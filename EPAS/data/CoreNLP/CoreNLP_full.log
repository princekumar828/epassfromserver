Number of raw rules: 12
Number of raw rules: 8
Number of raw rules: 15
Number of raw rules: 10
Number of raw rules: 9
Number of raw rules: 11
Number of raw rules: 13
Number of raw rules: 7
Number of raw rules: 14
Number of raw rules: 6
Number of raw rules: 16
Number of raw rules: 5
Number of raw rules: 17
Number of raw rules: 4
Number of raw rules: 18
minimaldebug Tuning thresholds to keep running. New Pattern threshold is 0.5
minimaldebug Tuning thresholds to keep running. New Pattern threshold is 0.6
minimaldebug Tuning thresholds to keep running. New Pattern threshold is 0.7
minimaldebug Tuning thresholds to keep running. New Pattern threshold is 0.8
minimaldebug Tuning thresholds to keep running. New Pattern threshold is 0.9
minimaldebug Tuning thresholds to keep running. New Pattern threshold is 1.0
minimaldebug Tuning thresholds to keep running. New Pattern threshold is 1.1
minimaldebug Tuning thresholds to keep running. New Pattern threshold is 1.2
minimaldebug Tuning thresholds to keep running. New Pattern threshold is 1.3
minimaldebug Tuning thresholds to keep running. New Pattern threshold is 1.4
minimaldebug Tuning thresholds to keep running. New Pattern threshold is 1.5
minimaldebug Tuning thresholds to keep running. New Pattern threshold is 1.6
minimaldebug Tuning thresholds to keep running. New Pattern threshold is 1.7
minimaldebug Tuning thresholds to keep running. New Pattern threshold is 1.8
minimaldebug Tuning thresholds to keep running. New Pattern threshold is 1.9
allowEmptySentences= true
allowEmptySentences= false
allowEmptySentences= 1
allowEmptySentences= 0
allowEmptySentences= yes
allowEmptySentences= no
allowEmptySentences= on
allowEmptySentences= off
allowEmptySentences= enabled
allowEmptySentences= disabled
allowEmptySentences= allow
allowEmptySentences= deny
allowEmptySentences= null
allowEmptySentences= empty
allowEmptySentences= default
c_stop_hTWds: 0.34 ; c_hTWds: 0.67 ; c_stop_hTds: 0.45 ; c_hTds: 0.56
c_stop_hTWds: 0.28 ; c_hTWds: 0.72 ; c_stop_hTds: 0.39 ; c_hTds: 0.61
c_stop_hTWds: 0.31 ; c_hTWds: 0.69 ; c_stop_hTds: 0.42 ; c_hTds: 0.58
c_stop_hTWds: 0.35 ; c_hTWds: 0.65 ; c_stop_hTds: 0.46 ; c_hTds: 0.54
c_stop_hTWds: 0.29 ; c_hTWds: 0.71 ; c_stop_hTds: 0.40 ; c_hTds: 0.60
c_stop_hTWds: 0.32 ; c_hTWds: 0.68 ; c_stop_hTds: 0.43 ; c_hTds: 0.57
c_stop_hTWds: 0.36 ; c_hTWds: 0.64 ; c_stop_hTds: 0.47 ; c_hTds: 0.53
c_stop_hTWds: 0.30 ; c_hTWds: 0.70 ; c_stop_hTds: 0.41 ; c_hTds: 0.59
c_stop_hTWds: 0.33 ; c_hTWds: 0.67 ; c_stop_hTds: 0.44 ; c_hTds: 0.56
c_stop_hTWds: 0.37 ; c_hTWds: 0.63 ; c_stop_hTds: 0.48 ; c_hTds: 0.52
c_stop_hTWds: 0.27 ; c_hTWds: 0.73 ; c_stop_hTds: 0.38 ; c_hTds: 0.62
c_stop_hTWds: 0.34 ; c_hTWds: 0.66 ; c_stop_hTds: 0.45 ; c_hTds: 0.55
c_stop_hTWds: 0.38 ; c_hTWds: 0.62 ; c_stop_hTds: 0.49 ; c_hTds: 0.51
c_stop_hTWds: 0.26 ; c_hTWds: 0.74 ; c_stop_hTds: 0.37 ; c_hTds: 0.63
c_stop_hTWds: 0.35 ; c_hTWds: 0.65 ; c_stop_hTds: 0.46 ; c_hTds: 0.54
#tags: 5
#tags: 12
#tags: 3
#tags: 9
#tags: 7
#tags: 10
#tags: 4
#tags: 8
#tags: 6
#tags: 11
#tags: 2
#tags: 13
#tags: 1
#tags: 14
#tags: 0
length of sTemplates keys: 0
length of sTemplates keys: 1
length of sTemplates keys: 2
length of sTemplates keys: 3
length of sTemplates keys: 4
length of sTemplates keys: 5
length of sTemplates keys: 6
length of sTemplates keys: 7
length of sTemplates keys: 8
length of sTemplates keys: 9
length of sTemplates keys: 10
length of sTemplates keys: 11
length of sTemplates keys: 12
length of sTemplates keys: 13
length of sTemplates keys: 14
ConstantsAndVariables.extremedebug For pattern with index 5 extracted the following sentences from the index 23
ConstantsAndVariables.extremedebug For pattern with index 12 extracted the following sentences from the index 7
ConstantsAndVariables.extremedebug For pattern with index 9 extracted the following sentences from the index 15
ConstantsAndVariables.extremedebug For pattern with index 3 extracted the following sentences from the index 19
ConstantsAndVariables.extremedebug For pattern with index 8 extracted the following sentences from the index 11
ConstantsAndVariables.extremedebug For pattern with index 10 extracted the following sentences from the index 4
ConstantsAndVariables.extremedebug For pattern with index 6 extracted the following sentences from the index 21
ConstantsAndVariables.extremedebug For pattern with index 4 extracted the following sentences from the index 17
ConstantsAndVariables.extremedebug For pattern with index 7 extracted the following sentences from the index 13
ConstantsAndVariables.extremedebug For pattern with index 11 extracted the following sentences from the index 9
ConstantsAndVariables.extremedebug For pattern with index 2 extracted the following sentences from the index 25
ConstantsAndVariables.extremedebug For pattern with index 1 extracted the following sentences from the index 27
ConstantsAndVariables.extremedebug For pattern with index 13 extracted the following sentences from the index 5
ConstantsAndVariables.extremedebug For pattern with index 14 extracted the following sentences from the index 3
ConstantsAndVariables.extremedebug For pattern with index 15 extracted the following sentences from the index 1
Speaker: Alice Mention: Bob
Speaker: John Mention: himself
Speaker: Emma Mention: the weather
Speaker: David Mention: his dog
Speaker: Lisa Mention: her project
Speaker: Mark Mention: the news
Speaker: Anna Mention: her sister
Speaker: Tom Mention: his car
Speaker: Kate Mention: her book
Speaker: Eric Mention: his boss
Speaker: Lucy Mention: her friend
Speaker: Sam Mention: his guitar
Speaker: Zoe Mention: her trip
Speaker: Mike Mention: his team
Speaker: Amy Mention: her birthday
Speaker: Alice Mention: Bob
Speaker: John Mention: Mary
Speaker: David Mention: himself
Speaker: Emma Mention: her mother
Speaker: Frank Mention: the president
Speaker: Grace Mention: the teacher
Speaker: Harry Mention: his friend
Speaker: Irene Mention: the doctor
Speaker: Jack Mention: the dog
Speaker: Kate Mention: the book
Speaker: Leo Mention: the movie
Speaker: Mia Mention: the weather
Speaker: Noah Mention: his sister
Speaker: Olivia Mention: the cake
Speaker: Paul Mention: the game
Speaker: Alice Mention: Bob
Speaker: John Mention: Mary
Speaker: David Mention: himself
Speaker: Emma Mention: her boss
Speaker: Frank Mention: the president
Speaker: Grace Mention: her friend
Speaker: Harry Mention: his dog
Speaker: Irene Mention: the weather
Speaker: Jack Mention: his book
Speaker: Kelly Mention: her project
Speaker: Leo Mention: his guitar
Speaker: Mia Mention: her sister
Speaker: Noah Mention: his teacher
Speaker: Olivia Mention: her dream
Speaker: Paul Mention: his car
ABC STATE_INDEX
XYZ STATE_INDEX
MNO STATE_INDEX
PQR STATE_INDEX
GHI STATE_INDEX
JKL STATE_INDEX
STU STATE_INDEX
VWX STATE_INDEX
DEF STATE_INDEX
RST STATE_INDEX
LMN STATE_INDEX
CDE STATE_INDEX
OPQ STATE_INDEX
WYZ STATE_INDEX
BCD STATE_INDEX
Quantifiable.processEntity: 0.5
Quantifiable.processEntity: 1.0
Quantifiable.processEntity: 0.75
Quantifiable.processEntity: 0.25
Quantifiable.processEntity: 0.9
Quantifiable.processEntity: 0.1
Quantifiable.processEntity: 0.6
Quantifiable.processEntity: 0.4
Quantifiable.processEntity: 0.8
Quantifiable.processEntity: 0.3
Quantifiable.processEntity: 0.7
Quantifiable.processEntity: 0.2
Quantifiable.processEntity: 1.1
Quantifiable.processEntity: -0.1
Quantifiable.processEntity: 0.55
Recalculating temporary betas for tree 1
Recalculating temporary betas for tree 7
Recalculating temporary betas for tree 12
Recalculating temporary betas for tree 4
Recalculating temporary betas for tree 9
Recalculating temporary betas for tree 3
Recalculating temporary betas for tree 10
Recalculating temporary betas for tree 5
Recalculating temporary betas for tree 8
Recalculating temporary betas for tree 6
Recalculating temporary betas for tree 11
Recalculating temporary betas for tree 2
Recalculating temporary betas for tree 13
Recalculating temporary betas for tree 14
Recalculating temporary betas for tree 15
Old Local Tree: sb1
Old Local Tree: sb1-2
Old Local Tree: sb1-3
Old Local Tree: sb1-4
Old Local Tree: sb1-5
Old Local Tree: sb1-6
Old Local Tree: sb1-7
Old Local Tree: sb1-8
Old Local Tree: sb1-9
Old Local Tree: sb1-10
Old Local Tree: sb1-11
Old Local Tree: sb1-12
Old Local Tree: sb1-13
Old Local Tree: sb1-14
Old Local Tree: sb1-15
New Local Tree: sb2-1
New Local Tree: sb2-2
New Local Tree: sb2-3
New Local Tree: sb2-4
New Local Tree: sb2-5
New Local Tree: sb2-6
New Local Tree: sb2-7
New Local Tree: sb2-8
New Local Tree: sb2-9
New Local Tree: sb2-10
New Local Tree: sb2-11
New Local Tree: sb2-12
New Local Tree: sb2-13
New Local Tree: sb2-14
New Local Tree: sb2-15
Couldn't instantiate: args[2]
Couldn't instantiate: args[0]
Couldn't instantiate: args[5]
Couldn't instantiate: args[1]
Couldn't instantiate: args[3]
Couldn't instantiate: args[4]
Couldn't instantiate: args[6]
Couldn't instantiate: args[7]
Couldn't instantiate: args[8]
Couldn't instantiate: args[9]
Couldn't instantiate: args[10]
Couldn't instantiate: args[11]
Couldn't instantiate: args[12]
Couldn't instantiate: args[13]
Couldn't instantiate: args[14]
Removed from vertical splitters: 5
Removed from vertical splitters: 12
Removed from vertical splitters: 0
Removed from vertical splitters: 8
Removed from vertical splitters: 3
Removed from vertical splitters: 10
Removed from vertical splitters: 1
Removed from vertical splitters: 7
Removed from vertical splitters: 4
Removed from vertical splitters: 9
Removed from vertical splitters: 6
Removed from vertical splitters: 2
Removed from vertical splitters: 11
Removed from vertical splitters: 13
Removed from vertical splitters: 14
Parent split categories: [Books, Movies, Music]
Parent split categories: [Sports, Games, Toys]
Parent split categories: [Clothing, Shoes, Accessories]
Parent split categories: [Electronics, Computers, Software]
Parent split categories: [Home, Garden, Pets]
Parent split categories: [Health, Beauty, Personal Care]
Parent split categories: [Food, Beverages, Grocery]
Parent split categories: [Travel, Hotels, Flights]
Parent split categories: [Arts, Crafts, Hobbies]
Parent split categories: [Business, Finance, Education]
Parent split categories: [News, Media, Entertainment]
Parent split categories: [Science, Technology, Engineering]
Parent split categories: [Social, Family, Relationships]
Parent split categories: [Religion, Spirituality, Philosophy]
Parent split categories: [Politics, Law, Government]
LatticeParser: parsing failed for lattice 12
LatticeBuilder: parsing failed for lattice 7
LatticeAnalyzer: parsing failed for lattice 15
LatticeValidator: parsing failed for lattice 9
LatticeConverter: parsing failed for lattice 3
LatticeReader: parsing failed for lattice 11
LatticeWriter: parsing failed for lattice 8
LatticeTransformer: parsing failed for lattice 6
LatticeFilter: parsing failed for lattice 10
LatticeSorter: parsing failed for lattice 4
LatticeMerger: parsing failed for lattice 13
LatticeSplitter: parsing failed for lattice 5
LatticeExtractor: parsing failed for lattice 14
LatticeInserter: parsing failed for lattice 2
LatticeUpdater: parsing failed for lattice 1
com.example.parser.Parser: Factored parse succeeded!
org.apache.commons.lang3.StringUtils: Factored parse succeeded!
java.util.Scanner: Factored parse succeeded!
javax.swing.JFrame: Factored parse succeeded!
android.app.Activity: Factored parse succeeded!
edu.stanford.nlp.trees.Tree: Factored parse succeeded!
net.sourceforge.jtds.jdbc.Driver: Factored parse succeeded!
org.json.JSONObject: Factored parse succeeded!
java.math.BigDecimal: Factored parse succeeded!
org.springframework.context.ApplicationContext: Factored parse succeeded!
java.security.MessageDigest: Factored parse succeeded!
java.io.File: Factored parse succeeded!
java.net.URL: Factored parse succeeded!
java.awt.Color: Factored parse succeeded!
java.util.regex.Pattern: Factored parse succeeded!
Removing already identified patterns removeIdentifiedPatterns in favor of pat
Removing already identified patterns removeIdentifiedPatterns in favor of pat_1
Removing already identified patterns removeIdentifiedPatterns in favor of pat_2
Removing already identified patterns removeIdentifiedPatterns in favor of pat_3
Removing already identified patterns removeIdentifiedPatterns in favor of pat_4
Removing already identified patterns removeIdentifiedPatterns in favor of pat_5
Removing already identified patterns removeIdentifiedPatterns in favor of pat_6
Removing already identified patterns removeIdentifiedPatterns in favor of pat_7
Removing already identified patterns removeIdentifiedPatterns in favor of pat_8
Removing already identified patterns removeIdentifiedPatterns in favor of pat_9
Removing already identified patterns removeIdentifiedPatterns in favor of pat_10
Removing already identified patterns removeIdentifiedPatterns in favor of pat_11
Removing already identified patterns removeIdentifiedPatterns in favor of pat_12
Removing already identified patterns removeIdentifiedPatterns in favor of pat_13
Removing already identified patterns removeIdentifiedPatterns in favor of pat_14
Deserializing classifier from /home/user/model.pkl...
Deserializing classifier from C:\Users\user\Documents\model.h5...
Deserializing classifier from /tmp/model.pt...
Deserializing classifier from /opt/model/model.json...
Deserializing classifier from /var/lib/model/model.npz...
Deserializing classifier from /mnt/model/model.sav...
Deserializing classifier from /usr/local/model/model.joblib...
Deserializing classifier from /data/model/model.onnx...
Deserializing classifier from /root/model/model.tflite...
Deserializing classifier from /media/user/USB/model.model...
Deserializing classifier from /dev/shm/model/model.dat...
Deserializing classifier from /etc/model/model.ini...
Deserializing classifier from /run/user/1000/model/model.pickle...
Deserializing classifier from /srv/model/model.xml...
Deserializing classifier from /proc/1234/fd/3...
Serializing classifier to /home/user/model.pkl... done.
Serializing classifier to C:\Users\user\Documents\model.h5... done.
Serializing classifier to /tmp/model.joblib... done.
Serializing classifier to /mnt/data/model.pt... done.
Serializing classifier to /Users/user/Desktop/model.sav... done.
Serializing classifier to /var/lib/model.json... done.
Serializing classifier to /opt/model/model.onnx... done.
Serializing classifier to /data/model/model.tflite... done.
Serializing classifier to /root/model/model.ckpt... done.
Serializing classifier to /usr/local/model/model.npz... done.
Serializing classifier to /dev/shm/model.model... done.
Serializing classifier to /media/user/USB/model.bin... done.
Serializing classifier to /etc/model/model.xml... done.
Serializing classifier to /srv/model/model.yaml... done.
Serializing classifier to /run/user/1000/model.model... done.
Error attempting to save classifier to file=classifier_2023_10_25.pkl
Error attempting to save classifier to file=/home/user/Documents/classifier.h5
Error attempting to save classifier to file=C:\Users\user\Desktop\classifier.sav
Error attempting to save classifier to file=classifier.joblib
Error attempting to save classifier to file=/tmp/classifier.pickle
Error attempting to save classifier to file=classifier.pt
Error attempting to save classifier to file=/var/log/classifier.log
Error attempting to save classifier to file=classifier.json
Error attempting to save classifier to file=/opt/classifier/model.bin
Error attempting to save classifier to file=classifier.xml
Error attempting to save classifier to file=/usr/local/classifier/classifier.tar.gz
Error attempting to save classifier to file=classifier.csv
Error attempting to save classifier to file=/mnt/classifier/classifier.npz
Error attempting to save classifier to file=classifier.mat
Error attempting to save classifier to file=classifier.yaml
shuffled 10 sentences and selecting 2 sentences per thread
shuffled 12 sentences and selecting 3 sentences per thread
shuffled 15 sentences and selecting 4 sentences per thread
shuffled 9 sentences and selecting 2 sentences per thread
shuffled 11 sentences and selecting 3 sentences per thread
shuffled 13 sentences and selecting 4 sentences per thread
shuffled 14 sentences and selecting 5 sentences per thread
shuffled 16 sentences and selecting 6 sentences per thread
shuffled 17 sentences and selecting 7 sentences per thread
shuffled 18 sentences and selecting 8 sentences per thread
Writing results to output.txt
Writing results to report.csv
Writing results to data.json
Writing results to summary.docx
Writing results to log.txt
Writing results to results.xlsx
Writing results to backup.zip
Writing results to stats.pdf
Writing results to graph.png
Writing results to table.html
Writing results to config.ini
Writing results to error.log
Writing results to notes.md
Writing results to archive.tar.gz
Writing results to image.jpg
Saving the learned patterns for label **spam** in **patternsOutputFileSpam.txt**
Saving the learned patterns for label **ham** in **patternsOutputFileHam.txt**
Saving the learned patterns for label **positive** in **patternsOutputFilePositive.csv**
Saving the learned patterns for label **negative** in **patternsOutputFileNegative.csv**
Saving the learned patterns for label **neutral** in **patternsOutputFileNeutral.csv**
Saving the learned patterns for label **cat** in **patternsOutputFileCat.json**
Saving the learned patterns for label **dog** in **patternsOutputFileDog.json**
Saving the learned patterns for label **bird** in **patternsOutputFileBird.json**
Saving the learned patterns for label **red** in **patternsOutputFileRed.pkl**
Saving the learned patterns for label **green** in **patternsOutputFileGreen.pkl**
Saving the learned patterns for label **blue** in **patternsOutputFileBlue.pkl**
Saving the learned patterns for label **male** in **patternsOutputFileMale.h5**
Saving the learned patterns for label **female** in **patternsOutputFileFemale.h5**
Saving the learned patterns for label **unknown** in **patternsOutputFileUnknown.h5**
Saving the learned patterns for label **happy** in **patternsOutputFileHappy.xml**
Processing text "Hello world" with dateString = 2023-10-25
Processing text "The quick brown fox jumps over the lazy dog" with dateString = 2023-10-24
Processing text "This is a test sentence" with dateString = 2023-10-23
Processing text "How are you today?" with dateString = 2023-10-22
Processing text "I love coding in Python" with dateString = 2023-10-21
Processing text "What is the meaning of life?" with dateString = 2023-10-20
Processing text "To be or not to be, that is the question" with dateString = 2023-10-19
Processing text "Lorem ipsum dolor sit amet" with dateString = 2023-10-18
Processing text "May the force be with you" with dateString = 2023-10-17
Processing text "Winter is coming" with dateString = 2023-10-16
Processing text "Houston, we have a problem" with dateString = 2023-10-15
Processing text "It's a beautiful day" with dateString = 2023-10-14
Processing text "Let it go, let it go" with dateString = 2023-10-13
Processing text "Do you want to build a snowman?" with dateString = 2023-10-12
Processing text "Hakuna matata, it means no worries" with dateString = 2023-10-11
ABORTING: Input file and output is the same - report.pdf
ABORTING: Input file and output is the same - image.jpg
ABORTING: Input file and output is the same - data.csv
ABORTING: Input file and output is the same - music.mp3
ABORTING: Input file and output is the same - video.mp4
ABORTING: Input file and output is the same - document.docx
ABORTING: Input file and output is the same - archive.zip
ABORTING: Input file and output is the same - code.py
ABORTING: Input file and output is the same - slides.pptx
ABORTING: Input file and output is the same - text.txt
ABORTING: Input file and output is the same - graph.png
ABORTING: Input file and output is the same - email.eml
ABORTING: Input file and output is the same - calendar.ics
ABORTING: Input file and output is the same - book.epub
ABORTING: Input file and output is the same - game.exe
unknown attribute value 0.5 of attribute 3
unknown attribute value 1.2 of attribute 7
unknown attribute value -0.8 of attribute 4
unknown attribute value 2.3 of attribute 9
unknown attribute value 0.9 of attribute 6
unknown attribute value -1.5 of attribute 2
unknown attribute value 1.8 of attribute 8
unknown attribute value -0.4 of attribute 5
unknown attribute value 0.7 of attribute 1
unknown attribute value -1.2 of attribute 10
unknown attribute value 2.1 of attribute 12
unknown attribute value -0.6 of attribute 11
unknown attribute value 1.4 of attribute 14
unknown attribute value -0.2 of attribute 13
unknown attribute value 0.3 of attribute 15
Error resolving temporal, using docDate=2023-10-25
Error resolving temporal, using docDate=2023-10-24
Error resolving temporal, using docDate=2023-10-23
Error resolving temporal, using docDate=2023-10-22
Error resolving temporal, using docDate=2023-10-21
Error resolving temporal, using docDate=2023-10-20
Error resolving temporal, using docDate=2023-10-19
Error resolving temporal, using docDate=2023-10-18
Error resolving temporal, using docDate=2023-10-17
Error resolving temporal, using docDate=2023-10-16
Error resolving temporal, using docDate=2023-10-15
Error resolving temporal, using docDate=2023-10-14
Error resolving temporal, using docDate=2023-10-13
Error resolving temporal, using docDate=2023-10-12
Error resolving temporal, using docDate=2023-10-11
Redwood.DBG Reading seed words from fin for label 0
Redwood.DBG Reading seed words from fin for label 1
Redwood.DBG Reading seed words from fin for label 2
Redwood.DBG Reading seed words from fin for label 3
Redwood.DBG Reading seed words from fin for label 4
Redwood.DBG Reading seed words from fin for label 5
Redwood.DBG Reading seed words from fin for label 6
Redwood.DBG Reading seed words from fin for label 7
Redwood.DBG Reading seed words from fin for label 8
Redwood.DBG Reading seed words from fin for label 9
Redwood.DBG Reading seed words from fin for label 10
Redwood.DBG Reading seed words from fin for label 11
Redwood.DBG Reading seed words from fin for label 12
Redwood.DBG Reading seed words from fin for label 13
Redwood.DBG Reading seed words from fin for label 14
Find marked head method returned **NP** as head of **S**
Find marked head method returned **VBD** as head of **VP**
Find marked head method returned **DT** as head of **NP**
Find marked head method returned **JJ** as head of **ADJP**
Find marked head method returned **IN** as head of **PP**
Find marked head method returned **RB** as head of **ADVP**
Find marked head method returned **NN** as head of **NP**
Find marked head method returned **VBZ** as head of **VP**
Find marked head method returned **PRP** as head of **NP**
Find marked head method returned **CC** as head of **CONJP**
Find marked head method returned **MD** as head of **VP**
Find marked head method returned **NNS** as head of **NP**
Find marked head method returned **VBG** as head of **VP**
Find marked head method returned **CD** as head of **QP**
Find marked head method returned **TO** as head of **S**
Redwood.DBG Adding 12 eval sents to the training set
Redwood.DBG Adding 8 eval sents to the training set
Redwood.DBG Adding 10 eval sents to the training set
Redwood.DBG Adding 9 eval sents to the training set
Redwood.DBG Adding 11 eval sents to the training set
Redwood.DBG Adding 7 eval sents to the training set
Redwood.DBG Adding 13 eval sents to the training set
Redwood.DBG Adding 14 eval sents to the training set
Redwood.DBG Adding 6 eval sents to the training set
Redwood.DBG Adding 15 eval sents to the training set
Redwood.DBG Adding 5 eval sents to the training set
Redwood.DBG Adding 4 eval sents to the training set
Redwood.DBG Adding 3 eval sents to the training set
Redwood.DBG Adding 2 eval sents to the training set
Warning: pattern startingClasses.get(0) matched nothing
Warning: pattern startingClasses.get(1) matched nothing
Warning: pattern startingClasses.get(2) matched nothing
Warning: pattern startingClasses.get(3) matched nothing
Warning: pattern startingClasses.get(4) matched nothing
Warning: pattern startingClasses.get(5) matched nothing
Warning: pattern startingClasses.get(6) matched nothing
Warning: pattern startingClasses.get(7) matched nothing
Warning: pattern startingClasses.get(8) matched nothing
Warning: pattern startingClasses.get(9) matched nothing
Warning: pattern startingClasses.get(10) matched nothing
Warning: pattern startingClasses.get(11) matched nothing
Warning: pattern startingClasses.get(12) matched nothing
Warning: pattern startingClasses.get(13) matched nothing
Warning: pattern startingClasses.get(14) matched nothing
File or directory not found /home/user/documents
File or directory not found /var/log/syslog
File or directory not found /etc/passwd
File or directory not found /mnt/usb
File or directory not found /opt/bin
File or directory not found /dev/sda1
File or directory not found /tmp/cache
File or directory not found /usr/local/lib
File or directory not found /root/.bashrc
File or directory not found /media/cdrom
File or directory not found /proc/cpuinfo
File or directory not found /boot/grub
File or directory not found /srv/http
File or directory not found /lib/modules
File or directory not found /bin/sh
Adding folder entry images
Adding folder entry documents
Adding folder entry music
Adding folder entry videos
Adding folder entry downloads
Adding folder entry backup
Adding folder entry projects
Adding folder entry games
Adding folder entry temp
Adding folder entry desktop
Adding folder entry recycle bin
Adding folder entry system
Adding folder entry fonts
Adding folder entry drivers
Adding folder entry logs
SUCCESS: wrote model to /home/user/models/model_1.h5
SUCCESS: wrote model to /var/tmp/models/model_2.pkl
SUCCESS: wrote model to /mnt/data/models/model_3.pt
SUCCESS: wrote model to /usr/local/models/model_4.onnx
SUCCESS: wrote model to /opt/models/model_5.json
SUCCESS: wrote model to /home/user/models/model_6.tflite
SUCCESS: wrote model to /var/tmp/models/model_7.joblib
SUCCESS: wrote model to /mnt/data/models/model_8.npz
SUCCESS: wrote model to /usr/local/models/model_9.sav
SUCCESS: wrote model to /opt/models/model_10.pb
SUCCESS: wrote model to /home/user/models/model_11.pth
SUCCESS: wrote model to /var/tmp/models/model_12.sklearn
SUCCESS: wrote model to /mnt/data/models/model_13.keras
SUCCESS: wrote model to /usr/local/models/model_14.mlmodel
SUCCESS: wrote model to /opt/models/model_15.dill
No extractions in: "stdin".equals(docid) ? document : docid
No extractions in: "stdin".equals("test.txt") ? document : "test.txt"
No extractions in: "stdin".equals("report.pdf") ? document : "report.pdf"
No extractions in: "stdin".equals("image.jpg") ? document : "image.jpg"
No extractions in: "stdin".equals(document) ? document : document
No extractions in: "stdin".equals("data.csv") ? document : "data.csv"
No extractions in: "stdin".equals("video.mp4") ? document : "video.mp4"
No extractions in: "stdin".equals("audio.wav") ? document : "audio.wav"
No extractions in: "stdin".equals("code.py") ? document : "code.py"
No extractions in: "stdin".equals("email.eml") ? document : "email.eml"
No extractions in: "stdin".equals("web.html") ? document : "web.html"
No extractions in: "stdin".equals("book.epub") ? document : "book.epub"
No extractions in: "stdin".equals("slide.pptx") ? document : "slide.pptx"
No extractions in: "stdin".equals("sheet.xlsx") ? document : "sheet.xlsx"
No extractions in: "stdin".equals("form.docx") ? document : "form.docx"
After MWETransform:               t=0.34
After MWETransform:               t=0.67
After MWETransform:               t=0.12
After MWETransform:               t=0.89
After MWETransform:               t=0.45
After MWETransform:               t=0.76
After MWETransform:               t=0.23
After MWETransform:               t=0.98
After MWETransform:               t=0.51
After MWETransform:               t=0.64
After MWETransform:               t=0.37
After MWETransform:               t=0.82
After MWETransform:               t=0.29
After MWETransform:               t=0.94
After MWETransform:               t=0.56
After prepCCTransform:               t = 0.34
After prepCCTransform:               t = 0.67
After prepCCTransform:               t = 0.12
After prepCCTransform:               t = 0.89
After prepCCTransform:               t = 0.45
After prepCCTransform:               t = 0.76
After prepCCTransform:               t = 0.23
After prepCCTransform:               t = 0.98
After prepCCTransform:               t = 0.51
After prepCCTransform:               t = 0.64
After prepCCTransform:               t = 0.37
After prepCCTransform:               t = 0.82
After prepCCTransform:               t = 0.29
After prepCCTransform:               t = 0.94
After prepCCTransform:               t = 0.56
After UCPTransformer:             t=0.001
After UCPTransformer:             t=0.002
After UCPTransformer:             t=0.003
After UCPTransformer:             t=0.004
After UCPTransformer:             t=0.005
After UCPTransformer:             t=0.006
After UCPTransformer:             t=0.007
After UCPTransformer:             t=0.008
After UCPTransformer:             t=0.009
After UCPTransformer:             t=0.01
After UCPTransformer:             t=0.011
After UCPTransformer:             t=0.012
After UCPTransformer:             t=0.013
After UCPTransformer:             t=0.014
After UCPTransformer:             t=0.015
After CCTransformer:              t = 0.34
After CCTransformer:              t = 0.56
After CCTransformer:              t = 0.78
After CCTransformer:              t = 0.12
After CCTransformer:              t = 0.45
After CCTransformer:              t = 0.67
After CCTransformer:              t = 0.89
After CCTransformer:              t = 0.23
After CCTransformer:              t = 0.46
After CCTransformer:              t = 0.69
After CCTransformer:              t = 0.91
After CCTransformer:              t = 0.14
After CCTransformer:              t = 0.37
After CCTransformer:              t = 0.59
After CCTransformer:              t = 0.81
After SQ flattening:              0.0032
After SQ flattening:              0.0015
After SQ flattening:              0.0028
After SQ flattening:              0.0041
After SQ flattening:              0.0036
After SQ flattening:              0.0021
After SQ flattening:              0.0039
After SQ flattening:              0.0018
After SQ flattening:              0.0025
After SQ flattening:              0.0043
After SQ flattening:              0.0034
After SQ flattening:              0.0023
After SQ flattening:              0.0037
After SQ flattening:              0.0016
After SQ flattening:              0.0029
After removeXoverX:               0.0
After removeXoverX:               0.5
After removeXoverX:               1.0
After removeXoverX:               1.5
After removeXoverX:               2.0
After removeXoverX:               2.5
After removeXoverX:               3.0
After removeXoverX:               3.5
After removeXoverX:               4.0
After removeXoverX:               4.5
After removeXoverX:               5.0
After removeXoverX:               5.5
After removeXoverX:               6.0
After removeXoverX:               6.5
After removeXoverX:               7.0
domain count 0 for google.com
domain count 0 for bing.com
domain count 0 for yahoo.com
domain count 0 for amazon.com
domain count 0 for facebook.com
domain count 0 for twitter.com
domain count 0 for youtube.com
domain count 0 for wikipedia.org
domain count 0 for reddit.com
domain count 0 for netflix.com
domain count 0 for spotify.com
domain count 0 for instagram.com
domain count 0 for linkedin.com
domain count 0 for ebay.com
domain count 0 for apple.com
After combineConjp:               t(1,2)
After combineConjp:               t(3,4)
After combineConjp:               t(5,6)
After combineConjp:               t(7,8)
After combineConjp:               t(9,10)
After combineConjp:               t(11,12)
After combineConjp:               t(13,14)
After combineConjp:               t(15,16)
After combineConjp:               t(17,18)
After combineConjp:               t(19,20)
After combineConjp:               t(21,22)
After combineConjp:               t(23,24)
After combineConjp:               t(25,26)
After combineConjp:               t(27,28)
After combineConjp:               t(29,30)
debug-preprocessorWARNING: gold mentions with the same offsets: ip=192.168.0.1 mentions=g.mentionID=1,existingMentions=2, g.spanToString()=John Smith
debug-preprocessorWARNING: gold mentions with the same offsets: ip=10.0.0.2 mentions=g.mentionID=3,existingMentions=4, g.spanToString()=Apple Inc.
debug-preprocessorWARNING: gold mentions with the same offsets: ip=172.16.0.3 mentions=g.mentionID=5,existingMentions=6, g.spanToString()=New York
debug-preprocessorWARNING: gold mentions with the same offsets: ip=127.0.0.1 mentions=g.mentionID=7,existingMentions=8, g.spanToString()=Jane Doe
debug-preprocessorWARNING: gold mentions with the same offsets: ip=192.168.1.4 mentions=g.mentionID=9,existingMentions=10, g.spanToString()=Microsoft
debug-preprocessorWARNING: gold mentions with the same offsets: ip=10.0.1.5 mentions=g.mentionID=11,existingMentions=12, g.spanToString()=London
debug-preprocessorWARNING: gold mentions with the same offsets: ip=172.16.1.6 mentions=g.mentionID=13,existingMentions=14, g.spanToString()=Bob Jones
debug-preprocessorWARNING: gold mentions with the same offsets: ip=127.0.1.7 mentions=g.mentionID=15,existingMentions=16, g.spanToString()=Amazon
debug-preprocessorWARNING: gold mentions with the same offsets: ip=192.168.2.8 mentions=g.mentionID=17,existingMentions=18, g.spanToString()=Paris
debug-preprocessorWARNING: gold mentions with the same offsets: ip=10.0.2.9 mentions=g.mentionID=19,existingMentions=20, g.spanToString()=Alice Cooper
debug-preprocessorWARNING: gold mentions with the same offsets: ip=172.16.2.10 mentions=g.mentionID=21,existingMentions=22, g.spanToString()=Google
debug-preprocessorWARNING: gold mentions with the same offsets: ip=127.0.2.11 mentions=g.mentionID=23,existingMentions=24, g.spanToString()=Tokyo
debug-preprocessorWARNING: gold mentions with the same offsets: ip=192.168.3.12 mentions=g.mentionID=25,existingMentions=26, g.spanToString()=Tom Cruise
debug-preprocessorWARNING: gold mentions with the same offsets: ip=10.0.3.13 mentions=g.mentionID=27,existingMentions=28, g.spanToString()=Facebook
debug-preprocessorWARNING: gold mentions with the same offsets: ip=172.16.3.14 mentions=g.mentionID=29,existingMentions=30, g.spanToString()=Beijing
After changeSbarToPP:             0.001
After changeSbarToPP:             0.01
After changeSbarToPP:             0.1
After changeSbarToPP:             1
After changeSbarToPP:             10
After changeSbarToPP:             100
After changeSbarToPP:             -0.001
After changeSbarToPP:             -0.01
After changeSbarToPP:             -0.1
After changeSbarToPP:             -1
After changeSbarToPP:             -10
After changeSbarToPP:             -100
After changeSbarToPP:             0.5
After changeSbarToPP:             -0.5
After changeSbarToPP:             0
file is domainNGramsFile_2023_10_25.txt
file is domainNGramsFile_2023_10_24.csv
file is domainNGramsFile_2023_10_23.json
file is domainNGramsFile_2023_10_22.xml
file is domainNGramsFile_2023_10_21.dat
file is domainNGramsFile_2023_10_20.log
file is domainNGramsFile_2023_10_19.zip
file is domainNGramsFile_2023_10_18.rar
file is domainNGramsFile_2023_10_17.docx
file is domainNGramsFile_2023_10_16.xlsx
file is domainNGramsFile_2023_10_15.pdf
file is domainNGramsFile_2023_10_14.pptx
file is domainNGramsFile_2023_10_13.html
file is domainNGramsFile_2023_10_12.md
file is domainNGramsFile_2023_10_11.java
transformCC in:  0.0
transformCC in:  0.5
transformCC in:  1.0
transformCC in:  -0.5
transformCC in:  -1.0
transformCC in:  0.25
transformCC in:  0.75
transformCC in:  -0.25
transformCC in:  -0.75
transformCC in:  0.1
transformCC in:  0.9
transformCC in:  -0.1
transformCC in:  -0.9
transformCC in:  0.01
transformCC in:  -0.01
transformCC out: 0.12
transformCC out: -0.34
transformCC out: 0.45
transformCC out: 0.67
transformCC out: -0.89
transformCC out: 0.01
transformCC out: -0.23
transformCC out: 0.56
transformCC out: 0.78
transformCC out: -0.90
transformCC out: 0.02
transformCC out: -0.45
transformCC out: 0.89
transformCC out: 0.99
transformCC out: -1.00
No span for 0.0
No span for 1.2
No span for -3.4
No span for 5.6
No span for 7.8
No span for -9.0
No span for 10.1
No span for -11.3
No span for 12.5
No span for -13.7
No span for 14.9
No span for -15.0
No span for 16.2
No span for -17.4
No span for 18.6
Tree span is 5, tree node is A
Tree span is 3, tree node is B
Tree span is 7, tree node is C
Tree span is 4, tree node is D
Tree span is 6, tree node is E
Tree span is 2, tree node is F
Tree span is 8, tree node is G
Tree span is 9, tree node is H
Tree span is 1, tree node is I
Tree span is 10, tree node is J
Tree span is 11, tree node is K
Tree span is 12, tree node is L
Tree span is 13, tree node is M
Tree span is 14, tree node is N
Tree span is 15, tree node is O
Writing to output.txt
Writing to report.csv
Writing to data.json
Writing to log.txt
Writing to results.xlsx
Writing to summary.docx
Writing to image.png
Writing to video.mp4
Writing to audio.wav
Writing to archive.zip
Writing to config.ini
Writing to script.py
Writing to error.log
Writing to backup.bak
Writing to index.html
Writing [com.example.DataWriter@7f31245a]
Writing [org.apache.spark.sql.DataFrameWriter@3cda1055]
Writing [java.io.BufferedWriter@6ce253f1]
Writing [com.mongodb.spark.MongoSpark$Writer@5b2133b1]
Writing [org.apache.hadoop.hbase.client.BufferedMutatorImpl@4b67cf4d]
Writing [org.apache.kafka.clients.producer.KafkaProducer@2f92e0f4]
Writing [org.apache.avro.file.DataFileWriter@7d4991ad]
Writing [org.apache.hadoop.io.SequenceFile$Writer@6d311334]
Writing [org.apache.poi.xssf.streaming.SXSSFWorkbook@2a139a55]
Writing [org.apache.flink.api.java.io.CsvOutputFormat@3d4eac69]
Writing [com.google.cloud.bigquery.BigQuery$TableDataWriteChannel@5c647e05]
Writing [org.neo4j.driver.internal.InternalTransaction@4f023edb]
Writing [com.amazonaws.services.s3.transfer.TransferManager@6f94fa3e]
Writing [org.apache.cassandra.io.sstable.SSTableSimpleUnsortedWriter@7a81197d]
Writing [com.google.gson.stream.JsonWriter@5e2de80c]
Evaluating: [1, 2, 3].toString()
Evaluating: {name: "Alice", age: 25}.toString()
Evaluating: Math.sqrt(16).toString()
Evaluating: new Date().toString()
Evaluating: "Hello".toString()
Evaluating: (x => x + 1).toString()
Evaluating: true.toString()
Evaluating: null.toString()
Evaluating: undefined.toString()
Evaluating: Symbol("foo").toString()
Evaluating: [4, 5, 6].join("-").toString()
Evaluating: JSON.stringify({color: "red", shape: "circle"}).toString()
Evaluating: (5 * 3).toString()
Evaluating: (function() {return "Hello World";}()).toString()
Evaluating: new RegExp("\\d+").toString()
PrepPoss Conj branch (two parallel PPs) adding: tdNew=0.45
PrepPoss Conj branch (two parallel PPs) adding: tdNew=0.67
PrepPoss Conj branch (two parallel PPs) adding: tdNew=0.32
PrepPoss Conj branch (two parallel PPs) adding: tdNew=0.51
PrepPoss Conj branch (two parallel PPs) adding: tdNew=0.76
PrepPoss Conj branch (two parallel PPs) adding: tdNew=0.28
PrepPoss Conj branch (two parallel PPs) adding: tdNew=0.59
PrepPoss Conj branch (two parallel PPs) adding: tdNew=0.43
PrepPoss Conj branch (two parallel PPs) adding: tdNew=0.62
PrepPoss Conj branch (two parallel PPs) adding: tdNew=0.37
PrepPoss Conj branch (two parallel PPs) adding: tdNew=0.54
PrepPoss Conj branch (two parallel PPs) adding: tdNew=0.71
PrepPoss Conj branch (two parallel PPs) adding: tdNew=0.25
PrepPoss Conj branch (two parallel PPs) adding: tdNew=0.56
PrepPoss Conj branch (two parallel PPs) adding: tdNew=0.48
Datum Extraction failed in Sieve.java while processing document: 1001 part: 1
Datum Extraction failed in Sieve.java while processing document: 2002 part: 2
Datum Extraction failed in Sieve.java while processing document: 3003 part: 3
Datum Extraction failed in Sieve.java while processing document: 4004 part: 4
Datum Extraction failed in Sieve.java while processing document: 5005 part: 5
Datum Extraction failed in Sieve.java while processing document: 6006 part: 6
Datum Extraction failed in Sieve.java while processing document: 7007 part: 7
Datum Extraction failed in Sieve.java while processing document: 8008 part: 8
Datum Extraction failed in Sieve.java while processing document: 9009 part: 9
Datum Extraction failed in Sieve.java while processing document: 1010 part: 10
Datum Extraction failed in Sieve.java while processing document: 1111 part: 11
Datum Extraction failed in Sieve.java while processing document: 1212 part: 12
Datum Extraction failed in Sieve.java while processing document: 1313 part: 13
Datum Extraction failed in Sieve.java while processing document: 1414 part: 14
Datum Extraction failed in Sieve.java while processing document: 1515 part: 15
Loading sieve: ExactStringMatch from HybridCorefProperties.getPathModel(props, ExactStringMatch) ...
Loading sieve: RelaxedStringMatch from HybridCorefProperties.getPathModel(props, RelaxedStringMatch) ...
Loading sieve: PreciseConstructs from HybridCorefProperties.getPathModel(props, PreciseConstructs) ...
Loading sieve: StrictHeadMatch1 from HybridCorefProperties.getPathModel(props, StrictHeadMatch1) ...
Loading sieve: StrictHeadMatch2 from HybridCorefProperties.getPathModel(props, StrictHeadMatch2) ...
Loading sieve: StrictHeadMatch3 from HybridCorefProperties.getPathModel(props, StrictHeadMatch3) ...
Loading sieve: StrictHeadMatch4 from HybridCorefProperties.getPathModel(props, StrictHeadMatch4) ...
Loading sieve: PronounSieve from HybridCorefProperties.getPathModel(props, PronounSieve) ...
Loading sieve: SpeakerMatch from HybridCorefProperties.getPathModel(props, SpeakerMatch) ...
Loading sieve: ChineseHeadMatch from HybridCorefProperties.getPathModel(props, ChineseHeadMatch) ...
Loading sieve: AliasSieve from HybridCorefProperties.getPathModel(props, AliasSieve) ...
Loading sieve: DiscourseMatch from HybridCorefProperties.getPathModel(props, DiscourseMatch) ...
Loading sieve: EntityHeadMatch from HybridCorefProperties.getPathModel(props, EntityHeadMatch) ...
Loading sieve: RoleAppositive from HybridCorefProperties.getPathModel(props, RoleAppositive) ...
Loading sieve: AcronymSieve from HybridCorefProperties.getPathModel(props, AcronymSieve) ...
MENTION FILTERING Removed nizhidao: [0, 8)
MENTION FILTERING Removed nizhidao: [12, 20)
MENTION FILTERING Removed nizhidao: [5, 13)
MENTION FILTERING Removed nizhidao: [9, 17)
MENTION FILTERING Removed nizhidao: [3, 11)
MENTION FILTERING Removed nizhidao: [10, 18)
MENTION FILTERING Removed nizhidao: [7, 15)
MENTION FILTERING Removed nizhidao: [4, 12)
MENTION FILTERING Removed nizhidao: [8, 16)
MENTION FILTERING Removed nizhidao: [6, 14)
MENTION FILTERING Removed nizhidao: [11, 19)
MENTION FILTERING Removed nizhidao: [2, 10)
MENTION FILTERING Removed nizhidao: [13, 21)
MENTION FILTERING Removed nizhidao: [1, 9)
MENTION FILTERING Removed nizhidao: [14, 22)
Expanding positives by adding [a:0.76, b:0.82, c:0.79] phrases
Expanding positives by adding [x:0.91, y:0.87, z:0.89] phrases
Expanding positives by adding [m:0.71, n:0.74, o:0.77] phrases
Expanding positives by adding [p:0.83, q:0.86, r:0.84] phrases
Expanding positives by adding [d:0.72, e:0.75, f:0.78] phrases
Expanding positives by adding [g:0.81, h:0.85, i:0.88] phrases
Expanding positives by adding [j:0.73, k:0.76, l:0.80] phrases
Expanding positives by adding [s:0.92, t:0.94, u:0.93] phrases
Expanding positives by adding [v:0.95, w:0.96, x:0.97] phrases
Expanding positives by adding [y:0.98, z:1.00, a:1.01] phrases
Expanding positives by adding [b:1.02, c:1.03, d:1.04] phrases
Expanding positives by adding [e:1.05, f:1.06, g:1.07] phrases
Expanding positives by adding [h:1.08, i:1.09, j:1.10] phrases
Expanding positives by adding [k:1.11, l:1.12, m:1.13] phrases
Expanding positives by adding [n:1.14, o:1.15, p:1.16] phrases
Removing dep killed in poss/prep (conj) collapse: td-1
Removing dep killed in poss/prep (conj) collapse: td-2
Removing dep killed in poss/prep (conj) collapse: td-3
Removing dep killed in poss/prep (conj) collapse: td-4
Removing dep killed in poss/prep (conj) collapse: td-5
Removing dep killed in poss/prep (conj) collapse: td-6
Removing dep killed in poss/prep (conj) collapse: td-7
Removing dep killed in poss/prep (conj) collapse: td-8
Removing dep killed in poss/prep (conj) collapse: td-9
Removing dep killed in poss/prep (conj) collapse: td-10
Removing dep killed in poss/prep (conj) collapse: td-11
Removing dep killed in poss/prep (conj) collapse: td-12
Removing dep killed in poss/prep (conj) collapse: td-13
Removing dep killed in poss/prep (conj) collapse: td-14
Removing dep killed in poss/prep (conj) collapse: td-15
Storing interim (float) weights to model_weights_1.h5 ...
Storing interim (float) weights to temp_weights_2023-10-25.npy ...
Storing interim (float) weights to backup_weights_0.75.pkl ...
Storing interim (float) weights to final_weights.pt ...
Storing interim (float) weights to checkpoint_weights_10.bin ...
Storing interim (float) weights to best_weights_0.82.mat ...
Storing interim (float) weights to weights_epoch_5.npz ...
Storing interim (float) weights to interim_weights_2023-10-25-10-55.sav ...
Storing interim (float) weights to current_weights.json ...
Storing interim (float) weights to updated_weights.csv ...
Storing interim (float) weights to weights_batch_50.tf ...
Storing interim (float) weights to trial_weights_0.78.joblib ...
Storing interim (float) weights to intermediate_weights.hdf5 ...
Storing interim (float) weights to latest_weights.txt ...
Storing interim (float) weights to optimal_weights.pth ...
Storing interim (double) weights to model_weights_1.bin ...
Storing interim (double) weights to temp_weights_2023_10_25.dat ...
Storing interim (double) weights to backup_weights_dnn_15.h5 ...
Storing interim (double) weights to weights_double_10_25_11_56.npz ...
Storing interim (double) weights to interim_weights_dnn_layer_3.pkl ...
Storing interim (double) weights to model_weights_double_2023_10_25_11_56.mat ...
Storing interim (double) weights to temp_weights_dnn_layer_4.npy ...
Storing interim (double) weights to backup_weights_double_2023_10_25.hdf5 ...
Storing interim (double) weights to model_weights_dnn_layer_2.pt ...
Storing interim (double) weights to temp_weights_double_2023_10_25.pth ...
Storing interim (double) weights to backup_weights_dnn_layer_1.npz ...
Storing interim (double) weights to model_weights_double_layer_3.pkl ...
Storing interim (double) weights to temp_weights_dnn_layer_2.h5 ...
Storing interim (double) weights to backup_weights_double_layer_4.mat ...
Storing interim (double) weights to model_weights_dnn_layer_1.npy ...
needle found in index.html tree 5
needle found in main.css tree 3
needle found in script.js tree 7
needle found in logo.png tree 2
needle found in data.json tree 4
needle found in readme.md tree 1
needle found in style.css tree 6
needle found in app.js tree 8
needle found in favicon.ico tree 9
needle found in config.xml tree 10
needle found in report.pdf tree 11
needle found in image.jpg tree 12
needle found in test.py tree 13
needle found in hello.c tree 14
needle found in music.mp3 tree 15
model is linear regression
model is convolutional neural network
model is random forest
model is support vector machine
model is k-means clustering
model is logistic regression
model is recurrent neural network
model is decision tree
model is principal component analysis
model is naive Bayes
model is gradient boosting
model is deep Q-learning
model is latent semantic analysis
model is generative adversarial network
model is long short-term memory
Serializing weights to /home/user/model/weights.h5...
Serializing weights to C:\Users\user\model\weights.pkl...
Serializing weights to /tmp/model/weights.pt...
Serializing weights to /var/lib/model/weights.npz...
Serializing weights to /mnt/model/weights.bin...
Serializing weights to /Users/user/model/weights.json...
Serializing weights to /data/model/weights.onnx...
Serializing weights to /model/weights.tf...
Serializing weights to /opt/model/weights.mat...
Serializing weights to /root/model/weights.joblib...
Serializing weights to /media/model/weights.npz...
Serializing weights to /dev/model/weights.hdf5...
Serializing weights to /etc/model/weights.pickle...
Serializing weights to /usr/local/model/weights.pth...
Serializing weights to /srv/model/weights.npz...
Reading saved model from /home/user/models/model_1.h5
Reading saved model from C:\Users\user\models\model_2.pkl
Reading saved model from /mnt/s3/models/model_3.pt
Reading saved model from /tmp/models/model_4.onnx
Reading saved model from /var/lib/models/model_5.joblib
Reading saved model from /opt/models/model_6.tflite
Reading saved model from /usr/local/models/model_7.npz
Reading saved model from /data/models/model_8.sav
Reading saved model from /models/model_9.json
Reading saved model from /root/models/model_10.bin
Reading saved model from /media/user/models/model_11.dill
Reading saved model from /etc/models/model_12.xml
Reading saved model from /run/models/model_13.pb
Reading saved model from /srv/models/model_14.net
Reading saved model from /dev/models/model_15.model
Loading weights from /home/user/model/weights.pth...
Loading weights from C:\Users\user\model\weights.h5...
Loading weights from /mnt/data/model/weights.npz...
Loading weights from https://drive.google.com/file/d/1a2b3c4d5e6f7g8h9i0j1k2l3m4n5o6p7/view?usp=sharing...
Loading weights from s3://bucket-name/model/weights.pkl...
Loading weights from /tmp/model/weights.bin...
Loading weights from /var/lib/model/weights.pt...
Loading weights from /usr/local/share/model/weights.mat...
Loading weights from ftp://user:password@host:port/path/to/model/weights.npz...
Loading weights from /opt/model/weights.hdf5...
Loading weights from /dev/shm/model/weights.npy...
Loading weights from /run/user/1000/model/weights.ckpt...
Loading weights from /media/user/USB/model/weights.torch...
Loading weights from /etc/model/weights.onnx...
Loading weights from /root/model/weights.tf...
Loading tuned wv from     word2vec_tuned_100d.bin
Loading tuned wv from     glove_tuned_300d.txt
Loading tuned wv from     fasttext_tuned_50d.vec
Loading tuned wv from     bert_tuned_768d.hdf5
Loading tuned wv from     elmo_tuned_1024d.pkl
Loading tuned wv from     gpt2_tuned_768d.pt
Loading tuned wv from     roberta_tuned_768d.pth
Loading tuned wv from     xlnet_tuned_1024d.tf
Loading tuned wv from     albert_tuned_4096d.npz
Loading tuned wv from     electra_tuned_256d.json
Loading tuned wv from     t5_tuned_512d.torch
Loading tuned wv from     bart_tuned_1024d.onnx
Loading tuned wv from     bigbird_tuned_4096d.npz
Loading tuned wv from     longformer_tuned_4096d.h5
Loading tuned wv from     reformer_tuned_2048d.bin
Loading anaphoricity from anaphoricity.txt
Loading anaphoricity from data/anaphoricity.csv
Loading anaphoricity from /home/user/anaphoricity.json
Loading anaphoricity from anaphoricity.pkl
Loading anaphoricity from C:\Users\user\anaphoricity.xlsx
Loading anaphoricity from https://example.com/anaphoricity.xml
Loading anaphoricity from anaphoricity.h5
Loading anaphoricity from /tmp/anaphoricity.npz
Loading anaphoricity from anaphoricity.mat
Loading anaphoricity from data/2023-10-25-anaphoricity.tsv
Loading anaphoricity from /opt/anaphora/anaphoricity.db
Loading anaphoricity from s3://anaphora-bucket/anaphoricity.parquet
Loading anaphoricity from anaphora/anaphoricity.rds
Loading anaphoricity from /var/log/anaphora/anaphoricity.log
Loading anaphoricity from ftp://anaphora.org/anaphoricity.zip
Loading pairwise from     data/pairwise_01.csv
Loading pairwise from     /home/user/pairwise_data.txt
Loading pairwise from     C:\Users\user\Documents\pairwise.xlsx
Loading pairwise from     https://example.com/pairwise.json
Loading pairwise from     s3://bucket/pairwise.parquet
Loading pairwise from     /tmp/pairwise.pkl
Loading pairwise from     data/pairwise_02.csv
Loading pairwise from     /home/user/pairwise_data2.txt
Loading pairwise from     C:\Users\user\Downloads\pairwise.zip
Loading pairwise from     https://example.com/pairwise.xml
Loading pairwise from     s3://bucket/pairwise2.parquet
Loading pairwise from     /tmp/pairwise2.pkl
Loading pairwise from     data/pairwise_03.csv
Loading pairwise from     /home/user/pairwise_data3.txt
Loading pairwise from     C:\Users\user\Desktop\pairwise.rar
Saving model to model_2023_10_25_11_07.h5
Saving model to best_model.hdf5
Saving model to final_model.pkl
Saving model to trained_model.pt
Saving model to model_checkpoint.ckpt
Saving model to my_model.json
Saving model to model_weights.npz
Saving model to model_architecture.yaml
Saving model to last_model.sav
Saving model to new_model.onnx
Saving model to model_config.ini
Saving model to model_state_dict.pth
Saving model to best_model.joblib
Saving model to latest_model.tflite
Saving embedding to       embedding_1.npy
Saving embedding to       word2vec_model.bin
Saving embedding to       bert_embeddings.h5
Saving embedding to       glove.6B.300d.txt
Saving embedding to       fasttext.vec
Saving embedding to       elmo_embeddings.pkl
Saving embedding to       gpt2_embeddings.pt
Saving embedding to       roberta_embeddings.json
Saving embedding to       flair_embeddings.csv
Saving embedding to       xlnet_embeddings.tfrecord
Saving embedding to       albert_embeddings.npz
Saving embedding to       electra_embeddings.mat
Saving embedding to       t5_embeddings.hd5f
Saving embedding to       distilbert_embeddings.parquet
Saving embedding to       longformer_embeddings.hdf
scoref.trainBERT
scoref.trainGPT-3
scoref.trainXLNet
scoref.trainTransformer-XL
scoref.trainALBERT
scoref.trainRoBERTa
scoref.trainELECTRA
scoref.trainT5
scoref.trainBART
scoref.trainDeBERTa
scoref.trainReformer
scoref.trainLongformer
scoref.trainBigBird
scoref.trainGPT-Neo
scoref.trainGPT-J
Warning: Invalid Parent Id 0 Sentence Length: 12
Warning: Invalid Parent Id -1 Sentence Length: 9
Warning: Invalid Parent Id 3 Sentence Length: 15
Warning: Invalid Parent Id 5 Sentence Length: 11
Warning: Invalid Parent Id 2 Sentence Length: 10
Warning: Invalid Parent Id -2 Sentence Length: 8
Warning: Invalid Parent Id 4 Sentence Length: 13
Warning: Invalid Parent Id 1 Sentence Length: 14
Warning: Invalid Parent Id -3 Sentence Length: 7
Warning: Invalid Parent Id 6 Sentence Length: 16
Warning: Invalid Parent Id -4 Sentence Length: 6
Warning: Invalid Parent Id 7 Sentence Length: 17
Warning: Invalid Parent Id -5 Sentence Length: 5
Warning: Invalid Parent Id 8 Sentence Length: 18
Warning: Invalid Parent Id -6 Sentence Length: 4
Read the sents file: sents.txt
Read the sents file: sents_2023_10_25.csv
Read the sents file: sents_en.json
Read the sents file: sents_fr.xml
Read the sents file: sents_de.tsv
Read the sents file: sents_es.xlsx
Read the sents file: sents_it.pkl
Read the sents file: sents_pt.hdf5
Read the sents file: sents_ru.npz
Read the sents file: sents_zh.parquet
Read the sents file: sents_ja.feather
Read the sents file: sents_ko.orc
Read the sents file: sents_ar.avro
Read the sents file: sents_hi.mat
Read the sents file: sents_swf.zip
Deleting any existing index at /home/user/allPatternsDir
Deleting any existing index at C:\Users\user\allPatternsDir
Deleting any existing index at /var/www/html/allPatternsDir
Deleting any existing index at /mnt/data/allPatternsDir
Deleting any existing index at /opt/app/allPatternsDir
Deleting any existing index at /Users/user/Desktop/allPatternsDir
Deleting any existing index at D:\Documents\allPatternsDir
Deleting any existing index at /tmp/allPatternsDir
Deleting any existing index at /usr/local/bin/allPatternsDir
Deleting any existing index at E:\Projects\allPatternsDir
Deleting any existing index at /root/allPatternsDir
Deleting any existing index at /home/user/Documents/allPatternsDir
Deleting any existing index at C:\Program Files\allPatternsDir
Deleting any existing index at /etc/allPatternsDir
Error reading train.conllX
Error reading test.conllX
Error reading dev.conllX
Error reading input.conllX
Error reading output.conllX
Error reading gold.conllX
Error reading sample.conllX
Error reading data.conllX
Error reading config.conllX
Error reading model.conllX
Error reading features.conllX
Error reading labels.conllX
Error reading scores.conllX
Error reading results.conllX
Error reading eval.conllX
test properties are {testProps: {name: "foo", value: "bar"}}
test properties are {testProps: {name: "baz", value: "qux"}}
test properties are {testProps: {name: "alice", value: "bob"}}
test properties are {testProps: {name: "hello", value: "world"}}
test properties are {testProps: {name: "color", value: "red"}}
test properties are {testProps: {name: "shape", value: "circle"}}
test properties are {testProps: {name: "animal", value: "cat"}}
test properties are {testProps: {name: "fruit", value: "apple"}}
test properties are {testProps: {name: "number", value: "42"}}
test properties are {testProps: {name: "letter", value: "A"}}
test properties are {testProps: {name: "planet", value: "Earth"}}
test properties are {testProps: {name: "sport", value: "soccer"}}
test properties are {testProps: {name: "car", value: "Tesla"}}
test properties are {testProps: {name: "book", value: "Harry Potter"}}
test properties are {testProps: {name: "movie", value: "Avatar"}}
input value is "Hello world"
input value is "42"
input value is "null"
input value is "true"
input value is "3.14"
input value is "foo"
input value is "[]"
input value is "{}"
input value is "\"bar\""
input value is "-1"
input value is "undefined"
input value is "false"
input value is "NaN"
input value is "0"
input value is "\"\""
Processed 12 documents in C:\Users\Alice\Documents\Project1
Processed 8 documents in C:\Users\Bob\Downloads\Report2
Processed 15 documents in C:\Users\Charlie\Desktop\Homework3
Processed 10 documents in C:\Users\David\Documents\Thesis4
Processed 9 documents in C:\Users\Eve\Downloads\Invoice5
Processed 11 documents in C:\Users\Frank\Desktop\Memo6
Processed 13 documents in C:\Users\Grace\Documents\Resume7
Processed 7 documents in C:\Users\Harry\Downloads\Contract8
Processed 14 documents in C:\Users\Irene\Desktop\Presentation9
Processed 6 documents in C:\Users\Jack\Documents\Article10
Processed 16 documents in C:\Users\Kate\Downloads\Essay11
Processed 5 documents in C:\Users\Larry\Desktop\Schedule12
Processed 17 documents in C:\Users\Mary\Documents\Proposal13
Processed 4 documents in C:\Users\Nick\Downloads\Certificate14
Processed 18 documents in C:\Users\Olivia\Desktop\Letter15
Lexicon.score 12/3 as known word.
Lexicon.score 8/5 as known word.
Lexicon.score 15/4 as known word.
Lexicon.score 9/2 as known word.
Lexicon.score 11/6 as known word.
Lexicon.score 13/1 as known word.
Lexicon.score 10/7 as known word.
Lexicon.score 14/8 as known word.
Lexicon.score 7/9 as known word.
Lexicon.score 6/10 as known word.
Lexicon.score 5/11 as known word.
Lexicon.score 4/12 as known word.
Lexicon.score 3/13 as known word.
Lexicon.score 2/14 as known word.
Lexicon.score 1/15 as known word.
Error setting up NER_COMBINER_NAME! Not applying NER tags!
Error setting up NER_COMBINER_NAME! Not applying NER tags!
Error setting up NER_COMBINER_NAME! Not applying NER tags!
Error setting up NER_COMBINER_NAME! Not applying NER tags!
Error setting up NER_COMBINER_NAME! Not applying NER tags!
Error setting up NER_COMBINER_NAME! Not applying NER tags!
Error setting up NER_COMBINER_NAME! Not applying NER tags!
Error setting up NER_COMBINER_NAME! Not applying NER tags!
Error setting up NER_COMBINER_NAME! Not applying NER tags!
Error setting up NER_COMBINER_NAME! Not applying NER tags!
Error setting up NER_COMBINER_NAME! Not applying NER tags!
Error setting up NER_COMBINER_NAME! Not applying NER tags!
Error setting up NER_COMBINER_NAME! Not applying NER tags!
Error setting up NER_COMBINER_NAME! Not applying NER tags!
Error setting up NER_COMBINER_NAME! Not applying NER tags!
Couldn't build object from line: name=John age=25 gender=male
Couldn't build object from line: id=1234 type=book title=The Catcher in the Rye
Couldn't build object from line: x=3.14 y=2.71 z=1.62
Couldn't build object from line: date=2023-10-25 time=11:19:42
Couldn't build object from line: color=red shape=circle radius=10
Couldn't build object from line: username=admin password=123456
Couldn't build object from line: email=john@example.com phone=+1-234-567-8901
Couldn't build object from line: country=USA state=Washington city=Moses Lake
Couldn't build object from line: animal=cat breed=Persian name=Fluffy
Couldn't build object from line: product=iPhone 12 price=$799 quantity=2
Couldn't build object from line: score=95 grade=A comment=Excellent
Couldn't build object from line: movie=The Matrix year=1999 genre=Sci-Fi
Couldn't build object from line: sport=basketball team=Lakers player=LeBron James
Couldn't build object from line: food=pizza size=large toppings=cheese, pepperoni, mushrooms
Compacting grammars...
Compacting grammars...
Compacting grammars...
Compacting grammars...
Compacting grammars...
Compacting grammars...
Compacting grammars...
Compacting grammars...
Compacting grammars...
Compacting grammars...
Compacting grammars...
Compacting grammars...
Compacting grammars...
Compacting grammars...
Compacting grammars...
Macro-averaged accuracy:
Macro-averaged accuracy:
Macro-averaged accuracy:
Macro-averaged accuracy:
Macro-averaged accuracy:
Macro-averaged accuracy:
Macro-averaged accuracy:
Macro-averaged accuracy:
Macro-averaged accuracy:
Macro-averaged accuracy:
Macro-averaged accuracy:
Macro-averaged accuracy:
Macro-averaged accuracy:
Macro-averaged accuracy:
Macro-averaged accuracy:
Tregex: tregex
Tregex: tregex -n -o -w
Tregex: tregex -f trees.txt NP < NN
Tregex: tregex -C -t "(S < (VP < VBD|VBN) !$,, (VP < VBD|VBN))"
Tregex: tregex -s "(NP < NNS|NNPS) > NP|PP"
Tregex: tregex -c "(S < (NP $.. VP)) !< VP"
Tregex: tregex -e "(NP < (DT < the)) > S"
Tregex: tregex -p "(VP !< /VB.?/)"
Tregex: tregex -m "(S < (NP !< PRP) $.. (VP < TO))"
Tregex: tregex -l "(ADJP < JJ|JJR|JJS)"
Tregex: tregex -v "(S < (VP < VBG) $+ NP)"
Tregex: tregex -a "(NP < (DT !< /^(a|an)$/))"
Tregex: tregex -h "(S < (VP !< /VB.?/))"
Tregex: tregex -i "(NP !< EX)"
Tregex: tregex -x "(S < (VP < MD) $+ VP)"
normalizeDate: 2023-10-25T11:21:49Z to October 25, 2023
normalizeDate: 2023-10-24T23:59:59Z to October 24, 2023
normalizeDate: 2023-10-31T00:00:00Z to October 31, 2023
normalizeDate: 2023-11-01T12:34:56Z to November 1, 2023
normalizeDate: 2023-09-30T18:45:32Z to September 30, 2023
normalizeDate: 2023-12-25T09:00:00Z to December 25, 2023
normalizeDate: 2024-01-01T00:00:01Z to January 1, 2024
normalizeDate: 2023-08-15T15:15:15Z to August 15, 2023
normalizeDate: 2023-07-04T04:44:44Z to July 4, 2023
normalizeDate: 2023-06-30T23:23:23Z to June 30, 2023
normalizeDate: 2023-05-12T12:12:12Z to May 12, 2023
normalizeDate: 2023-04-01T01:01:01Z to April 1, 2023
normalizeDate: 2023-03-14T03:14:15Z to March 14, 2023
normalizeDate: 2023-02-28T22:22:22Z to February 28, 2023
normalizeDate: 2023-01-31T11:11:11Z to January 31, 2023
12 phrasal category types, 45 tag types, and 678 word types
8 phrasal category types, 36 tag types, and 543 word types
10 phrasal category types, 42 tag types, and 621 word types
9 phrasal category types, 39 tag types, and 587 word types
11 phrasal category types, 43 tag types, and 654 word types
7 phrasal category types, 33 tag types, and 512 word types
13 phrasal category types, 47 tag types, and 701 word types
6 phrasal category types, 30 tag types, and 489 word types
14 phrasal category types, 49 tag types, and 724 word types
5 phrasal category types, 27 tag types, and 467 word types
15 phrasal category types, 51 tag types, and 746 word types
4 phrasal category types, 24 tag types, and 445 word types
16 phrasal category types, 53 tag types, and 769 word types
3 phrasal category types, 21 tag types, and 423 word types
hypg( 12 ; 20 , 30 , 50 ) = 0.023
hypg( 5 ; 10 , 15 , 40 ) = 0.147
hypg( 8 ; 15 , 25 , 60 ) = 0.056
hypg( 10 ; 18 , 35 , 70 ) = 0.012
hypg( 7 ; 12 , 20 , 45 ) = 0.089
hypg( 9 ; 16 , 28 , 55 ) = 0.034
hypg( 6 ; 14 , 22 , 50 ) = 0.075
hypg( 11 ; 17 , 32 , 65 ) = 0.017
hypg( 4 ; 8 , 12 , 30 ) = 0.198
hypg( 13 ; 21 , 36 , 75 ) = 0.009
hypg( 3 ; 6 , 9 , 25 ) = 0.267
hypg( 14 ; 23 , 40 , 80 ) = 0.006
hypg( 2 ; 4 , 6 , 20 ) = 0.357
hypg( 5 ; 10 , 20 , 30 ) = 0.021
hypg( 8 ; 15 , 25 , 40 ) = 0.056
hypg( 3 ; 12 , 18 , 24 ) = 0.132
hypg( 6 ; 9 , 30 , 45 ) = 0.005
hypg( 4 ; 7 , 10 , 20 ) = 0.198
hypg( 10 ; 20 , 40 , 60 ) = 0.015
hypg( 7 ; 14 , 28 , 35 ) = 0.087
hypg( 2 ; 5 , 15 , 25 ) = 0.039
hypg( 9 ; 18 , 36 , 50 ) = 0.024
hypg( 1 ; 4 , 12 , 16 ) = 0.083
hypg( 11 ; 22 , 44 , 66 ) = 0.009
hypg( 3 ; 6 , 9 , 18 ) = 0.216
PRECISION (macro average): 0.875%
PRECISION (macro average): 0.921%
PRECISION (macro average): 0.904%
PRECISION (macro average): 0.892%
PRECISION (macro average): 0.883%
PRECISION (macro average): 0.897%
PRECISION (macro average): 0.909%
PRECISION (macro average): 0.916%
PRECISION (macro average): 0.887%
PRECISION (macro average): 0.899%
PRECISION (macro average): 0.881%
PRECISION (macro average): 0.894%
PRECISION (macro average): 0.902%
PRECISION (macro average): 0.907%
PRECISION (macro average): 0.912%
PRECISION (macro average): 0.875%
PRECISION (macro average): 0.921%
PRECISION (macro average): 0.789%
PRECISION (macro average): 0.834%
PRECISION (macro average): 0.902%
PRECISION (macro average): 0.856%
PRECISION (macro average): 0.811%
PRECISION (macro average): 0.878%
PRECISION (macro average): 0.894%
PRECISION (macro average): 0.823%
PRECISION (macro average): 0.842%
PRECISION (macro average): 0.867%
PRECISION (macro average): 0.907%
PRECISION (macro average): 0.796%
PRECISION (macro average): 0.885%
ABC F1: 12.34
XYZ F1: 45.67
PQR F1: 89.01
LMN F1: 23.45
GHI F1: 67.89
DEF F1: 34.56
UVW F1: 78.90
RST F1: 90.12
KLM F1: 56.78
NOP F1: 12.56
JKL F1: 45.12
EFG F1: 78.34
CDE F1: 34.78
BCD F1: 67.45
MNO F1: 89.67
ABC F1: 12.34
XYZ F1: 45.67
PQR F1: 89.01
LMN F1: 23.45
RST F1: 67.89
UVW F1: 34.56
GHI F1: 78.90
JKL F1: 90.12
DEF F1: 56.78
MNO F1: 12.56
QRS F1: 45.23
TUV F1: 67.45
WXY F1: 89.67
BCD F1: 34.78
EFG F1: 56.90
Finished 10 total batches, running evaluation cycle
Finished 25 total batches, running evaluation cycle
Finished 50 total batches, running evaluation cycle
Finished 75 total batches, running evaluation cycle
Finished 100 total batches, running evaluation cycle
Finished 150 total batches, running evaluation cycle
Finished 200 total batches, running evaluation cycle
Finished 250 total batches, running evaluation cycle
Finished 300 total batches, running evaluation cycle
Finished 350 total batches, running evaluation cycle
Finished 400 total batches, running evaluation cycle
Finished 450 total batches, running evaluation cycle
Finished 500 total batches, running evaluation cycle
Finished 550 total batches, running evaluation cycle
Finished 600 total batches, running evaluation cycle
Finished 10 total batches, running evaluation cycle
Finished 25 total batches, running evaluation cycle
Finished 50 total batches, running evaluation cycle
Finished 75 total batches, running evaluation cycle
Finished 100 total batches, running evaluation cycle
Finished 150 total batches, running evaluation cycle
Finished 200 total batches, running evaluation cycle
Finished 250 total batches, running evaluation cycle
Finished 300 total batches, running evaluation cycle
Finished 400 total batches, running evaluation cycle
Finished 500 total batches, running evaluation cycle
Finished 600 total batches, running evaluation cycle
Finished 700 total batches, running evaluation cycle
Finished 800 total batches, running evaluation cycle
Finished 1000 total batches, running evaluation cycle
Finished 10 total batches, running evaluation cycle
Finished 25 total batches, running evaluation cycle
Finished 50 total batches, running evaluation cycle
Finished 75 total batches, running evaluation cycle
Finished 100 total batches, running evaluation cycle
Finished 150 total batches, running evaluation cycle
Finished 200 total batches, running evaluation cycle
Finished 250 total batches, running evaluation cycle
Finished 300 total batches, running evaluation cycle
Finished 350 total batches, running evaluation cycle
Finished 400 total batches, running evaluation cycle
Finished 450 total batches, running evaluation cycle
Finished 500 total batches, running evaluation cycle
Finished 550 total batches, running evaluation cycle
Finished 600 total batches, running evaluation cycle
Forced FlexiTagging 0 UTF-8
Forced FlexiTagging 1 UTF-8
Forced FlexiTagging 2 UTF-8
Forced FlexiTagging 3 UTF-8
Forced FlexiTagging 4 UTF-8
Forced FlexiTagging 5 UTF-8
Forced FlexiTagging 6 UTF-8
Forced FlexiTagging 7 UTF-8
Forced FlexiTagging 8 UTF-8
Forced FlexiTagging 9 UTF-8
Forced FlexiTagging 10 UTF-8
Forced FlexiTagging 11 UTF-8
Forced FlexiTagging 12 UTF-8
Forced FlexiTagging 13 UTF-8
Forced FlexiTagging 14 UTF-8
Forced FlexiTagging 0 UTF-8
Forced FlexiTagging 1 UTF-8
Forced FlexiTagging 2 UTF-8
Forced FlexiTagging 3 UTF-8
Forced FlexiTagging 4 UTF-8
Forced FlexiTagging 5 UTF-8
Forced FlexiTagging 6 UTF-8
Forced FlexiTagging 7 UTF-8
Forced FlexiTagging 8 UTF-8
Forced FlexiTagging 9 UTF-8
Forced FlexiTagging 10 UTF-8
Forced FlexiTagging 11 UTF-8
Forced FlexiTagging 12 UTF-8
Forced FlexiTagging 13 UTF-8
Forced FlexiTagging 14 UTF-8
ERROR Track names do not match: expected: Hello found: Hallo
ERROR Track names do not match: expected: Despacito found: Despacitoo
ERROR Track names do not match: expected: Bohemian Rhapsody found: Bohemian Rap City
ERROR Track names do not match: expected: Shape of You found: Shape of U
ERROR Track names do not match: expected: Bad Guy found: Bad Boy
ERROR Track names do not match: expected: Old Town Road found: Old Town Load
ERROR Track names do not match: expected: Blinding Lights found: Blinding Flights
ERROR Track names do not match: expected: Someone You Loved found: Someone You Love
ERROR Track names do not match: expected: Dance Monkey found: Dance Donkey
ERROR Track names do not match: expected: Believer found: Beleaver
ERROR Track names do not match: expected: Closer found: Closure
ERROR Track names do not match: expected: Perfect found: Prefect
ERROR Track names do not match: expected: Havana found: Havanna
ERROR Track names do not match: expected: Happy found: Hapy
ERROR Track names do not match: expected: Rolling in the Deep found: Rolling in the Sheep
Used {pty ltd} to recognize hello.txt
Used {pty ltd} to recognize report.pdf
Used {pty ltd} to recognize image.jpg
Used {pty ltd} to recognize data.csv
Used {pty ltd} to recognize music.mp3
Used {pty ltd} to recognize video.mp4
Used {pty ltd} to recognize resume.docx
Used {pty ltd} to recognize notes.txt
Used {pty ltd} to recognize presentation.pptx
Used {pty ltd} to recognize code.py
Used {pty ltd} to recognize book.pdf
Used {pty ltd} to recognize photo.png
Used {pty ltd} to recognize song.wav
Used {pty ltd} to recognize movie.mkv
Used {pty ltd} to recognize script.sh
CleanXML: ending tokens: [word=the, tag=DT, lemma=the, ner=O, speaker=PER0, beginOffset=0, endOffset=3]
CleanXML: ending tokens: [word=., tag=., lemma=., ner=O, speaker=PER0, beginOffset=15, endOffset=16]
CleanXML: ending tokens: [word=is, tag=VBZ, lemma=is, ner=O, speaker=PER0, beginOffset=5, endOffset=7]
CleanXML: ending tokens: [word=a, tag=DT, lemma=a, ner=O, speaker=PER0, beginOffset=8, endOffset=9]
CleanXML: ending tokens: [word=test, tag=NN, lemma=test, ner=O, speaker=PER0, beginOffset=10, endOffset=14]
CleanXML: ending tokens: [word=sentences, tag=NNS, lemma=sentence, ner=O, speaker=PER0, beginOffset=17, endOffset=26]
CleanXML: ending tokens: [word=this, tag=DT, lemma=this, ner=O, speaker=PER1, beginOffset=27, endOffset=31]
CleanXML: ending tokens: [word=isn't, tag=VBZ+RB+PUNCTUATION+PUNCTUATION+PUNCTUATION+PUNCTUATION+PUNCTUATION+PUNCTUATION+PUNCTUATION+PUNCTUATION+PUNCTUATION+PUNCTUATION+PUNCTUATION+PUNCTUATION+PUNCTUATION+PUNCTUATION+PUNCTUATION+PUNCTUATION
CleanXML: ending tokens: [word=a valid token list., tag=NIL
CleanXML: ending tokens: [word="Hello", tag="NN", lemma="hello", ner="O", speaker="PER0", beginOffset="0", endOffset="7"]
CleanXML: ending tokens: [word="world", tag="NN", lemma="world", ner="LOCATION", speaker="PER0", beginOffset="8", endOffset="13"]
CleanXML: ending tokens: [word="!", tag=".", lemma="!", ner="O", speaker="PER0", beginOffset="13", endOffset="14"]
CleanXML: ending tokens: [word="How", tag="WRB", lemma="how", ner="O", speaker="PER1", beginOffset="15", endOffset="18"]
CleanXML: ending tokens: [word="are", tag="VBP", lemma="are", ner="O", speaker="PER1", beginOffset="19", endOffset="22"]
CleanXML: ending tokens: [word="you?", tag="PRP+.", lemma="you?", ner="O", speaker="PER1", beginOffset="23", endOffset="27"]
CleanXML: ending tokens: [This, is, a, test, .]
CleanXML: ending tokens: [The, weather, is, nice, today, .]
CleanXML: ending tokens: [She, loves, reading, books, .]
CleanXML: ending tokens: [He, said, hello, to, me, .]
CleanXML: ending tokens: [They, are, going, to, the, park, .]
CleanXML: ending tokens: [What, time, is, it, ?]
CleanXML: ending tokens: [I, have, a, dog, and, a, cat, .]
CleanXML: ending tokens: [She, is, very, smart, and, kind, .]
CleanXML: ending tokens: [He, likes, playing, soccer, .]
CleanXML: ending tokens: [They, have, been, friends, for, a, long, time, .]
CleanXML: ending tokens: [Where, are, you, from, ?]
CleanXML: ending tokens: [I, am, happy, to, meet, you, .]
CleanXML: ending tokens: [She, has, a, beautiful, voice, .]
CleanXML: ending tokens: [He, is, studying, hard, for, the exam.]
CleanXML: ending tokens: [They are having fun at the party.]
0 percent complete
10 percent complete
25 percent complete
33 percent complete
50 percent complete
60 percent complete
75 percent complete
80 percent complete
90 percent complete
95 percent complete
99 percent complete
100 percent complete
101 percent complete
110 percent complete
120 percent complete
Adjusting end char offset from 1024 to 0
Adjusting end char offset from 512 to 0
Adjusting end char offset from 768 to 0
Adjusting end char offset from 256 to 0
Adjusting end char offset from 128 to 0
Adjusting end char offset from 64 to 0
Adjusting end char offset from 32 to 0
Adjusting end char offset from 16 to 0
Adjusting end char offset from 8 to 0
Adjusting end char offset from 4 to 0
Adjusting end char offset from -4 to 0
Adjusting end char offset from -8 to 0
Adjusting end char offset from -16 to 0
Adjusting end char offset from -32 to 0
Adjusting end char offset from -64 to 0
Deleting errant node 5 as if -NONE-: 0.3 GB2312
Deleting errant node 12 as if -NONE-: 0.5 UTF-8
Deleting errant node 9 as if -NONE-: 0.4 Big5
Deleting errant node 3 as if -NONE-: 0.2 GBK
Deleting errant node 7 as if -NONE-: 0.6 ISO-8859-1
Deleting errant node 10 as if -NONE-: 0.7 UTF-16
Deleting errant node 4 as if -NONE-: 0.1 ASCII
Deleting errant node 8 as if -NONE-: 0.8 UTF-32
Deleting errant node 6 as if -NONE-: 0.9 EUC-CN
Deleting errant node 11 as if -NONE-: 1.0 EUC-JP
Deleting errant node 2 as if -NONE-: 0.3 Shift-JIS
Deleting errant node 13 as if -NONE-: 1.1 EUC-KR
Deleting errant node 14 as if -NONE-: 1.2 ISO-2022-JP
Deleting errant node 15 as if -NONE-: 1.3 ISO-2022-KR
Deleting errant node 1 as if -NONE-: 0.4 ISO-2022-CN
Replacing relation argument: [John] with predicted mention [he]
Replacing relation argument: [Sweden] with predicted mention [this country]
Replacing relation argument: [Alice] with predicted mention [she]
Replacing relation argument: [Microsoft] with predicted mention [the company]
Replacing relation argument: [Bob] with predicted mention [him]
Replacing relation argument: [Bing] with predicted mention [the search engine]
Replacing relation argument: [Emma] with predicted mention [her]
Replacing relation argument: [France] with predicted mention [that country]
Replacing relation argument: [David] with predicted mention [he]
Replacing relation argument: [Amazon] with predicted mention [the online retailer]
Replacing relation argument: [Stockholm] with predicted mention [the capital]
Replacing relation argument: [Eva] with predicted mention [she]
Replacing relation argument: [China] with predicted mention [this nation]
Replacing relation argument: [Tom] with predicted mention [him]
Number of iterations of trees: 100
Number of iterations of trees: 50
Number of iterations of trees: 200
Number of iterations of trees: 75
Number of iterations of trees: 150
Number of iterations of trees: 25
Number of iterations of trees: 300
Number of iterations of trees: 125
Number of iterations of trees: 175
Number of iterations of trees: 10
Number of iterations of trees: 250
Number of iterations of trees: 80
Number of iterations of trees: 120
Number of iterations of trees: 40
Number of iterations of trees: 160
Searching for resource: config.json ... not found.
Searching for resource: image.jpg ... not found.
Searching for resource: index.html ... not found.
Searching for resource: data.csv ... not found.
Searching for resource: style.css ... not found.
Searching for resource: script.js ... not found.
Searching for resource: logo.png ... not found.
Searching for resource: report.pdf ... not found.
Searching for resource: video.mp4 ... not found.
Searching for resource: audio.mp3 ... not found.
Searching for resource: font.ttf ... not found.
Searching for resource: readme.md ... not found.
Searching for resource: backup.zip ... not found.
Searching for resource: favicon.ico ... not found.
Searching for resource: test.txt ... not found.
Number of minimized rules: 12
Number of minimized rules: 8
Number of minimized rules: 10
Number of minimized rules: 9
Number of minimized rules: 11
Number of minimized rules: 7
Number of minimized rules: 13
Number of minimized rules: 6
Number of minimized rules: 14
Number of minimized rules: 5
Number of minimized rules: 15
Number of minimized rules: 4
Number of minimized rules: 16
Number of minimized rules: 3
Number of minimized rules: 17
unknown attribute value 0 of attribute 1
unknown attribute value 1 of attribute 2
unknown attribute value 2 of attribute 3
unknown attribute value 3 of attribute 4
unknown attribute value 4 of attribute 5
unknown attribute value 5 of attribute 6
unknown attribute value 6 of attribute 7
unknown attribute value 7 of attribute 8
unknown attribute value 8 of attribute 9
unknown attribute value 9 of attribute 10
unknown attribute value -1 of attribute -1
unknown attribute value -2 of attribute -2
unknown attribute value -3 of attribute -3
unknown attribute value -4 of attribute -4
unknown attribute value -5 of attribute -5
12 labels incorrect
0 labels incorrect
7 labels incorrect
3 labels incorrect
9 labels incorrect
1 label incorrect
5 labels incorrect
4 labels incorrect
8 labels incorrect
2 labels incorrect
6 labels incorrect
10 labels incorrect
11 labels incorrect
13 labels incorrect
14 labels incorrect
after deleting 2 Graph{V=[1, 2, 3], E=[(1,2), (2,3), (3,1)]}
after deleting 2 Graph{V=[a, b, c, d], E=[(a,b), (b,c), (c,d), (d,a)]}
after deleting 2 Graph{V=[x, y, z], E=[(x,y), (y,z), (z,x), (x,z)]}
after deleting 2 Graph{V=[p, q, r, s, t], E=[(p,q), (q,r), (r,s), (s,t), (t,p)]}
after deleting 2 Graph{V=[m, n, o], E=[(m,n), (n,o)]}
after deleting 2 Graph{V=[u, v, w, x], E=[(u,v), (v,w), (w,x), (x,u), (u,w)]}
after deleting 2 Graph{V=[h, i, j, k], E=[(h,i), (i,j), (j,k), (k,h)]}
after deleting 2 Graph{V=[e, f, g], E=[(e,f), (f,g), (g,e)]}
after deleting 2 Graph{V=[l, m, n, o], E=[(l,m), (m,n), (n,o), (o,l)]}
after deleting 2 Graph{V=[s, t, u], E=[(s,t), (t,u), (u,s)]}
after deleting 2 Graph{V=[v, w, x, y], E=[(v,w), (w,x), (x,y), (y,v)]}
after deleting 2 Graph{V=[r, s, t], E=[(r,s), (s,t)]}
after deleting 2 Graph{V=[q, r, s, t], E=[(q,r), (r,s), (s,t), (t,q)]}
after deleting 2 Graph{V=[g, h, i], E=[(g,h), (h,i)]}
after deleting 2 Graph{V=[c, d, e], E=[(c,d), (d,e)]}
Document Date: 2023-10-26
Document Date: 2023-09-15
Document Date: 2023-08-31
Document Date: 2023-07-04
Document Date: 2023-06-12
Document Date: 2023-05-29
Document Date: 2023-04-17
Document Date: 2023-03-08
Document Date: 2023-02-21
Document Date: 2023-01-01
Document Date: 2022-12-25
Document Date: 2022-11-11
Document Date: 2022-10-31
Document Date: 2022-09-30
Document Date: 2022-08-15
Reading 5 CoreNLP files from /home/user/documents
Reading 12 CoreNLP files from /mnt/c/Users/user/Desktop
Reading 0 CoreNLP files from /dev/null
Reading 8 CoreNLP files from /tmp/corenlp
Reading 3 CoreNLP files from /usr/local/share/corenlp
Reading 10 CoreNLP files from /var/log/corenlp
Reading 6 CoreNLP files from /opt/corenlp/data
Reading 4 CoreNLP files from /home/user/.corenlp
Reading 9 CoreNLP files from /media/user/USB/corenlp
Reading 7 CoreNLP files from /etc/corenlp/config
Reading 2 CoreNLP files from /root/corenlp
Reading 11 CoreNLP files from /lib/corenlp
Reading 13 CoreNLP files from /srv/corenlp
Reading 14 CoreNLP files from /run/corenlp
sigmoids: [0.73, 0.81, 0.67, 0.89, 0.76]
sigmoids: [0.62, 0.92, 0.71, 0.64, 0.85]
sigmoids: [0.78, 0.69, 0.83, 0.74, 0.88]
sigmoids: [0.66, 0.86, 0.77, 0.68, 0.91]
sigmoids: [0.82, 0.72, 0.87, 0.79, 0.93]
sigmoids: [0.58, 0.94, 0.65, 0.61, 0.83]
sigmoids: [0.75, 0.84, 0.69, 0.86, 0.78]
sigmoids: [0.63, 0.91, 0.73, 0.66, 0.87]
sigmoids: [0.81, 0.71, 0.85, 0.76, 0.92]
sigmoids: [0.67, 0.89, 0.79, 0.7 , 0.9 ]
sigmoids: [0.83, 0.74, 0.88, 0.8 , 0.95]
sigmoids: [0.59, 0.93, 0.68, 0.63, 0.86]
sigmoids: [0.77, 0.8 , 0.72, 0.88, 0.81]
sigmoids: [0.64, 0.9 , 0.76, 0.69, 0.89]
sigmoids: [0.8 , 0.7 , 0.84, 0.75, 0.91]
Nothing to do for /home/user/data/input.csv
Nothing to do for C:\Users\user\Documents\data.xlsx
Nothing to do for https://example.com/api/data.json
Nothing to do for s3://bucket-name/data.parquet
Nothing to do for hdfs://namenode:8020/user/hive/warehouse/data.orc
Nothing to do for ftp://user:password@host/data.txt
Nothing to do for file:///tmp/data.xml
Nothing to do for gs://project-id/data.ndjson
Nothing to do for az://account-name/container-name/data.avro
Nothing to do for mongodb://user:password@host:port/database/collection
Nothing to do for redis://user:password@host:port/db-number/key
Nothing to do for kafka://user:password@host:port/topic
Nothing to do for cassandra://user:password@host:port/keyspace/table
Nothing to do for elasticsearch://user:password@host:port/index/type
Nothing to do for mysql://user:password@host:port/database/table
The first sentence overall sentiment rating is Positive
The first sentence overall sentiment rating is Neutral
The first sentence overall sentiment rating is Negative
The first sentence overall sentiment rating is Very positive
The first sentence overall sentiment rating is Very negative
The first sentence overall sentiment rating is Mixed
The first sentence overall sentiment rating is Unknown
The first sentence overall sentiment rating is Positive
The first sentence overall sentiment rating is Negative
The first sentence overall sentiment rating is Neutral
The first sentence overall sentiment rating is Very positive
The first sentence overall sentiment rating is Very negative
The first sentence overall sentiment rating is Mixed
The first sentence overall sentiment rating is Unknown
The first sentence overall sentiment rating is Positive
no name found :( 45721
no name found :( 98234
no name found :( 61020
no name found :( 75843
no name found :( 02345
no name found :( 99901
no name found :( 11223
no name found :( 33058
no name found :( 44190
no name found :( 00012
no name found :( 26781
no name found :( 88876
no name found :( 55340
no name found :( 12098
no name found :( 77420
3 were skipped as length 0 or greater than 100
7 were skipped as length 0 or greater than 50
12 were skipped as length 0 or greater than 200
0 were skipped as length 0 or greater than 150
5 were skipped as length 0 or greater than 75
9 were skipped as length 0 or greater than 25
4 were skipped as length 0 or greater than 125
8 were skipped as length 0 or greater than 80
10 were skipped as length 0 or greater than 40
6 were skipped as length 0 or greater than 60
2 were skipped as length 0 or greater than 90
1 were skipped as length 0 or greater than 140
11 were skipped as length 0 or greater than 30
13 were skipped as length 0 or greater than 10
14 were skipped as length 0 or greater than 20
LexicalizedParser weight 0.45 : labeled NP tag NNP
LexicalizedParser weight 0.32 : labeled VP tag VBD
LexicalizedParser weight 0.67 : labeled PP tag IN
LexicalizedParser weight 0.12 : labeled ADJP tag JJ
LexicalizedParser weight 0.54 : labeled SBAR tag WHNP
LexicalizedParser weight 0.28 : labeled S tag VP
LexicalizedParser weight 0.76 : labeled NP tag DT
LexicalizedParser weight 0.43 : labeled VP tag VBZ
LexicalizedParser weight 0.39 : labeled PP tag TO
LexicalizedParser weight 0.21 : labeled ADVP tag RB
LexicalizedParser weight 0.59 : labeled SBAR tag S
LexicalizedParser weight 0.34 : labeled S tag NP
LexicalizedParser weight 0.51 : labeled NP tag PRP
LexicalizedParser weight 0.37 : labeled VP tag MD
LexicalizedParser weight 0.46 : labeled PP tag RP
Number of documents added are 456
Number of documents added are 789
Number of documents added are 123
Number of documents added are 321
Number of documents added are 654
Number of documents added are 987
Number of documents added are 147
Number of documents added are 258
Number of documents added are 369
Number of documents added are 963
Number of documents added are 852
Number of documents added are 741
Number of documents added are 159
Number of documents added are 357
Number of documents added are 951
addNormalizedQuantitiesToEntities: wi= 0.5 , collector= 1
addNormalizedQuantitiesToEntities: wi= 0.75 , collector= 2
addNormalizedQuantitiesToEntities: wi= 0.25 , collector= 3
addNormalizedQuantitiesToEntities: wi= 0.6 , collector= 4
addNormalizedQuantitiesToEntities: wi= 0.4 , collector= 5
addNormalizedQuantitiesToEntities: wi= 0.8 , collector= 6
addNormalizedQuantitiesToEntities: wi= 0.3 , collector= 7
addNormalizedQuantitiesToEntities: wi= 0.7 , collector= 8
addNormalizedQuantitiesToEntities: wi= 0.2 , collector= 9
addNormalizedQuantitiesToEntities: wi= 0.9 , collector= 10
addNormalizedQuantitiesToEntities: wi= 0.1 , collector= 11
addNormalizedQuantitiesToEntities: wi= 1.0 , collector= 12
addNormalizedQuantitiesToEntities: wi= 0.05 , collector= 13
addNormalizedQuantitiesToEntities: wi= 0.95 , collector= 14
addNormalizedQuantitiesToEntities: wi= 0.55 , collector= 15
Couldn't instantiate: -f : java.lang.IllegalArgumentException
Couldn't instantiate: --help : java.lang.NullPointerException
Couldn't instantiate: config.json : java.io.FileNotFoundException
Couldn't instantiate: 42 : java.lang.NumberFormatException
Couldn't instantiate: "Hello" : java.lang.ClassNotFoundException
Couldn't instantiate: null : java.lang.NullPointerException
Couldn't instantiate: [1, 2, 3] : java.lang.IllegalStateException
Couldn't instantiate: true : java.lang.UnsupportedOperationException
Couldn't instantiate: new Object() : java.lang.InstantiationException
Couldn't instantiate: System.out : java.lang.SecurityException
Couldn't instantiate: Math.PI : java.lang.NoSuchFieldException
Couldn't instantiate: Thread.currentThread() : java.lang.IllegalThreadStateException
Couldn't instantiate: this : java.lang.StackOverflowError
Couldn't instantiate: args[0] : java.lang.ArrayIndexOutOfBoundsException
Couldn't instantiate: main : java.lang.NoSuchMethodException
Parent is word "book" index 5 via nsubj
Parent is word "likes" index 2 via obj
Parent is word "red" index 4 via amod
Parent is word "she" index 1 via nsubjpass
Parent is word "gave" index 3 via iobj
Parent is word "dog" index 6 via nmod
Parent is word "happy" index 7 via xcomp
Parent is word "saw" index 2 via ccomp
Parent is word "him" index 4 via dobj
Parent is word "fast" index 5 via advmod
Parent is word "ran" index 3 via acl
Parent is word "big" index 6 via det
Parent is word "house" index 7 via nsubj
Parent is word "is" index 2 via cop
Updating lucene index at /var/lib/lucene/index
Updating lucene index at C:\Users\lucene\index
Updating lucene index at /home/lucene/index
Updating lucene index at /mnt/lucene/index
Updating lucene index at /opt/lucene/index
Updating lucene index at /tmp/lucene/index
Updating lucene index at /usr/local/lucene/index
Updating lucene index at /data/lucene/index
Updating lucene index at /dev/lucene/index
Updating lucene index at /media/lucene/index
Updating lucene index at /root/lucene/index
Updating lucene index at /srv/lucene/index
Updating lucene index at /etc/lucene/index
Updating lucene index at D:\lucene\index
Updating lucene index at E:\lucene\index
(logs) -Inf + -Inf = NaN
(logs) -Inf + -Inf = -Infinity
(logs) -Inf + -Inf = Undefined
(logs) -Inf + -Inf = Error
(logs) -Inf + -Inf = Exception
(logs) -Inf + -Inf = Invalid
(logs) -Inf + -Inf = Null
(logs) -Inf + -Inf = 0
(logs) -Inf + -Inf = -0
(logs) -Inf + -Inf = 1
(logs) -Inf + -Inf = -1
(logs) -Inf + -Inf = 2
(logs) -Inf + -Inf = -2
(logs) -Inf + -Inf = 3
(logs) -Inf + -Inf = -3
set1 left with: newSet1
set1 left with: newSet1 - 3
set1 left with: newSet1 + 5
set1 left with: newSet1 * 2
set1 left with: newSet1 / 4
set1 left with: newSet1 ^ 2
set1 left with: newSet1 % 10
set1 left with: newSet1 & 7
set1 left with: newSet1 | 8
set1 left with: newSet1 ~ 9
set1 left with: newSet1 << 2
set1 left with: newSet1 >> 3
set1 left with: newSet1 == 6
set1 left with: newSet1 != 4
set1 left with: newSet1 < 10
Document: ID= 4567 ( 12 sentences, 234 tokens)
Document: ID= 8910 ( 8 sentences, 145 tokens)
Document: ID= 1123 ( 10 sentences, 189 tokens)
Document: ID= 4456 ( 9 sentences, 172 tokens)
Document: ID= 7789 ( 11 sentences, 201 tokens)
Document: ID= 9900 ( 7 sentences, 134 tokens)
Document: ID= 3322 ( 13 sentences, 256 tokens)
Document: ID= 6644 ( 14 sentences, 278 tokens)
Document: ID= 5566 ( 6 sentences, 123 tokens)
Document: ID= 8877 ( 15 sentences, 293 tokens)
Document: ID= 1199 ( 16 sentences, 312 tokens)
Document: ID= 2233 ( 5 sentences, 111 tokens)
Document: ID= 3377 ( 17 sentences, 331 tokens)
Document: ID= 4411 ( 18 sentences, 349 tokens)
Document: ID= 5588 ( 4 sentences, 98 tokens)
NOUN SENTENCE VERB
ADJECTIVE SENTENCE PRONOUN
QUESTION SENTENCE ADVERB
COMMAND SENTENCE PREPOSITION
EXCLAMATION SENTENCE CONJUNCTION
NEGATION SENTENCE ARTICLE
QUOTE SENTENCE NOUN
INTERJECTION SENTENCE ADJECTIVE
DECLARATION SENTENCE QUESTION
SUGGESTION SENTENCE COMMAND
COMPLIMENT SENTENCE EXCLAMATION
APOLOGY SENTENCE NEGATION
REQUEST SENTENCE QUOTE
JOKE SENTENCE INTERJECTION
FACT SENTENCE DECLARATION
isOneSentence= true
isOneSentence= false
isOneSentence= 1
isOneSentence= 0
isOneSentence= yes
isOneSentence= no
isOneSentence= TRUE
isOneSentence= FALSE
isOneSentence= Yes
isOneSentence= No
isOneSentence= T
isOneSentence= F
isOneSentence= Y
isOneSentence= N
isOneSentence= t
(logs) -7 + -Inf = -Inf
(logs) -7 + -Inf = NaN
(logs) -7 + -Inf = Double.NEGATIVE_INFINITY
(logs) -7 + -Inf = undefined
(logs) -7 + -Inf = null
(logs) -7 + -Inf = error
(logs) -7 + -Inf = exception
(logs) -7 + -Inf = invalid
(logs) -7 + -Inf = not a number
(logs) -7 + -Inf = negative infinity
(logs) -7 + -Inf = unsupported operation
(logs) -7 + -Inf = overflow
(logs) -7 + -Inf = underflow
(logs) -7 + -Inf = indeterminate
(logs) -7 + -Inf = unknown
Finished translating 500 words (
Finished translating 1200 words (
Finished translating 300 words (
Finished translating 800 words (
Finished translating 1500 words (
Finished translating 400 words (
Finished translating 1000 words (
Finished translating 600 words (
Finished translating 900 words (
Finished translating 700 words (
Finished translating 1300 words (
Finished translating 200 words (
Finished translating 1400 words (
Finished translating 1100 words (
Finished translating 100 words (
Processed 100 documents
Processed 57 documents
Processed 23 documents
Processed 76 documents
Processed 34 documents
Processed 89 documents
Processed 45 documents
Processed 67 documents
Processed 12 documents
Processed 98 documents
Processed 54 documents
Processed 29 documents
Processed 81 documents
Processed 41 documents
Processed 92 documents
Writing 10 trees to file1.txt
Writing 15 trees to file2.txt
Writing 8 trees to file3.txt
Writing 12 trees to file4.txt
Writing 9 trees to file5.txt
Writing 11 trees to file6.txt
Writing 13 trees to file7.txt
Writing 14 trees to file8.txt
Writing 7 trees to file9.txt
Writing 6 trees to file10.txt
Writing 16 trees to file11.txt
Writing 5 trees to file12.txt
Writing 4 trees to file13.txt
Writing 3 trees to file14.txt
Writing 2 trees to file15.txt
#feattag+lemmas: 0
#feattag+lemmas: 1
#feattag+lemmas: 2
#feattag+lemmas: 3
#feattag+lemmas: 4
#feattag+lemmas: 5
#feattag+lemmas: 6
#feattag+lemmas: 7
#feattag+lemmas: 8
#feattag+lemmas: 9
#feattag+lemmas: 10
#feattag+lemmas: 11
#feattag+lemmas: 12
#feattag+lemmas: 13
#feattag+lemmas: 14
Getting data from "Hello world" (default encoding)
Getting data from "https://www.bing.com" (default encoding)
Getting data from "C:\\Users\\Documents\\file.txt" (default encoding)
Getting data from "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA..." (default encoding)
Getting data from "3.141592653589793" (default encoding)
Getting data from "user@example.com" (default encoding)
Getting data from "[1, 2, 3, 4, 5]" (default encoding)
Getting data from "null" (default encoding)
Getting data from "true" (default encoding)
Getting data from "false" (default encoding)
Getting data from "" (default encoding)
Getting data from "Lorem ipsum dolor sit amet" (default encoding)
Getting data from "2021-10-22T08:12:32Z" (default encoding)
Getting data from "Bing is a web search engine" (default encoding)
Getting data from "42" (default encoding)
Final score (Excellent) Reading = 9.87
Final score (Poor) Writing = 4.32
Final score (Good) Listening = 7.65
Final score (Fair) Speaking = 6.12
Final score (Very Good) Math = 8.54
Final score (Average) Science = 6.78
Final score (Outstanding) Art = 10.00
Final score (Below Average) History = 5.43
Final score (Satisfactory) Music = 7.21
Final score (Unsatisfactory) Geography = 4.89
Final score (Superb) Programming = 9.99
Final score (Bad) Sports = 3.56
Final score (Great) Literature = 8.76
Final score (Mediocre) Economics = 6.34
Final score (Awesome) Physics = 9.45
Error deserializing /home/user/data.csv
Error deserializing C:\Users\user\Documents\report.pdf
Error deserializing /var/log/syslog
Error deserializing /tmp/file.tmp
Error deserializing /etc/passwd
Error deserializing /dev/null
Error deserializing /opt/app/config.json
Error deserializing /usr/local/bin/script.sh
Error deserializing /media/user/USB/file.txt
Error deserializing C:\Windows\System32\drivers\etc\hosts
Error deserializing /root/.ssh/id_rsa
Error deserializing /mnt/share/folder/file.docx
Error deserializing C:\Program Files\app\file.exe
Error deserializing /Library/Preferences/com.apple.plist
Error deserializing /proc/cpuinfo
Unknown argument -v
Unknown argument --help
Unknown argument /f
Unknown argument 42
Unknown argument foo
Unknown argument -name
Unknown argument --version
Unknown argument /c
Unknown argument 3.14
Unknown argument bar
Unknown argument -h
Unknown argument --verbose
Unknown argument /d
Unknown argument 0
Unknown argument baz
ChineseUtils.normalize warning: unmatched high surrogate character U+ D800 in "Hello \uD800 World"
ChineseUtils.normalize warning: unmatched high surrogate character U+ D801 in "\uD801\uDC00\uD801\uDC01\uD801"
ChineseUtils.normalize warning: unmatched high surrogate character U+ D803 in "\uD803\uDC00\uD803\uDC01\uD803\uDC02\uD803"
ChineseUtils.normalize warning: unmatched high surrogate character U+ D805 in "\uD805\uDC00\uD805\uDC01\uD805\uDC02\uD805"
ChineseUtils.normalize warning: unmatched high surrogate character U+ D807 in "\uD807\uDC00\uD807\uDC01\uD807\uDC02\uD807"
ChineseUtils.normalize warning: unmatched high surrogate character U+ D809 in "\uD809\uDC00\uD809\uDC01\uD809\uDC02\uD809"
ChineseUtils.normalize warning: unmatched high surrogate character U+ D80B in "\uD80B\uDC00\uD80B\uDC01\uD80B\uDC02\uD80B"
ChineseUtils.normalize warning: unmatched high surrogate character U+ D80D in "\uD80D\uDC00\uD80D\uDC01\uD80D\uDC02\uD80D"
Serializing parsed sentences to /home/user1/sentences.ser ...
Serializing parsed sentences to C:\Users\user2\Documents\sentences.ser ...
Serializing parsed sentences to /tmp/sentences.ser ...
Serializing parsed sentences to /var/log/sentences.ser ...
Serializing parsed sentences to /Users/user3/Desktop/sentences.ser ...
Serializing parsed sentences to D:\sentences.ser ...
Serializing parsed sentences to /opt/sentences.ser ...
Serializing parsed sentences to /dev/null ...
Serializing parsed sentences to /mnt/sentences.ser ...
Serializing parsed sentences to E:\sentences.ser ...
Serializing parsed sentences to /home/user4/.sentences.ser ...
Serializing parsed sentences to C:\Windows\Temp\sentences.ser ...
Serializing parsed sentences to /usr/local/sentences.ser ...
Serializing parsed sentences to /root/sentences.ser ...
Serializing parsed sentences to F:\sentences.ser ...
Initialized tokenizer factory: org.apache.lucene.analysis.standard.StandardTokenizerFactory
Initialized tokenizer factory: com.ibm.icu.text.BreakIterator
Initialized tokenizer factory: org.apache.solr.analysis.SolrTokenizerFactory
Initialized tokenizer factory: edu.stanford.nlp.process.PTBTokenizer
Initialized tokenizer factory: org.apache.lucene.analysis.core.WhitespaceTokenizerFactory
Initialized tokenizer factory: org.apache.lucene.analysis.ngram.NGramTokenizerFactory
Initialized tokenizer factory: org.apache.lucene.analysis.cjk.CJKTokenizerFactory
Initialized tokenizer factory: org.apache.lucene.analysis.th.ThaiTokenizerFactory
Initialized tokenizer factory: org.apache.lucene.analysis.ar.ArabicTokenizerFactory
Initialized tokenizer factory: org.apache.lucene.analysis.hi.HindiTokenizerFactory
Initialized tokenizer factory: org.apache.lucene.analysis.tr.TurkishTokenizerFactory
Initialized tokenizer factory: org.apache.lucene.analysis.ru.RussianTokenizerFactory
Initialized tokenizer factory: org.apache.lucene.analysis.de.GermanTokenizerFactory
Initialized tokenizer factory: org.apache.lucene.analysis.fr.FrenchTokenizerFactory
Initialized tokenizer factory: org.apache.lucene.analysis.es.SpanishTokenizerFactory
Line 12 of config.txt has leading/trailing whitespace: |  port = 8080 |UTF-8
Line 3 of main.py has leading/trailing whitespace: | import os |UTF-8
Line 7 of data.csv has leading/trailing whitespace: | 123,456,789 |UTF-8
Line 15 of index.html has leading/trailing whitespace: | <title>My Website</title> |UTF-8
Line 9 of style.css has leading/trailing whitespace: | font-size: 16px; |UTF-8
Line 4 of script.js has leading/trailing whitespace: | var x = 10; |UTF-8
Line 6 of README.md has leading/trailing whitespace: | This is a sample project. |UTF-8
Line 10 of log.txt has leading/trailing whitespace: | [INFO] Server started. |UTF-8
Line 8 of test.java has leading/trailing whitespace: | public static void main(String[] args) { |UTF-8
Line 11 of report.docx has leading/trailing whitespace: | Introduction |UTF-8
Line 5 of poem.txt has leading/trailing whitespace: | And miles to go before I sleep. |UTF-8
Line 13 of song.mp3 has leading/trailing whitespace: | I'm a believer, I couldn't leave her if I tried. |UTF-8
Line 14 of image.jpg has leading/trailing whitespace: | #FF0000 #00FF00 #0000FF |UTF-8
Line 2 of puzzle.txt has leading/trailing whitespace: | A B C D E F G H I J K L M N O P Q R S T U V W X Y Z |UTF-8
Line 1 of code.c has leading/trailing whitespace: | #include <stdio.h> |UTF-8
Output format: JSON
Output format: XML
Output format: CSV
Output format: HTML
Output format: PDF
Output format: TXT
Output format: PNG
Output format: JPEG
Output format: MP3
Output format: MP4
Output format: ZIP
Output format: DOCX
Output format: XLSX
Output format: PPTX
Output format: GIF
Known nonterms are: [S, A, B, C]
Known nonterms are: [E, F, G, H, I]
Known nonterms are: [X, Y, Z]
Known nonterms are: [P, Q, R, S, T]
Known nonterms are: [M, N, O]
Known nonterms are: [U, V, W]
Known nonterms are: [D, E, F]
Known nonterms are: [L, M, N, O, P]
Known nonterms are: [A, B, C, D]
Known nonterms are: [Q, R, S]
Known nonterms are: [G, H, I]
Known nonterms are: [W, X, Y, Z]
Known nonterms are: [J, K, L]
Known nonterms are: [T, U, V]
Known nonterms are: [K, L, M]
Filtered tree because of unary rule: S
Filtered tree because of unary rule: NP
Filtered tree because of unary rule: VP
Filtered tree because of unary rule: PP
Filtered tree because of unary rule: ADJP
Filtered tree because of unary rule: ADVP
Filtered tree because of unary rule: SBAR
Filtered tree because of unary rule: QP
Filtered tree because of unary rule: NAC
Filtered tree because of unary rule: WHNP
Filtered tree because of unary rule: WHPP
Filtered tree because of unary rule: WHADVP
Filtered tree because of unary rule: WHADJP
Filtered tree because of unary rule: PRN
Filtered tree because of unary rule: FRAG
Model loaded with 3 unary and 5 binary
Model loaded with 6 unary and 2 binary
Model loaded with 4 unary and 4 binary
Model loaded with 2 unary and 6 binary
Model loaded with 5 unary and 3 binary
Model loaded with 7 unary and 1 binary
Model loaded with 1 unary and 7 binary
Model loaded with 8 unary and 0 binary
Model loaded with 0 unary and 8 binary
Model loaded with 9 unary and 1 binary
Model loaded with 1 unary and 9 binary
Model loaded with 10 unary and 0 binary
Model loaded with 0 unary and 10 binary
Model loaded with 11 unary and 1 binary
Model loaded with 1 unary and 11 binary
number is 42
number is -3
number is 0.5
number is 100
number is -12.34
number is 7
number is 3.14159
number is -0.001
number is 9999
number is 1.23e4
number is -5.67e-2
number is 2/3
number is -4/5
number is sqrt(2)
number is -sqrt(3)
The reader log level is set to DEBUG
The reader log level is set to INFO
The reader log level is set to WARN
The reader log level is set to ERROR
The reader log level is set to FATAL
The reader log level is set to TRACE
The reader log level is set to OFF
The reader log level is set to ALL
The reader log level is set to VERBOSE
The reader log level is set to NOTICE
The reader log level is set to CRITICAL
The reader log level is set to ALERT
The reader log level is set to EMERGENCY
The reader log level is set to CUSTOM
The reader log level is set to NONE
NO corefcluster id 0
NO corefcluster id 1
NO corefcluster id 2
NO corefcluster id 3
NO corefcluster id 4
NO corefcluster id 5
NO corefcluster id 6
NO corefcluster id 7
NO corefcluster id 8
NO corefcluster id 9
NO corefcluster id 10
NO corefcluster id 11
NO corefcluster id 12
NO corefcluster id 13
NO corefcluster id 14
Max non-zero y values for an x: 5
Max non-zero y values for an x: 3
Max non-zero y values for an x: 4
Max non-zero y values for an x: 2
Max non-zero y values for an x: 6
Max non-zero y values for an x: 7
Max non-zero y values for an x: 8
Max non-zero y values for an x: 9
Max non-zero y values for an x: 10
Max non-zero y values for an x: 1
Max non-zero y values for an x: 11
Max non-zero y values for an x: 12
Max non-zero y values for an x: 13
Max non-zero y values for an x: 14
Max non-zero y values for an x: 15
Result: Success
Result: Failure
Result: 42
Result: Hello world
Result: 3.14
Result: True
Result: False
Result: [1, 2, 3]
Result: "foo"
Result: null
Result: NaN
Result: undefined
Result: Error
Result: OK
Result: Cancelled
Done. Serialized 100 sentences.
Done. Serialized 50 sentences.
Done. Serialized 150 sentences.
Done. Serialized 25 sentences.
Done. Serialized 75 sentences.
Done. Serialized 125 sentences.
Done. Serialized 10 sentences.
Done. Serialized 60 sentences.
Done. Serialized 110 sentences.
Done. Serialized 30 sentences.
Done. Serialized 80 sentences.
Done. Serialized 140 sentences.
Done. Serialized 20 sentences.
Done. Serialized 70 sentences.
Done. Serialized 130 sentences.
numDatums: 100
numDatums: 50
numDatums: 150
numDatums: 25
numDatums: 75
numDatums: 125
numDatums: 10
numDatums: 60
numDatums: 110
numDatums: 30
numDatums: 80
numDatums: 140
numDatums: 20
numDatums: 70
numDatums: 130
DBG final size of the patterns is 12
DBG final size of the patterns is 15
DBG final size of the patterns is 10
DBG final size of the patterns is 14
DBG final size of the patterns is 11
DBG final size of the patterns is 13
DBG final size of the patterns is 16
DBG final size of the patterns is 9
DBG final size of the patterns is 17
DBG final size of the patterns is 8
DBG final size of the patterns is 18
DBG final size of the patterns is 7
DBG final size of the patterns is 19
DBG final size of the patterns is 6
DBG final size of the patterns is 20
trees.txt : a file of trees to process
input.csv : a file of trees to process
data.json : a file of trees to process
tree_list.xlsx : a file of trees to process
forest.dat : a file of trees to process
output.xml : a file of trees to process
tree_data.pkl : a file of trees to process
input_trees.hdf5 : a file of trees to process
tree_info.tsv : a file of trees to process
tree_file.zip : a file of trees to process
tree_data.mat : a file of trees to process
tree_list.rds : a file of trees to process
tree_file.tar.gz : a file of trees to process
tree_data.npz : a file of trees to process
NO TAGGINGS: apple NN
NO TAGGINGS: quickly RB
NO TAGGINGS: beautiful JJ
NO TAGGINGS: ran VBD
NO TAGGINGS: the DT
NO TAGGINGS: happy JJ
NO TAGGINGS: dog NN
NO TAGGINGS: slowly RB
NO TAGGINGS: ugly JJ
NO TAGGINGS: walked VBD
NO TAGGINGS: a DT
NO TAGGINGS: sad JJ
NO TAGGINGS: cat NN
NO TAGGINGS: fast RB
NO TAGGINGS: pretty JJ
NORM: Hello world! UTF-8
NORM: Bonjour le monde! UTF-8
NORM: Hallo Welt! UTF-8
NORM: Ciao mondo! UTF-8
NORM: Salut lume! UTF-8
Warning: pattern com.example.Main matched nothing
Warning: pattern org.apache.commons.lang3.StringUtils matched nothing
Warning: pattern java.util.Scanner matched nothing
Warning: pattern javax.swing.JFrame matched nothing
Warning: pattern edu.stanford.nlp.tagger.maxent.MaxentTagger matched nothing
Warning: pattern org.junit.Test matched nothing
Warning: pattern android.app.Activity matched nothing
Warning: pattern org.springframework.boot.SpringApplication matched nothing
Warning: pattern scala.collection.immutable.List matched nothing
Warning: pattern spark.Spark matched nothing
Warning: pattern org.apache.hadoop.mapreduce.Mapper matched nothing
Warning: pattern org.apache.spark.sql.SparkSession matched nothing
Warning: pattern kotlin.io.ConsoleKt matched nothing
Warning: pattern groovy.lang.GroovyShell matched nothing
Warning: pattern clojure.lang.RT matched nothing
Datum: 2023-04-05
Datum: 3.14159
Datum: Hello World
Datum: true
Datum: [1, 2, 3]
Datum: {"name": "Alice", "age": 25}
Datum: null
Datum: 42
Datum: foo
Datum: 1.23e+10
Datum: (a + b) * c
Datum: <html><head><title>Test</title></head><body><h1>Test</h1></body></html>
Datum: \u0048\u0065\u006C\u006C\u006F
Datum: 0b101010
Datum: 0xABCDEF
directed path nodes btw 1 and 5 is [2, 3, 4]
directed path nodes btw 1 and 5 is [2, 6, 7, 4]
directed path nodes btw 1 and 5 is [3, 4]
directed path nodes btw 1 and 5 is [3, 6, 7, 4]
directed path nodes btw 1 and 5 is [2, 4]
directed path nodes btw 1 and 5 is [3, 7, 4]
directed path nodes btw 1 and 5 is [2, 6, 4]
directed path nodes btw 1 and 5 is [2, 7, 4]
directed path nodes btw 1 and 5 is [3, 8, 9, 4]
directed path nodes btw 1 and 5 is [2, 8, 9, 4]
directed path nodes btw 1 and 5 is [3, 9, 4]
directed path nodes btw 1 and 5 is [2, 9, 4]
directed path nodes btw 1 and 5 is [3, 8, 4]
directed path nodes btw 1 and 5 is [2, 8, 4]
directed path nodes btw 1 and 5 is [2,3,6,7,4]
Document Title: How to Write a Good Essay
Document Title: Introduction to Machine Learning
Document Title: The History of Ancient Rome
Document Title: A Guide to Healthy Eating
Document Title: Harry Potter and the Philosopher's Stone
Document Title: The Art of War by Sun Tzu
Document Title: The Lord of the Rings: The Fellowship of the Ring
Document Title: Principles of Economics
Document Title: The Catcher in the Rye
Document Title: Pride and Prejudice
Document Title: The Adventures of Sherlock Holmes
Document Title: The Da Vinci Code
Document Title: To Kill a Mockingbird
Document Title: The Hunger Games
Document Title: The Hitchhiker's Guide to the Galaxy
batch cost: 0.56
batch cost: 1.23
batch cost: 0.87
batch cost: 1.45
batch cost: 0.32
batch cost: 1.67
batch cost: 0.98
batch cost: 1.12
batch cost: 0.74
batch cost: 1.39
batch cost: 0.64
batch cost: 1.54
batch cost: 0.91
batch cost: 1.28
batch cost: 0.49
Mention Tag: PERSON Speaker Tag: PERSON
Mention Tag: PRONOMINAL Speaker Tag: PRONOMINAL
Mention Tag: ANIMATE Speaker Tag: ANIMATE
Mention Tag: INANIMATE Speaker Tag: INANIMATE
Mention Tag: UNKNOWN Speaker Tag: UNKNOWN
Mention Tag: PERSON Speaker Tag: PRONOMINAL
Mention Tag: PRONOMINAL Speaker Tag: PERSON
Mention Tag: ANIMATE Speaker Tag: INANIMATE
Mention Tag: INANIMATE Speaker Tag: ANIMATE
Mention Tag: UNKNOWN Speaker Tag: PERSON
Mention Tag: PERSON Speaker Tag: UNKNOWN
Mention Tag: PRONOMINAL Speaker Tag: ANIMATE
Mention Tag: ANIMATE Speaker Tag: PRONOMINAL
Mention Tag: INANIMATE Speaker Tag: UNKNOWN
Mention Tag: UNKNOWN Speaker Tag: INANIMATE
First character is: a
First character is: B
First character is: 3
First character is: $
First character is: *
First character is: z
First character is: Q
First character is: 9
First character is: +
First character is: _
First character is: g
First character is: R
First character is: 0
First character is: #
First character is: ?
ySize is 10
ySize is 15.6
ySize is 8.9
ySize is 12.3
ySize is 9.7
ySize is 14.2
ySize is 11.4
ySize is 13.8
ySize is 7.5
ySize is 16.1
ySize is 10.8
ySize is 9.2
ySize is 12.7
ySize is 8.3
ySize is 14.9
total feats 12
total feats 9
total feats 15
total feats 11
total feats 10
total feats 13
total feats 8
total feats 14
total feats 7
total feats 16
total feats 6
total feats 17
total feats 5
total feats 18
total feats 4
in isWH, decision: true for node 1
in isWH, decision: false for node 2
in isWH, decision: true for node 3
in isWH, decision: false for node 4
in isWH, decision: true for node 5
in isWH, decision: false for node 6
in isWH, decision: true for node 7
in isWH, decision: false for node 8
in isWH, decision: true for node 9
in isWH, decision: false for node 10
in isWH, decision: true for node 11
in isWH, decision: false for node 12
in isWH, decision: true for node 13
in isWH, decision: false for node 14
in isWH, decision: true for node 15
Equal? false
Equal? true
Equal? false
Equal? false
Equal? true
Equal? false
Equal? true
Equal? true
Equal? false
Equal? false
Equal? true
Equal? false
Equal? true
Equal? false
Equal? true
Adding folder entry images
Adding folder entry documents
Adding folder entry music
Adding folder entry videos
Adding folder entry downloads
Adding folder entry backup
Adding folder entry projects
Adding folder entry games
Adding folder entry homework
Adding folder entry photos
Adding folder entry ebooks
Adding folder entry podcasts
Adding folder entry recipes
Adding folder entry invoices
Adding folder entry fonts
Child is book (4) via obj
Child is happy (3) via amod
Child is to (5) via mark
Child is she (2) via nsubj
Child is very (4) via advmod
Child is him (6) via iobj
Child is red (3) via det
Child is and (4) via cc
Child is quickly (5) via advmod
Child is a (2) via det
Child is likes (3) via acl
Child is the (2) via case
Child is of (4) via case
Child is running (5) via xcomp
Tagging CD: hello
Tagging CD: world
Tagging CD: music
Tagging CD: python
Tagging CD: bing
Tagging CD: apple
Tagging CD: java
Tagging CD: linux
Tagging CD: pizza
Tagging CD: coffee
Tagging CD: book
Tagging CD: movie
Tagging CD: game
Tagging CD: art
Tagging CD: math
Tsurgeon: relabel NP VP
Tsurgeon: delete foo
Tsurgeon: move bar $- baz
Tsurgeon: replace qux quux
Tsurgeon: insert (DT a) >0 foobar
Tsurgeon: coindex foo bar
Tsurgeon: adjoinF (S (NP (PRP he)) (VP (VBD said))) < VP=foo
Tsurgeon: prune bar
Tsurgeon: excise foo foo
Tsurgeon: createSubtree FOO < NP=bar
Tsurgeon: flatten foo
Tsurgeon: adjoin (PP (IN with) (NP (NN cheese))) >1 NP=foo
Tsurgeon: moveTree foo >0 bar
Tsurgeon: adjoinH (ADJP (JJ big)) < NP=foo
Tsurgeon: coindexNew foo bar
illegal access NullPointerException
illegal access ArrayIndexOutOfBoundsException
illegal access FileNotFoundException
illegal access SecurityException
illegal access ClassNotFoundException
illegal access IOException
illegal access ArithmeticException
illegal access IllegalArgumentException
illegal access NoSuchMethodException
illegal access SQLException
illegal access NumberFormatException
illegal access InterruptedException
illegal access MalformedURLException
illegal access OutOfMemoryError
illegal access StackOverflowError
Learning rate: 0.01
Learning rate: 0.05
Learning rate: 0.1
Learning rate: 0.02
Learning rate: 0.03
Learning rate: 0.04
Learning rate: 0.06
Learning rate: 0.07
Learning rate: 0.08
Learning rate: 0.09
Learning rate: 0.012
Learning rate: 0.015
Learning rate: 0.018
Learning rate: 0.025
Learning rate: 0.035
name : Cluster does not contain mention: 456
date : Cluster does not contain mention: 2023-10-24
type : Cluster does not contain mention: error
id : Cluster does not contain mention: 7890
status : Cluster does not contain mention: active
source : Cluster does not contain mention: web
user : Cluster does not contain mention: Alice
time : Cluster does not contain mention: 07:46
level : Cluster does not contain mention: warning
code : Cluster does not contain mention: 404
message : Cluster does not contain mention: Not Found
location : Cluster does not contain mention: US
category : Cluster does not contain mention: network
priority : Cluster does not contain mention: high
action : Cluster does not contain mention: retry
These sentences will be serialized to /home/user/documents/sentences.txt
These sentences will be serialized to C:\Users\user\Desktop\sentences.dat
These sentences will be serialized to /tmp/sentences.ser
These sentences will be serialized to /var/log/sentences.log
These sentences will be serialized to /opt/sentences/sentences.bin
These sentences will be serialized to /Users/user/Documents/sentences.csv
These sentences will be serialized to D:\sentences\sentence.json
These sentences will be serialized to /dev/null
These sentences will be serialized to /mnt/sentences/sentences.xml
These sentences will be serialized to /usr/local/sentences/sentences.pkl
These sentences will be serialized to E:\sentences\sentence.yaml
These sentences will be serialized to /root/sentences/sentences.md
These sentences will be serialized to /etc/sentences/sentences.conf
These sentences will be serialized to F:\sentences\sentence.ini
These sentences will be serialized to /media/sentences/sentences.html
outputLayerWeights4Edge deriv( 0 , 1 ) = 0.23 - 0.18 = 0.05
outputLayerWeights4Edge deriv( 1 , 2 ) = 0.17 - 0.15 = 0.02
outputLayerWeights4Edge deriv( 2 , 3 ) = 0.21 - 0.19 = 0.02
outputLayerWeights4Edge deriv( 3 , 4 ) = 0.25 - 0.22 = 0.03
outputLayerWeights4Edge deriv( 4 , 5 ) = 0.19 - 0.16 = 0.03
outputLayerWeights4Edge deriv( 5 , 6 ) = 0.22 - 0.20 = 0.02
outputLayerWeights4Edge deriv( 6 , 7 ) = 0.24 - 0.21 = 0.03
outputLayerWeights4Edge deriv( 7 , 8 ) = 0.18 - 0.17 = 0.01
outputLayerWeights4Edge deriv( 8 , 9 ) = 0.20 - 0.18 = 0.02
outputLayerWeights4Edge deriv( 9 , 10 ) = 0.23 - 0.20 = 0.03
outputLayerWeights4Edge deriv( 10 , 11 ) = 0.26 - 0.24 = 0.02
outputLayerWeights4Edge deriv( 11 , 12 ) = 0.28 - 0.25 = 0.03
outputLayerWeights4Edge deriv( 12 , 13 ) = 0.27 - 0.23 = 0.04
outputLayerWeights4Edge deriv( 13 , 14 ) = 0.29 - 0.26 = 0.03
empirical 0.56 expected 0.58
empirical 0.67 expected 0.65
empirical 0.49 expected 0.51
empirical 0.72 expected 0.74
empirical 0.61 expected 0.59
empirical 0.53 expected 0.55
empirical 0.69 expected 0.71
empirical 0.46 expected 0.48
empirical 0.75 expected 0.77
empirical 0.64 expected 0.62
empirical 0.51 expected 0.53
empirical 0.66 expected 0.68
empirical 0.48 expected 0.50
empirical 0.73 expected 0.75
empirical 0.62 expected 0.60
contingency name found 7A3F
contingency name found 9C2B
contingency name found 4D1E
contingency name found 6B4C
contingency name found 8E5A
contingency name found 3F6D
contingency name found 5C8E
contingency name found 1D7B
contingency name found 2A9F
contingency name found B3C4
contingency name found E5D6
contingency name found F7A8
contingency name found C9B1
contingency name found D4E2
contingency name found A6F3
Illegal access java.lang.SecurityException: Permission denied
Illegal access java.lang.IllegalAccessException: Class com.example.Main can not access a member of class com.example.Private with modifiers "private"
Illegal access java.lang.reflect.InvocationTargetException: Method threw 'java.lang.IllegalArgumentException' exception.
Illegal access java.io.FileNotFoundException: /data/data/com.example/files/config.txt (No such file or directory)
Illegal access java.net.SocketException: Connection reset by peer
Illegal access javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No subject alternative names present
Illegal access android.content.ActivityNotFoundException: No Activity found to handle Intent { act=android.intent.action.VIEW dat=content://com.example.provider/images/1 }
Illegal access org.json.JSONException: No value for name
Illegal access android.database.sqlite.SQLiteException: no such table: users (code 1 SQLITE_ERROR)
Illegal access java.lang.NullPointerException: Attempt to invoke virtual method 'int java.lang.String.length()' on a null object reference
Illegal access java.lang.ClassNotFoundException: com.example.MyClass
Illegal access java.lang.OutOfMemoryError
Illegal access java.util.concurrent.TimeoutException
Illegal access android.os.NetworkOnMainThreadException
Illegal access java.lang.ArithmeticException: / by zero
Initializing JollyDayHoliday for SUTime from CLASSPATH jollyday/Holidays_sutime.xml as sutime.holidays
Initializing JollyDayHoliday for SUTime from FILE C:/Users/John/Documents/holidays.xml as holidays
Initializing JollyDayHoliday for SUTime from URL https://example.com/sutime/holidays.xml as sutime
Initializing JollyDayHoliday for SUTime from RESOURCE org/stanford/nlp/time/jollyday/Holidays_sutime.xml as nlp.time.holidays
Initializing JollyDayHoliday for SUTime from STREAM java.io.ByteArrayInputStream@12345678 as stream.holidays
Initializing JollyDayHoliday for SUTime from CLASSPATH custom/Holidays_custom.xml as custom.holidays
Initializing JollyDayHoliday for SUTime from FILE D:/Projects/SUTime/holidays.xml as sutime.projects
Initializing JollyDayHoliday for SUTime from URL https://sutime.org/holidays.xml as org.holidays
Initializing JollyDayHoliday for SUTime from RESOURCE com/example/sutime/Holidays_example.xml as example.holidays
Initializing JollyDayHoliday for SUTime from STREAM java.io.ByteArrayInputStream@87654321 as stream.example
Initializing JollyDayHoliday for SUTime from CLASSPATH default/Holidays_default.xml as default.holidays
Initializing JollyDayHoliday for SUTime from FILE E:/Data/SUTime/holidays.xml as sutime.data
Initializing JollyDayHoliday for SUTime from URL https://sutime.com/holidays.xml as com.holidays
Initializing JollyDayHoliday for SUTime from RESOURCE net/sutime/Holidays_net.xml as net.holidays
Responding unauthorized to /192.168.1.101:8080
Responding unauthorized to /10.0.0.5:443
Responding unauthorized to /172.16.0.23:80
Responding unauthorized to /127.0.0.1:8000
Responding unauthorized to /192.168.2.34:3000
Responding unauthorized to /10.0.1.12:8443
Responding unauthorized to /172.16.1.45:8081
Responding unauthorized to /127.0.0.2:9000
Responding unauthorized to /192.168.3.67:5000
Responding unauthorized to /10.0.2.78:8888
Responding unauthorized to /172.16.2.90:8008
Responding unauthorized to /127.0.0.3:7000
Responding unauthorized to /192.168.4.123:4000
Responding unauthorized to /10.0.3.156:8088
Responding unauthorized to /172.16.3.189:9090
Iteration 1 : [name, age, gender]
Iteration 2 : [id, score, grade]
Iteration 3 : [city, state, country]
Iteration 4 : [title, author, genre]
Iteration 5 : [product, price, quantity]
Iteration 6 : [color, shape, size]
Iteration 7 : [date, time, event]
Iteration 8 : [email, password, username]
Iteration 9 : [phone, address, contact]
Iteration 10 : [animal, food, habitat]
Iteration 11 : [movie, director, rating]
Iteration 12 : [sport, team, player]
Iteration 13 : [book, publisher, year]
Iteration 14 : [song, artist, album]
Iteration 15 : [car, model, year]
input is hello
input is 42
input is [1, 2, 3]
input is true
input is null
input is "Bing"
input is 3.14
input is {name: "Alice", age: 25}
input is -1
input is []
input is false
input is "log"
input is 0
input is {x: 10, y: 20}
input is goodbye
DBG Adding TokensRegexAnnotator annotation...
DBG Adding TokensRegexAnnotator annotation...
DBG Adding TokensRegexAnnotator annotation...
DBG Adding TokensRegexAnnotator annotation...
DBG Adding TokensRegexAnnotator annotation...
DBG Adding TokensRegexAnnotator annotation...
DBG Adding TokensRegexAnnotator annotation...
DBG Adding TokensRegexAnnotator annotation...
DBG Adding TokensRegexAnnotator annotation...
DBG Adding TokensRegexAnnotator annotation...
DBG Adding TokensRegexAnnotator annotation...
DBG Adding TokensRegexAnnotator annotation...
DBG Adding TokensRegexAnnotator annotation...
DBG Adding TokensRegexAnnotator annotation...
DBG Adding TokensRegexAnnotator annotation...
differing NER tags detected in entity: John
differing NER tags detected in entity: 42
differing NER tags detected in entity: New York
differing NER tags detected in entity: $100
differing NER tags detected in entity: October 24
differing NER tags detected in entity: Apple
differing NER tags detected in entity: China
differing NER tags detected in entity: Harry Potter
differing NER tags detected in entity: 3.14
differing NER tags detected in entity: COVID-19
differing NER tags detected in entity: Tesla
differing NER tags detected in entity: Monday
differing NER tags detected in entity: Bing
differing NER tags detected in entity: 2023
differing NER tags detected in entity: Paris
Failed to process timex "today" with attributes "type=date,value=2023-10-24"
Failed to process timex "next week" with attributes "type=duration,value=P1W"
Failed to process timex "last month" with attributes "type=date,value=2023-09"
Failed to process timex "5pm" with attributes "type=time,value=T17"
Failed to process timex "tomorrow morning" with attributes "type=datetime,value=2023-10-25T08"
Failed to process timex "every Friday" with attributes "type=set,value=P1W,anchor=TGIF"
Failed to process timex "two hours ago" with attributes "type=datetime,value=2023-10-24T16:30:41,mod=before"
Failed to process timex "Christmas" with attributes "type=date,value=2023-12-25"
Failed to process timex "in 10 minutes" with attributes "type=datetime,value=2023-10-24T18:40:41,mod=after"
Failed to process timex "the third quarter of 2023" with attributes "type=date,value=2023-Q3"
Failed to process timex "noon" with attributes "type=time,value=T12"
Failed to process timex "the day after tomorrow" with attributes "type=date,value=2023-10-26"
Failed to process timex "three days later" with attributes "type=datetime,value=2023-10-27T18:30:41,mod=after"
Failed to process timex "the end of this year" with attributes "type=date,value=2023-12-31"
Failed to process timex "the beginning of next month" with attributes "type=date,value=2023-11-01"
undirected edges btw 2 and 4 is 3
undirected edges btw 2 and 4 is 0
undirected edges btw 2 and 4 is 1
undirected edges btw 2 and 4 is 2
undirected edges btw 2 and 4 is 4
undirected edges btw 2 and 4 is 5
undirected edges btw 2 and 4 is 6
undirected edges btw 2 and 4 is 7
undirected edges btw 2 and 4 is 8
undirected edges btw 2 and 4 is 9
undirected edges btw 2 and 4 is -1
undirected edges btw 2 and 4 is -2
undirected edges btw 2 and 4 is -3
undirected edges btw 2 and 4 is -4
undirected edges btw 2 and 4 is -5
F1 (macro average): 0.765%
F1 (macro average): 0.921%
F1 (macro average): 0.847%
F1 (macro average): 0.689%
F1 (macro average): 0.794%
F1 (macro average): 0.912%
F1 (macro average): 0.731%
F1 (macro average): 0.876%
F1 (macro average): 0.803%
F1 (macro average): 0.948%
F1 (macro average): 0.716%
F1 (macro average): 0.891%
F1 (macro average): 0.834%
F1 (macro average): 0.702%
F1 (macro average): 0.779%
PCFG only: 0.456
PCFG only: 0.987
PCFG only: 0.123
PCFG only: 0.789
PCFG only: 0.654
PCFG only: 0.321
PCFG only: 0.876
PCFG only: 0.432
PCFG only: 0.765
PCFG only: 0.543
PCFG only: 0.210
PCFG only: 0.951
PCFG only: 0.294
PCFG only: 0.837
PCFG only: 0.628
Matched 12 chinese year vectors
Matched 5 chinese year vectors
Matched 9 chinese year vectors
Matched 7 chinese year vectors
Matched 10 chinese year vectors
Matched 8 chinese year vectors
Matched 6 chinese year vectors
Matched 11 chinese year vectors
Matched 4 chinese year vectors
Matched 3 chinese year vectors
Matched 2 chinese year vectors
Matched 15 chinese year vectors
Matched 14 chinese year vectors
Matched 13 chinese year vectors
Loaded serialized sentences from /home/user/data/sentences.ser ...
Loaded serialized sentences from /var/tmp/sentences_20211025.ser ...
Loaded serialized sentences from /opt/app/sentences.ser ...
Loaded serialized sentences from C:\Users\user\Documents\sentence_data.ser ...
Loaded serialized sentences from /mnt/usb/sentences_backup.ser ...
Loaded serialized sentences from /Users/user/Desktop/sentences_test.ser ...
Loaded serialized sentences from /data/sentences.ser ...
Loaded serialized sentences from /dev/shm/sentences_cache.ser ...
Loaded serialized sentences from /tmp/sentences_20211025_104431.ser ...
Loaded serialized sentences from /home/user/.sentences/sentences.ser ...
Loaded serialized sentences from /usr/local/share/sentences.ser ...
Loaded serialized sentences from D:\sentences_data\sentence_data.ser ...
Loaded serialized sentences from /media/cdrom/sentences.iso ...
Loaded serialized sentences from /root/sentences.ser ...
Loaded serialized sentences from /etc/sentences.conf ...
normalizedMoneyString: Normalizing $12.34
normalizedMoneyString: Normalizing 100.00
normalizedMoneyString: Normalizing -12.34
normalizedMoneyString: Normalizing $0.00
normalizedMoneyString: Normalizing 1,234.56
normalizedMoneyString: Normalizing 12,34.56
normalizedMoneyString: Normalizing $12,345.67
Relation extraction results AFTER consistency checks for partition #0 using printer org.apache.log4j.ConsoleAppender : Success
Relation extraction results AFTER consistency checks for partition #1 using printer org.apache.log4j.FileAppender : Error: File not found
Relation extraction results AFTER consistency checks for partition #2 using printer org.apache.log4j.DailyRollingFileAppender : Warning: File size exceeded
Relation extraction results AFTER consistency checks for partition #3 using printer org.apache.log4j.AsyncAppender : Success
Relation extraction results AFTER consistency checks for partition #4 using printer org.apache.log4j.SMTPAppender : Error: SMTP server not reachable
Relation extraction results AFTER consistency checks for partition #5 using printer org.apache.log4j.JMSAppender : Success
Relation extraction results AFTER consistency checks for partition #6 using printer org.apache.log4j.SocketAppender : Warning: Socket connection timeout
Relation extraction results AFTER consistency checks for partition #7 using printer org.apache.log4j.WriterAppender : Success
Relation extraction results AFTER consistency checks for partition #8 using printer org.apache.log4j.SyslogAppender : Error: Syslog daemon not running
Relation extraction results AFTER consistency checks for partition #9 using printer org.apache.log4j.NTEventLogAppender : Warning: Event log full
DBG The max weight of candidate patterns is 0.75 so not adding anymore patterns
DBG The max weight of candidate patterns is 0.64 so not adding anymore patterns
DBG The max weight of candidate patterns is 0.82 so not adding anymore patterns
DBG The max weight of candidate patterns is 0.69 so not adding anymore patterns
DBG The max weight of candidate patterns is 0.77 so not adding anymore patterns
DBG The max weight of candidate patterns is 0.71 so not adding anymore patterns
DBG The max weight of candidate patterns is 0.66 so not adding anymore patterns
DBG The max weight of candidate patterns is 0.79 so not adding anymore patterns
DBG The max weight of candidate patterns is 0.68 so not adding anymore patterns
DBG The max weight of candidate patterns is 0.73 so not adding anymore patterns
DBG The max weight of candidate patterns is 0.76 so not adding anymore patterns
DBG The max weight of candidate patterns is 0.67 so not adding anymore patterns
DBG The max weight of candidate patterns is 0.74 so not adding anymore patterns
DBG The max weight of candidate patterns is 0.70 so not adding anymore patterns
DBG The max weight of candidate patterns is 0.72 so not adding anymore patterns
Speaker: John Predicted: Mary POSITIVE
Speaker: Alice Predicted: Alice TRUE_POSITIVE
Speaker: Bob Predicted: Bob TRUE_POSITIVE
Speaker: David Predicted: Lisa NEGATIVE
Speaker: Emma Predicted: Emma TRUE_POSITIVE
Speaker: Frank Predicted: Frank TRUE_POSITIVE
Speaker: Grace Predicted: Henry NEGATIVE
Speaker: Ian Predicted: Ian TRUE_POSITIVE
Speaker: Julia Predicted: Kevin NEGATIVE
Speaker: Leo Predicted: Leo TRUE_POSITIVE
Speaker: Mia Predicted: Noah NEGATIVE
Speaker: Oscar Predicted: Oscar TRUE_POSITIVE
Speaker: Ruby Predicted: Paul NEGATIVE
Speaker: Sam Predicted: Sam TRUE_POSITIVE
Speaker: Tina Predicted: Tina TRUE_POSITIVE
Using relation extraction post processor: edu.stanford.nlp.ie.machinereading.MachineReadingProperties$DefaultRelationExtractorPostProcessor
Using relation extraction post processor: edu.stanford.nlp.ie.machinereading.MachineReadingProperties$NoOpRelationExtractorPostProcessor
Using relation extraction post processor: edu.stanford.nlp.ie.machinereading.MachineReadingProperties$CorefRelationExtractorPostProcessor
Using relation extraction post processor: edu.stanford.nlp.ie.machinereading.MachineReadingProperties$EntityMentionRelationExtractorPostProcessor
Using relation extraction post processor: edu.stanford.nlp.ie.machinereading.MachineReadingProperties$SemanticGraphRelationExtractorPostProcessor
Using relation extraction post processor: edu.stanford.nlp.ie.machinereading.MachineReadingProperties$DependencyRelationExtractorPostProcessor
Using relation extraction post processor: edu.stanford.nlp.ie.machinereading.MachineReadingProperties$NERRelationExtractorPostProcessor
Using relation extraction post processor: edu.stanford.nlp.ie.machinereading.MachineReadingProperties$SRLRelationExtractorPostProcessor
Using relation extraction post processor: edu.stanford.nlp.ie.machinereading.MachineReadingProperties$OpenIERelationExtractorPostProcessor
Using relation extraction post processor: edu.stanford.nlp.ie.machinereading.MachineReadingProperties$KBPRelationExtractorPostProcessor
Using relation extraction post processor: edu.stanford.nlp.ie.machinereading.MachineReadingProperties$ReVerbRelationExtractorPostProcessor
Using relation extraction post processor: edu.stanford.nlp.ie.machinereading.MachineReadingProperties$ClueWebRelationExtractorPostProcessor
Using relation extraction post processor: edu.stanford.nlp.ie.machinereading.MachineReadingProperties$WikiRelationExtractorPostProcessor
Using relation extraction post processor: edu.stanford.nlp.ie.machinereading.MachineReadingProperties$BioNLPRelationExtractorPostProcessor
Using relation extraction post processor: edu.stanford.nlp.ie.machinereading.MachineReadingProperties$CustomRelationExtractorPostProcessor
Still waiting... 5 minutes have passed.
Still waiting... 12 minutes have passed.
Still waiting... 3 minutes have passed.
Still waiting... 8 minutes have passed.
Still waiting... 10 minutes have passed.
Still waiting... 7 minutes have passed.
Still waiting... 9 minutes have passed.
Still waiting... 4 minutes have passed.
Still waiting... 6 minutes have passed.
Still waiting... 11 minutes have passed.
Still waiting... 2 minutes have passed.
Still waiting... 13 minutes have passed.
Still waiting... 14 minutes have passed.
Still waiting... 15 minutes have passed.
Relevant variables: [0, 1, 2, 3]
Relevant variables: [4, 5, 6, 7]
Relevant variables: [8, 9, 10, 11]
Relevant variables: [12, 13, 14, 15]
Relevant variables: [16, 17, 18, 19]
Relevant variables: [20, 21, 22, 23]
Relevant variables: [24, 25, 26, 27]
Relevant variables: [28, 29, 30, 31]
Relevant variables: [32, 33, 34, 35]
Relevant variables: [36, 37, 38, 39]
Relevant variables: [40, 41, 42, 43]
Relevant variables: [44, 45, 46, 47]
Relevant variables: [48, 49, 50, 51]
Relevant variables: [52, 53, 54, 55]
Relevant variables: [56, 57, 58, 59]
baseClassifiers index 0
baseClassifiers index 1
baseClassifiers index 2
baseClassifiers index 3
baseClassifiers index 4
baseClassifiers index 5
baseClassifiers index 6
baseClassifiers index 7
baseClassifiers index 8
baseClassifiers index 9
baseClassifiers index 10
baseClassifiers index 11
baseClassifiers index 12
baseClassifiers index 13
baseClassifiers index 14
Patterns, num = 0
Patterns, num = 1
Patterns, num = 2
Patterns, num = 3
Patterns, num = 4
Patterns, num = 5
Patterns, num = 6
Patterns, num = 7
Patterns, num = 8
Patterns, num = 9
Patterns, num = 10
Patterns, num = 11
Patterns, num = 12
Patterns, num = 13
Patterns, num = 14
r (interm): 0.56
r (interm): 0.67
r (interm): 0.48
r (interm): 0.73
r (interm): 0.62
r (interm): 0.51
r (interm): 0.69
r (interm): 0.58
r (interm): 0.64
r (interm): 0.76
r (interm): 0.53
r (interm): 0.71
r (interm): 0.59
r (interm): 0.66
r (interm): 0.74
Starting class: com.example.Main
Starting class: org.apache.commons.lang.StringUtils
Starting class: java.util.ArrayList
Starting class: net.minecraft.server.MinecraftServer
Starting class: javax.swing.JFrame
Starting class: edu.stanford.nlp.parser.lexparser.LexicalizedParser
Starting class: android.app.Activity
Starting class: org.junit.Test
Starting class: org.springframework.boot.SpringApplication
Starting class: kotlin.io.ConsoleKt
Starting class: scala.collection.immutable.List
Starting class: org.tensorflow.Graph
Starting class: javax.xml.parsers.SAXParser
Starting class: org.apache.spark.SparkContext
Starting class: com.google.gson.Gson
Getting data from "Hello world" ( UTF-8 encoding)
Getting data from "Bing" ( ASCII encoding)
Getting data from "Bonjour" ( ISO-8859-1 encoding)
Getting data from "Hola" ( UTF-16 encoding)
Getting data from "Ciao" ( CP1252 encoding)
Getting data from "Hej" ( ISO-8859-15 encoding)
Getting data from "Merhaba" ( UTF-32 encoding)
Getting data from "Sawubona" ( CP850 encoding)
score: 97
score: 82
score: 65
score: 76
score: 88
score: 91
score: 54
score: 73
score: 68
score: 84
score: 79
score: 92
score: 61
score: 86
score: 74
latticeDensity: 0.45 cost: 12
latticeDensity: 0.67 cost: 9
latticeDensity: 0.32 cost: 15
latticeDensity: 0.54 cost: 11
latticeDensity: 0.76 cost: 7
latticeDensity: 0.29 cost: 16
latticeDensity: 0.61 cost: 10
latticeDensity: 0.43 cost: 13
latticeDensity: 0.69 cost: 8
latticeDensity: 0.35 cost: 14
latticeDensity: 0.51 cost: 12
latticeDensity: 0.73 cost: 8
latticeDensity: 0.27 cost: 17
latticeDensity: 0.59 cost: 10
latticeDensity: 0.41 cost: 14
tokens size : 0
tokens size : 1
tokens size : 2
tokens size : 3
tokens size : 4
tokens size : 5
tokens size : 6
tokens size : 7
tokens size : 8
tokens size : 9
tokens size : 10
tokens size : 11
tokens size : 12
tokens size : 13
tokens size : 14
DBG Done labeling provided sents in 0.34 seconds. Total # of tokens labeled: 456
DBG Done labeling provided sents in 0.27 seconds. Total # of tokens labeled: 389
DBG Done labeling provided sents in 0.31 seconds. Total # of tokens labeled: 412
DBG Done labeling provided sents in 0.29 seconds. Total # of tokens labeled: 398
DBG Done labeling provided sents in 0.32 seconds. Total # of tokens labeled: 405
DBG Done labeling provided sents in 0.28 seconds. Total # of tokens labeled: 391
DBG Done labeling provided sents in 0.33 seconds. Total # of tokens labeled: 419
DBG Done labeling provided sents in 0.35 seconds. Total # of tokens labeled: 431
DBG Done labeling provided sents in 0.30 seconds. Total # of tokens labeled: 402
DBG Done labeling provided sents in 0.36 seconds. Total # of tokens labeled: 437
DBG Done labeling provided sents in 0.37 seconds. Total # of tokens labeled: 443
DBG Done labeling provided sents in 0.38 seconds. Total # of tokens labeled: 449
DBG Done labeling provided sents in 0.39 seconds. Total # of tokens labeled: 455
DBG Done labeling provided sents in 0.40 seconds. Total # of tokens labeled: 461
Warning, could not find matching GrammaticalRelation for reln= nsubj
Warning, could not find matching GrammaticalRelation for reln= dobj
Warning, could not find matching GrammaticalRelation for reln= aux
Warning, could not find matching GrammaticalRelation for reln= advmod
Warning, could not find matching GrammaticalRelation for reln= amod
Warning, could not find matching GrammaticalRelation for reln= det
Warning, could not find matching GrammaticalRelation for reln= conj
Warning, could not find matching GrammaticalRelation for reln= cc
Warning, could not find matching GrammaticalRelation for reln= mark
Warning, could not find matching GrammaticalRelation for reln= case
Warning, could not find matching GrammaticalRelation for reln= acl
Warning, could not find matching GrammaticalRelation for reln= xcomp
Warning, could not find matching GrammaticalRelation for reln= ccomp
Warning, could not find matching GrammaticalRelation for reln= cop
Warning, could not find matching GrammaticalRelation for reln= neg
FileSplitter : Error reading split file /home/user/data/file1.txt (line 23)
FileSplitter : Error reading split file /home/user/data/file2.txt (line 17)
FileSplitter : Error reading split file /home/user/data/file3.txt (line 42)
FileSplitter : Error reading split file /home/user/data/file4.txt (line 35)
FileSplitter : Error reading split file /home/user/data/file5.txt (line 28)
FileMerger : Error reading split file /home/user/data/file6.txt (line 12)
FileMerger : Error reading split file /home/user/data/file7.txt (line 19)
FileMerger : Error reading split file /home/user/data/file8.txt (line 26)
FileMerger : Error reading split file /home/user/data/file9.txt (line 33)
FileMerger : Error reading split file /home/user/data/file10.txt (line 40)
FileReader : Error reading split file /home/user/data/file11.txt (line 15)
FileReader : Error reading split file /home/user/data/file12.txt (line 22)
FileReader : Error reading split file /home/user/data/file13.txt (line 29)
FileReader : Error reading split file /home/user/data/file14.txt (line 36)
Number of qn iterations per batch: 5
Number of qn iterations per batch: 10
Number of qn iterations per batch: 7
Number of qn iterations per batch: 12
Number of qn iterations per batch: 8
Number of qn iterations per batch: 6
Number of qn iterations per batch: 9
Number of qn iterations per batch: 11
Number of qn iterations per batch: 4
Number of qn iterations per batch: 3
Number of qn iterations per batch: 13
Number of qn iterations per batch: 14
Number of qn iterations per batch: 2
Number of qn iterations per batch: 15
Number of qn iterations per batch: 1
Could not parse subnet: 192.168.0.1/24
Could not parse subnet: 10.0.0.0/8
Could not parse subnet: 172.16.1.1/16
Could not parse subnet: 255.255.255.0
Could not parse subnet: 127.0.0.1
Could not parse subnet: 169.254.0.0/16
Could not parse subnet: 224.0.0.0/4
Could not parse subnet: 2001:db8::/32
Could not parse subnet: fe80::/10
Could not parse subnet: ::1
Could not parse subnet: ff00::/8
Could not parse subnet: 192.168.x.y
Could not parse subnet: 10.x.y.z
Could not parse subnet: 172.a.b.c
Could not parse subnet: abc.def.ghi.jkl
size of q 0
size of q 1
size of q 2
size of q 3
size of q 4
size of q 5
size of q 6
size of q 7
size of q 8
size of q 9
size of q 10
size of q 11
size of q 12
size of q 13
size of q 14
Overall: Precision: 0.76, Recall: 0.82, F1: 0.79
Overall: Precision: 0.81, Recall: 0.77, F1: 0.79
Overall: Precision: 0.78, Recall: 0.79, F1: 0.78
Overall: Precision: 0.83, Recall: 0.74, F1: 0.78
Overall: Precision: 0.75, Recall: 0.84, F1: 0.79
Overall: Precision: 0.82, Recall: 0.76, F1: 0.79
Overall: Precision: 0.77, Recall: 0.81, F1: 0.79
Overall: Precision: 0.84, Recall: 0.73, F1: 0.78
Overall: Precision: 0.74, Recall: 0.85, F1: 0.79
Overall: Precision: 0.80, Recall: 0.78, F1: 0.79
Overall: Precision: 0.79, Recall: 0.80, F1: 0.79
Overall: Precision: 0.85, Recall: 0.72, F1: 0.78
Overall: Precision: 0.73, Recall: 0.86, F1: 0.79
Overall: Precision: 0.79, Recall: 0.79, F1: 0.79
Overall: Precision: 0.86, Recall: 0.71, F1: 0.78
After removeXoverX: 0.5
After removeXoverX: 0.75
After removeXoverX: 0.25
After removeXoverX: 0.9
After removeXoverX: 0.1
After removeXoverX: 0.6
After removeXoverX: 0.8
After removeXoverX: 0.4
After removeXoverX: 0.7
After removeXoverX: 0.3
After removeXoverX: 1.0
After removeXoverX: 0.2
After removeXoverX: 0.95
After removeXoverX: 0.05
After removeXoverX: 0.85
Loading normalized dictionary from words.txt
Loading unnormalized dictionary from names.csv
Loading normalized dictionary from synonyms.json
Loading unnormalized dictionary from acronyms.xml
Loading normalized dictionary from definitions.db
Loading unnormalized dictionary from slang.tsv
Loading normalized dictionary from categories.ini
Loading unnormalized dictionary from codes.bin
Loading normalized dictionary from phrases.html
Loading unnormalized dictionary from keywords.docx
Loading normalized dictionary from translations.xlsx
Loading unnormalized dictionary from abbreviations.rtf
Loading normalized dictionary from antonyms.pptx
Loading unnormalized dictionary from idioms.pdf
Loading normalized dictionary from examples.zip
Estimate: 0.95
Estimate: 0.67
Estimate: 0.76
Estimate: 0.88
Estimate: 0.81
Estimate: 0.92
Estimate: 0.73
Estimate: 0.84
Estimate: 0.69
Estimate: 0.78
Estimate: 0.86
Estimate: 0.91
Estimate: 0.74
Estimate: 0.82
Estimate: 0.97
Empty top speakers list for: "I have a dream" [no candidate top speakers found – just ignore!
Empty top speakers list for: "To be or not to be" [no candidate top speakers found – just ignore!
Empty top speakers list for: "The only thing we have to fear is fear itself" [no candidate top speakers found – just ignore!
Empty top speakers list for: "Ask not what your country can do for you" [no candidate top speakers found – just ignore!
Empty top speakers list for: "You miss 100% of the shots you don't take" [no candidate top speakers found – just ignore!
Empty top speakers list for: "The journey of a thousand miles begins with a single step" [no candidate top speakers found – just ignore!
Empty top speakers list for: "Be the change that you wish to see in the world" [no candidate top speakers found – just ignore!
Empty top speakers list for: "Don't cry because it's over, smile because it happened" [no candidate top speakers found – just ignore!
Empty top speakers list for: "The truth is rarely pure and never simple" [no candidate top speakers found – just ignore!
Empty top speakers list for: "Live as if you were to die tomorrow" [no candidate top speakers found – just ignore!
Empty top speakers list for: "The unexamined life is not worth living" [no candidate top speakers found – just ignore!
Empty top speakers list for: "Do or do not, there is no try" [no candidate top speakers found – just ignore!
Empty top speakers list for: "May the Force be with you" [no candidate top speakers found – just ignore!
Empty top speakers list for: "Houston, we have a problem" [no candidate top speakers found – just ignore!
Empty top speakers list for: "I'm the king of the world!" [no candidate top speakers found – just ignore!
normalizedTimeString new str: 2023-10-27T05:47:31Z
normalizedTimeString new str: 2023-10-27T06:00:00Z
normalizedTimeString new str: 2023-10-27T05:30:00Z
normalizedTimeString new str: 2023-10-27T05:45:00Z
normalizedTimeString new str: 2023-10-27T05:50:00Z
normalizedTimeString new str: 2023-10-27T05:55:00Z
normalizedTimeString new str: 2023-10-27T06:05:00Z
normalizedTimeString new str: 2023-10-27T06:10:00Z
normalizedTimeString new str: 2023-10-27T06:15:00Z
normalizedTimeString new str: 2023-10-27T06:20:00Z
normalizedTimeString new str: 2023-10-27T06:25:00Z
normalizedTimeString new str: 2023-10-27T06:30:00Z
normalizedTimeString new str: 2023-10-27T06:35:00Z
normalizedTimeString new str: 2023-10-27T06:40:00Z
normalizedTimeString new str: 2023-10-27T06:45:00Z
ABCD TAG_INDEX
1234 TAG_INDEX
Xyzw TAG_INDEX
9876 TAG_INDEX
Qwer TAG_INDEX
5678 TAG_INDEX
Asdf TAG_INDEX
4321 TAG_INDEX
Zxcv TAG_INDEX
8765 TAG_INDEX
Tyui TAG_INDEX
3456 TAG_INDEX
Ghjk TAG_INDEX
6543 TAG_INDEX
Bnmk TAG_INDEX
Something is really wrong. Your system doesn't even support UTF-8! java.nio.charset.UnsupportedCharsetException: UTF-8
Something is really wrong. Your system doesn't even support UTF-8! UnicodeEncodeError: 'ascii' codec can't encode character '\u03b1' in position 0: ordinal not in range(128)
Something is really wrong. Your system doesn't even support UTF-8! System.ArgumentException: 'utf-8' is not a supported encoding name.
Something is really wrong. Your system doesn't even support UTF-8! Encoding::UndefinedConversionError: "\xC3" from ASCII-8BIT to UTF-8
Something is really wrong. Your system doesn't even support UTF-8! System.Text.EncoderFallbackException: Unable to translate Unicode character \u00E9 at index 3 to specified code page.
Something is really wrong. Your system doesn't even support UTF-8! System.NotSupportedException: No data is available for encoding 65001.
Something is really wrong. Your system doesn't even support UTF-8! System.InvalidOperationException: The current stream does not support writing.
Something is really wrong. Your system doesn't even support UTF-8! System.FormatException: Input string was not in a correct format.
Something is really wrong. Your system doesn't even support UTF-8! System.Security.SecurityException: Request for the permission of type 'System.Security.Permissions.FileIOPermission, mscorlib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089' failed.
Something is really wrong. Your system doesn't even support UTF-8! System.NullReferenceException: Object reference not set to an instance of an object.
Something is really wrong. Your system doesn't even support UTF-8! System.IO.FileNotFoundException: Could not find file 'C:\Users\John\Documents\test.txt'.
Something is really wrong. Your system doesn't even support UTF-8! System.IO.DirectoryNotFoundException: Could not find a part of the path 'C:\Users\John\Documents\test.txt'.
Something is really wrong. Your system doesn't even support UTF-8! System.UnauthorizedAccessException: Access to the path 'C:\Users\John\Documents\test.txt' is denied.
Something is really wrong. Your system doesn't even support UTF-8! System.IO.PathTooLongException: The specified path, file name, or both are too long.
numWeights: orig1= 12 , orig2= 15 , combined= 27
numWeights: orig1= 8 , orig2= 10 , combined= 18
numWeights: orig1= 20 , orig2= 25 , combined= 45
numWeights: orig1= 16 , orig2= 12 , combined= 28
numWeights: orig1= 10 , orig2= 14 , combined= 24
numWeights: orig1= 18 , orig2= 20 , combined= 38
numWeights: orig1= 14 , orig2= 16 , combined= 30
numWeights: orig1= 22 , orig2= 18 , combined= 40
numWeights: orig1= 24 , orig2= 21 , combined= 45
numWeights: orig1= 26 , orig2= 19 , combined= 45
numWeights: orig1= 28 , orig2= 17 , combined= 45
numWeights: orig1= 30 , orig2= 15 , combined= 45
numWeights: orig1= 32 , orig2= 13 , combined= 45
numWeights: orig1= 34 , orig2= 11 , combined= 45
numWeights: orig1= 36 , orig2= 9 , combined= 45
Extracting year from: | 2021-10-27
Extracting year from: | 1999-12-31
Extracting year from: | 2023-01-01
Extracting year from: | 1984-06-04
Extracting year from: | 2000-02-29
Extracting year from: | 2018-07-15
Extracting year from: | 1970-01-01
Extracting year from: | 2024-02-29
Extracting year from: | 2016-11-08
Extracting year from: | 2008-08-08
Extracting year from: | 1969-07-20
Extracting year from: | 1945-08-06
Extracting year from: | 1492-10-12
Extracting year from: | 1776-07-04
Entry 0 is entry 1 of binary transform 1010 : 10
Entry 1 is entry 2 of binary transform 1100 : 12
Entry 2 is entry 3 of binary transform 1110 : 14
Entry 3 is entry 4 of binary transform 1001 : 9
Entry 4 is entry 5 of binary transform 0110 : 6
Entry 5 is entry 6 of binary transform 0101 : 5
Entry 6 is entry 7 of binary transform 0011 : 3
Entry 7 is entry 8 of binary transform 0001 : 1
Entry 8 is entry 9 of binary transform 0010 : 2
Entry 9 is entry 10 of binary transform 0100 : 4
Entry 10 is entry 11 of binary transform 0111 : 7
Entry 11 is entry 12 of binary transform 1000 : 8
Entry 12 is entry 13 of binary transform 1011 : 11
Entry 13 is entry 14 of binary transform 1101 : 13
Entry -1 is entry -2 of binary transform -1111 : -15
Number of minimized states: 12
Number of minimized states: 8
Number of minimized states: 15
Number of minimized states: 10
Number of minimized states: 9
Number of minimized states: 11
Number of minimized states: 7
Number of minimized states: 13
Number of minimized states: 14
Number of minimized states: 6
Number of minimized states: 16
Number of minimized states: 5
Number of minimized states: 4
Number of minimized states: 17
Number of minimized states: 3
IO error while reading: data.csv FileNotFoundException
IO error while reading: report.docx IOException
IO error while reading: image.png EOFException
IO error while reading: music.mp3 CorruptDataException
IO error while reading: video.mp4 UnsupportedEncodingException
IO error while reading: config.ini InvalidFormatException
IO error while reading: log.txt AccessDeniedException
IO error while reading: game.exe VirusDetectedException
IO error while reading: resume.pdf MalformedURLException
IO error while reading: notes.txt EncodingException
IO error while reading: backup.zip ZipException
IO error while reading: calendar.ics ParseException
IO error while reading: contacts.vcf SyncException
IO error while reading: messages.db SQLiteException
IO error while reading: settings.json JSONException
PennTreeReader: warning: incomplete tree (extra left parentheses in input): (S (NP (DT The) (NN cat)) (VP (VBD ate) (NP (DT a) (NN fish)))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((S (NP (PRP He) (VP (VBD said) (SBAR (IN that) (S (NP (PRP she))))))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((NP (NNP John) (NNP Smith)) (VP (VBZ likes) (NP (NN pizza)))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((ADJP (JJ very) (JJ smart)) (NP (DT the) (NN boy)))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((PP (IN on) (NP (DT the) (NN table))) (S (NP-SBJ (-NONE- *)) (VP-PRD (-NONE- *))))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((SINV (-NONE- *) (-NONE- *) (-NONE- *) (-NONE- *) (-NONE- *) (-NONE- *) (-NONE- *) (-NONE- *) (-NONE- *) (-NONE- *))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((FRAG (-NONE- *T*) (-NONE- *T*) (-NONE- *T*) (-NONE- *T*) (-NONE- *T*) (-NONE- *T*) (-NONE- *T*) (-NONE- *T*))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((QP ($ $) (-LRB- -LRB-) CD 1.5) billion)
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((INTJ UH oh) ,)
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((LST (: :) (: :) (: :) (: :) (: :) (: :) (: :) (: :) (: :) (: :) (: :) (: :) (: :) (: :) (: :))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((WHADVP WRB how) S SQ do PRP you VB know))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((SBARQ WHNP WP what VP MD will PRP you VB do))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((ADVP RB quickly) RB too)
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((CONJP CC but JJ also JJ not))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((X XX XXX XXXX XXXXX XXXXXX XXXXXXX XXXXXXXX XXXXXXXXX XXXXXXXXXX XXXXXXXXXXX XXXXXXXXXXXX XXXXXXXXXXXXX XXXXXXXXXXXXXX XXXXXXXXXXXXXXX))
undirected nodes btw 1 and 5 is 3
undirected nodes btw 1 and 5 is 4
undirected nodes btw 1 and 5 is 2
undirected nodes btw 1 and 5 is 5
undirected nodes btw 1 and 5 is 1
undirected nodes btw 1 and 5 is 0
undirected nodes btw 1 and 5 is -1
undirected nodes btw 1 and 5 is -2
undirected nodes btw 1 and 5 is -3
undirected nodes btw 1 and 5 is -4
undirected nodes btw 1 and 5 is -5
undirected nodes btw 1 and 5 is -6
undirected nodes btw 1 and 5 is -7
undirected nodes btw 1 and 5 is -8
undirected nodes btw 1 and 5 is -9
Binary transform 1010 : 0101
Binary transform 1100 : 0011
Binary transform 0111 : 1000
Binary transform 1111 : 0000
Binary transform 0001 : 1110
Binary transform 0100 : 1011
Binary transform 1001 : 0110
Binary transform 0010 : 1101
Binary transform 0110 : 1001
Binary transform 1011 : 0100
Binary transform 1101 : 0010
Binary transform 1000 : 0111
Binary transform 0000 : 1111
Binary transform 1110 : 0001
Binary transform 0011 : 1100
Random seed not set, using randomly chosen seed of 42
Random seed not set, using randomly chosen seed of 123456789
Random seed not set, using randomly chosen seed of 314159265
Random seed not set, using randomly chosen seed of 271828182
Random seed not set, using randomly chosen seed of 161803398
Random seed not set, using randomly chosen seed of 141421356
Random seed not set, using randomly chosen seed of 173205080
Random seed not set, using randomly chosen seed of 223606797
Random seed not set, using randomly chosen seed of 244948974
Random seed not set, using randomly chosen seed of 316227766
Random seed not set, using randomly chosen seed of 387298334
Random seed not set, using randomly chosen seed of 458257569
Random seed not set, using randomly chosen seed of 529150262
Random seed not set, using randomly chosen seed of 600000000
Random seed not set, using randomly chosen seed of 670820393
B has length 5 and mean 3.2
B has length 10 and mean 4.5
B has length 8 and mean 2.7
B has length 12 and mean 6.1
B has length 7 and mean 3.8
B has length 9 and mean 5.4
B has length 6 and mean 4.2
B has length 11 and mean 7.3
B has length 4 and mean 2.9
B has length 15 and mean 8.6
B has length 13 and mean 6.7
B has length 3 and mean 1.5
B has length 14 and mean 9.2
B has length 2 and mean 0.8
B has length 16 and mean 10.4
Scanning with cache UNfriendly lookups took 12 ms
Scanning with cache UNfriendly lookups took 34 ms
Scanning with cache UNfriendly lookups took 56 ms
Scanning with cache UNfriendly lookups took 78 ms
Scanning with cache UNfriendly lookups took 90 ms
Scanning with cache UNfriendly lookups took 15 ms
Scanning with cache UNfriendly lookups took 37 ms
Scanning with cache UNfriendly lookups took 59 ms
Scanning with cache UNfriendly lookups took 81 ms
Scanning with cache UNfriendly lookups took 103 ms
Scanning with cache UNfriendly lookups took 25 ms
Scanning with cache UNfriendly lookups took 47 ms
Scanning with cache UNfriendly lookups took 69 ms
Scanning with cache UNfriendly lookups took 91 ms
Scanning with cache UNfriendly lookups took 113 ms
Gold entity list does not contain words "John Smith" for label "PERSON". *****Assuming them as negative.******
Gold entity list does not contain words "New York" for label "LOCATION". *****Assuming them as negative.******
Gold entity list does not contain words "Microsoft" for label "ORGANIZATION". *****Assuming them as negative.******
Gold entity list does not contain words "October 31" for label "DATE". *****Assuming them as negative.******
Gold entity list does not contain words "iPhone 14" for label "PRODUCT". *****Assuming them as negative.******
Gold entity list does not contain words "Harry Potter" for label "WORK_OF_ART". *****Assuming them as negative.******
Gold entity list does not contain words "3.14" for label "NUMBER". *****Assuming them as negative.******
Gold entity list does not contain words "red" for label "COLOR". *****Assuming them as negative.******
Gold entity list does not contain words "pizza" for label "FOOD". *****Assuming them as negative.******
Gold entity list does not contain words "soccer" for label "SPORT". *****Assuming them as negative.******
Gold entity list does not contain words "Bing" for label "WEBSITE". *****Assuming them as negative.******
Gold entity list does not contain words "Tesla" for label "VEHICLE". *****Assuming them as negative.******
Gold entity list does not contain words "dog" for label "ANIMAL". *****Assuming them as negative.******
Gold entity list does not contain words "France" for label "COUNTRY". *****Assuming them as negative.******
Loaded 12 lattices
Loaded 8 lattices
Loaded 15 lattices
Loaded 10 lattices
Loaded 9 lattices
Loaded 11 lattices
Loaded 13 lattices
Loaded 7 lattices
Loaded 14 lattices
Loaded 6 lattices
Loaded 16 lattices
Loaded 5 lattices
Loaded 17 lattices
Loaded 4 lattices
Loaded 18 lattices
Joint marginals: [0, 1, 2]
Joint marginals: [3, 4, 5]
Joint marginals: [6, 7, 8]
Joint marginals: [0, 3, 6]
Joint marginals: [1, 4, 7]
Joint marginals: [2, 5, 8]
Joint marginals: [0, 4, 8]
Joint marginals: [2, 4, 6]
Joint marginals: [0, 2, 4]
Joint marginals: [1, 3, 5]
Joint marginals: [0, 5, 7]
Joint marginals: [1, 6, 8]
Joint marginals: [2, 3, 7]
Joint marginals: [4, 5, 6]
Joint marginals: [3, 4, 8]
Entity extraction results for partition #0 using printer com.example.Printer@1a2b3c : Success
Entity extraction results for partition #1 using printer com.example.Printer@4d5e6f : Error: Printer not found
Entity extraction results for partition #2 using printer com.example.Printer@7g8h9i : Success
Entity extraction results for partition #3 using printer com.example.Printer@6p7q8r : Success
Entity extraction results for partition #4 using printer com.example.Printer@9s0t1u : Error: Printer out of paper
Entity extraction results for partition #5 using printer com.example.Printer@2v3w4x : Success
Entity extraction results for partition #6 using printer com.example.Printer@8b9c0d : Success
Entity extraction results for partition #7 using printer com.example.Printer@1e2f3g : Error: Printer overheated
Entity extraction results for partition #8 using printer com.example.Printer@7k8l9m : Error: Printer out of ink
Entity extraction results for partition #9 using printer com.example.Printer@0n1o2p : Success
truecase.bias" - class bias of the true case model; default: 0.5
truecase.bias" - class bias of the true case model; default: 0.75
truecase.bias" - class bias of the true case model; default: 0.25
truecase.bias" - class bias of the true case model; default: 0.1
truecase.bias" - class bias of the true case model; default: 0.9
truecase.bias" - class bias of the true case model; default: 0.4
truecase.bias" - class bias of the true case model; default: 0.6
truecase.bias" - class bias of the true case model; default: 0.3
truecase.bias" - class bias of the true case model; default: 0.7
truecase.bias" - class bias of the true case model; default: 0.2
truecase.bias" - class bias of the true case model; default: 0.8
truecase.bias" - class bias of the true case model; default: 0.05
truecase.bias" - class bias of the true case model; default: 0.95
truecase.bias" - class bias of the true case model; default: 0.15
truecase.bias" - class bias of the true case model; default: 0.85
Training linear classifier with 100 features and 2 labels
Training linear classifier with 50 features and 3 labels
Training linear classifier with 200 features and 4 labels
Training linear classifier with 75 features and 5 labels
Training linear classifier with 150 features and 6 labels
Training linear classifier with 25 features and 7 labels
Training linear classifier with 300 features and 8 labels
Training linear classifier with 125 features and 9 labels
Training linear classifier with 250 features and 10 labels
Training linear classifier with 175 features and 11 labels
Training linear classifier with 350 features and 12 labels
Training linear classifier with 225 features and 13 labels
Training linear classifier with 400 features and 14 labels
Training linear classifier with 275 features and 15 labels
Training linear classifier with 450 features and 16 labels
Relation extraction results for partition #0 using printer com.example.Printer@1a2b3c : Success
Relation extraction results for partition #1 using printer com.example.Printer@4d5e6f : Error: Printer not found
Relation extraction results for partition #2 using printer com.example.Printer@1a2b3c : Error: Out of paper
Relation extraction results for partition #3 using printer com.example.Printer@4d5e6f : Success
Relation extraction results for partition #4 using printer com.example.Printer@1a2b3c : Success
Relation extraction results for partition #5 using printer com.example.Printer@4d5e6f : Error: Invalid format
Relation extraction results for partition #6 using printer com.example.Printer@1a2b3c : Error: Printer busy
Relation extraction results for partition #7 using printer com.example.Printer@4d5e6f : Success
Relation extraction results for partition #8 using printer com.example.Printer@1a2b3c : Success
Relation extraction results for partition #9 using printer com.example.Printer@4d5e6f : Error: Printer jammed
Iteration 1 batch 32
Iteration 2 batch 64
Iteration 3 batch 128
Iteration 4 batch 256
Iteration 5 batch 512
Iteration 6 batch 1024
Iteration 7 batch 2048
Iteration 8 batch 4096
Iteration 9 batch 8192
Iteration 10 batch 16384
Iteration 11 batch 32768
Iteration 12 batch 65536
Iteration 13 batch 131072
Iteration 14 batch 262144
Iteration 15 batch 524288
Number of quotes + ascii - single : 0
Number of quotes + ascii - single : 3
Number of quotes + ascii - single : 6
Number of quotes + ascii - single : 9
Number of quotes + ascii - single : 12
Number of quotes + ascii - single : 15
Number of quotes + ascii - single : 18
Number of quotes + ascii - single : 21
Number of quotes + ascii - single : 24
Number of quotes + ascii - single : 27
Number of quotes + ascii - single : 30
Number of quotes + ascii - single : 33
Number of quotes + ascii - single : 36
Number of quotes + ascii - single : 39
Number of quotes + ascii - single : 42
sentenceBoundaryMultiTokenPattern= \n
sentenceBoundaryMultiTokenPattern= .!?
sentenceBoundaryMultiTokenPattern= \r\n
sentenceBoundaryMultiTokenPattern= \s
sentenceBoundaryMultiTokenPattern= \t
sentenceBoundaryMultiTokenPattern= \n\n
sentenceBoundaryMultiTokenPattern= .!?\n
sentenceBoundaryMultiTokenPattern= \r\n\r\n
sentenceBoundaryMultiTokenPattern= \s\s
sentenceBoundaryMultiTokenPattern= \t\t
sentenceBoundaryMultiTokenPattern= \n\s
sentenceBoundaryMultiTokenPattern= .!?\s
sentenceBoundaryMultiTokenPattern= \r\n\s
sentenceBoundaryMultiTokenPattern= \s\t
sentenceBoundaryMultiTokenPattern= \t\n
Successfully mapped all arguments in relation mention: rm-1
Successfully mapped all arguments in relation mention: rm-23
Successfully mapped all arguments in relation mention: rm-456
Successfully mapped all arguments in relation mention: rm-7890
Successfully mapped all arguments in relation mention: rm-12
Successfully mapped all arguments in relation mention: rm-345
Successfully mapped all arguments in relation mention: rm-6789
Successfully mapped all arguments in relation mention: rm-1011
Successfully mapped all arguments in relation mention: rm-1213
Successfully mapped all arguments in relation mention: rm-1415
Successfully mapped all arguments in relation mention: rm-1617
Successfully mapped all arguments in relation mention: rm-1819
Successfully mapped all arguments in relation mention: rm-2021
Successfully mapped all arguments in relation mention: rm-2223
Successfully mapped all arguments in relation mention: rm-2425
Number of active feature types: 12
Number of active feature types: 9
Number of active feature types: 15
Number of active feature types: 11
Number of active feature types: 10
Number of active feature types: 13
Number of active feature types: 8
Number of active feature types: 14
Number of active feature types: 7
Number of active feature types: 16
Number of active feature types: 6
Number of active feature types: 17
Number of active feature types: 5
Number of active feature types: 18
Number of active feature types: 4
Unfiltered hooks: relaxHook2
Unfiltered hooks: relaxHook2 (skipped)
Unfiltered hooks: relaxHook2 (failed)
Unfiltered hooks: relaxHook2 (timeout)
Unfiltered hooks: relaxHook2 (success)
Unfiltered hooks: relaxHook2 (retry)
Unfiltered hooks: relaxHook2 (invalid)
Unfiltered hooks: relaxHook2 (error)
Unfiltered hooks: relaxHook2 (pending)
Unfiltered hooks: relaxHook2 (cancelled)
Unfiltered hooks: relaxHook2 (duplicate)
Unfiltered hooks: relaxHook2 (ignored)
Unfiltered hooks: relaxHook2 (updated)
Unfiltered hooks: relaxHook2 (expired)
Unfiltered hooks: relaxHook2 (completed)
Times through each training batch: 10
Times through each training batch: 25
Times through each training batch: 50
Times through each training batch: 100
Times through each training batch: 200
Times through each training batch: 500
Times through each training batch: 1000
Times through each training batch: 1500
Times through each training batch: 2500
Times through each training batch: 5000
Times through each training batch: 7500
Times through each training batch: 10000
Times through each training batch: 15000
Times through each training batch: 20000
Times through each training batch: 30000
Failed to match relation argument, so keeping gold: goldEnt_1
Failed to match relation argument, so keeping gold: goldEnt_5
Failed to match relation argument, so keeping gold: goldEnt_9
Failed to match relation argument, so keeping gold: goldEnt_12
Failed to match relation argument, so keeping gold: goldEnt_16
Failed to match relation argument, so keeping gold: goldEnt_20
Failed to match relation argument, so keeping gold: goldEnt_23
Failed to match relation argument, so keeping gold: goldEnt_27
Failed to match relation argument, so keeping gold: goldEnt_31
Failed to match relation argument, so keeping gold: goldEnt_34
Failed to match relation argument, so keeping gold: goldEnt_38
Failed to match relation argument, so keeping gold: goldEnt_42
Failed to match relation argument, so keeping gold: goldEnt_45
Failed to match relation argument, so keeping gold: goldEnt_49
Failed to match relation argument, so keeping gold: goldEnt_53
unrecognized flag: --verbose
unrecognized flag: -h
unrecognized flag: /f
unrecognized flag: --help
unrecognized flag: -v
unrecognized flag: /?
unrecognized flag: --version
unrecognized flag: -a
unrecognized flag: /c
unrecognized flag: --all
unrecognized flag: -d
unrecognized flag: /r
unrecognized flag: --delete
unrecognized flag: -s
unrecognized flag: /q
Done: 1634819061000 - 0
Done: 1634819062000 - 1000
Done: 1634819063000 - 2000
Done: 1634819064000 - 3000
Done: 1634819065000 - 4000
Done: 1634819066000 - 5000
Done: 1634819067000 - 6000
Done: 1634819068000 - 7000
Done: 1634819069000 - 8000
Done: 1634819070000 - 9000
Done: 1634819071000 - 10000
Done: 1634819072000 - 11000
Done: 1634819073000 - 12000
Done: 1634819074000 - 13000
Done: 1634819075000 - 14000
Read in 100 trees from /home/user/data/tree1.csv
Read in 50 trees from /var/log/tree2.log
Read in 200 trees from /tmp/tree3.tmp
Read in 75 trees from /opt/tree4.dat
Read in 150 trees from /usr/local/tree5.txt
Read in 25 trees from /etc/tree6.conf
Read in 125 trees from /dev/tree7.bin
Read in 175 trees from /mnt/tree8.bak
Read in 300 trees from /root/tree9.json
Read in 400 trees from /media/tree10.xml
Read in 350 trees from /srv/tree11.html
Read in 250 trees from /run/tree12.pid
Read in 450 trees from /sys/tree13.ko
Read in 500 trees from /proc/tree14.stat
Read in 550 trees from /boot/tree15.img
(logs) -11 + -7 = -18.0
(logs) -11 + -7 = logAdd(-11.0, -7.0)
(logs) -11 + -7 = logAdd(-11, -7)
(logs) -11 + -7 = logAdd(-11, -7.0)
(logs) -11 + -7 = logAdd(-11.0, -7)
(logs) -11 + -7 = logAdd(-1.1E1, -7)
(logs) -11 + -7 = logAdd(-1.1E1, -7.0)
(logs) -11 + -7 = logAdd(-11, -7E0)
(logs) -11 + -7 = logAdd(-11.0, -7E0)
(logs) -11 + -7 = logAdd(-1.1E1, -7E0)
(logs) -11 + -7 = logAdd(-110E-1, -70E-1)
(logs) -11 + -7 = logAdd(-110E-1, -7)
(logs) -11 + -7 = logAdd(-110E-1, -7.0)
(logs) -11 + -7 = logAdd(-11, -70E-1)
(logs) -11 + -7 = logAdd(-11.0, -70E-1)
changed to: newCollocationChild(0, 1)
changed to: newCollocationChild(3, 2)
changed to: newCollocationChild(1, 4)
changed to: newCollocationChild(2, 0)
changed to: newCollocationChild(4, 3)
changed to: newCollocationChild(5, 6)
changed to: newCollocationChild(6, 5)
changed to: newCollocationChild(7, 8)
changed to: newCollocationChild(8, 7)
changed to: newCollocationChild(9, 10)
changed to: newCollocationChild(10, 9)
changed to: newCollocationChild(11, 12)
changed to: newCollocationChild(12, 11)
changed to: newCollocationChild(13, 14)
changed to: newCollocationChild(14, 13)
Loaded 234 phrases from wordf
Loaded 189 phrases from wordf
Loaded 256 phrases from wordf
Loaded 212 phrases from wordf
Loaded 241 phrases from wordf
Loaded 198 phrases from wordf
Loaded 227 phrases from wordf
Loaded 205 phrases from wordf
Loaded 236 phrases from wordf
Loaded 217 phrases from wordf
Loaded 244 phrases from wordf
Loaded 202 phrases from wordf
Loaded 231 phrases from wordf
Loaded 219 phrases from wordf
Loaded 248 phrases from wordf
There were 12 matches in total.
There were 0 matches in total.
There were 5 matches in total.
There were 9 matches in total.
There were 3 matches in total.
There were 7 matches in total.
There were 10 matches in total.
There were 4 matches in total.
There were 8 matches in total.
There were 6 matches in total.
There were 2 matches in total.
There were 11 matches in total.
There were 15 matches in total.
There were 13 matches in total.
|intersect|: 0
|intersect|: 1
|intersect|: 2
|intersect|: 3
|intersect|: 4
|intersect|: 5
|intersect|: 6
|intersect|: 7
|intersect|: 8
|intersect|: 9
|intersect|: 10
|intersect|: 11
|intersect|: 12
|intersect|: 13
|intersect|: 14
Not hypothesizing a boundary at pos 5, since between two ASCII letters (a and b).
Not hypothesizing a boundary at pos 17, since between two ASCII letters (T and h).
Not hypothesizing a boundary at pos 32, since between two ASCII letters (n and o).
Not hypothesizing a boundary at pos 9, since between two ASCII letters (e and f).
Not hypothesizing a boundary at pos 24, since between two ASCII letters (r and s).
Not hypothesizing a boundary at pos 13, since between two ASCII letters (m and n).
Not hypothesizing a boundary at pos 28, since between two ASCII letters (k and l).
Not hypothesizing a boundary at pos 4, since between two ASCII letters (c and d).
Not hypothesizing a boundary at pos 19, since between two ASCII letters (g and h).
Not hypothesizing a boundary at pos 36, since between two ASCII letters (p and q).
Not hypothesizing a boundary at pos 7, since between two ASCII letters (d and e).
Not hypothesizing a boundary at pos 21, since between two ASCII letters (s and t).
Not hypothesizing a boundary at pos 34, since between two ASCII letters (o and p).
Not hypothesizing a boundary at pos 11, since between two ASCII letters (f and g).
Parseable: true
Parseable: false
Parseable: yes
Parseable: no
Parseable: 1
Parseable: 0
Parseable: Y
Parseable: N
Parseable: T
Parseable: F
Parseable: OK
Parseable: ERROR
Parseable: PASS
Parseable: FAIL
Parseable: NULL
Training event extraction using predicted entitities: entity scores using printer HP LaserJet 1020 : Success
Training event extraction using predicted entitities: entity scores using printer Canon PIXMA MG3620 : Error: Printer not found
Training event extraction using predicted entitities: entity scores using printer Epson WorkForce WF-2850 : Warning: Low ink level
Training event extraction using predicted entitities: entity scores using printer Brother MFC-J995DW : Success
Training event extraction using predicted entitities: entity scores using printer Samsung Xpress M2020W : Error: Paper jam
Training event extraction using predicted entitities: entity scores using printer Lexmark B2236dw : Success
Training event extraction using predicted entitities: entity scores using printer Dell E310dw : Warning: Toner low
Training event extraction using predicted entitities: entity scores using printer HP DeskJet 3755 : Success
Training event extraction using predicted entitities: entity scores using printer Canon imageCLASS MF236n : Error: Network connection failed
Training event extraction using predicted entitities: entity scores using printer Epson EcoTank ET-2760 : Success
Training event extraction using predicted entitities: entity scores using printer Brother HL-L2350DW : Warning: Drum unit near end of life
Training event extraction using predicted entitities: entity scores using printer Samsung ProXpress M3820DW : Success
Training event extraction using predicted entitities: entity scores using printer Lexmark MC3224dwe : Error: Invalid cartridge
Training event extraction using predicted entitities: entity scores using printer Dell C1760nw : Success
Training event extraction using predicted entitities: entity scores using printer HP OfficeJet Pro 9015e : Warning: Scanner error
com.example.service.WordService : Invalid unknown word signature! (3)
com.example.dao.WordDao : Invalid unknown word signature! (2)
com.example.controller.WordController : Invalid unknown word signature! (1)
com.example.model.Word : Invalid unknown word signature! (0)
com.example.util.WordUtils : Invalid unknown word signature! (4)
com.example.exception.WordException : Invalid unknown word signature! (5)
com.example.config.WordConfig : Invalid unknown word signature! (6)
com.example.dto.WordDto : Invalid unknown word signature! (7)
com.example.mapper.WordMapper : Invalid unknown word signature! (8)
com.example.validator.WordValidator : Invalid unknown word signature! (9)
com.example.repository.WordRepository : Invalid unknown word signature! (10)
com.example.converter.WordConverter : Invalid unknown word signature! (11)
com.example.handler.WordHandler : Invalid unknown word signature! (12)
com.example.aspect.WordAspect : Invalid unknown word signature! (13)
com.example.listener.WordListener : Invalid unknown word signature! (14)
Using unknown word vector for capitalized words: 0.5
Using unknown word vector for capitalized words: 0.3
Using unknown word vector for capitalized words: 0.7
Using unknown word vector for capitalized words: 0.4
Using unknown word vector for capitalized words: 0.6
Using unknown word vector for capitalized words: 0.2
Using unknown word vector for capitalized words: 0.8
Using unknown word vector for capitalized words: 0.1
Using unknown word vector for capitalized words: 0.9
Using unknown word vector for capitalized words: 0.45
Using unknown word vector for capitalized words: 0.35
Using unknown word vector for capitalized words: 0.65
Using unknown word vector for capitalized words: 0.55
Using unknown word vector for capitalized words: 0.25
Using unknown word vector for capitalized words: 0.75
Created index with size 0. Don't worry if it's zero and you are using batch process sents.
Created index with size 1. Don't worry if it's zero and you are using batch process sents.
Created index with size 10. Don't worry if it's zero and you are using batch process sents.
Created index with size 100. Don't worry if it's zero and you are using batch process sents.
Created index with size 1000. Don't worry if it's zero and you are using batch process sents.
Created index with size 2. Don't worry if it's zero and you are using batch process sents.
Created index with size 20. Don't worry if it's zero and you are using batch process sents.
Created index with size 200. Don't worry if it's zero and you are using batch process sents.
Created index with size 2000. Don't worry if it's zero and you are using batch process sents.
Created index with size 3. Don't worry if it's zero and you are using batch process sents.
Created index with size 30. Don't worry if it's zero and you are using batch process sents.
Created index with size 300. Don't worry if it's zero and you are using batch process sents.
Created index with size 3000. Don't worry if it's zero and you are using batch process sents.
Created index with size 4. Don't worry if it's zero and you are using batch process sents.
Created index with size 40. Don't worry if it's zero and you are using batch process sents.
Failed to correctly process tree 1
Failed to correctly process tree 7
Failed to correctly process tree 12
Failed to correctly process tree 4
Failed to correctly process tree 9
Failed to correctly process tree 3
Failed to correctly process tree 10
Failed to correctly process tree 6
Failed to correctly process tree 11
Failed to correctly process tree 2
Failed to correctly process tree 8
Failed to correctly process tree 5
Failed to correctly process tree 13
Failed to correctly process tree 14
Failed to correctly process tree 15
Number of zero feature x,y pairs: 0
Number of zero feature x,y pairs: 3
Number of zero feature x,y pairs: 7
Number of zero feature x,y pairs: 12
Number of zero feature x,y pairs: 1
Number of zero feature x,y pairs: 5
Number of zero feature x,y pairs: 9
Number of zero feature x,y pairs: 15
Number of zero feature x,y pairs: 2
Number of zero feature x,y pairs: 4
Number of zero feature x,y pairs: 8
Number of zero feature x,y pairs: 10
Number of zero feature x,y pairs: 6
Number of zero feature x,y pairs: 11
Number of zero feature x,y pairs: 14
sizes different: 12 vs. 15
sizes different: 9 vs. 10
sizes different: 0 vs. 3
sizes different: 5 vs. 5
sizes different: 8 vs. 7
sizes different: 11 vs. 13
sizes different: 4 vs. 6
sizes different: 10 vs. 9
sizes different: 7 vs. 8
sizes different: 6 vs. 4
sizes different: 3 vs. 0
sizes different: 15 vs. 12
sizes different: 13 vs. 11
sizes different: 14 vs. 14
sizes different: 2 vs. 1
TRAIN corpus size reduced from 10000 to 8000
TRAIN corpus size reduced from 5000 to 4000
TRAIN corpus size reduced from 12000 to 9000
TRAIN corpus size reduced from 8000 to 6000
TRAIN corpus size reduced from 6000 to 4500
TRAIN corpus size reduced from 9000 to 7000
TRAIN corpus size reduced from 7000 to 5000
TRAIN corpus size reduced from 4000 to 3000
TRAIN corpus size reduced from 1000 to 800
TRAIN corpus size reduced from 3000 to 2000
TRAIN corpus size reduced from 15000 to 12000
TRAIN corpus size reduced from 20000 to 16000
TRAIN corpus size reduced from 25000 to 20000
TRAIN corpus size reduced from 30000 to 24000
TRAIN corpus size reduced from 35000 to 28000
Error reading train.conllX
Error reading dev.conllX
Error reading test.conllX
Error reading input.conllX
Error reading output.conllX
Error reading gold.conllX
Error reading config.conllX
Error reading data.conllX
Error reading model.conllX
Error reading features.conllX
Error reading labels.conllX
Error reading sentences.conllX
Error reading tokens.conllX
Error reading dependencies.conllX
Error reading predictions.conllX
Normalizing integer part: 0
Normalizing integer part: 1
Normalizing integer part: -1
Normalizing integer part: 10
Normalizing integer part: -10
Normalizing integer part: 100
Normalizing integer part: -100
Normalizing integer part: 123
Normalizing integer part: -123
Normalizing integer part: 999
Normalizing integer part: -999
Normalizing integer part: 4567
Normalizing integer part: -4567
Normalizing integer part: 1234567890
Normalizing integer part: -1234567890
Running 5 threads of different taggers
Running 3 threads of different taggers
Running 7 threads of different taggers
Running 4 threads of different taggers
Running 6 threads of different taggers
Running 2 threads of different taggers
Running 8 threads of different taggers
Running 9 threads of different taggers
Running 10 threads of different taggers
Running 11 threads of different taggers
Running 12 threads of different taggers
Running 13 threads of different taggers
Running 14 threads of different taggers
Running 15 threads of different taggers
Processing text "Hello world" with dateString = 2023-10-23
Processing text "The quick brown fox jumps over the lazy dog" with dateString = 2023-10-22
Processing text "This is a test sentence" with dateString = 2023-10-21
Processing text "How are you today?" with dateString = 2023-10-20
Processing text "I love Bing" with dateString = 2023-10-19
Processing text "What is the meaning of life?" with dateString = 2023-10-18
Processing text "To be or not to be, that is the question" with dateString = 2023-10-17
Processing text "Lorem ipsum dolor sit amet" with dateString = 2023-10-16
Processing text "May the force be with you" with dateString = 2023-10-15
Processing text "Winter is coming" with dateString = 2023-10-14
Processing text "It was a dark and stormy night" with dateString = 2023-10-13
Processing text "Once upon a time, there was a princess" with dateString = 2023-10-12
Processing text "Let it go, let it go, can't hold it back anymore" with dateString = 2023-10-11
Processing text "Houston, we have a problem" with dateString = 2023-10-10
sampled 50 sentences ( 10 %)
sampled 25 sentences ( 5 %)
sampled 100 sentences ( 20 %)
sampled 75 sentences ( 15 %)
sampled 40 sentences ( 8 %)
sampled 60 sentences ( 12 %)
sampled 30 sentences ( 6 %)
sampled 80 sentences ( 16 %)
sampled 20 sentences ( 4 %)
sampled 90 sentences ( 18 %)
sampled 45 sentences ( 9 %)
sampled 70 sentences ( 14 %)
sampled 35 sentences ( 7 %)
sampled 55 sentences ( 11 %)
sampled 10 sentences ( 2 %)
Loading KBP classifier from: model/kbp-bert-base.bin
Loading KBP classifier from: model/kbp-roberta-large.bin
Loading KBP classifier from: model/kbp-xlnet-base.bin
Loading KBP classifier from: model/kbp-albert-xxlarge.bin
Loading KBP classifier from: model/kbp-electra-small.bin
Loading KBP classifier from: model/kbp-distilbert-base.bin
Loading KBP classifier from: model/kbp-gpt2-medium.bin
Loading KBP classifier from: model/kbp-bart-large.bin
Loading KBP classifier from: model/kbp-t5-small.bin
Loading KBP classifier from: model/kbp-deberta-v2-xlarge.bin
Loading KBP classifier from: model/kbp-longformer-base.bin
Loading KBP classifier from: model/kbp-bigbird-base.bin
Loading KBP classifier from: model/kbp-mpnet-base.bin
Loading KBP classifier from: model/kbp-reformer-base.bin
Loading KBP classifier from: model/kbp-clip-vit-base.bin
Removing patterns from iteration 0
Removing patterns from iteration 1
Removing patterns from iteration 2
Removing patterns from iteration 3
Removing patterns from iteration 4
Removing patterns from iteration 5
Removing patterns from iteration 6
Removing patterns from iteration 7
Removing patterns from iteration 8
Removing patterns from iteration 9
Removing patterns from iteration 10
Removing patterns from iteration 11
Removing patterns from iteration 12
Removing patterns from iteration 13
Removing patterns from iteration 14
Class is java.lang.String
Class is java.util.ArrayList
Class is java.lang.Integer
Class is java.io.File
Class is java.util.HashMap
Class is java.lang.Object
Class is java.math.BigDecimal
Class is java.net.URL
Class is java.time.LocalDate
Class is java.util.Scanner
Class is java.lang.Thread
Class is java.awt.Color
Class is javax.swing.JFrame
Class is java.sql.Connection
Class is org.junit.Test
For cInfo.get(US) , features: 0x01 UTF-8
For cInfo.get(CN) , features: 0x02 UTF-8
For cInfo.get(FR) , features: 0x03 UTF-8
For cInfo.get(UK) , features: 0x04 UTF-8
For cInfo.get(JP) , features: 0x05 UTF-8
For cInfo.get(IN) , features: 0x06 UTF-8
For cInfo.get(BR) , features: 0x07 UTF-8
For cInfo.get(DE) , features: 0x08 UTF-8
For cInfo.get(AU) , features: 0x09 UTF-8
For cInfo.get(CA) , features: 0x0A UTF-8
For cInfo.get(RU) , features: 0x0B UTF-8
For cInfo.get(ES) , features: 0x0C UTF-8
For cInfo.get(IT) , features: 0x0D UTF-8
For cInfo.get(MX) , features: 0x0E UTF-8
For cInfo.get(KR) , features: 0x0F UTF-8
Loading entity extraction model from NER_BERT.pt ...
Loading entity extraction model from Spacy_en_core_web_sm ...
Loading entity extraction model from Stanford_NER.jar ...
Loading entity extraction model from OpenNLP_NameFinder.bin ...
Loading entity extraction model from Flair_NER_tagger.pkl ...
Loading entity extraction model from Polyglot_NER_model.h5 ...
Loading entity extraction model from NLTK_Chunking_model.pickle ...
Loading entity extraction model from AllenNLP_ELMo_model.tar.gz ...
Loading entity extraction model from HuggingFace_Transformers_model.pth ...
Loading entity extraction model from spaCy_en_core_web_lg ...
Loading entity extraction model from MITIE_NER_model.dat ...
Loading entity extraction model from Gensim_Word2Vec_model.bin ...
Loading entity extraction model from FastText_NER_model.vec ...
Loading entity extraction model from CRF_NER_model.crf ...
Loading entity extraction model from DeepPavlov_BERT_model.zip ...
Prec: 0.76 , Recall: 0.81 , F1: 0.78
Prec: 0.65 , Recall: 0.72 , F1: 0.68
Prec: 0.83 , Recall: 0.86 , F1: 0.84
Prec: 0.71 , Recall: 0.75 , F1: 0.73
Prec: 0.79 , Recall: 0.82 , F1: 0.80
Prec: 0.68 , Recall: 0.74 , F1: 0.71
Prec: 0.86 , Recall: 0.89 , F1: 0.87
Prec: 0.74 , Recall: 0.78 , F1: 0.76
Prec: 0.81 , Recall: 0.84 , F1: 0.82
Prec: 0.66 , Recall: 0.73 , F1: 0.69
Prec: 0.84 , Recall: 0.87 , F1: 0.85
Prec: 0.72 , Recall: 0.76 , F1: 0.74
Prec: 0.80 , Recall: 0.83 , F1: 0.81
Prec: 0.69 , Recall: 0.75 , F1: 0.72
Prec: 0.85 , Recall: 0.88 , F1: 0.86
Removing 12 patterns that do not meet minPosPhraseSupportForPat requirement of >= 0.5
Removing 8 patterns that do not meet minPosPhraseSupportForPat requirement of >= 0.6
Removing 15 patterns that do not meet minPosPhraseSupportForPat requirement of >= 0.4
Removing 10 patterns that do not meet minPosPhraseSupportForPat requirement of >= 0.7
Removing 9 patterns that do not meet minPosPhraseSupportForPat requirement of >= 0.8
Removing 11 patterns that do not meet minPosPhraseSupportForPat requirement of >= 0.3
Removing 13 patterns that do not meet minPosPhraseSupportForPat requirement of >= 0.9
Removing 7 patterns that do not meet minPosPhraseSupportForPat requirement of >= 0.2
Removing 14 patterns that do not meet minPosPhraseSupportForPat requirement of >= 1.0
Removing 6 patterns that do not meet minPosPhraseSupportForPat requirement of >= 1.1
Removing 16 patterns that do not meet minPosPhraseSupportForPat requirement of >= 0.1
Removing 5 patterns that do not meet minPosPhraseSupportForPat requirement of >= 1.2
Removing 17 patterns that do not meet minPosPhraseSupportForPat requirement of >= -0.1
Removing 4 patterns that do not meet minPosPhraseSupportForPat requirement of >= -0.2
Removing 18 patterns that do not meet minPosPhraseSupportForPat requirement of >= -0.3
UserMapper : Unable to instantiate mapper type com.example.model.User
OrderMapper : Unable to instantiate mapper type com.example.model.Order
ProductMapper : Unable to instantiate mapper type com.example.model.Product
CustomerMapper : Unable to instantiate mapper type com.example.model.Customer
CategoryMapper : Unable to instantiate mapper type com.example.model.Category
ReviewMapper : Unable to instantiate mapper type com.example.model.Review
CartMapper : Unable to instantiate mapper type com.example.model.Cart
PaymentMapper : Unable to instantiate mapper type com.example.model.Payment
AddressMapper : Unable to instantiate mapper type com.example.model.Address
InvoiceMapper : Unable to instantiate mapper type com.example.model.Invoice
EmployeeMapper : Unable to instantiate mapper type com.example.model.Employee
DepartmentMapper : Unable to instantiate mapper type com.example.model.Department
RoleMapper : Unable to instantiate mapper type com.example.model.Role
PermissionMapper : Unable to instantiate mapper type com.example.model.Permission
LogMapper : Unable to instantiate mapper type com.example.model.Log
Sentence: The/cat/NP/NNS/SBJ/VP/VBD/quickly/RB/ADV/VP/VBD/jumped/over/the/fence/NP/NN/OBJ
Sentence: She/PRP/NP/SBJ/VP/VBZ/a/good/NP/JJ/teacher/NP/NN/PRED
Sentence: He/PRP/NP/SBJ/VP/VBD/to/the/store/NP/NN/OBJ/with/his/friend/NP/NN/OBJ
Sentence: They/PRP/NP/SBJ/VP/VBP/enjoying/the/movie/NP/NN/OBJ
Sentence: It/PRP/NP/SBJ/VP/VBZ/raining/outside/NP/RB/ADV
Sentence: I/PRP/NP/SBJ/VP/VBP/want/some/coffee/NP/NN/OBJ
Sentence: You/PRP/NP/SBJ/VP/VBP/have/a/nice/day/NP/JJ/OBJ
Sentence: We/PRP/NP/SBJ/VP/VBP/go/to/the/park/NP/NN/OBJ/tomorrow/NP/RB/TMP
Sentence: She/PRP/NP/SBJ/VP/VBD/saw/a/bird/NP/NN/OBJ/flying/in/the/sky/NP/VBG/OBJ
Sentence: He/PRP/NP/SBJ/VP/VBD/gave/her/a/book/NP/JJ/OBJ/as/a/gift/NP/JJ/OBJ
Sentence: They/PRP/NP/SBJ/VP/VBP/happy/to/meet/you/NP/JJ/PRED
Sentence: It/PRP/NP/SBJ/VP/VBZ/a/beautiful/day/NP/JJ/PRED
Sentence: I/PRP/NP/SBJ/VP/VBP/thankful/for/everything/NP/JJ/PRED
Sentence: You/PRP/NP/SBJ/VP/VBP/the/best/friend/NP/JJ/PRED/I/could/have
Sentence: We/PRP/NP/SBJ/VP/VBP/make/a/difference/NP/JJ/PRED/if/we/work/together
Annotating dataset with NLP
Annotating dataset with OCR
Annotating dataset with face detection
Annotating dataset with sentiment analysis
Annotating dataset with speech recognition
Annotating dataset with image segmentation
Annotating dataset with object recognition
Annotating dataset with text summarization
Annotating dataset with machine translation
Annotating dataset with pose estimation
Annotating dataset with keyword extraction
Annotating dataset with topic modeling
Annotating dataset with named entity recognition
Annotating dataset with relation extraction
Annotating dataset with question answering
*** Optimizing Sieve ordering for pass 3 ***
*** Optimizing Sieve ordering for pass 7 ***
*** Optimizing Sieve ordering for pass 1 ***
*** Optimizing Sieve ordering for pass 5 ***
*** Optimizing Sieve ordering for pass 9 ***
*** Optimizing Sieve ordering for pass 2 ***
*** Optimizing Sieve ordering for pass 4 ***
*** Optimizing Sieve ordering for pass 6 ***
*** Optimizing Sieve ordering for pass 8 ***
*** Optimizing Sieve ordering for pass 10 ***
*** Optimizing Sieve ordering for pass 12 ***
*** Optimizing Sieve ordering for pass 11 ***
*** Optimizing Sieve ordering for pass 13 ***
*** Optimizing Sieve ordering for pass 14 ***
*** Optimizing Sieve ordering for pass 15 ***
Loading file treeFile_1.txt
Loading file treeFile_2.csv
Loading file treeFile_3.json
Loading file treeFile_4.xml
Loading file treeFile_5.bin
Loading file treeFile_6.dat
Loading file treeFile_7.zip
Loading file treeFile_8.rar
Loading file treeFile_9.docx
Loading file treeFile_10.xlsx
Loading file treeFile_11.pdf
Loading file treeFile_12.pptx
Loading file treeFile_13.jpg
Loading file treeFile_14.png
Loading file treeFile_15.gif
Failed to use the given parser to reparse sentence "The dog barked loudly."
Failed to use the given parser to reparse sentence "She loves to read books."
Failed to use the given parser to reparse sentence "He ran as fast as he could."
Failed to use the given parser to reparse sentence "They went to the park yesterday."
Failed to use the given parser to reparse sentence "I have a blue car."
Failed to use the given parser to reparse sentence "She is very smart and kind."
Failed to use the given parser to reparse sentence "He likes to play soccer and basketball."
Failed to use the given parser to reparse sentence "They have two cats and a dog."
Failed to use the given parser to reparse sentence "I ate pizza for lunch."
Failed to use the given parser to reparse sentence "She is studying for the exam."
Failed to use the given parser to reparse sentence "He works as a teacher."
Failed to use the given parser to reparse sentence "They are going on a vacation next week."
Failed to use the given parser to reparse sentence "I am happy today."
Failed to use the given parser to reparse sentence "She has a beautiful voice."
Sigma used: 0.5
Sigma used: 1.0
Sigma used: 0.75
Sigma used: 1.5
Sigma used: 0.25
Sigma used: 2.0
Sigma used: 1.25
Sigma used: 0.1
Sigma used: 1.8
Sigma used: 0.4
Sigma used: 1.2
Sigma used: 0.6
Sigma used: 1.4
Sigma used: 0.3
Sigma used: 1.6
Final conll score (accuracy) micro = 0.93
Final conll score (precision) macro = 0.87
Final conll score (recall) micro = 0.91
Final conll score (f1) macro = 0.89
Final conll score (accuracy) macro = 0.92
Final conll score (precision) micro = 0.94
Final conll score (recall) macro = 0.86
Final conll score (f1) micro = 0.92
Final conll score (accuracy) weighted = 0.93
Final conll score (precision) weighted = 0.94
Final conll score (recall) weighted = 0.91
Final conll score (f1) weighted = 0.92
Final conll score (accuracy) binary = 0.95
Final conll score (precision) binary = 0.96
Final conll score (recall) binary = 0.94
Using unknown dashed word vector heuristics: true
Using unknown dashed word vector heuristics: false
Using unknown dashed word vector heuristics: 0.5
Using unknown dashed word vector heuristics: 0.75
Using unknown dashed word vector heuristics: 0.25
Using unknown dashed word vector heuristics: 1
Using unknown dashed word vector heuristics: 0
Using unknown dashed word vector heuristics: 0.1
Using unknown dashed word vector heuristics: 0.9
Using unknown dashed word vector heuristics: 0.3
Using unknown dashed word vector heuristics: 0.7
Using unknown dashed word vector heuristics: 0.4
Using unknown dashed word vector heuristics: 0.6
Using unknown dashed word vector heuristics: 0.2
Using unknown dashed word vector heuristics: 0.8
False Positives: apple:0.12;banana:0.23;carrot:0.34
False Positives: dog:0.45;elephant:0.56;fish:0.67
False Positives: grape:0.78;hat:0.89;ice:0.90
False Positives: jar:0.91;key:0.92;lion:0.93
False Positives: moon:0.94;nose:0.95;orange:0.96
False Positives: pear:0.97;queen:0.98;rainbow:0.99
False Positives: sun:1.00;tree:0.01;umbrella:0.02
False Positives: vase:0.03;water:0.04;x-ray:0.05
False Positives: yarn:0.06;zoo:0.07;ant:0.08
False Positives: bear:0.09;cat:0.10;dinosaur:0.11
False Positives: eggplant:0.13;flower:0.14;giraffe:0.15
False Positives: honey:0.16;igloo:0.17;jump:0.18
False Positives: kite:0.19;lamp:0.20;mango:0.21
False Positives: nail:0.22;ocean:0.24;pizza:0.25
False Positives: quilt:0.26;rose:0.27;starfish:0.28
WARNING: Number of extracted dependencies (5) does not match yield (4):
WARNING: Number of extracted dependencies (3) does not match yield (2):
WARNING: Number of extracted dependencies (7) does not match yield (6):
WARNING: Number of extracted dependencies (4) does not match yield (5):
WARNING: Number of extracted dependencies (6) does not match yield (7):
WARNING: Number of extracted dependencies (8) does not match yield (9):
WARNING: Number of extracted dependencies (9) does not match yield (8):
WARNING: Number of extracted dependencies (10) does not match yield (11):
WARNING: Number of extracted dependencies (11) does not match yield (10):
WARNING: Number of extracted dependencies (12) does not match yield (13):
WARNING: Number of extracted dependencies (13) does not match yield (12):
WARNING: Number of extracted dependencies (14) does not match yield (15):
WARNING: Number of extracted dependencies (15) does not match yield (14):
WARNING: Number of extracted dependencies (16) does not match yield (17):
WARNING: Number of extracted dependencies (17) does not match yield (16):
x size 640 ysize 480
x size 800 ysize 600
x size 1024 ysize 768
x size 1280 ysize 720
x size 1366 ysize 768
x size 1440 ysize 900
x size 1600 ysize 900
x size 1680 ysize 1050
x size 1920 ysize 1080
x size 2560 ysize 1440
x size 3840 ysize 2160
x size 512 ysize 384
x size 320 ysize 240
x size 400 ysize 300
x size 2048 ysize 1536
Chose null node
Chose A
Chose B
Chose C
Chose D
Chose E
Chose F
Chose G
Chose H
Chose I
Chose J
Chose K
Chose L
Chose M
Chose N
Using fraction of train: 0.5
Using fraction of train: 0.75
Using fraction of train: 0.25
Using fraction of train: 0.1
Using fraction of train: 0.9
Using fraction of train: 0.6
Using fraction of train: 0.4
Using fraction of train: 0.8
Using fraction of train: 0.3
Using fraction of train: 0.7
Using fraction of train: 0.2
Using fraction of train: 0.95
Using fraction of train: 0.05
Using fraction of train: 0.85
Using fraction of train: 0.15
Grammar size: 1024
Grammar size: 512
Grammar size: 2048
Grammar size: 256
Grammar size: 4096
Grammar size: 128
Grammar size: 8192
Grammar size: 64
Grammar size: 16384
Grammar size: 32
Grammar size: 32768
Grammar size: 16
Grammar size: 65536
Grammar size: 8
Grammar size: 131072
Read in 100 training trees
Read in 50 training trees
Read in 75 training trees
Read in 25 training trees
Read in 150 training trees
Read in 200 training trees
Read in 10 training trees
Read in 5 training trees
Read in 80 training trees
Read in 120 training trees
Read in 60 training trees
Read in 40 training trees
Read in 30 training trees
Read in 90 training trees
Read in 70 training trees
Dummy column: Lorem ipsum dolor sit amet
Dummy column: The quick brown fox jumps over the lazy dog
Dummy column: Hello world!
Dummy column: To be or not to be, that is the question
Dummy column: May the force be with you
Dummy column: Winter is coming
Dummy column: I have a dream
Dummy column: E=mc^2
Dummy column: In God we trust
Dummy column: Hakuna matata
Dummy column: Carpe diem
Dummy column: Veni, vidi, vici
Dummy column: Et tu, Brute?
Dummy column: Cogito ergo sum
Dummy column: Don't panic
Used {TBSPEC2} to recognize hello.txt
Used {TBSPEC2} to recognize report.txt
Used {TBSPEC2} to recognize image.txt
Used {TBSPEC2} to recognize data.txt
Used {TBSPEC2} to recognize notes.txt
Used {TBSPEC2} to recognize config.txt
Used {TBSPEC2} to recognize readme.txt
Used {TBSPEC2} to recognize log.txt
Used {TBSPEC2} to recognize test.txt
Used {TBSPEC2} to recognize code.txt
Used {TBSPEC2} to recognize book.txt
Used {TBSPEC2} to recognize poem.txt
Used {TBSPEC2} to recognize song.txt
Used {TBSPEC2} to recognize story.txt
Used {TBSPEC2} to recognize essay.txt
position: 5 , entities[position-1] = Person(John, 3)
position: 2 , entities[position-1] = Location(London, 7)
position: 8 , entities[position-1] = Organization(IBM, 4)
position: 3 , entities[position-1] = Date(2023-10-27, 6)
position: 6 , entities[position-1] = Product(iPhone 14, 2)
position: 4 , entities[position-1] = Event(Olympics, 5)
position: 7 , entities[position-1] = Animal(dog, 1)
position: 9 , entities[position-1] = Person(Mary, 8)
position: 10 , entities[position-1] = Location(Paris, 9)
position: 1 , entities[position-1] = Organization(Google, 10)
position: 11 , entities[position-1] = Date(2023-11-01, 11)
position: 12 , entities[position-1] = Product(MacBook Pro, 12)
position: 13 , entities[position-1] = Event(Halloween, 13)
position: 14 , entities[position-1] = Animal(cat, 14)
position: 15 , entities[position-1] = Person(Alice, 15)
shuffled 100 sentences and selecting 10 sentences per thread
shuffled 50 sentences and selecting 5 sentences per thread
shuffled 200 sentences and selecting 20 sentences per thread
shuffled 75 sentences and selecting 7 sentences per thread
shuffled 25 sentences and selecting 3 sentences per thread
shuffled 150 sentences and selecting 15 sentences per thread
shuffled 125 sentences and selecting 12 sentences per thread
shuffled 175 sentences and selecting 17 sentences per thread
shuffled 80 sentences and selecting 8 sentences per thread
shuffled 40 sentences and selecting 4 sentences per thread
shuffled 60 sentences and selecting 6 sentences per thread
shuffled 90 sentences and selecting 9 sentences per thread
shuffled 30 sentences and selecting 2 sentences per thread
shuffled 140 sentences and selecting 14 sentences per thread
shuffled 45 sentences and selecting 5 sentences per thread
Training relation extraction using predicted entities: entity scores using printer HP LaserJet 1020 : 0.76
Training relation extraction using predicted entities: entity scores using printer Canon PIXMA MG3620 : 0.81
Training relation extraction using predicted entities: entity scores using printer Epson EcoTank ET-2760 : 0.79
Training relation extraction using predicted entities: entity scores using printer Brother MFC-L2710DW : 0.83
Training relation extraction using predicted entities: entity scores using printer Samsung Xpress M2020W : 0.77
Training relation extraction using predicted entities: entity scores using printer Lexmark B2236dw : 0.78
Training relation extraction using predicted entities: entity scores using printer Dell E310dw : 0.82
Training relation extraction using predicted entities: entity scores using printer Ricoh SP 150SUw : 0.80
Training relation extraction using predicted entities: entity scores using printer Kyocera ECOSYS P5021cdw : 0.84
Training relation extraction using predicted entities: entity scores using printer Xerox Phaser 3260 : 0.75
Training relation extraction using predicted entities: entity scores using printer OKI C332dn : 0.86
Training relation extraction using predicted entities: entity scores using printer Panasonic KX-MB2030 : 0.74
Training relation extraction using predicted entities: entity scores using printer Kodak Verite 55W Eco : 0.85
Training relation extraction using predicted entities: entity scores using printer Sharp MX-2614N : 0.87
LINK ( 0 , 1 ) -> ( 2 , 3 )
LINK ( 4 , 5 ) -> ( 6 , 7 )
LINK ( 8 , 9 ) -> ( 10 , 11 )
LINK ( 12 , 13 ) -> ( 14 , 15 )
LINK ( 16 , 17 ) -> ( 18 , 19 )
LINK ( 20 , 21 ) -> ( 22 , 23 )
LINK ( 24 , 25 ) -> ( 26 , 27 )
LINK ( 28 , 29 ) -> ( 30 , 31 )
LINK ( 32 , 33 ) -> ( 34 , 35 )
LINK ( 36 , 37 ) -> ( 38 , 39 )
LINK ( -1 , -2 ) -> ( -3 , -4 )
LINK ( -5 , -6 ) -> ( -7 , -8 )
LINK ( -9 , -10 ) -> ( -11 , -12 )
LINK ( -13 , -14 ) -> ( -15 , -16 )
LINK ( -17 , -18 ) -> ( -19 , -20 )
##best sigma: 0.01
##best sigma: 0.05
##best sigma: 0.1
##best sigma: 0.15
##best sigma: 0.2
##best sigma: 0.25
##best sigma: 0.3
##best sigma: 0.35
##best sigma: 0.4
##best sigma: 0.45
##best sigma: 0.5
##best sigma: 0.55
##best sigma: 0.6
##best sigma: 0.65
##best sigma: 0.7
Visited: [true, false, false, true, false]
Visited: [false, false, true, true, true]
Visited: [true, true, false, false, false]
Visited: [false, true, true, false, true]
Visited: [true, false, true, false, false]
Visited: [false, false, false, true, true]
Visited: [true, true, true, false, false]
Visited: [false, true, false, true, false]
Visited: [true, false, false, false, true]
Visited: [false, false, true, false, false]
Visited: [true, true, false, true, true]
Visited: [false, true, false, false, true]
Visited: [true, false, true, true, false]
Visited: [false, true, true, true,false]
Visited: [true,true,true,true,true]
Ignoring the following labels: [spam]
Ignoring the following labels: [urgent]
Ignoring the following labels: [duplicate]
Ignoring the following labels: [outdated]
Ignoring the following labels: [invalid]
Ignoring the following labels: [low priority]
Ignoring the following labels: [test]
Ignoring the following labels: [bug]
Ignoring the following labels: [feature request]
Ignoring the following labels: [enhancement]
Ignoring the following labels: [question]
Ignoring the following labels: [feedback]
Ignoring the following labels: [suggestion]
Ignoring the following labels: [help wanted]
Ignoring the following labels: [documentation]
Checking 0 head is the / DT
Checking 1 head is a / DT
Checking 2 head is an / DT
Checking 3 head is dog / NN
Checking 4 head is cat / NN
Checking 5 head is big / JJ
Checking 6 head is small / JJ
Checking 7 head is red / JJ
Checking 8 head is blue / JJ
Checking 9 head is ran / VBD
Checking 10 head is run / VB
Checking 11 head is saw / VBD
Checking 12 head is see / VB
Checking 13 head is and / CC
Checking 14 head is or / CC
Before feature count thresholding, numFeatures = 1024
Before feature count thresholding, numFeatures = 512
Before feature count thresholding, numFeatures = 256
Before feature count thresholding, numFeatures = 128
Before feature count thresholding, numFeatures = 64
Before feature count thresholding, numFeatures = 32
Before feature count thresholding, numFeatures = 16
Before feature count thresholding, numFeatures = 8
Before feature count thresholding, numFeatures = 4
Before feature count thresholding, numFeatures = 2
Before feature count thresholding, numFeatures = 1
Before feature count thresholding, numFeatures = 2048
Before feature count thresholding, numFeatures = 4096
Before feature count thresholding, numFeatures = 8192
Before feature count thresholding, numFeatures = 16384
en DEPENDENCY_GRAMMAR
zh DEPENDENCY_GRAMMAR
fr DEPENDENCY_GRAMMAR
de DEPENDENCY_GRAMMAR
es DEPENDENCY_GRAMMAR
ja DEPENDENCY_GRAMMAR
ru DEPENDENCY_GRAMMAR
ar DEPENDENCY_GRAMMAR
hi DEPENDENCY_GRAMMAR
pt DEPENDENCY_GRAMMAR
ko DEPENDENCY_GRAMMAR
it DEPENDENCY_GRAMMAR
nl DEPENDENCY_GRAMMAR
sv DEPENDENCY_GRAMMAR
tr DEPENDENCY_GRAMMAR
Pipeline setup: 0.12 sec.
Pipeline setup: 0.05 sec.
Pipeline setup: 0.09 sec.
Pipeline setup: 0.07 sec.
Pipeline setup: 0.11 sec.
Pipeline setup: 0.08 sec.
Pipeline setup: 0.06 sec.
Pipeline setup: 0.10 sec.
Pipeline setup: 0.04 sec.
Pipeline setup: 0.13 sec.
Pipeline setup: 0.03 sec.
Pipeline setup: 0.14 sec.
Pipeline setup: 0.15 sec.
Pipeline setup: 0.02 sec.
Pipeline setup: 0.01 sec.
Before condense, activeFeatures = 12
Before condense, activeFeatures = 9
Before condense, activeFeatures = 15
Before condense, activeFeatures = 10
Before condense, activeFeatures = 8
Before condense, activeFeatures = 11
Before condense, activeFeatures = 13
Before condense, activeFeatures = 7
Before condense, activeFeatures = 14
Before condense, activeFeatures = 6
Before condense, activeFeatures = 16
Before condense, activeFeatures = 5
Before condense, activeFeatures = 17
Before condense, activeFeatures = 4
Before condense, activeFeatures = 18
Trying sieves: SieveA,SieveB,SieveC
Trying sieves: SieveD,SieveE
Trying sieves: SieveF,SieveG,SieveH,SieveI
Trying sieves: SieveJ
Trying sieves: SieveK,SieveL,SieveM
Trying sieves: SieveN,SieveO,SieveP
Trying sieves: SieveQ,SieveR
Trying sieves: SieveS,SieveT,SieveU
Trying sieves: SieveV,SieveW
Trying sieves: SieveX,SieveY,SieveZ
Trying sieves: SieveA1,SieveA2,SieveA3
Trying sieves: SieveB1,SieveB2
Trying sieves: SieveC1,SieveC2,SieveC3,SieveC4
Trying sieves: SieveD1
Trying sieves: SieveE1,SieveE2,SieveE3
Received unhanded output format in scenegraph ' jpg '
Received unhanded output format in scenegraph ' png '
Received unhanded output format in scenegraph ' bmp '
Received unhanded output format in scenegraph ' gif '
Received unhanded output format in scenegraph ' tiff '
Received unhanded output format in scenegraph ' svg '
Received unhanded output format in scenegraph ' pdf '
Received unhanded output format in scenegraph ' docx '
Received unhanded output format in scenegraph ' txt '
Received unhanded output format in scenegraph ' csv '
Received unhanded output format in scenegraph ' json '
Received unhanded output format in scenegraph ' xml '
Received unhanded output format in scenegraph ' html '
Received unhanded output format in scenegraph ' mp4 '
Received unhanded output format in scenegraph ' wav '
CleanXML: starting tokens: 12
CleanXML: starting tokens: 34
CleanXML: starting tokens: 56
CleanXML: starting tokens: 78
CleanXML: starting tokens: 90
CleanXML: starting tokens: 23
CleanXML: starting tokens: 45
CleanXML: starting tokens: 67
CleanXML: starting tokens: 89
CleanXML: starting tokens: 10
CleanXML: starting tokens: 32
CleanXML: starting tokens: 54
CleanXML: starting tokens: 76
CleanXML: starting tokens: 98
CleanXML: starting tokens: 11
Batch size: 32
Batch size: 64
Batch size: 128
Batch size: 256
Batch size: 512
Batch size: 16
Batch size: 48
Batch size: 96
Batch size: 192
Batch size: 384
Batch size: 24
Batch size: 72
Batch size: 144
Batch size: 288
Batch size: 576
hasPassiveProgressiveAuxiliary returns true
hasPassiveProgressiveAuxiliary returns false
hasPassiveProgressiveAuxiliary returns null
hasPassiveProgressiveAuxiliary returns undefined
hasPassiveProgressiveAuxiliary returns error
hasPassiveProgressiveAuxiliary returns 0
hasPassiveProgressiveAuxiliary returns 1
hasPassiveProgressiveAuxiliary returns -1
hasPassiveProgressiveAuxiliary returns NaN
hasPassiveProgressiveAuxiliary returns "true"
hasPassiveProgressiveAuxiliary returns "false"
hasPassiveProgressiveAuxiliary returns "null"
hasPassiveProgressiveAuxiliary returns "undefined"
hasPassiveProgressiveAuxiliary returns "error"
hasPassiveProgressiveAuxiliary returns ""
No merge: c is ' 0 '
No merge: c is ' 1 '
No merge: c is ' a '
No merge: c is ' b '
No merge: c is ' x '
No merge: c is ' y '
No merge: c is ' z '
No merge: c is ' * '
No merge: c is ' ? '
No merge: c is ' ! '
No merge: c is ' @ '
No merge: c is ' # '
No merge: c is ' $ '
No merge: c is ' % '
No merge: c is ' & '
Checking Document 4567 span ( 12 , 34 )
Checking Document 8910 span ( 56 , 78 )
Checking Document 1123 span ( 90 , 112 )
Checking Document 4456 span ( 134 , 156 )
Checking Document 7789 span ( 178 , 200 )
Checking Document 9900 span ( 222 , 244 )
Checking Document 1234 span ( 266 , 288 )
Checking Document 5678 span ( 310 , 332 )
Checking Document 9012 span ( 354 , 376 )
Checking Document 3456 span ( 398 , 420 )
Checking Document 7890 span ( 442 , 464 )
Checking Document 1122 span ( 486 , 508 )
Checking Document 3344 span ( 530 , 552 )
Checking Document 5566 span ( 574 , 596 )
Checking Document 7788 span ( 618 , 640 )
# curWordMinFeatureThresh = 0.1
# curWordMinFeatureThresh = 0.05
# curWordMinFeatureThresh = 0.2
# curWordMinFeatureThresh = 0.15
# curWordMinFeatureThresh = 0.25
# curWordMinFeatureThresh = 0.3
# curWordMinFeatureThresh = 0.35
# curWordMinFeatureThresh = 0.4
# curWordMinFeatureThresh = 0.45
# curWordMinFeatureThresh = 0.5
# curWordMinFeatureThresh = 0.55
# curWordMinFeatureThresh = 0.6
# curWordMinFeatureThresh = 0.65
# curWordMinFeatureThresh = 0.7
# curWordMinFeatureThresh = 0.75
Non-zero parameters: 12 / 100 ( 12.0 %)
Non-zero parameters: 23 / 150 ( 15.3 %)
Non-zero parameters: 34 / 200 ( 17.0 %)
Non-zero parameters: 45 / 250 ( 18.0 %)
Non-zero parameters: 56 / 300 ( 18.7 %)
Non-zero parameters: 67 / 350 ( 19.1 %)
Non-zero parameters: 78 / 400 ( 19.5 %)
Non-zero parameters: 89 / 450 ( 19.8 %)
Non-zero parameters: 100 / 500 ( 20.0 %)
Non-zero parameters: 111 / 550 ( 20.2 %)
Non-zero parameters: 122 / 600 ( 20.3 %)
Non-zero parameters: 133 / 650 ( 20.5 %)
Non-zero parameters: 144 / 700 ( 20.6 %)
Non-zero parameters: 155 / 750 ( 20.7 %)
Non-zero parameters: 166 / 800 ( 20.8 %)
Parsing of sentence ran out of memory (length= 23 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 47 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 12 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 36 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 29 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 41 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 18 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 32 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 25 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 39 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 15 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 28 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 21 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 34 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 17 ). Will ignore and try to continue.
Binary score 1010 : 1111
Binary score 0101 : 1001
Binary score 1100 : 0011
Binary score 0110 : 0100
Binary score 1001 : 1010
Binary score 0011 : 1100
Binary score 1110 : 0111
Binary score 0001 : 0000
Binary score 1011 : 1000
Binary score 0100 : 0010
Binary score 1101 : 1110
Binary score 0111 : 0101
Binary score 1000 : 1011
Binary score 0010 : 1101
Binary score 1111 : 0001
Warning: unparseable date 2023-10-32
Warning: unparseable date 2023-02-29
Warning: unparseable date 2023-13-01
Warning: unparseable date 2023-04-31
Warning: unparseable date 2023-06-31
Warning: unparseable date 2023-09-31
Warning: unparseable date 2023-11-31
Warning: unparseable date 2023-01-00
Warning: unparseable date 2023-03-00
Warning: unparseable date 2023-05-00
Warning: unparseable date 2023-07-00
Warning: unparseable date 2023-08-00
Warning: unparseable date 2023-10-00
Warning: unparseable date 2023-12-00
Warning: unparseable date 2023/10/26
-> average Score: 78.5
-> average Score: 63.2
-> average Score: 85.9
-> average Score: 72.4
-> average Score: 90.1
-> average Score: 66.7
-> average Score: 81.3
-> average Score: 74.6
-> average Score: 69.8
-> average Score: 87.4
-> average Score: 76.2
-> average Score: 82.7
-> average Score: 79.3
-> average Score: 68.5
-> average Score: 84.6
0 . Compacted grammar for NP from 12 arcs to 4 arcs.
1 . Compacted grammar for VP from 8 arcs to 3 arcs.
2 . Compacted grammar for S from 15 arcs to 5 arcs.
3 . Compacted grammar for PP from 6 arcs to 2 arcs.
4 . Compacted grammar for ADJP from 9 arcs to 3 arcs.
5 . Compacted grammar for ADVP from 7 arcs to 2 arcs.
6 . Compacted grammar for NNP from 10 arcs to 4 arcs.
7 . Compacted grammar for VBD from 11 arcs to 4 arcs.
9 . Compacted grammar for JJ from 8 arcs to 3 arcs.
10 . Compacted grammar for RB from 6 arcs to 2 arcs.
11 . Compacted grammar for IN from 7 arcs to 2 arcs.
12 . Compacted grammar for NN from 9 arcs to 3 arcs.
13 . Compacted grammar for PRP from 6 arcs to 2 arcs.
Skipping non-boundary at pos 0 , since space in the input.
Skipping non-boundary at pos 5 , since space in the input.
Skipping non-boundary at pos 12 , since space in the input.
Skipping non-boundary at pos 17 , since space in the input.
Skipping non-boundary at pos 23 , since space in the input.
Skipping non-boundary at pos 29 , since space in the input.
Skipping non-boundary at pos 34 , since space in the input.
Skipping non-boundary at pos 41 , since space in the input.
Skipping non-boundary at pos 46 , since space in the input.
Skipping non-boundary at pos 52 , since space in the input.
Skipping non-boundary at pos 58 , since space in the input.
Skipping non-boundary at pos 63 , since space in the input.
Skipping non-boundary at pos 69 , since space in the input.
Skipping non-boundary at pos 74 , since space in the input.
Skipping non-boundary at pos 81 , since space in the input.
top: A bottom: B
top: C bottom: D
top: E bottom: F
top: G bottom: H
top: I bottom: J
top: K bottom: L
top: M bottom: N
top: O bottom: P
top: Q bottom: R
top: S bottom: T
top: U bottom: V
top: W bottom: X
top: Y bottom: Z
top: AA bottom: BB
Serializing class index to /home/user1/index.ser ... done.
Serializing class index to C:\Users\user2\index.ser ... done.
Serializing class index to /tmp/index.ser ... done.
Serializing class index to /var/lib/index.ser ... done.
Serializing class index to /opt/index.ser ... done.
Serializing class index to /data/index.ser ... done.
Serializing class index to /mnt/index.ser ... done.
Serializing class index to /dev/index.ser ... done.
Serializing class index to /usr/local/index.ser ... done.
Serializing class index to /etc/index.ser ... done.
Serializing class index to /root/index.ser ... done.
Serializing class index to /media/index.ser ... done.
Serializing class index to /srv/index.ser ... done.
Serializing class index to /run/index.ser ... done.
Serializing class index to /sys/index.ser ... done.
baseClassifiers index 0 : /home/user/data/train.csv
baseClassifiers index 1 : /home/user/data/test.csv
baseClassifiers index 2 : /home/user/data/valid.csv
baseClassifiers index 3 : /home/user/models/model1.pkl
baseClassifiers index 4 : /home/user/models/model2.pkl
baseClassifiers index 5 : /home/user/models/model3.pkl
baseClassifiers index 6 : /home/user/results/result1.txt
baseClassifiers index 7 : /home/user/results/result2.txt
baseClassifiers index 8 : /home/user/results/result3.txt
baseClassifiers index 9 : /home/user/logs/log1.log
baseClassifiers index 10 : /home/user/logs/log2.log
baseClassifiers index 11 : /home/user/logs/log3.log
baseClassifiers index 12 : /home/user/config/config1.json
baseClassifiers index 13 : /home/user/config/config2.json
baseClassifiers index 14 : /home/user/config/config3.json
baseClassifiers index 0 : /home/user/data/train.csv
baseClassifiers index 1 : /home/user/data/test.csv
baseClassifiers index 2 : /home/user/data/valid.csv
baseClassifiers index 3 : /home/user/models/model1.pkl
baseClassifiers index 4 : /home/user/models/model2.pkl
baseClassifiers index 5 : /home/user/models/model3.pkl
baseClassifiers index 6 : /home/user/results/result1.txt
baseClassifiers index 7 : /home/user/results/result2.txt
baseClassifiers index 8 : /home/user/results/result3.txt
baseClassifiers index 9 : /home/user/logs/log1.log
baseClassifiers index 10 : /home/user/logs/log2.log
baseClassifiers index 11 : /home/user/logs/log3.log
baseClassifiers index 12 : /home/user/config/config1.ini
baseClassifiers index 13 : /home/user/config/config2.ini
baseClassifiers index 14 : /home/user/config/config3.ini
For sentence s.get(CoreAnnotations.TextAnnotation.class) = "The quick brown fox jumps over the lazy dog."
For sentence s.get(CoreAnnotations.TextAnnotation.class) = "She sells seashells by the seashore."
For sentence s.get(CoreAnnotations.TextAnnotation.class) = "How much wood would a woodchuck chuck if a woodchuck could chuck wood?"
For sentence s.get(CoreAnnotations.TextAnnotation.class) = "To be or not to be, that is the question."
For sentence s.get(CoreAnnotations.TextAnnotation.class) = "May the force be with you."
For sentence s.get(CoreAnnotations.TextAnnotation.class) = "I have a dream that one day this nation will rise up and live out the true meaning of its creed."
For sentence s.get(CoreAnnotations.TextAnnotation.class) = "It was the best of times, it was the worst of times."
For sentence s.get(CoreAnnotations.TextAnnotation.class) = "You're a wizard, Harry."
For sentence s.get(CoreAnnotations.TextAnnotation.class) = "Houston, we have a problem."
For sentence s.get(CoreAnnotations.TextAnnotation.class) = "Elementary, my dear Watson."
For sentence s.get(CoreAnnotations.TextAnnotation.class) = "Frankly, my dear, I don't give a damn."
For sentence s.get(CoreAnnotations.TextAnnotation.class) = "I'm the king of the world!"
For sentence s.get(CoreAnnotations.TextAnnotation.class) = "E.T. phone home."
For sentence s.get(CoreAnnotations.TextAnnotation.class) = "In the beginning God created the heaven and the earth."
For sentence s.get(CoreAnnotations.TextAnnotation.class) = "Four score and seven years ago our fathers brought forth on this continent, a new nation."
All pairs marked: 1634719186460
All pairs marked: 1634719186461
All pairs marked: 1634719186462
All pairs marked: 1634719186463
All pairs marked: 1634719186464
All pairs marked: 1634719186465
All pairs marked: 1634719186466
All pairs marked: 1634719186467
All pairs marked: 1634719186468
All pairs marked: 1634719186469
All pairs marked: 1634719186470
All pairs marked: 1634719186471
All pairs marked: 1634719186472
All pairs marked: 1634719186473
All pairs marked: 1634719186474
Lexicon not found: words.txt
Lexicon not found: names.csv
Lexicon not found: phrases.json
Lexicon not found: synonyms.xml
Lexicon not found: acronyms.dat
Lexicon not found: slang.yaml
Lexicon not found: categories.ini
Lexicon not found: verbs.tsv
Lexicon not found: nouns.xlsx
Lexicon not found: adjectives.docx
Lexicon not found: idioms.pdf
Lexicon not found: proverbs.rtf
Lexicon not found: antonyms.html
Lexicon not found: abbreviations.md
Lexicon not found: metaphors.pptx
Entry 5 is entry 1 of unary score 0.67
Entry 12 is entry 4 of unary score 0.54
Entry 9 is entry 3 of unary score 0.76
Entry 3 is entry 2 of unary score 0.62
Entry 7 is entry 1 of unary score 0.71
Entry 10 is entry 2 of unary score 0.59
Entry 8 is entry 4 of unary score 0.65
Entry 6 is entry 3 of unary score 0.73
Entry 4 is entry 1 of unary score 0.68
Entry 11 is entry 2 of unary score 0.57
Entry 2 is entry 4 of unary score 0.61
Entry 1 is entry 3 of unary score 0.74
Entry 13 is entry 1 of unary score 0.53
Entry 15 is entry 4 of unary score 0.55
Entry 14 is entry 2 of unary score 0.58
Language: English
Language: Deutsch
Language: Italiano
Language: Polski
Language: Nederlands
Length: 5
Length: 12
Length: 9
Length: 7
Length: 10
Length: 6
Length: 8
Length: 11
Length: 4
Length: 13
Length: 3
Length: 14
Length: 2
Length: 15
Length: 1
total feats: 12 , populated: 8
total feats: 9 , populated: 6
total feats: 15 , populated: 10
total feats: 11 , populated: 7
total feats: 10 , populated: 9
total feats: 13 , populated: 11
total feats: 8 , populated: 5
total feats: 14 , populated: 12
total feats: 7 , populated: 4
total feats: 16 , populated: 13
total feats: 6 , populated: 3
total feats: 17 , populated: 14
total feats: 5 , populated: 2
total feats: 18 , populated: 15
total feats: 4 , populated: 1
Longest sentence is of length: 23
Longest sentence is of length: 18
Longest sentence is of length: 27
Longest sentence is of length: 21
Longest sentence is of length: 25
Longest sentence is of length: 19
Longest sentence is of length: 24
Longest sentence is of length: 22
Longest sentence is of length: 26
Longest sentence is of length: 20
Longest sentence is of length: 28
Longest sentence is of length: 17
Longest sentence is of length: 29
Longest sentence is of length: 16
Longest sentence is of length: 30
Longest sentence is of length: 27
Longest sentence is of length: 32
Longest sentence is of length: 19
Longest sentence is of length: 24
Longest sentence is of length: 29
Longest sentence is of length: 22
Longest sentence is of length: 31
Longest sentence is of length: 26
Longest sentence is of length: 18
Longest sentence is of length: 28
Longest sentence is of length: 21
Longest sentence is of length: 25
Longest sentence is of length: 30
Longest sentence is of length: 20
Longest sentence is of length: 23
HashMap @ 1000 keys: 0.12 secs/2million gets
HashMap @ 5000 keys: 0.25 secs/2million gets
HashMap @ 10000 keys: 0.35 secs/2million gets
HashMap @ 20000 keys: 0.48 secs/2million gets
HashMap @ 30000 keys: 0.62 secs/2million gets
HashMap @ 40000 keys: 0.75 secs/2million gets
HashMap @ 50000 keys: 0.88 secs/2million gets
HashMap @ 60000 keys: 1.02 secs/2million gets
HashMap @ 70000 keys: 1.15 secs/2million gets
HashMap @ 80000 keys: 1.28 secs/2million gets
HashMap @ 90000 keys: 1.42 secs/2million gets
HashMap @ 100000 keys: 1.55 secs/2million gets
HashMap @ 110000 keys: 1.68 secs/2million gets
HashMap @ 120000 keys: 1.82 secs/2million gets
HashMap @ 130000 keys: 1.95 secs/2million gets
WARNING: buildGraphsProperty set to true, but com.example.parser.TLPParams does not support dependencies
WARNING: buildGraphsProperty set to true, but org.apache.commons.cli.TLPParams does not support dependencies
WARNING: buildGraphsProperty set to true, but net.sourceforge.argparse4j.TLPParams does not support dependencies
WARNING: buildGraphsProperty set to true, but this.parser.getTLPParams().getClass() does not support dependencies
WARNING: buildGraphsProperty set to true, but java.lang.Object.TLPParams does not support dependencies
WARNING: buildGraphsProperty set to true, but null.TLPParams does not support dependencies
WARNING: buildGraphsProperty set to true, but com.google.gson.TLPParams does not support dependencies
WARNING: buildGraphsProperty set to true, but edu.stanford.nlp.trees.TLPParams does not support dependencies
WARNING: buildGraphsProperty set to true, but com.fasterxml.jackson.databind.TLPParams does not support dependencies
WARNING: buildGraphsProperty set to true, but org.json.simple.TLPParams does not support dependencies
WARNING: buildGraphsProperty set to true, but org.yaml.snakeyaml.TLPParams does not support dependencies
WARNING: buildGraphsProperty set to true, but javax.xml.parsers.TLPParams does not support dependencies
WARNING: buildGraphsProperty set to true, but org.w3c.dom.TLPParams does not support dependencies
WARNING: buildGraphsProperty set to true, but org.xml.sax.TLPParams does not support dependencies
WARNING: buildGraphsProperty set to true, but java.util.Properties.TLPParams does not support dependencies
CleanXML: process returning: [token: "The lemma: "the pos: "DT"]
CleanXML: process returning: [token: "dog lemma: "dog pos: "NN"]
CleanXML: process returning: [token: ". lemma: ". pos: "."]
CleanXML: process returning: [token: "She lemma: "she pos: "PRP"]
CleanXML: process returning: [token: "likes lemma: "like pos: "VBZ"]
CleanXML: process returning: [token: "apples lemma: "apple pos: "NNS"]
CleanXML: process returning: [token:  lemma:  pos: "]
CleanXML: process returning: [token: "but lemma: "but pos: "CC"]
CleanXML: process returning: [token: "he lemma: "he pos: "PRP"]
CleanXML: process returning: [token: "prefers lemma: "prefer pos: "VBZ"]
CleanXML: process returning: [token: "bananas lemma: "banana pos: "NNS"]
CleanXML: process returning: [token: ". lemma ". pos "."]
Built edges: 548
Built edges: 1056
Built edges: 732
Built edges: 369
Built edges: 812
Built edges: 240
Built edges: 965
Built edges: 604
Built edges: 472
Built edges: 159
Built edges: 624
Built edges: 875
Built edges: 213
Built edges: 456
Built edges: 754
Id: 123 Quote: "Lorem ipsum dolor sit amet.
Id: 987 Quote: "Quisque ullamcorper justo eget ligula dapibus, at venenatis sem tincidunt.
Id: 532 Quote: "Sed non odio sit amet ligula tristique aliquet.
Id: 765 Quote: "Fusce auctor purus vitae justo egestas, vel tincidunt metus auctor.
Id: 111 Quote: "Pellentesque fermentum ligula at ex bibendum, at malesuada velit rhoncus.
Id: 444 Quote: "Nam vel nisi eu odio fermentum tincidunt.
Id: 666 Quote: "Vestibulum eu purus sit amet tortor suscipit pharetra.
Id: 222 Quote: "Morbi bibendum urna ac elit aliquam, in euismod orci ultricies.
Id: 999 Quote: "Praesent vulputate metus ac sem auctor, vel consequat justo efficitur.
Id: 777 Quote: "Suspendisse nec velit eget tortor aliquam sodales vel sit amet mauris.
Id: 888 Quote: "Curabitur tristique orci in libero eleifend, id varius metus hendrerit.
Id: 333 Quote: "Proin vitae sem eu nunc convallis varius nec eu tellus.
Id: 456 Quote: "Duis ac ligula sit amet felis aliquet suscipit in ut purus.
Id: 678 Quote: "Integer sed urna at odio bibendum iaculis.
Id: 135 Quote: "Ut eu purus eget ligula pharetra tristique sit amet a neque.
#lemmas: 12
#lemmas: 8
#lemmas: 16
#lemmas: 10
#lemmas: 7
#lemmas: 11
#lemmas: 9
#lemmas: 14
#lemmas: 13
#lemmas: 15
#lemmas: 10
#lemmas: 17
#lemmas: 6
#lemmas: 12
#lemmas: 8
Derivative norm derivativeNorm < 0.52 : quitting
Derivative norm convergenceDerivativeNorm < 1.21 : quitting
Derivative norm derivativeNorm < 0.88 : quitting
Derivative norm convergenceDerivativeNorm < 1.05 : quitting
Derivative norm derivativeNorm < 0.69 : quitting
Derivative norm convergenceDerivativeNorm < 1.34 : quitting
Derivative norm derivativeNorm < 0.77 : quitting
Derivative norm convergenceDerivativeNorm < 1.12 : quitting
Derivative norm derivativeNorm < 0.93 : quitting
Derivative norm convergenceDerivativeNorm < 1.18 : quitting
Derivative norm derivativeNorm < 0.61 : quitting
Derivative norm convergenceDerivativeNorm < 1.29 : quitting
Derivative norm derivativeNorm < 0.75 : quitting
Derivative norm convergenceDerivativeNorm < 1.09 : quitting
Derivative norm derivativeNorm < 0.99 : quitting
Loading character dictionary file from filename [done].
Loading character dictionary file from data_file [done].
Loading character dictionary file from dict123.txt [done].
Loading character dictionary file from wordlist.txt [done].
Loading character dictionary file from dictionary.dat [done].
Loading character dictionary file from lexicon.db [done].
Loading character dictionary file from custom_dict.bin [done].
Loading character dictionary file from dict.txt [done].
Loading character dictionary file from lookup_table.csv [done].
Loading character dictionary file from lang_dict.json [done].
Loading character dictionary file from character_data.xml [done].
Loading character dictionary file from dictionary_data.ini [done].
Loading character dictionary file from glossary_file.txt [done].
Loading character dictionary file from terms.csv [done].
Loading character dictionary file from vocab.txt [done].
depth: 12.5 {min: 10.2 max: 14.7 } s: 1.2 0.8 0.9
depth: 8.3 {min: 6.5 max: 10.1 } s: 0.9 0.7 0.6
depth: 15.7 {min: 13.4 max: 18.2 } s: 1.5 1.1 1.2
depth: 10.4 {min: 8.6 max: 12.3 } s: 1.0 0.9 0.8
depth: 9.6 {min: 7.8 max: 11.5 } s: 0.8 0.6 0.7
depth: 13.2 {min: 11.1 max: 15.4 } s: 1.3 1.0 1.1
depth: 11.8 {min: 9.9 max: 13.6 } s: 1.1 0.9 1.0
depth: 14.3 {min: 12.5 max: 16.2 } s: 1.4 1.2 1.3
depth: 7.5 {min: 5.8 max: 9.3 } s: 0.7 0.5 0.6
depth: 16.4 {min: 14.2 max: 18.7 } s: 1.6 1.3 1.4
mergeDocuments: Background symbol is #
mergeDocuments: Background symbol is @
mergeDocuments: Background symbol is &
mergeDocuments: Background symbol is *
mergeDocuments: Background symbol is $
mergeDocuments: Background symbol is %
mergeDocuments: Background symbol is +
mergeDocuments: Background symbol is =
mergeDocuments: Background symbol is -
mergeDocuments: Background symbol is _
mergeDocuments: Background symbol is /
mergeDocuments: Background symbol is \
mergeDocuments: Background symbol is ?
mergeDocuments: Background symbol is !
mergeDocuments: Background symbol is ~
quote w/ no mention. ID: 5678
quote w/ no mention. ID: 4321
quote w/ no mention. ID: 9876
quote w/ no mention. ID: 2468
quote w/ no mention. ID: 1357
quote w/ no mention. ID: 8642
quote w/ no mention. ID: 9513
quote w/ no mention. ID: 7294
quote w/ no mention. ID: 1836
quote w/ no mention. ID: 4725
quote w/ no mention. ID: 6190
quote w/ no mention. ID: 3048
quote w/ no mention. ID: 7561
quote w/ no mention. ID: 8290
quote w/ no mention. ID: 1407
Quantifiable: error! tag is missing
Quantifiable: error! tag is invalid
Quantifiable: error! tag is empty
Quantifiable: error! tag is too long
Quantifiable: error! tag is not supported
Quantifiable: error! tag is duplicated
Quantifiable: error! tag is corrupted
Quantifiable: error! tag is outdated
Quantifiable: error! tag is mismatched
Quantifiable: error! tag is unauthorized
Quantifiable: error! tag is undefined
Quantifiable: error! tag is locked
Quantifiable: error! tag is expired
Quantifiable: error! tag is incompatible
Quantifiable: error! tag is malformed
regCost: 0.01
regCost: 0.05
regCost: 0.1
regCost: 0.2
regCost: 0.3
regCost: 0.4
regCost: 0.5
regCost: 0.6
regCost: 0.7
regCost: 0.8
regCost: 0.9
regCost: 1.0
regCost: 1.5
regCost: 2.0
regCost: 2.5
Remapping labels as follows: remap(0, 1, 2, 3) -> (a, b, c, d)
Remapping labels as follows: remap(4, 5, 6) -> (x, y, z)
Remapping labels as follows: remap(7, 8) -> (m, n)
Remapping labels as follows: remap(9) -> (w)
Remapping labels as follows: remap(10, 11) -> (p, q)
Remapping labels as follows: remap(12, 13, 14) -> (e, f, g)
Remapping labels as follows: remap(15, 16) -> (r, s)
Remapping labels as follows: remap(17) -> (t)
Remapping labels as follows: remap(18, 19) -> (u, v)
Remapping labels as follows: remap(20) -> (o)
Remapping labels as follows: remap(21, 22) -> (h, i)
Remapping labels as follows: remap(23) -> (j)
Remapping labels as follows: remap(24) -> (k)
Remapping labels as follows: remap(25) -> (l)
Remapping labels as follows: remap() -> ()
Sentence 1 quoted= "The sky is blue."
Sentence 2 quoted= "To be or not to be, that is the question."
Sentence 3 quoted= "I have a dream today."
Sentence 4 quoted= "You can't handle the truth!"
Sentence 5 quoted= "May the Force be with you."
Sentence 6 quoted= "It was the best of times, it was the worst of times."
Sentence 7 quoted= "Four score and seven years ago..."
Sentence 8 quoted= "Houston, we have a problem."
Sentence 9 quoted= "I'm the king of the world!"
Sentence 10 quoted= "Elementary, my dear Watson."
Sentence 11 quoted= "Frankly, my dear, I don't give a damn."
Sentence 12 quoted= "That's one small step for man, one giant leap for mankind."
Sentence 13 quoted= "The only thing we have to fear is fear itself."
Sentence 14 quoted= "E.T. phone home."
Sentence 15 quoted= "Carpe diem. Seize the day, boys."
Tagging of sentence ran out of memory. Will ignore and continue: [The, quick, brown, fox, jumps, over, the, lazy, dog]
Tagging of sentence ran out of memory. Will ignore and continue: [She, loves, to, read, books, of, all, genres]
Tagging of sentence ran out of memory. Will ignore and continue: [This, is, a, very, long, and, complex, sentence, that, has, many, words, and, clauses]
Tagging of sentence ran out of memory. Will ignore and continue: [What, do, you, think, of, the, new, movie?]
Tagging of sentence ran out of memory. Will ignore and continue: [He, went, to, the, store, to, buy, some, milk]
Tagging of sentence ran out of memory. Will ignore and continue: [They, are, having, a, great, time, at, the, party]
Tagging of sentence ran out of memory. Will ignore and continue: [She, is, a, talented, singer]
Tagging of sentence ran out of memory. Will ignore and continue: [It's a beautiful day today]
Tagging of sentence ran out of memory. Will ignore and continue: [How are you doing?]
Tagging of sentence ran out of memory. Will ignore and continue: [I like pizza with cheese and pepperoni]
Tagging of sentence ran out of memory. Will ignore and continue: [He is studying hard for the exam]
Tagging of sentence ran out of memory. Will ignore and continue: [She has a cute cat named Fluffy]
Tagging of sentence ran out of memory. Will ignore and continue: [They went to the park to play soccer]
Tagging of sentence ran out of memory. Will ignore and continue: [This is a short sentence]
Tagging of sentence ran out of memory. Will ignore and continue: [Who are you?]
False Negatives: the:0.12;of:0.08;and:0.07;to:0.06;a:0.05
False Negatives: is:0.11;in:0.09;that:0.07;for:0.06;it:0.04
False Negatives: he:0.13;was:0.10;his:0.08;with:0.07;as:0.05
False Negatives: she:0.14;her:0.11;had:0.09;but:0.06;not:0.04
False Negatives: they:0.12;them:0.10;their:0.08;were:0.07;by:0.05
False Negatives: be:0.11;have:0.09;are:0.07;on:0.06;at:0.04
False Negatives: this:0.13;that:0.10;these:0.08;those:0.07;if:0.05
False Negatives: from:0.12;or:0.09;an:0.07;will:0.06;can:0.04
False Negatives: which:0.14;who:0.11;what:0.09;when:0.07;where:0.05
False Negatives: how:0.13;why:0.10;because:0.08;so:0.06;then:0.04
False Negatives: you:0.12;yours:0.09;yourself:0.07;yourselves :  006 ;  you're :  004
Experiments error: for y= 0.5 , ptildeY(y)= 0.25 but Sum_x ptildeXY(x,y)= 0.3
Experiments error: for y= 1 , ptildeY(y)= 0.4 but Sum_x ptildeXY(x,y)= 0.35
Experiments error: for y= -1 , ptildeY(y)= 0.1 but Sum_x ptildeXY(x,y)= 0.15
Experiments error: for y= 2 , ptildeY(y)= 0.05 but Sum_x ptildeXY(x,y)= 0.08
Experiments error: for y= -2 , ptildeY(y)= 0.02 but Sum_x ptildeXY(x,y)= 0.03
Experiments error: for y= 0.75 , ptildeY(y)= 0.18 but Sum_x ptildeXY(x,y)= 0.2
Experiments error: for y= -0.75 , ptildeY(y)= 0.07 but Sum_x ptildeXY(x,y)= 0.1
Experiments error: for y= 1.5 , ptildeY(y)= 0.03 but Sum_x ptildeXY(x,y)= 0.04
Experiments error: for y= -1.5 , ptildeY(y)= 0.01 but Sum_x ptildeXY(x,y)= 0.02
Experiments error: for y= 0.25 , ptildeY(y)= 0.3 but Sum_x ptildeXY(x,y)= 0.28
Experiments error: for y= -0.25 , ptildeY(y)= 0.15 but Sum_x ptildeXY(x,y)= 0.18
Experiments error: for y= 1.25 , ptildeY(y)= 0.04 but Sum_x ptildeXY(x,y)= 0.05
Experiments error: for y= -1.25 , ptildeY(y)= 0.02 but Sum_x ptildeXY(x,y)= 0.03
Experiments error: for y= 1.75 , ptildeY(y)= 0.01 but Sum_x ptildeXY(x,y)= 0.02
Experiments error: for y= -1.75 , ptildeY(y)= 0 but Sum_x ptildeXY(x,y)= 0
c_aW= 12 , numWordTokens= 34 , p(aW)= 0.35
c_aW= 9 , numWordTokens= 27 , p(aW)= 0.33
c_aW= 15 , numWordTokens= 45 , p(aW)= 0.33
c_aW= 10 , numWordTokens= 30 , p(aW)= 0.34
c_aW= 8 , numWordTokens= 24 , p(aW)= 0.33
c_aW= 11 , numWordTokens= 33 , p(aW)= 0.33
c_aW= 13 , numWordTokens= 39 , p(aW)= 0.33
c_aW= 14 , numWordTokens= 42 , p(aW)= 0.33
c_aW= 7 , numWordTokens= 21 , p(aW)= 0.33
c_aW= 16 , numWordTokens= 48 , p(aW)= 0.33
c_aW= 6 , numWordTokens= 18 , p(aW)= 0.33
c_aW= 17 , numWordTokens= 51 , p(aW)= 0.33
c_aW= 5 , numWordTokens= 15 , p(aW)= 0.33
c_aW= 18 , numWordTokens= 54 , p(aW)= 0.33
c_aW= 4 , numWordTokens= 12 , p(aW)= 0.33
Redwood.FORCE Saving sentences in output1.txt
Redwood.FORCE Saving sentences in report.csv
Redwood.FORCE Saving sentences in data.json
Redwood.FORCE Saving sentences in log.txt
Redwood.FORCE Saving sentences in summary.docx
Redwood.FORCE Saving sentences in backup.zip
Redwood.FORCE Saving sentences in results.xlsx
Redwood.FORCE Saving sentences in config.ini
Redwood.FORCE Saving sentences in error.log
Redwood.FORCE Saving sentences in output2.txt
Redwood.FORCE Saving sentences in analysis.pdf
Redwood.FORCE Saving sentences in input.txt
Redwood.FORCE Saving sentences in readme.md
Redwood.FORCE Saving sentences in test.py
Redwood.FORCE Saving sentences in output3.txt
Took too long parsing: 1000 words
Took too long parsing: 500 words
Took too long parsing: 300 words
Took too long parsing: 2000 words
Took too long parsing: 1500 words
Took too long parsing: 800 words
Took too long parsing: 1200 words
Took too long parsing: 400 words
Took too long parsing: 600 words
Took too long parsing: 2500 words
Took too long parsing: 1800 words
Took too long parsing: 900 words
Took too long parsing: 700 words
Took too long parsing: 1400 words
Took too long parsing: 1600 words
Problem: 87 vs. 92 on /home/user1/file1.txt
Problem: 76 vs. 81 on /var/log/syslog
Problem: 95 vs. 89 on /etc/passwd
Problem: 82 vs. 86 on /home/user2/file2.txt
Problem: 91 vs. 93 on /var/log/messages
Problem: 88 vs. 90 on /etc/hosts
Problem: 79 vs. 83 on /home/user3/file3.txt
Problem: 94 vs. 96 on /var/log/kern.log
Problem: 85 vs. 84 on /etc/shadow
Problem: 80 vs. 78 on /home/user4/file4.txt
Problem: 93 vs. 97 on /var/log/auth.log
Problem: 86 vs. 88 on /etc/fstab
Problem: 77 vs. 75 on /home/user5/file5.txt
Problem: 92 vs. 98 on /var/log/boot.log
Problem: 84 vs. 82 on /etc/crontab
Number of iterations of trees: 100
Number of iterations of trees: 50
Number of iterations of trees: 200
Number of iterations of trees: 75
Number of iterations of trees: 150
Number of iterations of trees: 25
Number of iterations of trees: 300
Number of iterations of trees: 125
Number of iterations of trees: 250
Number of iterations of trees: 175
Number of iterations of trees: 400
Number of iterations of trees: 350
Number of iterations of trees: 450
Number of iterations of trees: 325
Number of iterations of trees: 275
Parsing of sentence ran out of memory (length=45). Will ignore and try to continue.
Parsing of sentence ran out of memory (length=32). Will ignore and try to continue.
Parsing of sentence ran out of memory (length=51). Will ignore and try to continue.
Parsing of sentence ran out of memory (length=28). Will ignore and try to continue.
Parsing of sentence ran out of memory (length=37). Will ignore and try to continue.
Parsing of sentence ran out of memory (length=42). Will ignore and try to continue.
Parsing of sentence ran out of memory (length=39). Will ignore and try to continue.
Parsing of sentence ran out of memory (length=33). Will ignore and try to continue.
Parsing of sentence ran out of memory (length=48). Will ignore and try to continue.
Parsing of sentence ran out of memory (length=36). Will ignore and try to continue.
Parsing of sentence ran out of memory (length=40). Will ignore and try to continue.
Parsing of sentence ran out of memory (length=31). Will ignore and try to continue.
Parsing of sentence ran out of memory (length=44). Will ignore and try to continue.
Parsing of sentence ran out of memory (length=29). Will ignore and try to continue.
Depth first search from start node
Depth first search from start node
Depth first search from start node
Depth first search from start node
Depth first search from start node
Depth first search from start node
Depth first search from start node
Depth first search from start node
Depth first search from start node
Depth first search from start node
Depth first search from start node
Depth first search from start node
Depth first search from start node
Depth first search from start node
Depth first search from start node
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: "The quick brown fox jumps over the lazy dog."
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: "She sells seashells by the seashore."
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: "How much wood would a woodchuck chuck if a woodchuck could chuck wood?"
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: "To be or not to be, that is the question."
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: "I think therefore I am."
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: "May the force be with you."
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: "Houston, we have a problem."
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: "It was the best of times, it was the worst of times."
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: "You're a wizard, Harry."
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: "One small step for man, one giant leap for mankind."
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: "Four score and seven years ago..."
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: "Elementary, my dear Watson."
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: "Frankly, my dear, I don't give a damn."
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: "I have a dream..."
Number of qn iterations per batch: 10
Number of qn iterations per batch: 5
Number of qn iterations per batch: 8
Number of qn iterations per batch: 12
Number of qn iterations per batch: 7
Number of qn iterations per batch: 9
Number of qn iterations per batch: 6
Number of qn iterations per batch: 11
Number of qn iterations per batch: 4
Number of qn iterations per batch: 13
Number of qn iterations per batch: 3
Number of qn iterations per batch: 14
Number of qn iterations per batch: 2
Number of qn iterations per batch: 15
Number of qn iterations per batch: 1
Depth first search back from end node
Depth first search back from end node
Depth first search back from end node
Depth first search back from end node
Depth first search back from end node
Depth first search back from end node
Depth first search back from end node
Depth first search back from end node
Depth first search back from end node
Depth first search back from end node
Depth first search back from end node
Depth first search back from end node
Depth first search back from end node
Depth first search back from end node
Depth first search back from end node
usage: RadicalMap [-rebuild <dict_file>] [-infile <file>] [-encoding <encoding>] [char]+
usage: RadicalMap [-rebuild <dict_file>] [-infile <file>] [-encoding <encoding>] [char]+
usage: RadicalMap [-rebuild <dict_file>] [-infile <file>] [-encoding <encoding>] [char]+
usage: RadicalMap [-rebuild <dict_file>] [-infile <file>] [-encoding <encoding>] [char]+
usage: RadicalMap [-rebuild <dict_file>] [-infile <file>] [-encoding <encoding>] [char]+
usage: RadicalMap [-rebuild <dict_file>] [-infile <file>] [-encoding <encoding>] [char]+
usage: RadicalMap [-rebuild <dict_file>] [-infile <file>] [-encoding <encoding>] [char]+
usage: RadicalMap [-rebuild <dict_file>] [-infile <file>] [-encoding <encoding>] [char]+
usage: RadicalMap [-rebuild <dict_file>] [-infile <file>] [-encoding <encoding>] [char]+
usage: RadicalMap [-rebuild <dict_file>] [-infile <file>] [-encoding <encoding>] [char]+
usage: RadicalMap [-rebuild <dict_file>] [-infile <file>] [-encoding <encoding>] [char]+
usage: RadicalMap [-rebuild <dict_file>] [-infile <file>] [-encoding <encoding>] [char]+
usage: RadicalMap [-rebuild <dict_file>] [-infile <file>] [-encoding <encoding>] [char]+
usage: RadicalMap [-rebuild <dict_file>] [-infile <file>] [-encoding <encoding>] [char]+
usage: RadicalMap [-rebuild <dict_file>] [-infile <file>] [-encoding <encoding>] [char]+
usage: RadicalMap -infile file
usage: RadicalMap -infile file
usage: RadicalMap -infile file
usage: RadicalMap -infile file
usage: RadicalMap -infile file
usage: RadicalMap -infile file
usage: RadicalMap -infile file
usage: RadicalMap -infile file
usage: RadicalMap -infile file
usage: RadicalMap -infile file
usage: RadicalMap -infile file
usage: RadicalMap -infile file
usage: RadicalMap -infile file
usage: RadicalMap -infile file
usage: RadicalMap -infile file
usage: RadicalMap [-rebuild dictFile] [-infile file] [-encoding encoding] char+
usage: RadicalMap [-rebuild dictFile] [-infile file] [-encoding encoding] char+
usage: RadicalMap [-rebuild dictFile] [-infile file] [-encoding encoding] char+
usage: RadicalMap [-rebuild dictFile] [-infile file] [-encoding encoding] char+
usage: RadicalMap [-rebuild dictFile] [-infile file] [-encoding encoding] char+
usage: RadicalMap [-rebuild dictFile] [-infile file] [-encoding encoding] char+
usage: RadicalMap [-rebuild dictFile] [-infile file] [-encoding encoding] char+
usage: RadicalMap [-rebuild dictFile] [-infile file] [-encoding encoding] char+
usage: RadicalMap [-rebuild dictFile] [-infile file] [-encoding encoding] char+
usage: RadicalMap [-rebuild dictFile] [-infile file] [-encoding encoding] char+
usage: RadicalMap [-rebuild dictFile] [-infile file] [-encoding encoding] char+
usage: RadicalMap [-rebuild dictFile] [-infile file] [-encoding encoding] char+
usage: RadicalMap [-rebuild dictFile] [-infile file] [-encoding encoding] char+
usage: RadicalMap [-rebuild dictFile] [-infile file] [-encoding encoding] char+
usage: RadicalMap [-rebuild dictFile] [-infile file] [-encoding encoding] char+
String[] radLists = new String[] {
String[] radLists = new String[] {
String[] radLists = new String[] {
String[] radLists = new String[] {
String[] radLists = new String[] {
String[] radLists = new String[] {
String[] radLists = new String[] {
String[] radLists = new String[] {
String[] radLists = new String[] {
String[] radLists = new String[] {
String[] radLists = new String[] {
String[] radLists = new String[] {
String[] radLists = new String[] {
String[] radLists = new String[] {
String[] radLists = new String[] {
Learning rate: 0.01
Learning rate: 0.05
Learning rate: 0.1
Learning rate: 0.001
Learning rate: 0.02
Learning rate: 0.03
Learning rate: 0.04
Learning rate: 0.06
Learning rate: 0.07
Learning rate: 0.08
Learning rate: 0.09
Learning rate: 0.002
Learning rate: 0.003
Learning rate: 0.004
Learning rate: 0.005
Tagging of sentence ran out of memory. Will ignore and continue: The quick brown fox jumps over the lazy dog.
Tagging of sentence ran out of memory. Will ignore and continue: She sells seashells by the seashore.
Tagging of sentence ran out of memory. Will ignore and continue: How much wood would a woodchuck chuck if a woodchuck could chuck wood?
Tagging of sentence ran out of memory. Will ignore and continue: To be or not to be, that is the question.
Tagging of sentence ran out of memory. Will ignore and continue: I have a dream that one day this nation will rise up and live out the true meaning of its creed.
Tagging of sentence ran out of memory. Will ignore and continue: May the Force be with you.
Tagging of sentence ran out of memory. Will ignore and continue: It was the best of times, it was the worst of times.
Tagging of sentence ran out of memory. Will ignore and continue: You're a wizard, Harry.
Tagging of sentence ran out of memory. Will ignore and continue: In the beginning God created the heaven and the earth.
Tagging of sentence ran out of memory. Will ignore and continue: Four score and seven years ago our fathers brought forth on this continent a new nation.
Tagging of sentence ran out of memory. Will ignore and continue: Houston, we have a problem.
Tagging of sentence ran out of memory. Will ignore and continue: That's one small step for man, one giant leap for mankind.
Tagging of sentence ran out of memory. Will ignore and continue: I'm sorry, Dave. I'm afraid I can't do that.
Tagging of sentence ran out of memory. Will ignore and continue: Frankly, my dear, I don't give a damn.
Tagging of sentence ran out of memory. Will ignore and continue: Elementary, my dear Watson.
Delta margin: 0.1
Delta margin: 0.05
Delta margin: 0.15
Delta margin: 0.2
Delta margin: 0.12
Delta margin: 0.08
Delta margin: 0.18
Delta margin: 0.03
Delta margin: 0.13
Delta margin: 0.07
Delta margin: 0.17
Delta margin: 0.02
Delta margin: 0.14
Delta margin: 0.06
Delta margin: 0.16
WARNING: QuantifiableEntityNormalizingAnnotator does not work well with collapse=true
WARNING: QuantifiableEntityNormalizingAnnotator does not work well with collapse=true
WARNING: QuantifiableEntityNormalizingAnnotator does not work well with collapse=true
WARNING: QuantifiableEntityNormalizingAnnotator does not work well with collapse=true
WARNING: QuantifiableEntityNormalizingAnnotator does not work well with collapse=true
WARNING: QuantifiableEntityNormalizingAnnotator does not work well with collapse=true
WARNING: QuantifiableEntityNormalizingAnnotator does not work well with collapse=true
WARNING: QuantifiableEntityNormalizingAnnotator does not work well with collapse=true
WARNING: QuantifiableEntityNormalizingAnnotator does not work well with collapse=true
WARNING: QuantifiableEntityNormalizingAnnotator does not work well with collapse=true
WARNING: QuantifiableEntityNormalizingAnnotator does not work well with collapse=true
WARNING: QuantifiableEntityNormalizingAnnotator does not work well with collapse=true
WARNING: QuantifiableEntityNormalizingAnnotator does not work well with collapse=true
WARNING: QuantifiableEntityNormalizingAnnotator does not work well with collapse=true
WARNING: QuantifiableEntityNormalizingAnnotator does not work well with collapse=true
Normalizing quantifiable entities...
Normalizing quantifiable entities...
Normalizing quantifiable entities...
Normalizing quantifiable entities...
Normalizing quantifiable entities...
Normalizing quantifiable entities...
Normalizing quantifiable entities...
Normalizing quantifiable entities...
Normalizing quantifiable entities...
Normalizing quantifiable entities...
Normalizing quantifiable entities...
Normalizing quantifiable entities...
Normalizing quantifiable entities...
Normalizing quantifiable entities...
Normalizing quantifiable entities...
regCost: 0.01
regCost: 0.05
regCost: 0.1
regCost: 0.2
regCost: 0.3
regCost: 0.4
regCost: 0.5
regCost: 0.6
regCost: 0.7
regCost: 0.8
regCost: 0.9
regCost: 1.0
regCost: 1.5
regCost: 2.0
regCost: 2.5
Parse exception while reading config.json
Parse exception while reading data.csv
Parse exception while reading report.docx
Parse exception while reading image.png
Parse exception while reading script.py
Parse exception while reading log.txt
Parse exception while reading index.html
Parse exception while reading video.mp4
Parse exception while reading audio.mp3
Parse exception while reading archive.zip
Parse exception while reading settings.ini
Parse exception while reading database.db
Parse exception while reading readme.md
Parse exception while reading license.txt
Parse exception while reading code.java
Preparing quote annotator...
Preparing quote annotator...
Preparing quote annotator...
Preparing quote annotator...
Preparing quote annotator...
Preparing quote annotator...
Preparing quote annotator...
Preparing quote annotator...
Preparing quote annotator...
Preparing quote annotator...
Preparing quote annotator...
Preparing quote annotator...
Preparing quote annotator...
Preparing quote annotator...
Preparing quote annotator...
data.csv: 12 trees, 8 matched and printed
report.pdf: 5 trees, 3 matched and printed
image.png: 7 trees, 4 matched and printed
video.mp4: 10 trees, 6 matched and printed
music.mp3: 9 trees, 5 matched and printed
text.txt: 8 trees, 7 matched and printed
code.java: 11 trees, 9 matched and printed
graph.xls: 6 trees, 2 matched and printed
slide.ppt: 4 trees, 1 matched and printed
email.eml: 3 trees, 0 matched and printed
book.epub: 15 trees, 10 matched and printed
game.exe: 13 trees, 11 matched and printed
calendar.ics: 2 trees, 2 matched and printed
map.kml: 14 trees, 12 matched and printed
Probability distribution given to tokens (Counts for all class-token pairs; accuracy for this bin; examples are gold entity tokens in bin)
Probability distribution given to tokens (Counts for all class-token pairs; accuracy for this bin; examples are gold entity tokens in bin)
Probability distribution given to tokens (Counts for all class-token pairs; accuracy for this bin; examples are gold entity tokens in bin)
Probability distribution given to tokens (Counts for all class-token pairs; accuracy for this bin; examples are gold entity tokens in bin)
Probability distribution given to tokens (Counts for all class-token pairs; accuracy for this bin; examples are gold entity tokens in bin)
Probability distribution given to tokens (Counts for all class-token pairs; accuracy for this bin; examples are gold entity tokens in bin)
Probability distribution given to tokens (Counts for all class-token pairs; accuracy for this bin; examples are gold entity tokens in bin)
Probability distribution given to tokens (Counts for all class-token pairs; accuracy for this bin; examples are gold entity tokens in bin)
Probability distribution given to tokens (Counts for all class-token pairs; accuracy for this bin; examples are gold entity tokens in bin)
Probability distribution given to tokens (Counts for all class-token pairs; accuracy for this bin; examples are gold entity tokens in bin)
Probability distribution given to tokens (Counts for all class-token pairs; accuracy for this bin; examples are gold entity tokens in bin)
Probability distribution given to tokens (Counts for all class-token pairs; accuracy for this bin; examples are gold entity tokens in bin)
Probability distribution given to tokens (Counts for all class-token pairs; accuracy for this bin; examples are gold entity tokens in bin)
Probability distribution given to tokens (Counts for all class-token pairs; accuracy for this bin; examples are gold entity tokens in bin)
Probability distribution given to tokens (Counts for all class-token pairs; accuracy for this bin; examples are gold entity tokens in bin)
Number of quotes + unicode - single : 0
Number of quotes + unicode - single : 1
Number of quotes + unicode - single : 2
Number of quotes + unicode - single : 3
Number of quotes + unicode - single : 4
Number of quotes + unicode - single : 5
Number of quotes + unicode - single : 6
Number of quotes + unicode - single : 7
Number of quotes + unicode - single : 8
Number of quotes + unicode - single : 9
Number of quotes + unicode - single : 10
Number of quotes + unicode - single : 11
Number of quotes + unicode - single : 12
Number of quotes + unicode - single : 13
Number of quotes + unicode - single : 14
could not find SRL entries for file: config.json
could not find SRL entries for file: index.html
could not find SRL entries for file: data.csv
could not find SRL entries for file: logo.png
could not find SRL entries for file: report.docx
could not find SRL entries for file: script.py
could not find SRL entries for file: style.css
could not find SRL entries for file: video.mp4
could not find SRL entries for file: audio.mp3
could not find SRL entries for file: readme.md
could not find SRL entries for file: backup.zip
could not find SRL entries for file: invoice.pdf
could not find SRL entries for file: game.exe
could not find SRL entries for file: resume.txt
could not find SRL entries for file: photo.jpg
Number of quotes + ascii - single : 0
Number of quotes + ascii - single : 1
Number of quotes + ascii - single : 2
Number of quotes + ascii - single : 3
Number of quotes + ascii - single : 4
Number of quotes + ascii - single : 5
Number of quotes + ascii - single : 6
Number of quotes + ascii - single : 7
Number of quotes + ascii - single : 8
Number of quotes + ascii - single : 9
Number of quotes + ascii - single : 10
Number of quotes + ascii - single : 11
Number of quotes + ascii - single : 12
Number of quotes + ascii - single : 13
Number of quotes + ascii - single : 14
Using unknown word vector for numbers: [0.5, 0.3, 0.1, 0.7]
Using unknown word vector for numbers: [0.2, 0.4, 0.6, 0.8]
Using unknown word vector for numbers: [0.9, 0.1, 0.3, 0.5]
Using unknown word vector for numbers: [0.4, 0.2, 0.8, 0.6]
Using unknown word vector for numbers: [0.7, 0.9, 0.1, 0.3]
Using unknown word vector for numbers: [0.6, 0.8, 0.2, 0.4]
Using unknown word vector for numbers: [0.3, 0.5, 0.7, 0.9]
Using unknown word vector for numbers: [1.0, 1.2, 1.4, 1.6]
Using unknown word vector for numbers: [1.8, 2.0, 2.2, 2.4]
Using unknown word vector for numbers: [2.6, 2.8, 3.0, 3.2]
Using unknown word vector for numbers: [3.4, 3.6, 3.8, 4.0]
Using unknown word vector for numbers: [4.2, 4.4, 4.6, 4.8]
Using unknown word vector for numbers: [5.0, 5.2, 5.4, 5.6]
Using unknown word vector for numbers: [5.8, 6.0, 6.2, 6.4]
Using unknown word vector for numbers: [6.6, 6.8, 7.0, 7.2]
Number of quotes + ascii + single : 12
Number of quotes + ascii + single : 9
Number of quotes + ascii + single : 15
Number of quotes + ascii + single : 7
Number of quotes + ascii + single : 10
Number of quotes + ascii + single : 11
Number of quotes + ascii + single : 8
Number of quotes + ascii + single : 13
Number of quotes + ascii + single : 6
Number of quotes + ascii + single : 14
Number of quotes + ascii + single : 5
Number of quotes + ascii + single : 16
Number of quotes + ascii + single : 4
Number of quotes + ascii + single : 17
Number of quotes + ascii + single : 3
12 examples completed
7 examples completed
9 examples completed
11 examples completed
10 examples completed
8 examples completed
13 examples completed
6 examples completed
14 examples completed
5 examples completed
15 examples completed
4 examples completed
3 examples completed
2 examples completed
load IO Exception: java.io.FileNotFoundException: File not found
load IO Exception: java.io.IOException: Stream closed
load IO Exception: java.io.EOFException: End of input
load IO Exception: java.io.InterruptedIOException: Operation timed out
load IO Exception: java.io.SyncFailedException: Sync failed
load IO Exception: java.io.UTFDataFormatException: Malformed input
load IO Exception: java.io.CharConversionException: Invalid character encoding
load IO Exception: java.io.InvalidClassException: Incompatible class version
load IO Exception: java.io.NotSerializableException: Object not serializable
load IO Exception: java.io.OptionalDataException: Optional data not found
load IO Exception: java.io.WriteAbortedException: Write aborted by exception
load IO Exception: java.io.ObjectStreamException: Stream corrupted
load IO Exception: java.io.IOException: Socket closed
load IO Exception: java.io.IOException: Connection reset
load IO Exception: java.io.IOException: Broken pipe
Ignoring the following labels: spam
Ignoring the following labels: urgent
Ignoring the following labels: test
Ignoring the following labels: error
Ignoring the following labels: duplicate
Ignoring the following labels: low-priority
Ignoring the following labels: incomplete
Ignoring the following labels: outdated
Ignoring the following labels: irrelevant
Ignoring the following labels: confidential
Ignoring the following labels: feedback
Ignoring the following labels: draft
Ignoring the following labels: personal
Ignoring the following labels: debug
Ignoring the following labels: todo
sentence id=1 k=0 logProb=-2.34 prob=0.096
sentence id=1 k=1 logProb=-2.56 prob=0.077
sentence id=1 k=2 logProb=-2.78 prob=0.062
sentence id=2 k=0 logProb=-1.89 prob=0.151
sentence id=2 k=1 logProb=-2.12 prob=0.121
sentence id=2 k=2 logProb=-2.45 prob=0.087
sentence id=3 k=0 logProb=-3.01 prob=0.049
sentence id=3 k=1 logProb=-3.25 prob=0.039
sentence id=3 k=2 logProb=-3.51 prob=0.030
sentence id=4 k=0 logProb=-2.67 prob=0.069
sentence id=4 k=1 logProb=-2.91 prob=0.054
sentence id=4 k=2 logProb=-3.18 prob=0.042
sentence id=5 k=0 logProb=-1.76 prob=0.173
sentence id=5 k=1 logProb=-2.03 prob=0.131
Remapping labels as follows: (0, 1, 2, 3)
Remapping labels as follows: (4, 5, 6, 7)
Remapping labels as follows: (8, 9, 10, 11)
Remapping labels as follows: (12, 13, 14, 15)
Remapping labels as follows: (16, 17, 18, 19)
Remapping labels as follows: (20, 21, 22, 23)
Remapping labels as follows: (24, 25, 26, 27)
Remapping labels as follows: (28, 29, 30, 31)
Remapping labels as follows: (32, 33, 34, 35)
Remapping labels as follows: (36, 37, 38, 39)
Remapping labels as follows: (40, 41, 42, 43)
Remapping labels as follows: (44, 45, 46, 47)
Remapping labels as follows: (48, 49, 50, 51)
Remapping labels as follows: (52, 53, 54, 55)
Remapping labels as follows: (56, 57, 58, 59)
Loading QuoteAttribution coref [/home/user/coref_data.json]...
Loading QuoteAttribution coref [C:\Users\user\Documents\coref_data.json]...
Loading QuoteAttribution coref [https://example.com/coref_data.json]...
Loading QuoteAttribution coref [/mnt/coref_data.json]...
Loading QuoteAttribution coref [s3://bucket/coref_data.json]...
Loading QuoteAttribution coref [hdfs://namenode/coref_data.json]...
Loading QuoteAttribution coref [file:///tmp/coref_data.json]...
Loading QuoteAttribution coref [/var/lib/coref_data.json]...
Loading QuoteAttribution coref [ftp://user@host/coref_data.json]...
Loading QuoteAttribution coref [/opt/coref_data.json]...
Loading QuoteAttribution coref [gs://bucket/coref_data.json]...
Loading QuoteAttribution coref [jdbc:mysql://localhost/coref_data]...
Loading QuoteAttribution coref [mongodb://user:pass@host/coref_data]...
Loading QuoteAttribution coref [redis://host:port/coref_data]...
Loading QuoteAttribution coref [sqlite:///coref_data.db]...
Loading QuoteAttribution characters [characters.txt]...
Loading QuoteAttribution characters [char_data.csv]...
Loading QuoteAttribution characters [chars.json]...
Loading QuoteAttribution characters [character_list.xlsx]...
Loading QuoteAttribution characters [char_info.xml]...
Loading QuoteAttribution characters [characters.dat]...
Loading QuoteAttribution characters [char_names.tsv]...
Loading QuoteAttribution characters [chars.pkl]...
Loading QuoteAttribution characters [character_data.yaml]...
Loading QuoteAttribution characters [chars.hdf5]...
Loading QuoteAttribution characters [character_info.parquet]...
Loading QuoteAttribution characters [chars.mat]...
Loading QuoteAttribution characters [character_list.dbf]...
Loading QuoteAttribution characters [chars.feather]...
Loading QuoteAttribution characters [character_data.ods]...
Using unknown word vector for capitalized words: 0.5
Using unknown word vector for capitalized words: 0.3
Using unknown word vector for capitalized words: 0.7
Using unknown word vector for capitalized words: 0.4
Using unknown word vector for capitalized words: 0.6
Using unknown word vector for capitalized words: 0.2
Using unknown word vector for capitalized words: 0.8
Using unknown word vector for capitalized words: 0.1
Using unknown word vector for capitalized words: 0.9
Using unknown word vector for capitalized words: 1.0
Using unknown word vector for capitalized words: -0.1
Using unknown word vector for capitalized words: -0.2
Using unknown word vector for capitalized words: -0.3
Using unknown word vector for capitalized words: -0.4
Using unknown word vector for capitalized words: -0.5
Adding RegexNER annotations ...
Adding RegexNER annotations ...
Adding RegexNER annotations ...
Adding RegexNER annotations ...
Adding RegexNER annotations ...
Adding RegexNER annotations ...
Adding RegexNER annotations ...
Adding RegexNER annotations ...
Adding RegexNER annotations ...
Adding RegexNER annotations ...
Adding RegexNER annotations ...
Adding RegexNER annotations ...
Adding RegexNER annotations ...
Adding RegexNER annotations ...
Adding RegexNER annotations ...
sup.relation.verbose is DEPRECATED.  use relation.verbose instead
sup.relation.verbose is DEPRECATED.  use relation.verbose instead
sup.relation.verbose is DEPRECATED.  use relation.verbose instead
sup.relation.verbose is DEPRECATED.  use relation.verbose instead
sup.relation.verbose is DEPRECATED.  use relation.verbose instead
sup.relation.verbose is DEPRECATED.  use relation.verbose instead
sup.relation.verbose is DEPRECATED.  use relation.verbose instead
sup.relation.verbose is DEPRECATED.  use relation.verbose instead
sup.relation.verbose is DEPRECATED.  use relation.verbose instead
sup.relation.verbose is DEPRECATED.  use relation.verbose instead
sup.relation.verbose is DEPRECATED.  use relation.verbose instead
sup.relation.verbose is DEPRECATED.  use relation.verbose instead
sup.relation.verbose is DEPRECATED.  use relation.verbose instead
sup.relation.verbose is DEPRECATED.  use relation.verbose instead
sup.relation.verbose is DEPRECATED.  use relation.verbose instead
sup.relation.model is DEPRECATED.  use relation.model instead
sup.relation.model is DEPRECATED.  use relation.model instead
sup.relation.model is DEPRECATED.  use relation.model instead
sup.relation.model is DEPRECATED.  use relation.model instead
sup.relation.model is DEPRECATED.  use relation.model instead
sup.relation.model is DEPRECATED.  use relation.model instead
sup.relation.model is DEPRECATED.  use relation.model instead
sup.relation.model is DEPRECATED.  use relation.model instead
sup.relation.model is DEPRECATED.  use relation.model instead
sup.relation.model is DEPRECATED.  use relation.model instead
sup.relation.model is DEPRECATED.  use relation.model instead
sup.relation.model is DEPRECATED.  use relation.model instead
sup.relation.model is DEPRECATED.  use relation.model instead
sup.relation.model is DEPRECATED.  use relation.model instead
sup.relation.model is DEPRECATED.  use relation.model instead
PennTreeReader: warning: file has extra non-matching right parenthesis [ignored]
PennTreeReader: warning: file has extra non-matching right parenthesis [ignored]
PennTreeReader: warning: file has extra non-matching right parenthesis [ignored]
PennTreeReader: warning: file has extra non-matching right parenthesis [ignored]
PennTreeReader: warning: file has extra non-matching right parenthesis [ignored]
PennTreeReader: warning: file has extra non-matching right parenthesis [ignored]
PennTreeReader: warning: file has extra non-matching right parenthesis [ignored]
PennTreeReader: warning: file has extra non-matching right parenthesis [ignored]
PennTreeReader: warning: file has extra non-matching right parenthesis [ignored]
PennTreeReader: warning: file has extra non-matching right parenthesis [ignored]
PennTreeReader: warning: file has extra non-matching right parenthesis [ignored]
PennTreeReader: warning: file has extra non-matching right parenthesis [ignored]
PennTreeReader: warning: file has extra non-matching right parenthesis [ignored]
PennTreeReader: warning: file has extra non-matching right parenthesis [ignored]
PennTreeReader: warning: file has extra non-matching right parenthesis [ignored]
Extracted the following entities:
Extracted the following entities:
Extracted the following entities:
Extracted the following entities:
Extracted the following entities:
Extracted the following entities:
Extracted the following entities:
Extracted the following entities:
Extracted the following entities:
Extracted the following entities:
Extracted the following entities:
Extracted the following entities:
Extracted the following entities:
Extracted the following entities:
Extracted the following entities:
PennTreeReader: warning: incomplete tree (extra left parentheses in input): (S (NP (DT The) (NN cat)) (VP (VBD ate) (NP (DT a) (NN mouse))))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((S (NP (PRP He)) (VP (VBZ likes) (NP (NN cake)))))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): (((NP (NNP John)) (VP (VBD went) (PP (TO to) (NP (NNP Paris))))))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((S (NP-SBJ-1 (-NONE- *)) (VP-1 (-NONE- *T*-1)) (. .)))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((SINV-TPC-1 (-NONE- *T*-2) , ,) ((S (-NONE- *T*-1)) VP|<VBG Having> NP|<DT a> NN|<breakfast>))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): (((WHADVP-1 (-NONE- 0)) SQ|<MD Could> NP-SBJ|<PRP you> VP|<VB please> VP|<VB help> NP|<PRP me>))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((S-TPC-1 (-NONE- *T*-2) , ,) ((S (-NONE- *T*-1)) VP|<VBD said> S|<'' ''>))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): (((WHNP-1 (-NONE- 0)) SINV|<VBP Are> NP-SBJ|<DT there> ADJP|<JJ any> NNS|<questions>))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): (((WHPP-1 (-NONE- 0)) SBARQ|<VBP Do> NP-SBJ|<PRP you> VP|<VB know> SBAR|<IN where> NP-SBJ|<PRP he> VP|<VBD went>))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): ((S-TPC-1 (-NONE- *T*-2) , ,) ((S (-NONE- *T*-1)) VP|<VBD was> ADJP|<JJ happy>))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): (((WHADJP-1 (-NONE- 0)) SQ|<JJR How> NP-SBJ|<PRP you> VP|<VBP are>))
PennTreeReader: warning: incomplete tree (extra left parentheses in input): (((WHNP-1 (-NONE- 0)) SQ|<VBZ Is> NP-SBJ|<DT this> NP|<DT the> NN|<book> PP|<IN that> NP-SBJ|<PRP you> VP|<VBD wanted>)
PennTreeReader: warning: incomplete tree (extra left parentheses in input): (((WHPP-1 (-NONE- 0)) SINV|<MD Can> NP-SBJ|<PRP we> VP|<VB go> PP|<TO to> NP|<DT the> NN|<park>)
PennTreeReader: warning: incomplete tree (extra left parentheses in input): (((WHNP-1 (-NONE- 0)) SINV|<MD Will> NP-SBJ|<PRP she> VP|<VB come>
PennTreeReader: warning: incomplete tree (extra left parentheses in input): (((WHADVP-1 (-NONE- 0)) SQ|<WRB When>
Extracted the following relations:
Extracted the following relations:
Extracted the following relations:
Extracted the following relations:
Extracted the following relations:
Extracted the following relations:
Extracted the following relations:
Extracted the following relations:
Extracted the following relations:
Extracted the following relations:
Extracted the following relations:
Extracted the following relations:
Extracted the following relations:
Extracted the following relations:
Extracted the following relations:
Using unknown number vector for Chinese words: [0.1, 0.2, 0.3, 0.4]
Using unknown number vector for Chinese words: [0.5, 0.6, 0.7, 0.8]
Using unknown number vector for Chinese words: [0.9, 1.0, 1.1, 1.2]
Using unknown number vector for Chinese words: [1.3, 1.4, 1.5, 1.6]
Using unknown number vector for Chinese words: [1.7, 1.8, 1.9, 2.0]
Using unknown number vector for Chinese words: [2.1, 2.2, 2.3, 2.4]
Using unknown number vector for Chinese words: [2.5, 2.6, 2.7, 2.8]
Using unknown number vector for Chinese words: [2.9, 3.0, 3.1, 3.2]
Using unknown number vector for Chinese words: [3.3, 3.4, 3.5, 3.6]
Using unknown number vector for Chinese words: [3.7, 3.8, 3.9, 4.0]
Using unknown number vector for Chinese words: [4.1, 4.2, 4.3, 4.4]
Using unknown number vector for Chinese words: [4.5, 4.6, 4.7, 4.8]
Using unknown number vector for Chinese words: [4.9, 5.0, 5.1, 5.2]
Using unknown number vector for Chinese words: [5.3, 5.4, 5.5, 5.6]
Using unknown number vector for Chinese words: [5.7, 5.8, 5.9, 6.0]
For sentence: The dog barked loudly.
For sentence: She loves to read books.
For sentence: He ran as fast as he could.
For sentence: They went to the park together.
For sentence: It was a sunny day.
For sentence: She bought a new dress.
For sentence: He likes to play video games.
For sentence: They had a lot of fun.
For sentence: It rained all day.
For sentence: She made a delicious cake.
For sentence: He studied hard for the exam.
For sentence: They watched a movie last night.
For sentence: It was very cold outside.
For sentence: She sang a beautiful song.
For sentence: He gave her a nice gift.
preTokenized option set: Non-standard annotators list, preTokenized may not work in this case.
preTokenized option set: Non-standard annotators list, preTokenized may not work in this case.
preTokenized option set: Non-standard annotators list, preTokenized may not work in this case.
preTokenized option set: Non-standard annotators list, preTokenized may not work in this case.
preTokenized option set: Non-standard annotators list, preTokenized may not work in this case.
preTokenized option set: Non-standard annotators list, preTokenized may not work in this case.
preTokenized option set: Non-standard annotators list, preTokenized may not work in this case.
preTokenized option set: Non-standard annotators list, preTokenized may not work in this case.
preTokenized option set: Non-standard annotators list, preTokenized may not work in this case.
preTokenized option set: Non-standard annotators list, preTokenized may not work in this case.
preTokenized option set: Non-standard annotators list, preTokenized may not work in this case.
preTokenized option set: Non-standard annotators list, preTokenized may not work in this case.
preTokenized option set: Non-standard annotators list, preTokenized may not work in this case.
preTokenized option set: Non-standard annotators list, preTokenized may not work in this case.
preTokenized option set: Non-standard annotators list, preTokenized may not work in this case.
preTokenized option set: Adding tokenize,ssplit to beginning.
preTokenized option set: Adding tokenize,ssplit to beginning.
preTokenized option set: Adding tokenize,ssplit to beginning.
preTokenized option set: Adding tokenize,ssplit to beginning.
preTokenized option set: Adding tokenize,ssplit to beginning.
preTokenized option set: Adding tokenize,ssplit to beginning.
preTokenized option set: Adding tokenize,ssplit to beginning.
preTokenized option set: Adding tokenize,ssplit to beginning.
preTokenized option set: Adding tokenize,ssplit to beginning.
preTokenized option set: Adding tokenize,ssplit to beginning.
preTokenized option set: Adding tokenize,ssplit to beginning.
preTokenized option set: Adding tokenize,ssplit to beginning.
preTokenized option set: Adding tokenize,ssplit to beginning.
preTokenized option set: Adding tokenize,ssplit to beginning.
preTokenized option set: Adding tokenize,ssplit to beginning.
preTokenized option set: Annotators list starts with tokenize,ssplit, no change needed.
preTokenized option set: Annotators list starts with tokenize,ssplit, no change needed.
preTokenized option set: Annotators list starts with tokenize,ssplit, no change needed.
preTokenized option set: Annotators list starts with tokenize,ssplit, no change needed.
preTokenized option set: Annotators list starts with tokenize,ssplit, no change needed.
preTokenized option set: Annotators list starts with tokenize,ssplit, no change needed.
preTokenized option set: Annotators list starts with tokenize,ssplit, no change needed.
preTokenized option set: Annotators list starts with tokenize,ssplit, no change needed.
preTokenized option set: Annotators list starts with tokenize,ssplit, no change needed.
preTokenized option set: Annotators list starts with tokenize,ssplit, no change needed.
preTokenized option set: Annotators list starts with tokenize,ssplit, no change needed.
preTokenized option set: Annotators list starts with tokenize,ssplit, no change needed.
preTokenized option set: Annotators list starts with tokenize,ssplit, no change needed.
preTokenized option set: Annotators list starts with tokenize,ssplit, no change needed.
preTokenized option set: Annotators list starts with tokenize,ssplit, no change needed.
preTokenized option set: Changing annotators tokenize,ssplit,mwt to tokenize,ssplit
preTokenized option set: Changing annotators tokenize,ssplit,mwt to tokenize,ssplit
preTokenized option set: Changing annotators tokenize,ssplit,mwt to tokenize,ssplit
preTokenized option set: Changing annotators tokenize,ssplit,mwt to tokenize,ssplit
preTokenized option set: Changing annotators tokenize,ssplit,mwt to tokenize,ssplit
preTokenized option set: Changing annotators tokenize,ssplit,mwt to tokenize,ssplit
preTokenized option set: Changing annotators tokenize,ssplit,mwt to tokenize,ssplit
preTokenized option set: Changing annotators tokenize,ssplit,mwt to tokenize,ssplit
preTokenized option set: Changing annotators tokenize,ssplit,mwt to tokenize,ssplit
preTokenized option set: Changing annotators tokenize,ssplit,mwt to tokenize,ssplit
preTokenized option set: Changing annotators tokenize,ssplit,mwt to tokenize,ssplit
preTokenized option set: Changing annotators tokenize,ssplit,mwt to tokenize,ssplit
preTokenized option set: Changing annotators tokenize,ssplit,mwt to tokenize,ssplit
preTokenized option set: Changing annotators tokenize,ssplit,mwt to tokenize,ssplit
preTokenized option set: Changing annotators tokenize,ssplit,mwt to tokenize,ssplit
preTokenized option set: Changing annotators cdc_tokenize to tokenize,ssplit
preTokenized option set: Changing annotators cdc_tokenize to tokenize,ssplit
preTokenized option set: Changing annotators cdc_tokenize to tokenize,ssplit
preTokenized option set: Changing annotators cdc_tokenize to tokenize,ssplit
preTokenized option set: Changing annotators cdc_tokenize to tokenize,ssplit
preTokenized option set: Changing annotators cdc_tokenize to tokenize,ssplit
preTokenized option set: Changing annotators cdc_tokenize to tokenize,ssplit
preTokenized option set: Changing annotators cdc_tokenize to tokenize,ssplit
preTokenized option set: Changing annotators cdc_tokenize to tokenize,ssplit
preTokenized option set: Changing annotators cdc_tokenize to tokenize,ssplit
preTokenized option set: Changing annotators cdc_tokenize to tokenize,ssplit
preTokenized option set: Changing annotators cdc_tokenize to tokenize,ssplit
preTokenized option set: Changing annotators cdc_tokenize to tokenize,ssplit
preTokenized option set: Changing annotators cdc_tokenize to tokenize,ssplit
preTokenized option set: Changing annotators cdc_tokenize to tokenize,ssplit
Adding annotator ner
Adding annotator parse
Adding annotator sentiment
Adding annotator lemma
Adding annotator tokenize
Adding annotator pos
Adding annotator coref
Adding annotator ssplit
Adding annotator dcoref
Adding annotator depparse
Adding annotator quote
Adding annotator regexner
Adding annotator relation
Adding annotator natlog
Adding annotator openie
Using unknown year vector for Chinese words: [0.1, 0.2, 0.3, 0.4]
Using unknown year vector for Chinese words: [0.5, 0.6, 0.7, 0.8]
Using unknown year vector for Chinese words: [0.9, 1.0, 1.1, 1.2]
Using unknown year vector for Chinese words: [1.3, 1.4, 1.5, 1.6]
Using unknown year vector for Chinese words: [1.7, 1.8, 1.9, 2.0]
Using unknown year vector for Chinese words: [2.1, 2.2, 2.3, 2.4]
Using unknown year vector for Chinese words: [2.5, 2.6, 2.7, 2.8]
Using unknown year vector for Chinese words: [2.9, 3.0, 3.1, 3.2]
Using unknown year vector for Chinese words: [3.3, 3.4, 3.5, 3.6]
Using unknown year vector for Chinese words: [3.7, 3.8, 3.9, 4.0]
Using unknown year vector for Chinese words: [4.1, 4.2, 4.3, 4.4]
Using unknown year vector for Chinese words: [4.5, 4.6, 4.7, 4.8]
Using unknown year vector for Chinese words: [4.9, 5.0, 5.1, 5.2]
Using unknown year vector for Chinese words: [5.3, 5.4, 5.5, 5.6]
Using unknown year vector for Chinese words: [5.7, 5.8, 5.9, 6.0]
Using unknown percent vector for Chinese words: [0.1, 0.2, 0.3, 0.4]
Using unknown percent vector for Chinese words: [0.05, 0.1, 0.15, 0.2]
Using unknown percent vector for Chinese words: [0.2, 0.4, 0.6, 0.8]
Using unknown percent vector for Chinese words: [0.01, 0.02, 0.03, 0.04]
Using unknown percent vector for Chinese words: [0.25, 0.5, 0.75, 1]
Using unknown percent vector for Chinese words: [0.3, 0.6, 0.9, 1.2]
Using unknown percent vector for Chinese words: [0.15, 0.3, 0.45, 0.6]
Using unknown percent vector for Chinese words: [0.4, 0.8, 1.2, 1.6]
Using unknown percent vector for Chinese words: [0.05, 0.15, 0.25, 0.35]
Using unknown percent vector for Chinese words: [0.1, 0.3, 0.5, 0.7]
Using unknown percent vector for Chinese words: [0.2, 0.5, 0.8, 1]
Using unknown percent vector for Chinese words: [0.03, 0.06, 0.09, 0.12]
Using unknown percent vector for Chinese words: [0.35, 0.7, 1.05, 1.4]
Using unknown percent vector for Chinese words: [0.08, 0.16, 0.24, 0.32]
Using unknown percent vector for Chinese words: [0.4, 1 ,1 ,1]
At motherCat, my parent is parent
At motherCat, my parent is null
At motherCat, my parent is kitten
At motherCat, my parent is cat
At motherCat, my parent is animal
At motherCat, my parent is object
At motherCat, my parent is undefined
At motherCat, my parent is self
At motherCat, my parent is root
At motherCat, my parent is child
At motherCat, my parent is sibling
At motherCat, my parent is ancestor
At motherCat, my parent is descendant
At motherCat, my parent is friend
At motherCat, my parent is enemy
Initial matrices scaled by: 0.5
Initial matrices scaled by: 1.0
Initial matrices scaled by: 0.75
Initial matrices scaled by: 1.5
Initial matrices scaled by: 0.25
Initial matrices scaled by: 2.0
Initial matrices scaled by: 0.1
Initial matrices scaled by: 3.0
Initial matrices scaled by: 0.01
Initial matrices scaled by: 10.0
Initial matrices scaled by: 0.001
Initial matrices scaled by: 100.0
Initial matrices scaled by: 0.0001
Initial matrices scaled by: 1000.0
Initial matrices scaled by: 0.00001
Semantic head finder: at VP
Semantic head finder: at VP
Semantic head finder: at VP
Semantic head finder: at VP
Semantic head finder: at VP
Semantic head finder: at VP
Semantic head finder: at VP
Semantic head finder: at VP
Semantic head finder: at VP
Semantic head finder: at VP
Semantic head finder: at VP
Semantic head finder: at VP
Semantic head finder: at VP
Semantic head finder: at VP
Semantic head finder: at VP
Training will use 1 thread(s)
Training will use 4 thread(s)
Training will use 8 thread(s)
Training will use 16 thread(s)
Training will use 32 thread(s)
Training will use 2 thread(s)
Training will use 6 thread(s)
Training will use 12 thread(s)
Training will use 24 thread(s)
Training will use 48 thread(s)
Training will use 3 thread(s)
Training will use 5 thread(s)
Training will use 10 thread(s)
Training will use 20 thread(s)
Training will use 40 thread(s)
Quantifiable modifiers: next: 1
Quantifiable modifiers: next: 2
Quantifiable modifiers: next: 3
Quantifiable modifiers: next: 4
Quantifiable modifiers: next: 5
Quantifiable modifiers: next: 6
Quantifiable modifiers: next: 7
Quantifiable modifiers: next: 8
Quantifiable modifiers: next: 9
Quantifiable modifiers: next: 10
Quantifiable modifiers: next: 11
Quantifiable modifiers: next: 12
Quantifiable modifiers: next: 13
Quantifiable modifiers: next: 14
Quantifiable modifiers: next: 15
Context words are on
Context words are off
Context words are on
Context words are off
Context words are on
Context words are off
Context words are on
Context words are off
Context words are on
Context words are off
Context words are on
Context words are off
Context words are on
Context words are off
Context words are on
Quantifiable: not a quantity modifier
Quantifiable: not a quantity modifier
Quantifiable: not a quantity modifier
Quantifiable: not a quantity modifier
Quantifiable: not a quantity modifier
Quantifiable: not a quantity modifier
Quantifiable: not a quantity modifier
Quantifiable: not a quantity modifier
Quantifiable: not a quantity modifier
Quantifiable: not a quantity modifier
Quantifiable: not a quantity modifier
Quantifiable: not a quantity modifier
Quantifiable: not a quantity modifier
Quantifiable: not a quantity modifier
Quantifiable: not a quantity modifier
SemanticHeadFinder failed to reassign head for
SemanticHeadFinder failed to reassign head for
SemanticHeadFinder failed to reassign head for
SemanticHeadFinder failed to reassign head for
SemanticHeadFinder failed to reassign head for
SemanticHeadFinder failed to reassign head for
SemanticHeadFinder failed to reassign head for
SemanticHeadFinder failed to reassign head for
SemanticHeadFinder failed to reassign head for
SemanticHeadFinder failed to reassign head for
SemanticHeadFinder failed to reassign head for
SemanticHeadFinder failed to reassign head for
SemanticHeadFinder failed to reassign head for
SemanticHeadFinder failed to reassign head for
SemanticHeadFinder failed to reassign head for
Model will be simplified
Model will not be simplified
Model will be simplified
Model will not be simplified
Model will be simplified
Model will not be simplified
Model will be simplified
Model will not be simplified
Model will be simplified
Model will not be simplified
Model will be simplified
Model will not be simplified
Model will be simplified
Model will not be simplified
ChineseQuantifiableEntityNormalizer.processEntity: 0.5
ChineseQuantifiableEntityNormalizer.processEntity: 1.0
ChineseQuantifiableEntityNormalizer.processEntity: 0.75
ChineseQuantifiableEntityNormalizer.processEntity: 0.25
ChineseQuantifiableEntityNormalizer.processEntity: 0.9
ChineseQuantifiableEntityNormalizer.processEntity: 0.1
ChineseQuantifiableEntityNormalizer.processEntity: 0.6
ChineseQuantifiableEntityNormalizer.processEntity: 0.4
ChineseQuantifiableEntityNormalizer.processEntity: 0.8
ChineseQuantifiableEntityNormalizer.processEntity: 0.3
ChineseQuantifiableEntityNormalizer.processEntity: 0.7
ChineseQuantifiableEntityNormalizer.processEntity: 0.2
ChineseQuantifiableEntityNormalizer.processEntity: 0.95
ChineseQuantifiableEntityNormalizer.processEntity: 0.05
ChineseQuantifiableEntityNormalizer.processEntity: 0.85
Gradient check: converting 12 compressed trees
Gradient check: converting 7 compressed trees
Gradient check: converting 15 compressed trees
Gradient check: converting 9 compressed trees
Gradient check: converting 10 compressed trees
Gradient check: converting 8 compressed trees
Gradient check: converting 11 compressed trees
Gradient check: converting 6 compressed trees
Gradient check: converting 14 compressed trees
Gradient check: converting 13 compressed trees
Gradient check: converting 5 compressed trees
Gradient check: converting 4 compressed trees
Gradient check: converting 3 compressed trees
Gradient check: converting 2 compressed trees
Gradient check: converting 1 compressed trees
Searching for resource: image.jpg ... not found.
Searching for resource: report.pdf ... not found.
Searching for resource: video.mp4 ... not found.
Searching for resource: index.html ... not found.
Searching for resource: data.csv ... not found.
Searching for resource: music.mp3 ... not found.
Searching for resource: game.exe ... not found.
Searching for resource: logo.png ... not found.
Searching for resource: script.py ... not found.
Searching for resource: style.css ... not found.
Searching for resource: document.docx ... not found.
Searching for resource: presentation.pptx ... not found.
Searching for resource: archive.zip ... not found.
Searching for resource: font.ttf ... not found.
Searching for resource: icon.ico ... not found.
Quantifiable: Processing entity string "Hello"
Quantifiable: Processing entity string "Bing"
Quantifiable: Processing entity string "123"
Quantifiable: Processing entity string "log"
Quantifiable: Processing entity string "data"
Quantifiable: Processing entity string "simulated"
Quantifiable: Processing entity string "entity"
Quantifiable: Processing entity string "string"
Quantifiable: Processing entity string "processing"
Quantifiable: Processing entity string "quantifiable"
Quantifiable: Processing entity string "example"
Quantifiable: Processing entity string "output"
Quantifiable: Processing entity string "parameter"
Quantifiable: Processing entity string "template"
Quantifiable: Processing entity string "generate"
Searching for resource: index.html ... found.
Searching for resource: logo.png ... found.
Searching for resource: style.css ... found.
Searching for resource: data.json ... found.
Searching for resource: script.js ... found.
Searching for resource: favicon.ico ... found.
Searching for resource: image.jpg ... found.
Searching for resource: video.mp4 ... found.
Searching for resource: audio.mp3 ... found.
Searching for resource: font.ttf ... found.
Searching for resource: report.pdf ... found.
Searching for resource: config.ini ... found.
Searching for resource: readme.txt ... found.
Searching for resource: backup.zip ... found.
Searching for resource: error.log ... found.
Clearing CoreNLP annotation pool; this should be unnecessary in production
Clearing CoreNLP annotation pool; this should be unnecessary in production
Clearing CoreNLP annotation pool; this should be unnecessary in production
Clearing CoreNLP annotation pool; this should be unnecessary in production
Clearing CoreNLP annotation pool; this should be unnecessary in production
Clearing CoreNLP annotation pool; this should be unnecessary in production
Clearing CoreNLP annotation pool; this should be unnecessary in production
Clearing CoreNLP annotation pool; this should be unnecessary in production
Clearing CoreNLP annotation pool; this should be unnecessary in production
Clearing CoreNLP annotation pool; this should be unnecessary in production
Clearing CoreNLP annotation pool; this should be unnecessary in production
Clearing CoreNLP annotation pool; this should be unnecessary in production
Clearing CoreNLP annotation pool; this should be unnecessary in production
Clearing CoreNLP annotation pool; this should be unnecessary in production
Clearing CoreNLP annotation pool; this should be unnecessary in production
Done converting trees
Done converting trees
Done converting trees
Done converting trees
Done converting trees
Done converting trees
Done converting trees
Done converting trees
Done converting trees
Done converting trees
Done converting trees
Done converting trees
Done converting trees
Done converting trees
Done converting trees
Saving serialized model to model_1.h5
Saving serialized model to model_2.pkl
Saving serialized model to model_3.pt
Saving serialized model to model_4.json
Saving serialized model to model_5.sav
Saving serialized model to model_6.onnx
Saving serialized model to model_7.joblib
Saving serialized model to model_8.npz
Saving serialized model to model_9.tflite
Saving serialized model to model_10.bin
Saving serialized model to model_11.xml
Saving serialized model to model_12.dat
Saving serialized model to model_13.zip
Saving serialized model to model_14.tar.gz
Saving serialized model to model_15.dill
Loading serialized model from model.pkl
Loading serialized model from classifier.h5
Loading serialized model from bert.pt
Loading serialized model from mnist.npz
Loading serialized model from gpt2.bin
Loading serialized model from resnet.pth
Loading serialized model from vgg16.hdf5
Loading serialized model from transformer.torch
Loading serialized model from lstm.sav
Loading serialized model from cnn.json
Loading serialized model from word2vec.model
Loading serialized model from xgboost.pkl
Loading serialized model from svm.joblib
Loading serialized model from knn.pickle
Loading serialized model from rf.sklearn
Options supplied by this file:
Options supplied by this file:
Options supplied by this file:
Options supplied by this file:
Options supplied by this file:
Options supplied by this file:
Options supplied by this file:
Options supplied by this file:
Options supplied by this file:
Options supplied by this file:
Options supplied by this file:
Options supplied by this file:
Options supplied by this file:
Options supplied by this file:
Options supplied by this file:
-model <name>: When training, the name of the model to save.  Otherwise, the name of the model to load.
-model <name>: When training, the name of the model to save.  Otherwise, the name of the model to load.
-model <name>: When training, the name of the model to save.  Otherwise, the name of the model to load.
-model <name>: When training, the name of the model to save.  Otherwise, the name of the model to load.
-model <name>: When training, the name of the model to save.  Otherwise, the name of the model to load.
-model <name>: When training, the name of the model to save.  Otherwise, the name of the model to load.
-model <name>: When training, the name of the model to save.  Otherwise, the name of the model to load.
-model <name>: When training, the name of the model to save.  Otherwise, the name of the model to load.
-model <name>: When training, the name of the model to save.  Otherwise, the name of the model to load.
-model <name>: When training, the name of the model to save.  Otherwise, the name of the model to load.
-model <name>: When training, the name of the model to save.  Otherwise, the name of the model to load.
-model <name>: When training, the name of the model to save.  Otherwise, the name of the model to load.
-model <name>: When training, the name of the model to save.  Otherwise, the name of the model to load.
-model <name>: When training, the name of the model to save.  Otherwise, the name of the model to load.
-model <name>: When training, the name of the model to save.  Otherwise, the name of the model to load.
-parser <name>: When training, the LexicalizedParser to use as the base model.
-parser <name>: When training, the LexicalizedParser to use as the base model.
-parser <name>: When training, the LexicalizedParser to use as the base model.
-parser <name>: When training, the LexicalizedParser to use as the base model.
-parser <name>: When training, the LexicalizedParser to use as the base model.
-parser <name>: When training, the LexicalizedParser to use as the base model.
-parser <name>: When training, the LexicalizedParser to use as the base model.
-parser <name>: When training, the LexicalizedParser to use as the base model.
-parser <name>: When training, the LexicalizedParser to use as the base model.
-parser <name>: When training, the LexicalizedParser to use as the base model.
-parser <name>: When training, the LexicalizedParser to use as the base model.
-parser <name>: When training, the LexicalizedParser to use as the base model.
-parser <name>: When training, the LexicalizedParser to use as the base model.
-parser <name>: When training, the LexicalizedParser to use as the base model.
-parser <name>: When training, the LexicalizedParser to use as the base model.
-cachedTrees <name>: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java
-cachedTrees <name>: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java
-cachedTrees <name>: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java
-cachedTrees <name>: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java
-cachedTrees <name>: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java
-cachedTrees <name>: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java
-cachedTrees <name>: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java
-cachedTrees <name>: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java
-cachedTrees <name>: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java
-cachedTrees <name>: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java
-cachedTrees <name>: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java
-cachedTrees <name>: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java
-cachedTrees <name>: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java
-cachedTrees <name>: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java
-cachedTrees <name>: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java
-treebank <name> [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.
-treebank <name> [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.
-treebank <name> [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.
-treebank <name> [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.
-treebank <name> [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.
-treebank <name> [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.
-treebank <name> [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.
-treebank <name> [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.
-treebank <name> [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.
-treebank <name> [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.
-treebank <name> [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.
-treebank <name> [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.
-treebank <name> [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.
-treebank <name> [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.
-treebank <name> [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.
-testTreebank <name> [filter]: A treebank for testing the model.
-testTreebank <name> [filter]: A treebank for testing the model.
-testTreebank <name> [filter]: A treebank for testing the model.
-testTreebank <name> [filter]: A treebank for testing the model.
-testTreebank <name> [filter]: A treebank for testing the model.
-testTreebank <name> [filter]: A treebank for testing the model.
-testTreebank <name> [filter]: A treebank for testing the model.
-testTreebank <name> [filter]: A treebank for testing the model.
-testTreebank <name> [filter]: A treebank for testing the model.
-testTreebank <name> [filter]: A treebank for testing the model.
-testTreebank <name> [filter]: A treebank for testing the model.
-testTreebank <name> [filter]: A treebank for testing the model.
-testTreebank <name> [filter]: A treebank for testing the model.
-testTreebank <name> [filter]: A treebank for testing the model.
-testTreebank <name> [filter]: A treebank for testing the model.
-train: Run training over the treebank, testing on the testTreebank.
-train: Run training over the treebank, testing on the testTreebank.
-train: Run training over the treebank, testing on the testTreebank.
-train: Run training over the treebank, testing on the testTreebank.
-train: Run training over the treebank, testing on the testTreebank.
-train: Run training over the treebank, testing on the testTreebank.
-train: Run training over the treebank, testing on the testTreebank.
-train: Run training over the treebank, testing on the testTreebank.
-train: Run training over the treebank, testing on the testTreebank.
-train: Run training over the treebank, testing on the testTreebank.
-train: Run training over the treebank, testing on the testTreebank.
-train: Run training over the treebank, testing on the testTreebank.
-train: Run training over the treebank, testing on the testTreebank.
-train: Run training over the treebank, testing on the testTreebank.
-train: Run training over the treebank, testing on the testTreebank.
-continueTraining <name>: The name of a file to continue training.
-continueTraining <name>: The name of a file to continue training.
-continueTraining <name>: The name of a file to continue training.
-continueTraining <name>: The name of a file to continue training.
-continueTraining <name>: The name of a file to continue training.
-continueTraining <name>: The name of a file to continue training.
-continueTraining <name>: The name of a file to continue training.
-continueTraining <name>: The name of a file to continue training.
-continueTraining <name>: The name of a file to continue training.
-continueTraining <name>: The name of a file to continue training.
-continueTraining <name>: The name of a file to continue training.
-continueTraining <name>: The name of a file to continue training.
-continueTraining <name>: The name of a file to continue training.
-continueTraining <name>: The name of a file to continue training.
-continueTraining <name>: The name of a file to continue training.
-nofilter: Rules for the parser will not be filtered based on the training treebank.
-nofilter: Rules for the parser will not be filtered based on the training treebank.
-nofilter: Rules for the parser will not be filtered based on the training treebank.
-nofilter: Rules for the parser will not be filtered based on the training treebank.
-nofilter: Rules for the parser will not be filtered based on the training treebank.
-nofilter: Rules for the parser will not be filtered based on the training treebank.
-nofilter: Rules for the parser will not be filtered based on the training treebank.
-nofilter: Rules for the parser will not be filtered based on the training treebank.
-nofilter: Rules for the parser will not be filtered based on the training treebank.
-nofilter: Rules for the parser will not be filtered based on the training treebank.
-nofilter: Rules for the parser will not be filtered based on the training treebank.
-nofilter: Rules for the parser will not be filtered based on the training treebank.
-nofilter: Rules for the parser will not be filtered based on the training treebank.
-nofilter: Rules for the parser will not be filtered based on the training treebank.
-nofilter: Rules for the parser will not be filtered based on the training treebank.
-runGradientCheck: Run a gradient check.
-runGradientCheck: Run a gradient check.
-runGradientCheck: Run a gradient check.
-runGradientCheck: Run a gradient check.
-runGradientCheck: Run a gradient check.
-runGradientCheck: Run a gradient check.
-runGradientCheck: Run a gradient check.
-runGradientCheck: Run a gradient check.
-runGradientCheck: Run a gradient check.
-runGradientCheck: Run a gradient check.
-runGradientCheck: Run a gradient check.
-runGradientCheck: Run a gradient check.
-runGradientCheck: Run a gradient check.
-runGradientCheck: Run a gradient check.
-runGradientCheck: Run a gradient check.
-resultsRecord: A file for recording info on intermediate results
-resultsRecord: A file for recording info on intermediate results
-resultsRecord: A file for recording info on intermediate results
-resultsRecord: A file for recording info on intermediate results
-resultsRecord: A file for recording info on intermediate results
-resultsRecord: A file for recording info on intermediate results
-resultsRecord: A file for recording info on intermediate results
-resultsRecord: A file for recording info on intermediate results
-resultsRecord: A file for recording info on intermediate results
-resultsRecord: A file for recording info on intermediate results
-resultsRecord: A file for recording info on intermediate results
-resultsRecord: A file for recording info on intermediate results
-resultsRecord: A file for recording info on intermediate results
-resultsRecord: A file for recording info on intermediate results
-resultsRecord: A file for recording info on intermediate results
Options overlapping the parser:
Options overlapping the parser:
Options overlapping the parser:
Options overlapping the parser:
Options overlapping the parser:
Options overlapping the parser:
Options overlapping the parser:
Options overlapping the parser:
Options overlapping the parser:
Options overlapping the parser:
Options overlapping the parser:
Options overlapping the parser:
Options overlapping the parser:
Options overlapping the parser:
Options overlapping the parser:
100; batchSize
50; batchSize
200; batchSize
75; batchSize
150; batchSize
25; batchSize
125; batchSize
175; batchSize
10; batchSize
250; batchSize
80; batchSize
120; batchSize
40; batchSize
160; batchSize
60; batchSize
use discourse salience
use discourse salience
use discourse salience
use discourse salience
use discourse salience
use discourse salience
use discourse salience
use discourse salience
use discourse salience
use discourse salience
use discourse salience
use discourse salience
use discourse salience
use discourse salience
use discourse salience
Error handling client# 1001
Error handling client# 2056
Error handling client# 3178
Error handling client# 4023
Error handling client# 5129
Error handling client# 6214
Error handling client# 7350
Error handling client# 8497
Error handling client# 9002
Error handling client# 10134
Error handling client# 11056
Error handling client# 12078
Error handling client# 13001
Error handling client# 14023
Error handling client# 15045
----------- collapsed dependencies -----------
----------- collapsed dependencies -----------
----------- collapsed dependencies -----------
----------- collapsed dependencies -----------
----------- collapsed dependencies -----------
----------- collapsed dependencies -----------
----------- collapsed dependencies -----------
----------- collapsed dependencies -----------
----------- collapsed dependencies -----------
----------- collapsed dependencies -----------
----------- collapsed dependencies -----------
----------- collapsed dependencies -----------
----------- collapsed dependencies -----------
----------- collapsed dependencies -----------
----------- collapsed dependencies -----------
not use truecase annotator
not use truecase annotator
not use truecase annotator
not use truecase annotator
not use truecase annotator
not use truecase annotator
not use truecase annotator
not use truecase annotator
not use truecase annotator
not use truecase annotator
not use truecase annotator
not use truecase annotator
not use truecase annotator
not use truecase annotator
not use truecase annotator
---------- CCprocessed dependencies ----------
---------- CCprocessed dependencies ----------
---------- CCprocessed dependencies ----------
---------- CCprocessed dependencies ----------
---------- CCprocessed dependencies ----------
---------- CCprocessed dependencies ----------
---------- CCprocessed dependencies ----------
---------- CCprocessed dependencies ----------
---------- CCprocessed dependencies ----------
---------- CCprocessed dependencies ----------
---------- CCprocessed dependencies ----------
---------- CCprocessed dependencies ----------
---------- CCprocessed dependencies ----------
---------- CCprocessed dependencies ----------
---------- CCprocessed dependencies ----------
read "hello" and cannot understand
read "3.14" and cannot understand
read "true" and cannot understand
read "null" and cannot understand
read "[]" and cannot understand
read "foo" and cannot understand
read "42" and cannot understand
read "bar" and cannot understand
read "{}" and cannot understand
read "baz" and cannot understand
read "qux" and cannot understand
read "\"\"" and cannot understand
read "-1" and cannot understand
read "false" and cannot understand
read "0.0" and cannot understand
use truecase annotator
use truecase annotator
use truecase annotator
use truecase annotator
use truecase annotator
use truecase annotator
use truecase annotator
use truecase annotator
use truecase annotator
use truecase annotator
use truecase annotator
use truecase annotator
use truecase annotator
use truecase annotator
use truecase annotator
Graph is not connected for:
Graph is not connected for:
Graph is not connected for:
Graph is not connected for:
Graph is not connected for:
Graph is not connected for:
Graph is not connected for:
Graph is not connected for:
Graph is not connected for:
Graph is not connected for:
Graph is not connected for:
Graph is not connected for:
Graph is not connected for:
Graph is not connected for:
Graph is not connected for:
5; numBatches
12; numBatches
8; numBatches
10; numBatches
7; numBatches
9; numBatches
6; numBatches
11; numBatches
4; numBatches
13; numBatches
3; numBatches
14; numBatches
2; numBatches
15; numBatches
1; numBatches
USE_DISCOURSE_CONSTRAINTS off
USE_DISCOURSE_CONSTRAINTS off
USE_DISCOURSE_CONSTRAINTS off
USE_DISCOURSE_CONSTRAINTS off
USE_DISCOURSE_CONSTRAINTS off
USE_DISCOURSE_CONSTRAINTS off
USE_DISCOURSE_CONSTRAINTS off
USE_DISCOURSE_CONSTRAINTS off
USE_DISCOURSE_CONSTRAINTS off
USE_DISCOURSE_CONSTRAINTS off
USE_DISCOURSE_CONSTRAINTS off
USE_DISCOURSE_CONSTRAINTS off
USE_DISCOURSE_CONSTRAINTS off
USE_DISCOURSE_CONSTRAINTS off
USE_DISCOURSE_CONSTRAINTS off
USE_DISCOURSE_CONSTRAINTS on
USE_DISCOURSE_CONSTRAINTS on
USE_DISCOURSE_CONSTRAINTS on
USE_DISCOURSE_CONSTRAINTS on
USE_DISCOURSE_CONSTRAINTS on
USE_DISCOURSE_CONSTRAINTS on
USE_DISCOURSE_CONSTRAINTS on
USE_DISCOURSE_CONSTRAINTS on
USE_DISCOURSE_CONSTRAINTS on
USE_DISCOURSE_CONSTRAINTS on
USE_DISCOURSE_CONSTRAINTS on
USE_DISCOURSE_CONSTRAINTS on
USE_DISCOURSE_CONSTRAINTS on
USE_DISCOURSE_CONSTRAINTS on
USE_DISCOURSE_CONSTRAINTS on
----------- collapsed dependencies tree -----------
----------- collapsed dependencies tree -----------
----------- collapsed dependencies tree -----------
----------- collapsed dependencies tree -----------
----------- collapsed dependencies tree -----------
----------- collapsed dependencies tree -----------
----------- collapsed dependencies tree -----------
----------- collapsed dependencies tree -----------
----------- collapsed dependencies tree -----------
----------- collapsed dependencies tree -----------
----------- collapsed dependencies tree -----------
----------- collapsed dependencies tree -----------
----------- collapsed dependencies tree -----------
----------- collapsed dependencies tree -----------
----------- collapsed dependencies tree -----------
USE_GOLD_POS off
USE_GOLD_POS off
USE_GOLD_POS off
USE_GOLD_POS off
USE_GOLD_POS off
USE_GOLD_POS off
USE_GOLD_POS off
USE_GOLD_POS off
USE_GOLD_POS off
USE_GOLD_POS off
USE_GOLD_POS off
USE_GOLD_POS off
USE_GOLD_POS off
USE_GOLD_POS off
USE_GOLD_POS off
The modeling server is running.
The modeling server is running.
The modeling server is running.
The modeling server is running.
The modeling server is running.
The modeling server is running.
The modeling server is running.
The modeling server is running.
The modeling server is running.
The modeling server is running.
The modeling server is running.
The modeling server is running.
The modeling server is running.
The modeling server is running.
The modeling server is running.
----------- enhanced dependencies tree -----------
----------- enhanced dependencies tree -----------
----------- enhanced dependencies tree -----------
----------- enhanced dependencies tree -----------
----------- enhanced dependencies tree -----------
----------- enhanced dependencies tree -----------
----------- enhanced dependencies tree -----------
----------- enhanced dependencies tree -----------
----------- enhanced dependencies tree -----------
----------- enhanced dependencies tree -----------
----------- enhanced dependencies tree -----------
----------- enhanced dependencies tree -----------
----------- enhanced dependencies tree -----------
----------- enhanced dependencies tree -----------
----------- enhanced dependencies tree -----------
USE_GOLD_POS on
USE_GOLD_POS on
USE_GOLD_POS on
USE_GOLD_POS on
USE_GOLD_POS on
USE_GOLD_POS on
USE_GOLD_POS on
USE_GOLD_POS on
USE_GOLD_POS on
USE_GOLD_POS on
USE_GOLD_POS on
USE_GOLD_POS on
USE_GOLD_POS on
USE_GOLD_POS on
USE_GOLD_POS on
Total time for AnnotationPipeline: 0.45 sec.
Total time for AnnotationPipeline: 1.23 sec.
Total time for AnnotationPipeline: 0.67 sec.
Total time for AnnotationPipeline: 0.89 sec.
Total time for AnnotationPipeline: 1.56 sec.
Total time for AnnotationPipeline: 0.32 sec.
Total time for AnnotationPipeline: 1.78 sec.
Total time for AnnotationPipeline: 0.98 sec.
Total time for AnnotationPipeline: 1.34 sec.
Total time for AnnotationPipeline: 0.76 sec.
Total time for AnnotationPipeline: 1.11 sec.
Total time for AnnotationPipeline: 0.54 sec.
Total time for AnnotationPipeline: 1.67 sec.
Total time for AnnotationPipeline: 0.87 sec.
Total time for AnnotationPipeline: 1.44 sec.
----------- enhanced++ dependencies tree -----------
----------- enhanced++ dependencies tree -----------
----------- enhanced++ dependencies tree -----------
----------- enhanced++ dependencies tree -----------
----------- enhanced++ dependencies tree -----------
----------- enhanced++ dependencies tree -----------
----------- enhanced++ dependencies tree -----------
----------- enhanced++ dependencies tree -----------
----------- enhanced++ dependencies tree -----------
----------- enhanced++ dependencies tree -----------
----------- enhanced++ dependencies tree -----------
----------- enhanced++ dependencies tree -----------
----------- enhanced++ dependencies tree -----------
----------- enhanced++ dependencies tree -----------
----------- enhanced++ dependencies tree -----------
use Stanford NER
use Stanford NER
use Stanford NER
use Stanford NER
use Stanford NER
use Stanford NER
use Stanford NER
use Stanford NER
use Stanford NER
use Stanford NER
use Stanford NER
use Stanford NER
use Stanford NER
use Stanford NER
use Stanford NER
------------- GrammaticalStructure -------------
------------- GrammaticalStructure -------------
------------- GrammaticalStructure -------------
------------- GrammaticalStructure -------------
------------- GrammaticalStructure -------------
------------- GrammaticalStructure -------------
------------- GrammaticalStructure -------------
------------- GrammaticalStructure -------------
------------- GrammaticalStructure -------------
------------- GrammaticalStructure -------------
------------- GrammaticalStructure -------------
------------- GrammaticalStructure -------------
------------- GrammaticalStructure -------------
------------- GrammaticalStructure -------------
------------- GrammaticalStructure -------------
use gold NE type annotation
use gold NE type annotation
use gold NE type annotation
use gold NE type annotation
use gold NE type annotation
use gold NE type annotation
use gold NE type annotation
use gold NE type annotation
use gold NE type annotation
use gold NE type annotation
use gold NE type annotation
use gold NE type annotation
use gold NE type annotation
use gold NE type annotation
use gold NE type annotation
5; Iterations
10; Iterations
15; Iterations
20; Iterations
25; Iterations
30; Iterations
35; Iterations
40; Iterations
45; Iterations
50; Iterations
55; Iterations
60; Iterations
65; Iterations
70; Iterations
75; Iterations
Cached annotator will never GC -- this can cause OOM exceptions!
Cached annotator will never GC -- this can cause OOM exceptions!
Cached annotator will never GC -- this can cause OOM exceptions!
Cached annotator will never GC -- this can cause OOM exceptions!
Cached annotator will never GC -- this can cause OOM exceptions!
Cached annotator will never GC -- this can cause OOM exceptions!
Cached annotator will never GC -- this can cause OOM exceptions!
Cached annotator will never GC -- this can cause OOM exceptions!
Cached annotator will never GC -- this can cause OOM exceptions!
Cached annotator will never GC -- this can cause OOM exceptions!
Cached annotator will never GC -- this can cause OOM exceptions!
Cached annotator will never GC -- this can cause OOM exceptions!
Cached annotator will never GC -- this can cause OOM exceptions!
Cached annotator will never GC -- this can cause OOM exceptions!
Cached annotator will never GC -- this can cause OOM exceptions!
------------- non-collapsed dependencies (basic + extra) ---------------
------------- non-collapsed dependencies (basic + extra) ---------------
------------- non-collapsed dependencies (basic + extra) ---------------
------------- non-collapsed dependencies (basic + extra) ---------------
------------- non-collapsed dependencies (basic + extra) ---------------
------------- non-collapsed dependencies (basic + extra) ---------------
------------- non-collapsed dependencies (basic + extra) ---------------
------------- non-collapsed dependencies (basic + extra) ---------------
------------- non-collapsed dependencies (basic + extra) ---------------
------------- non-collapsed dependencies (basic + extra) ---------------
------------- non-collapsed dependencies (basic + extra) ---------------
------------- non-collapsed dependencies (basic + extra) ---------------
------------- non-collapsed dependencies (basic + extra) ---------------
------------- non-collapsed dependencies (basic + extra) ---------------
------------- non-collapsed dependencies (basic + extra) ---------------
USE_GOLD_PARSES off
USE_GOLD_PARSES off
USE_GOLD_PARSES off
USE_GOLD_PARSES off
USE_GOLD_PARSES off
USE_GOLD_PARSES off
USE_GOLD_PARSES off
USE_GOLD_PARSES off
USE_GOLD_PARSES off
USE_GOLD_PARSES off
USE_GOLD_PARSES off
USE_GOLD_PARSES off
USE_GOLD_PARSES off
USE_GOLD_PARSES off
USE_GOLD_PARSES off
Replacing old annotator "NER" with signature [StanfordCoreNLP, 3.9.2] with new annotator with signature [SpaCy, 3.1.3]
Replacing old annotator "POS" with signature [OpenNLP, 1.9.3] with new annotator with signature [NLTK, 3.6.5]
Replacing old annotator "Sentiment" with signature [VADER, 0.5] with new annotator with signature [TextBlob, 0.16.0]
Replacing old annotator "Coref" with signature [AllenNLP, 2.8.0] with new annotator with signature [NeuralCoref, 4.0]
Replacing old annotator "Summarizer" with signature [Gensim, 4.1.2] with new annotator with signature [BART, 0.4.0]
Replacing old annotator "Parser" with signature [BerkeleyParser, 1.7] with new annotator with signature [UDPipe, 2.6]
Replacing old annotator "Lemmatizer" with signature [WordNetLemmatizer, 2.0] with new annotator with signature [Pattern, 3.6]
Replacing old annotator "Tokenizer" with signature [WhitespaceTokenizer, 1.0] with new annotator with signature [SentencePiece, 0.1.96]
Replacing old annotator "SpellChecker" with signature [PyEnchant, 3.2.1] with new annotator with signature [SymSpell, 6.7.0]
Replacing old annotator "TopicModeler" with signature [LDA, 1.2] with new annotator with signature [BERTopic, 0.9.4]
Replacing old annotator "Translator" with signature [GoogleTranslate, 3.0] with new annotator with signature [MarianMT, 1.9]
Replacing old annotator "Stemmer" with signature [PorterStemmer, 2.0] with new annotator with signature [SnowballStemmer, 2.1]
Replacing old annotator "KeywordExtractor" with signature [RAKE, 1.0] with new annotator with signature [YAKE, 0.4]
Replacing old annotator "NER" with signature [SpaCy, 3.1.3] with new annotator with signature [Flair, 0.9]
Replacing old annotator "Sentiment" with signature [TextBlob, 0.16.0] with new annotator with signature [HuggingFace Transformers, 4.12]
------------- collapsed dependencies -----------
------------- collapsed dependencies -----------
------------- collapsed dependencies -----------
------------- collapsed dependencies -----------
------------- collapsed dependencies -----------
------------- collapsed dependencies -----------
------------- collapsed dependencies -----------
------------- collapsed dependencies -----------
------------- collapsed dependencies -----------
------------- collapsed dependencies -----------
------------- collapsed dependencies -----------
------------- collapsed dependencies -----------
------------- collapsed dependencies -----------
------------- collapsed dependencies -----------
------------- collapsed dependencies -----------
USE_GOLD_PARSES on
USE_GOLD_PARSES on
USE_GOLD_PARSES on
USE_GOLD_PARSES on
USE_GOLD_PARSES on
USE_GOLD_PARSES on
USE_GOLD_PARSES on
USE_GOLD_PARSES on
USE_GOLD_PARSES on
USE_GOLD_PARSES on
USE_GOLD_PARSES on
USE_GOLD_PARSES on
USE_GOLD_PARSES on
USE_GOLD_PARSES on
USE_GOLD_PARSES on
12.34; Completion Time
9.87; Completion Time
15.67; Completion Time
10.45; Completion Time
13.89; Completion Time
11.23; Completion Time
14.56; Completion Time
8.90; Completion Time
16.78; Completion Time
10.01; Completion Time
12.98; Completion Time
9.54; Completion Time
15.43; Completion Time
10.76; Completion Time
13.21; Completion Time
------------- collapsed dependencies tree -----------
------------- collapsed dependencies tree -----------
------------- collapsed dependencies tree -----------
------------- collapsed dependencies tree -----------
------------- collapsed dependencies tree -----------
------------- collapsed dependencies tree -----------
------------- collapsed dependencies tree -----------
------------- collapsed dependencies tree -----------
------------- collapsed dependencies tree -----------
------------- collapsed dependencies tree -----------
------------- collapsed dependencies tree -----------
------------- collapsed dependencies tree -----------
------------- collapsed dependencies tree -----------
------------- collapsed dependencies tree -----------
------------- collapsed dependencies tree -----------
USE_GOLD_SPEAKER_TAGS off
USE_GOLD_SPEAKER_TAGS off
USE_GOLD_SPEAKER_TAGS off
USE_GOLD_SPEAKER_TAGS off
USE_GOLD_SPEAKER_TAGS off
USE_GOLD_SPEAKER_TAGS off
USE_GOLD_SPEAKER_TAGS off
USE_GOLD_SPEAKER_TAGS off
USE_GOLD_SPEAKER_TAGS off
USE_GOLD_SPEAKER_TAGS off
USE_GOLD_SPEAKER_TAGS off
USE_GOLD_SPEAKER_TAGS off
USE_GOLD_SPEAKER_TAGS off
USE_GOLD_SPEAKER_TAGS off
USE_GOLD_SPEAKER_TAGS off
------------- CCprocessed dependencies --------
------------- CCprocessed dependencies --------
------------- CCprocessed dependencies --------
------------- CCprocessed dependencies --------
------------- CCprocessed dependencies --------
------------- CCprocessed dependencies --------
------------- CCprocessed dependencies --------
------------- CCprocessed dependencies --------
------------- CCprocessed dependencies --------
------------- CCprocessed dependencies --------
------------- CCprocessed dependencies --------
------------- CCprocessed dependencies --------
------------- CCprocessed dependencies --------
------------- CCprocessed dependencies --------
------------- CCprocessed dependencies --------
USE_GOLD_SPEAKER_TAGS on
USE_GOLD_SPEAKER_TAGS on
USE_GOLD_SPEAKER_TAGS on
USE_GOLD_SPEAKER_TAGS on
USE_GOLD_SPEAKER_TAGS on
USE_GOLD_SPEAKER_TAGS on
USE_GOLD_SPEAKER_TAGS on
USE_GOLD_SPEAKER_TAGS on
USE_GOLD_SPEAKER_TAGS on
USE_GOLD_SPEAKER_TAGS on
USE_GOLD_SPEAKER_TAGS on
USE_GOLD_SPEAKER_TAGS on
USE_GOLD_SPEAKER_TAGS on
USE_GOLD_SPEAKER_TAGS on
USE_GOLD_SPEAKER_TAGS on
USE_GOLD_MENTIONS off
USE_GOLD_MENTIONS off
USE_GOLD_MENTIONS off
USE_GOLD_MENTIONS off
USE_GOLD_MENTIONS off
USE_GOLD_MENTIONS off
USE_GOLD_MENTIONS off
USE_GOLD_MENTIONS off
USE_GOLD_MENTIONS off
USE_GOLD_MENTIONS off
USE_GOLD_MENTIONS off
USE_GOLD_MENTIONS off
USE_GOLD_MENTIONS off
USE_GOLD_MENTIONS off
USE_GOLD_MENTIONS off
Loading segmentation model ...
Loading segmentation model ...
Loading segmentation model ...
Loading segmentation model ...
Loading segmentation model ...
Loading segmentation model ...
Loading segmentation model ...
Loading segmentation model ...
Loading segmentation model ...
Loading segmentation model ...
Loading segmentation model ...
Loading segmentation model ...
Loading segmentation model ...
Loading segmentation model ...
Loading segmentation model ...
Output Files Closed
Output Files Closed
Output Files Closed
Output Files Closed
Output Files Closed
Output Files Closed
Output Files Closed
Output Files Closed
Output Files Closed
Output Files Closed
Output Files Closed
Output Files Closed
Output Files Closed
Output Files Closed
Output Files Closed
dependency graph NOT connected! possible offending nodes: [0, 1, 2]
dependency graph NOT connected! possible offending nodes: [3, 4, 5, 6]
dependency graph NOT connected! possible offending nodes: [7, 8, 9]
dependency graph NOT connected! possible offending nodes: [10, 11]
dependency graph NOT connected! possible offending nodes: [12, 13, 14]
dependency graph NOT connected! possible offending nodes: [15, 16, 17, 18]
dependency graph NOT connected! possible offending nodes: [19, 20]
dependency graph NOT connected! possible offending nodes: [21, 22, 23]
dependency graph NOT connected! possible offending nodes: [24, 25, 26, 27]
dependency graph NOT connected! possible offending nodes: [28, 29]
dependency graph NOT connected! possible offending nodes: [30, 31, 32]
dependency graph NOT connected! possible offending nodes: [33, 34, 35, 36]
dependency graph NOT connected! possible offending nodes: [37, 38]
dependency graph NOT connected! possible offending nodes: [39, 40, 41]
dependency graph NOT connected! possible offending nodes: [42]
USE_GOLD_MENTIONS on
USE_GOLD_MENTIONS on
USE_GOLD_MENTIONS on
USE_GOLD_MENTIONS on
USE_GOLD_MENTIONS on
USE_GOLD_MENTIONS on
USE_GOLD_MENTIONS on
USE_GOLD_MENTIONS on
USE_GOLD_MENTIONS on
USE_GOLD_MENTIONS on
USE_GOLD_MENTIONS on
USE_GOLD_MENTIONS on
USE_GOLD_MENTIONS on
USE_GOLD_MENTIONS on
USE_GOLD_MENTIONS on
Loading Segmentation Model ...
Loading Segmentation Model ...
Loading Segmentation Model ...
Loading Segmentation Model ...
Loading Segmentation Model ...
Loading Segmentation Model ...
Loading Segmentation Model ...
Loading Segmentation Model ...
Loading Segmentation Model ...
Loading Segmentation Model ...
Loading Segmentation Model ...
Loading Segmentation Model ...
Loading Segmentation Model ...
Loading Segmentation Model ...
Loading Segmentation Model ...
USE_GOLD_MENTION_BOUNDARIES off
USE_GOLD_MENTION_BOUNDARIES off
USE_GOLD_MENTION_BOUNDARIES off
USE_GOLD_MENTION_BOUNDARIES off
USE_GOLD_MENTION_BOUNDARIES off
USE_GOLD_MENTION_BOUNDARIES off
USE_GOLD_MENTION_BOUNDARIES off
USE_GOLD_MENTION_BOUNDARIES off
USE_GOLD_MENTION_BOUNDARIES off
USE_GOLD_MENTION_BOUNDARIES off
USE_GOLD_MENTION_BOUNDARIES off
USE_GOLD_MENTION_BOUNDARIES off
USE_GOLD_MENTION_BOUNDARIES off
USE_GOLD_MENTION_BOUNDARIES off
USE_GOLD_MENTION_BOUNDARIES off
Adding Segmentation annotation ...
Adding Segmentation annotation ...
Adding Segmentation annotation ...
Adding Segmentation annotation ...
Adding Segmentation annotation ...
Adding Segmentation annotation ...
Adding Segmentation annotation ...
Adding Segmentation annotation ...
Adding Segmentation annotation ...
Adding Segmentation annotation ...
Adding Segmentation annotation ...
Adding Segmentation annotation ...
Adding Segmentation annotation ...
Adding Segmentation annotation ...
Adding Segmentation annotation ...
dependencies form connected graphs.
dependencies form connected graphs.
dependencies form connected graphs.
dependencies form connected graphs.
dependencies form connected graphs.
dependencies form connected graphs.
dependencies form connected graphs.
dependencies form connected graphs.
dependencies form connected graphs.
dependencies form connected graphs.
dependencies form connected graphs.
dependencies form connected graphs.
dependencies form connected graphs.
dependencies form connected graphs.
dependencies form connected graphs.
USE_GOLD_MENTION_BOUNDARIES on
USE_GOLD_MENTION_BOUNDARIES on
USE_GOLD_MENTION_BOUNDARIES on
USE_GOLD_MENTION_BOUNDARIES on
USE_GOLD_MENTION_BOUNDARIES on
USE_GOLD_MENTION_BOUNDARIES on
USE_GOLD_MENTION_BOUNDARIES on
USE_GOLD_MENTION_BOUNDARIES on
USE_GOLD_MENTION_BOUNDARIES on
USE_GOLD_MENTION_BOUNDARIES on
USE_GOLD_MENTION_BOUNDARIES on
USE_GOLD_MENTION_BOUNDARIES on
USE_GOLD_MENTION_BOUNDARIES on
USE_GOLD_MENTION_BOUNDARIES on
USE_GOLD_MENTION_BOUNDARIES on
tree dependencies form a DAG: true
tree dependencies form a DAG: false
tree dependencies form a DAG: True
tree dependencies form a DAG: False
tree dependencies form a DAG: TRUE
tree dependencies form a DAG: FALSE
tree dependencies form a DAG: 1
tree dependencies form a DAG: 0
tree dependencies form a DAG: yes
tree dependencies form a DAG: no
tree dependencies form a DAG: Yes
tree dependencies form a DAG: No
tree dependencies form a DAG: YES
tree dependencies form a DAG: NO
tree dependencies form a DAG: error
use conll gold set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use gold
use conll gold set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use gold
use conll gold set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use gold
use conll gold set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use gold
use conll gold set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use gold
use conll gold set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use gold
use conll gold set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use gold
use conll gold set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use gold
use conll gold set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use gold
use conll gold set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use gold
use conll gold set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use gold
use conll gold set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use gold
use conll gold set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use gold
use conll gold set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use gold
use conll gold set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use gold
sentChars (length 5) is hello
sentChars (length 8) is goodbye
sentChars (length 3) is yes
sentChars (length 4) is nope
sentChars (length 6) is thanks
sentChars (length 7) is please
sentChars (length 9) is awesome
sentChars (length 10) is fantastic
sentChars (length 11) is wonderful
sentChars (length 12) is incredible
sentChars (length 13) is marvelous
sentChars (length 14) is outstanding
sentChars (length 15) is spectacular
sentChars (length 16) is magnificent
sentChars (length 17) is extraordinary
use conll auto set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use auto
use conll auto set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use auto
use conll auto set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use auto
use conll auto set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use auto
use conll auto set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use auto
use conll auto set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use auto
use conll auto set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use auto
use conll auto set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use auto
use conll auto set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use auto
use conll auto set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use auto
use conll auto set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use auto
use conll auto set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use auto
use conll auto set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use auto
use conll auto set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use auto
use conll auto set -> if GOLD_NE, GOLD_PARSE, GOLD_POS, etc turned on, use auto
REMOVE_SINGLETONS off
REMOVE_SINGLETONS off
REMOVE_SINGLETONS off
REMOVE_SINGLETONS off
REMOVE_SINGLETONS off
REMOVE_SINGLETONS off
REMOVE_SINGLETONS off
REMOVE_SINGLETONS off
REMOVE_SINGLETONS off
REMOVE_SINGLETONS off
REMOVE_SINGLETONS off
REMOVE_SINGLETONS off
REMOVE_SINGLETONS off
REMOVE_SINGLETONS off
REMOVE_SINGLETONS off
Sentence length=25 is longer than maximum set length 20
Sentence length=18 is longer than maximum set length 15
Sentence length=22 is longer than maximum set length 18
Sentence length=24 is longer than maximum set length 21
Sentence length=19 is longer than maximum set length 16
Sentence length=23 is longer than maximum set length 17
Sentence length=21 is longer than maximum set length 19
Sentence length=26 is longer than maximum set length 22
Sentence length=20 is longer than maximum set length 14
Sentence length=27 is longer than maximum set length 23
Sentence length=28 is longer than maximum set length 24
Sentence length=29 is longer than maximum set length 25
Sentence length=30 is longer than maximum set length 26
Sentence length=31 is longer than maximum set length 27
Sentence length=32 is longer than maximum set length 28
REMOVE_SINGLETONS on
REMOVE_SINGLETONS on
REMOVE_SINGLETONS on
REMOVE_SINGLETONS on
REMOVE_SINGLETONS on
REMOVE_SINGLETONS on
REMOVE_SINGLETONS on
REMOVE_SINGLETONS on
REMOVE_SINGLETONS on
REMOVE_SINGLETONS on
REMOVE_SINGLETONS on
REMOVE_SINGLETONS on
REMOVE_SINGLETONS on
REMOVE_SINGLETONS on
REMOVE_SINGLETONS on
REMOVE_APPOSITION_PREDICATENOMINATIVES off
REMOVE_APPOSITION_PREDICATENOMINATIVES off
REMOVE_APPOSITION_PREDICATENOMINATIVES off
REMOVE_APPOSITION_PREDICATENOMINATIVES off
REMOVE_APPOSITION_PREDICATENOMINATIVES off
REMOVE_APPOSITION_PREDICATENOMINATIVES off
REMOVE_APPOSITION_PREDICATENOMINATIVES off
REMOVE_APPOSITION_PREDICATENOMINATIVES off
REMOVE_APPOSITION_PREDICATENOMINATIVES off
REMOVE_APPOSITION_PREDICATENOMINATIVES off
REMOVE_APPOSITION_PREDICATENOMINATIVES off
REMOVE_APPOSITION_PREDICATENOMINATIVES off
REMOVE_APPOSITION_PREDICATENOMINATIVES off
REMOVE_APPOSITION_PREDICATENOMINATIVES off
REMOVE_APPOSITION_PREDICATENOMINATIVES off
Long Sentence: The quick brown fox jumps over the lazy dog.
Long Sentence: She sells seashells by the seashore.
Long Sentence: To be or not to be, that is the question.
Long Sentence: A stitch in time saves nine.
Long Sentence: All work and no play makes Jack a dull boy.
Long Sentence: May the force be with you.
Long Sentence: I have a dream that one day this nation will rise up and live out the true meaning of its creed.
Long Sentence: It was the best of times, it was the worst of times.
Long Sentence: Four score and seven years ago our fathers brought forth on this continent a new nation.
Long Sentence: Houston, we have a problem.
Long Sentence: In the beginning God created the heaven and the earth.
Long Sentence: You can't handle the truth!
Long Sentence: I'm sorry Dave, I'm afraid I can't do that.
Long Sentence: The only thing we have to fear is fear itself.
Long Sentence: That's one small step for man, one giant leap for mankind.
Working on word hello, sentChar f (sentChars index 0)
Working on word world, sentChar l (sentChars index 3)
Working on word Bing, sentChar g (sentChars index 3)
Working on word chat, sentChar c (sentChars index 0)
Working on word mode, sentChar e (sentChars index 3)
Working on word log, sentChar o (sentChars index 1)
Working on word data, sentChar a (sentChars index 1)
Working on word simulate, sentChar i (sentChars index 2)
Working on word generate, sentChar n (sentChars index 4)
Working on word template, sentChar m (sentChars index 2)
Working on word parameter, sentChar r (sentChars index 8)
Working on word example, sentChar x (sentChars index 1)
Working on word output, sentChar t (sentChars index 5)
Working on word realistic, sentChar s (sentChars index 6)
Working on word possible, sentChar b (sentChars index -1)
REMOVE_APPOSITION_PREDICATENOMINATIVES on
REMOVE_APPOSITION_PREDICATENOMINATIVES on
REMOVE_APPOSITION_PREDICATENOMINATIVES on
REMOVE_APPOSITION_PREDICATENOMINATIVES on
REMOVE_APPOSITION_PREDICATENOMINATIVES on
REMOVE_APPOSITION_PREDICATENOMINATIVES on
REMOVE_APPOSITION_PREDICATENOMINATIVES on
REMOVE_APPOSITION_PREDICATENOMINATIVES on
REMOVE_APPOSITION_PREDICATENOMINATIVES on
REMOVE_APPOSITION_PREDICATENOMINATIVES on
REMOVE_APPOSITION_PREDICATENOMINATIVES on
REMOVE_APPOSITION_PREDICATENOMINATIVES on
REMOVE_APPOSITION_PREDICATENOMINATIVES on
REMOVE_APPOSITION_PREDICATENOMINATIVES on
REMOVE_APPOSITION_PREDICATENOMINATIVES on
Encountered an empty word. Shouldn't happen?
Encountered an empty word. Shouldn't happen?
Encountered an empty word. Shouldn't happen?
Encountered an empty word. Shouldn't happen?
Encountered an empty word. Shouldn't happen?
Encountered an empty word. Shouldn't happen?
Encountered an empty word. Shouldn't happen?
Encountered an empty word. Shouldn't happen?
Encountered an empty word. Shouldn't happen?
Encountered an empty word. Shouldn't happen?
Encountered an empty word. Shouldn't happen?
Encountered an empty word. Shouldn't happen?
Encountered an empty word. Shouldn't happen?
Encountered an empty word. Shouldn't happen?
Encountered an empty word. Shouldn't happen?
WARNING: Already contain mention mentionID=4567
WARNING: Already contain mention mentionID=8910
WARNING: Already contain mention mentionID=1112
WARNING: Already contain mention mentionID=1314
WARNING: Already contain mention mentionID=1516
WARNING: Already contain mention mentionID=1718
WARNING: Already contain mention mentionID=1920
WARNING: Already contain mention mentionID=2122
WARNING: Already contain mention mentionID=2324
WARNING: Already contain mention mentionID=2526
WARNING: Already contain mention mentionID=2728
WARNING: Already contain mention mentionID=2930
WARNING: Already contain mention mentionID=3132
WARNING: Already contain mention mentionID=3334
WARNING: Already contain mention mentionID=3536
Reading cached parses from file.txt
Reading cached parses from data.json
Reading cached parses from input.csv
Reading cached parses from cache.db
Reading cached parses from stream.bin
Reading cached parses from log.xml
Reading cached parses from inputDesc
Reading cached parses from config.ini
Reading cached parses from output.txt
Reading cached parses from data.pkl
Reading cached parses from input.zip
Reading cached parses from cache.tmp
Reading cached parses from stream.log
Reading cached parses from log.jsonl
Reading cached parses from input.parquet
com.example.service.UserService: Unknown exception in input stream
org.apache.commons.io.FileUtils: Unknown exception in input stream
java.util.Scanner: Unknown exception in input stream
com.google.gson.JsonParser: Unknown exception in input stream
javax.xml.parsers.SAXParser: Unknown exception in input stream
org.springframework.web.client.RestTemplate: Unknown exception in input stream
java.io.ObjectInputStream: Unknown exception in input stream
java.net.Socket: Unknown exception in input stream
java.nio.channels.FileChannel: Unknown exception in input stream
com.mongodb.client.MongoCollection: Unknown exception in input stream
java.util.zip.ZipInputStream: Unknown exception in input stream
org.apache.poi.ss.usermodel.WorkbookFactory: Unknown exception in input stream
java.security.MessageDigest: Unknown exception in input stream
javax.crypto.CipherInputStream: Unknown exception in input stream
java.awt.image.BufferedImage: Unknown exception in input stream
Error: on word "hello" at position 5 trying to get at position 4
Error: on word "world" at position 10 trying to get at position 9
Error: on word "foo" at position 3 trying to get at position 2
Error: on word "bar" at position 6 trying to get at position 5
Error: on word "baz" at position 9 trying to get at position 8
Error: on word "qux" at position 12 trying to get at position 11
Error: on word "quux" at position 15 trying to get at position 14
Error: on word "corge" at position 18 trying to get at position 17
Error: on word "grault" at position 21 trying to get at position 20
Error: on word "garply" at position 24 trying to get at position 23
Error: on word "waldo" at position 27 trying to get at position 26
Error: on word "fred" at position 30 trying to get at position 29
Error: on word "plugh" at position 33 trying to get at position 32
Error: on word "xyzzy" at position 36 trying to get at position 35
Error: on word "thud" at position 39 trying to get at position 38
OLD mention: hello[0,5]
OLD mention: world[6,11]
OLD mention: foo[12,15]
OLD mention: bar[16,19]
OLD mention: test[20,24]
OLD mention: Bing[25,29]
OLD mention: chat[30,34]
OLD mention: mode[35,39]
OLD mention: log[40,43]
OLD mention: sim[44,47]
OLD mention: gen[48,51]
OLD mention: fun[52,55]
OLD mention: end[56,59]
OLD mention: bye[60,63]
OLD mention: see[64,67]
sentenceIds are not increasing (last=456, curr=123)
sentenceIds are not increasing (last=789, curr=456)
sentenceIds are not increasing (last=234, curr=111)
sentenceIds are not increasing (last=567, curr=345)
sentenceIds are not increasing (last=891, curr=678)
sentenceIds are not increasing (last=345, curr=234)
sentenceIds are not increasing (last=678, curr=567)
sentenceIds are not increasing (last=111, curr=789)
sentenceIds are not increasing (last=123, curr=891)
sentenceIds are not increasing (last=432, curr=321)
sentenceIds are not increasing (last=765, curr=654)
sentenceIds are not increasing (last=198, curr=987)
sentenceIds are not increasing (last=654, curr=543)
sentenceIds are not increasing (last=321, curr=432)
sentenceIds are not increasing (last=987, curr=198)
last element of sentChars is a
last element of sentChars is 9
last element of sentChars is Z
last element of sentChars is !
last element of sentChars is [
last element of sentChars is *
last element of sentChars is g
last element of sentChars is 0
last element of sentChars is Q
last element of sentChars is )
last element of sentChars is _
last element of sentChars is 5
last element of sentChars is t
last element of sentChars is =
Read 12 trees, from input1.txt in 0.34 secs
Read 8 trees, from input2.csv in 0.21 secs
Read 15 trees, from input3.json in 0.45 secs
Read 10 trees, from input4.xml in 0.32 secs
Read 9 trees, from input5.dat in 0.28 secs
Read 11 trees, from input6.bin in 0.35 secs
Read 13 trees, from input7.log in 0.39 secs
Read 7 trees, from input8.ini in 0.19 secs
Read 14 trees, from input9.conf in 0.42 secs
Read 6 trees, from input10.yaml in 0.18 secs
Read 16 trees, from input11.tsv in 0.48 secs
Read 5 trees, from input12.md in 0.17 secs
Read 17 trees, from input13.rtf in 0.51 secs
Read 4 trees, from input14.docx in 0.16 secs
Read 18 trees, from input15.pdf in 0.54 secs
Parser invoked with arguments:
Parser invoked with arguments:
Parser invoked with arguments:
Parser invoked with arguments:
Parser invoked with arguments:
Parser invoked with arguments:
Parser invoked with arguments:
Parser invoked with arguments:
Parser invoked with arguments:
Parser invoked with arguments:
Parser invoked with arguments:
Parser invoked with arguments:
Parser invoked with arguments:
Parser invoked with arguments:
Parser invoked with arguments:
Checking Document 4567 span (0,25)
Checking Document 8910 span (26,50)
Checking Document 2345 span (51,75)
Checking Document 6789 span (76,100)
Checking Document 1234 span (101,125)
Checking Document 4321 span (126,150)
Checking Document 8765 span (151,175)
Checking Document 1098 span (176,200)
Checking Document 5432 span (201,225)
Checking Document 9876 span (226,250)
Checking Document 3210 span (251,275)
Checking Document 7654 span (276,300)
Checking Document 2109 span (301,325)
Checking Document 6543 span (326,350)
Checking Document 0987 span (351,375)
***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***
***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***
***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***
***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***
***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***
***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***
***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***
***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***
***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***
***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***
***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***
***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***
***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***
***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***
***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***
Cannot find node in dependency for word "hello"
Cannot find node in dependency for word "algorithm"
Cannot find node in dependency for word "data"
Cannot find node in dependency for word "structure"
Cannot find node in dependency for word "analysis"
Cannot find node in dependency for word "machine"
Cannot find node in dependency for word "learning"
Cannot find node in dependency for word "artificial"
Cannot find node in dependency for word "intelligence"
Cannot find node in dependency for word "natural"
Cannot find node in dependency for word "language"
Cannot find node in dependency for word "processing"
Cannot find node in dependency for word "computer"
Cannot find node in dependency for word "vision"
Cannot find node in dependency for word "network"
***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***
***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***
***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***
***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***
***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***
***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***
***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***
***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***
***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***
***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***
***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***
***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***
***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***
***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***
***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***
***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***
***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***
***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***
***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***
***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***
***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***
***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***
***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***
***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***
***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***
***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***
***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***
***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***
***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***
***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***
WARNING: Document 4567 span does not match sentence
WARNING: Document 8910 span does not match sentence
WARNING: Document 2345 span does not match sentence
WARNING: Document 6789 span does not match sentence
WARNING: Document 1234 span does not match sentence
WARNING: Document 4321 span does not match sentence
WARNING: Document 9876 span does not match sentence
WARNING: Document 6543 span does not match sentence
WARNING: Document 1098 span does not match sentence
WARNING: Document 7654 span does not match sentence
WARNING: Document 3210 span does not match sentence
WARNING: Document 8765 span does not match sentence
WARNING: Document 5432 span does not match sentence
WARNING: Document 2109 span does not match sentence
WARNING: Document 0987 span does not match sentence
***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***
***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***
***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***
***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***
***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***
***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***
***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***
***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***
***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***
***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***
***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***
***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***
***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***
***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***
***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***
NO MENTIONS for cluster 1001
NO MENTIONS for cluster 2002
NO MENTIONS for cluster 3003
NO MENTIONS for cluster 4004
NO MENTIONS for cluster 5005
NO MENTIONS for cluster 6006
NO MENTIONS for cluster 7007
NO MENTIONS for cluster 8008
NO MENTIONS for cluster 9009
NO MENTIONS for cluster 1010
NO MENTIONS for cluster 1111
NO MENTIONS for cluster 1212
NO MENTIONS for cluster 1313
NO MENTIONS for cluster 1414
NO MENTIONS for cluster 1515
***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***
***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***
***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***
***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***
***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***
***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***
***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***
***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***
***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***
***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***
***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***
***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***
***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***
***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***
***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***
Caught exception creating Arabic normalizer map: java.lang.NullPointerException
Caught exception creating Arabic normalizer map: java.io.FileNotFoundException: arabic.map (No such file or directory)
Caught exception creating Arabic normalizer map: java.lang.IllegalArgumentException: Invalid character in map
Caught exception creating Arabic normalizer map: java.io.IOException: Stream closed
Caught exception creating Arabic normalizer map: java.lang.ClassNotFoundException: org.apache.lucene.analysis.ar.ArabicNormalizer
Caught exception creating Arabic normalizer map: java.lang.OutOfMemoryError: Java heap space
Caught exception creating Arabic normalizer map: java.lang.SecurityException: Permission denied to access file
Caught exception creating Arabic normalizer map: java.util.zip.ZipException: Invalid zip file format
Caught exception creating Arabic normalizer map: java.lang.UnsupportedOperationException: Not implemented
Caught exception creating Arabic normalizer map: java.lang.ArrayIndexOutOfBoundsException: Index 10 out of bounds for length 10
Caught exception creating Arabic normalizer map: java.lang.NumberFormatException: For input string: "arabic"
Caught exception creating Arabic normalizer map: java.nio.charset.MalformedInputException: Input length = 1
Caught exception creating Arabic normalizer map: java.util.concurrent.TimeoutException: Timeout waiting for map creation
Caught exception creating Arabic normalizer map: java.lang.StackOverflowError
Caught exception creating Arabic normalizer map: java.lang.Error
DocSpanText: "The quick brown fox jumps over the lazy dog"
DocSpanText: "Hello world! This is a test message."
DocSpanText: "Lorem ipsum dolor sit amet, consectetur adipiscing elit."
DocSpanText: "To be or not to be, that is the question."
DocSpanText: "May the Force be with you."
DocSpanText: "In the beginning God created the heaven and the earth."
DocSpanText: "It was a bright cold day in April, and the clocks were striking thirteen."
DocSpanText: "Four score and seven years ago our fathers brought forth on this continent, a new nation..."
DocSpanText: "I have a dream that one day this nation will rise up and live out the true meaning of its creed..."
DocSpanText: "Call me Ishmael. Some years ago - never mind how long precisely..."
DocSpanText: "It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife."
DocSpanText: "All animals are equal, but some animals are more equal than others."
DocSpanText: "You shall not pass!"
DocSpanText: "Houston, we have a problem."
DocSpanText: "Elementary, my dear Watson."
***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***
***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***
***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***
***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***
***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***
***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***
***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***
***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***
***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***
***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***
***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***
***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***
***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***
***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***
***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***
rDen is 0.5
rDen is 1.2
rDen is -0.3
rDen is 0.8
rDen is 0.9
rDen is 1.0
rDen is 0.4
rDen is 0.7
rDen is -0.1
rDen is 1.1
rDen is 0.6
rDen is -0.2
rDen is 0.3
rDen is -0.4
rDen is 1.3
Need to specify output model name with -output
Need to specify output model name with -output
Need to specify output model name with -output
Need to specify output model name with -output
Need to specify output model name with -output
Need to specify output model name with -output
Need to specify output model name with -output
Need to specify output model name with -output
Need to specify output model name with -output
Need to specify output model name with -output
Need to specify output model name with -output
Need to specify output model name with -output
Need to specify output model name with -output
Need to specify output model name with -output
Need to specify output model name with -output
Need to specify input model names with -input
Need to specify input model names with -input
Need to specify input model names with -input
Need to specify input model names with -input
Need to specify input model names with -input
Need to specify input model names with -input
Need to specify input model names with -input
Need to specify input model names with -input
Need to specify input model names with -input
Need to specify input model names with -input
Need to specify input model names with -input
Need to specify input model names with -input
Need to specify input model names with -input
Need to specify input model names with -input
Need to specify input model names with -input
doc.allGoldMentions.size() is 0
doc.allGoldMentions.size() is 1
doc.allGoldMentions.size() is 2
doc.allGoldMentions.size() is 3
doc.allGoldMentions.size() is 4
doc.allGoldMentions.size() is 5
doc.allGoldMentions.size() is 6
doc.allGoldMentions.size() is 7
doc.allGoldMentions.size() is 8
doc.allGoldMentions.size() is 9
doc.allGoldMentions.size() is 10
doc.allGoldMentions.size() is 11
doc.allGoldMentions.size() is 12
doc.allGoldMentions.size() is 13
doc.allGoldMentions.size() is 14
DocSpanTokens: "The quick brown fox jumps over the lazy dog"
DocSpanTokens: "She sells seashells by the seashore"
DocSpanTokens: "How much wood would a woodchuck chuck if a woodchuck could chuck wood"
DocSpanTokens: "Peter Piper picked a peck of pickled peppers"
DocSpanTokens: "To be or not to be, that is the question"
DocSpanTokens: "Four score and seven years ago our fathers brought forth on this continent a new nation"
DocSpanTokens: "In the beginning God created the heaven and the earth"
DocSpanTokens: "It was a bright cold day in April, and the clocks were striking thirteen"
DocSpanTokens: "Call me Ishmael"
DocSpanTokens: "It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife"
DocSpanTokens: "All happy families are alike; each unhappy family is unhappy in its own way"
DocSpanTokens: "You don't know about me without you have read a book by the name of The Adventures of Tom Sawyer; but that ain't no matter"
DocSpanTokens: "Mr and Mrs Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much"
DocSpanTokens: "In a hole in the ground there lived a hobbit"
DocSpanTokens: "It was the best of times, it was the worst of times"
MorphAnalyzer: Word contains malformed morph annotation: 'dog-NNS'
MorphParser: Word contains malformed morph annotation: 'walk-VBD'
MorphTagger: Word contains malformed morph annotation: 'the-DT'
MorphLemmatizer: Word contains malformed morph annotation: 'cats-NNS'
MorphGenerator: Word contains malformed morph annotation: 'run-VBG'
MorphFilter: Word contains malformed morph annotation: 'a-DT'
MorphTokenizer: Word contains malformed morph annotation: 'hello-UH'
MorphSegmenter: Word contains malformed morph annotation: 'book-NN'
MorphStemmer: Word contains malformed morph annotation: 'played-VBD'
MorphNormalizer: Word contains malformed morph annotation: 'isn't-VBZ'
MorphConverter: Word contains malformed morph annotation: 'she-PRP'
MorphSplitter: Word contains malformed morph annotation: 'don't-VBP'
MorphJoiner: Word contains malformed morph annotation: 'I-PRP'
MorphChecker: Word contains malformed morph annotation: 'are-VBP'
MorphCorrector: Word contains malformed morph annotation: 'it-PRP'
Outputting result to model_1.h5
Outputting result to results.csv
Outputting result to output.txt
Outputting result to model_2.pkl
Outputting result to report.pdf
Outputting result to model_3.pth
Outputting result to summary.json
Outputting result to output.log
Outputting result to model_4.onnx
Outputting result to results.xlsx
Outputting result to output.html
Outputting result to model_5.pt
Outputting result to report.docx
Outputting result to summary.md
Outputting result to output.xml
SentenceTokens: [The, quick, brown, fox, jumps, over, the, lazy, dog]
SentenceTokens: [She, loves, to, read, books, and, magazines]
SentenceTokens: [What, time, is, your, flight, tomorrow?]
SentenceTokens: [He, said, that, he, was, sorry]
SentenceTokens: [They, went, to, the, park, with, their, friends]
SentenceTokens: [How, are, you, feeling, today?]
SentenceTokens: [I, have, a, lot, of, work, to, do]
SentenceTokens: [This, is, a, very, long, and, complex, sentence]
SentenceTokens: [Can, you, please, help, me, with, this?]
SentenceTokens: [She's a very talented singer and dancer]
SentenceTokens: [He likes to play video games and watch movies]
SentenceTokens: [They have two cats and a dog]
SentenceTokens: [Where are you going for vacation?]
SentenceTokens: [He's not very good at math but he's great at art]
SentenceTokens: [She has a big exam tomorrow and she's very nervous]
doc.goldCorefClusters.values().size() is 0
doc.goldCorefClusters.values().size() is 1
doc.goldCorefClusters.values().size() is 2
doc.goldCorefClusters.values().size() is 3
doc.goldCorefClusters.values().size() is 4
doc.goldCorefClusters.values().size() is 5
doc.goldCorefClusters.values().size() is 6
doc.goldCorefClusters.values().size() is 7
doc.goldCorefClusters.values().size() is 8
doc.goldCorefClusters.values().size() is 9
doc.goldCorefClusters.values().size() is 10
doc.goldCorefClusters.values().size() is 11
doc.goldCorefClusters.values().size() is 12
doc.goldCorefClusters.values().size() is 13
doc.goldCorefClusters.values().size() is 14
Lemmas don't match tokens, ignoring lemmas: lemmas [the, cat, is, sleeping], tokens [cat, sleeps]
Lemmas don't match tokens, ignoring lemmas: lemmas [she, has, a, big, smile], tokens [she, smiling, big]
Lemmas don't match tokens, ignoring lemmas: lemmas [they, are, going, to, the, park], tokens [they, go, park]
Lemmas don't match tokens, ignoring lemmas: lemmas [he, likes, to, play, chess], tokens [he, play, chess]
Lemmas don't match tokens, ignoring lemmas: lemmas [I, am, happy], tokens [happy]
Lemmas don't match tokens, ignoring lemmas: lemmas [we, can, see, the, stars], tokens [we, see, star]
Lemmas don't match tokens, ignoring lemmas: lemmas [she, is, a, teacher], tokens [teacher]
Lemmas don't match tokens, ignoring lemmas: lemmas [he, ran, fast], tokens [he, run]
Lemmas don't match tokens, ignoring lemmas: lemmas [it, was, a, sunny, day], tokens [sunny]
Lemmas don't match tokens, ignoring lemmas: lemmas [you, are, my, friend], tokens [you, friend]
Lemmas don't match tokens, ignoring lemmas: lemmas [the, dog, barked], tokens [dog]
Lemmas don't match tokens, ignoring lemmas: lemmas [she, sang], tokens [sing]
Lemmas don't match tokens, ignoring lemmas: lemmas [he, ate], tokens [eat]
Lemmas don't match tokens, ignoring lemmas: lemmas [they, laughed], tokens [laugh]
Lemmas don't match tokens, ignoring lemmas: lemmas [I ,love ,you], tokens [love]
Filtered 3 trees
Filtered 5 trees
Filtered 7 trees
Filtered 2 trees
Filtered 4 trees
Filtered 6 trees
Filtered 8 trees
Filtered 9 trees
Filtered 10 trees
Filtered 11 trees
Filtered 12 trees
Filtered 13 trees
Filtered 14 trees
Filtered 15 trees
No merge: entity not at end
No merge: entity not at end
No merge: entity not at end
No merge: entity not at end
No merge: entity not at end
No merge: entity not at end
No merge: entity not at end
No merge: entity not at end
No merge: entity not at end
No merge: entity not at end
No merge: entity not at end
No merge: entity not at end
No merge: entity not at end
No merge: entity not at end
No merge: entity not at end
No mention finder specified, but not using gold mentions
No mention finder specified, but not using gold mentions
No mention finder specified, but not using gold mentions
No mention finder specified, but not using gold mentions
No mention finder specified, but not using gold mentions
No mention finder specified, but not using gold mentions
No mention finder specified, but not using gold mentions
No mention finder specified, but not using gold mentions
No mention finder specified, but not using gold mentions
No mention finder specified, but not using gold mentions
No mention finder specified, but not using gold mentions
No mention finder specified, but not using gold mentions
No mention finder specified, but not using gold mentions
No mention finder specified, but not using gold mentions
No mention finder specified, but not using gold mentions
WARNING: filtered all trees for oak
WARNING: filtered all trees for pine
WARNING: filtered all trees for maple
WARNING: filtered all trees for birch
WARNING: filtered all trees for palm
WARNING: filtered all trees for willow
WARNING: filtered all trees for cedar
WARNING: filtered all trees for spruce
WARNING: filtered all trees for apple
WARNING: filtered all trees for cherry
WARNING: filtered all trees for elm
WARNING: filtered all trees for ash
WARNING: filtered all trees for walnut
WARNING: filtered all trees for magnolia
WARNING: filtered all trees for eucalyptus
No merge: no char
No merge: no char
No merge: no char
No merge: no char
No merge: no char
No merge: no char
No merge: no char
No merge: no char
No merge: no char
No merge: no char
No merge: no char
No merge: no char
No merge: no char
No merge: no char
No merge: no char
No merge: c is 'true'
No merge: c is 'false'
No merge: c is 'null'
No merge: c is 'undefined'
No merge: c is '0'
No merge: c is '1'
No merge: c is '-1'
No merge: c is 'NaN'
No merge: c is 'Infinity'
No merge: c is '-Infinity'
No merge: c is '"hello"'
No merge: c is '"world"'
No merge: c is '[1, 2, 3]'
No merge: c is '{a: 1, b: 2}'
No merge: c is 'function() {return c;}'
CONLL MENTION GOLD FILE: /data/conll-2012/v4/data/train/data/english/annotations/bc/cctv/00/cctv_0001.gold_conll
CONLL MENTION GOLD FILE: /data/conll-2012/v4/data/dev/data/english/annotations/nw/wsj/23/wsj_2300.gold_conll
CONLL MENTION GOLD FILE: /data/conll-2012/v4/data/test/data/english/annotations/bn/abc/00/abc_0004.gold_conll
CONLL MENTION GOLD FILE: /data/conll-2012/v4/data/train/data/english/annotations/tc/ch/00/ch_0006.gold_conll
CONLL MENTION GOLD FILE: /data/conll-2012/v4/data/dev/data/english/annotations/mz/sinorama/10/ectb_1008.gold_conll
CONLL MENTION GOLD FILE: /data/conll-2012/v4/data/test/data/english/annotations/wb/a2e/00/a2e_0003.gold_conll
CONLL MENTION GOLD FILE: /data/conll-2012/v4/data/train/data/english/annotations/nw/xinhua/01/chtb_0100.gold_conll
CONLL MENTION GOLD FILE: /data/conll-2012/v4/data/dev/data/english/annotations/bc/msnbc/00/msnbc_0000.gold_conll
CONLL MENTION GOLD FILE: /data/conll-2012/v4/data/test/data/english/annotations/nw/wsj/24/wsj_2401.gold_conll
CONLL MENTION GOLD FILE: /data/conll-2012/v4/data/train/data/english/annotations/mz/sinorama/11/ectb_1105.gold_conll
CONLL MENTION GOLD FILE: /data/conll-2012/v4/data/dev/data/english/annotations/bn/cnn/03/cnn_0301.gold_conll
CONLL MENTION GOLD FILE: /data/conll-2012/v4/data/test/data/english/annotations/tc/ch/01/ch_0109.gold_conll
CONLL MENTION GOLD FILE: /data/conll-2012/v4/data/train/data/english/annotations/wb/a2e/01/a2e_0105.gold_conll
CONLL MENTION GOLD FILE: /data/conll-2012/v4/data/dev/data/english/annotations/nw/xinhua/02/chtb_0203.gold_conll
CONLL MENTION GOLD FILE: /data/conll-2012/v4/data/test/data/english/annotations/bc/cctv/01/cctv_0100.gold_conll
Loading parser model en_core_web_sm
Loading parser model zh_core_web_trf
Loading parser model de_core_news_lg
Loading parser model fr_dep_news_trf
Loading parser model es_core_news_md
Loading parser model ja_core_news_sm
Loading parser model ru_core_news_lg
Loading parser model pt_core_news_sm
Loading parser model nl_core_news_sm
Loading parser model it_core_news_sm
Loading parser model el_core_news_sm
Loading parser model ar_core_web_sm
Loading parser model hi_core_web_sm
Loading parser model sw_core_web_sm
Loading parser model ta_core_web_sm
CONLL MENTION PREDICTED FILE: /home/user1/data/conll2012/mention_pred_1.txt
CONLL MENTION PREDICTED FILE: /mnt/c/Users/user2/Documents/conll2012/mention_pred_2.csv
CONLL MENTION PREDICTED FILE: /data/user3/conll2012/mention_pred_3.json
CONLL MENTION PREDICTED FILE: /var/log/user4/conll2012/mention_pred_4.log
CONLL MENTION PREDICTED FILE: /tmp/user5/conll2012/mention_pred_5.tsv
CONLL MENTION PREDICTED FILE: /opt/user6/conll2012/mention_pred_6.xml
CONLL MENTION PREDICTED FILE: /usr/local/user7/conll2012/mention_pred_7.pkl
CONLL MENTION PREDICTED FILE: /home/user8/data/conll2012/mention_pred_8.hdf5
CONLL MENTION PREDICTED FILE: /mnt/d/Users/user9/Documents/conll2012/mention_pred_9.mat
CONLL MENTION PREDICTED FILE: /data/user10/conll2012/mention_pred_10.parquet
CONLL MENTION PREDICTED FILE: /var/log/user11/conll2012/mention_pred_11.npz
CONLL MENTION PREDICTED FILE: /tmp/user12/conll2012/mention_pred_12.feather
CONLL MENTION PREDICTED FILE: /opt/user13/conll2012/mention_pred_13.rds
CONLL MENTION PREDICTED FILE: /usr/local/user14/conll2012/mention_pred_14.nc
CONLL MENTION PREDICTED FILE: /home/user15/data/conll2012/mention_pred_15.arff
Merge chunks
Merge chunks
Merge chunks
Merge chunks
Merge chunks
Merge chunks
Merge chunks
Merge chunks
Merge chunks
Merge chunks
Merge chunks
Merge chunks
Merge chunks
Merge chunks
Merge chunks
Adjusting begin char offset from 12 to 0
Adjusting begin char offset from 45 to 0
Adjusting begin char offset from 78 to 0
Adjusting begin char offset from 23 to 0
Adjusting begin char offset from 56 to 0
Adjusting begin char offset from 89 to 0
Adjusting begin char offset from 34 to 0
Adjusting begin char offset from 67 to 0
Adjusting begin char offset from 90 to 0
Adjusting begin char offset from 45 to 0
Adjusting begin char offset from 12 to 0
Adjusting begin char offset from 78 to 0
Adjusting begin char offset from 34 to 0
Adjusting begin char offset from 90 to 0
Adjusting begin char offset from 56 to 0
CONLL MENTION EVAL FILE: /home/user/data/conll-2012-test.v4_gold_conll
CONLL MENTION EVAL FILE: /mnt/c/Users/John/Documents/eval/conll-2012-dev.v4_auto_conll
CONLL MENTION EVAL FILE: /tmp/conll-2012-train.v4_gold_conll
CONLL MENTION EVAL FILE: /data/user1/eval/conll-2012-test.v4_auto_conll
CONLL MENTION EVAL FILE: /var/log/conll-2012-dev.v4_gold_conll
CONLL MENTION EVAL FILE: /Users/Mary/Desktop/eval/conll-2012-train.v4_auto_conll
CONLL MENTION EVAL FILE: /opt/data/conll-2012-test.v4_gold_conll
CONLL MENTION EVAL FILE: /home/user/eval/conll-2012-dev.v4_auto_conll
CONLL MENTION EVAL FILE: /mnt/d/Data/eval/conll-2012-train.v4_gold_conll
CONLL MENTION EVAL FILE: /tmp/eval/conll-2012-test.v4_auto_conll
CONLL MENTION EVAL FILE: /data/user2/eval/conll-2012-dev.v4_gold_conll
CONLL MENTION EVAL FILE: /var/tmp/conll-2012-train.v4_auto_conll
CONLL MENTION EVAL FILE: /Users/Tom/Documents/eval/conll-2012-test.v4_gold_conll
CONLL MENTION EVAL FILE: /opt/eval/conll-2012-dev.v4_auto_conll
CONLL MENTION EVAL FILE: /home/user/data/eval/conll-2012-train.v4_gold_conll
Chunk begin offset: 0, Source text begin offset 0
Chunk begin offset: 15, Source text begin offset 10
Chunk begin offset: 25, Source text begin offset 20
Chunk begin offset: 35, Source text begin offset 30
Chunk begin offset: 50, Source text begin offset 45
Chunk begin offset: 60, Source text begin offset 55
Chunk begin offset: 75, Source text begin offset 70
Chunk begin offset: 85, Source text begin offset 80
Chunk begin offset: 100, Source text begin offset 95
Chunk begin offset: 110, Source text begin offset 105
Chunk begin offset: 125, Source text begin offset 120
Chunk begin offset: 135, Source text begin offset 130
Chunk begin offset: 150, Source text begin offset 145
Chunk begin offset: 160, Source text begin offset 155
Chunk begin offset: 175, Source text begin offset 170
CONLL MENTION PREDICTED WITH COREF FILE: /home/user/data/conll2012_coref_pred_001.txt
CONLL MENTION PREDICTED WITH COREF FILE: /mnt/c/Users/John/Documents/conll2012_coref_pred_002.txt
CONLL MENTION PREDICTED WITH COREF FILE: /var/log/conll2012_coref_pred_003.log
CONLL MENTION PREDICTED WITH COREF FILE: /tmp/conll2012_coref_pred_004.tmp
CONLL MENTION PREDICTED WITH COREF FILE: /opt/conll2012_coref_pred_005.dat
CONLL MENTION PREDICTED WITH COREF FILE: /usr/local/bin/conll2012_coref_pred_006.bin
CONLL MENTION PREDICTED WITH COREF FILE: /dev/null/conll2012_coref_pred_007.null
CONLL MENTION PREDICTED WITH COREF FILE: /etc/conll2012_coref_pred_008.conf
CONLL MENTION PREDICTED WITH COREF FILE: /lib/conll2012_coref_pred_009.lib
CONLL MENTION PREDICTED WITH COREF FILE: /media/user/USB/conll2012_coref_pred_010.usb
CONLL MENTION PREDICTED WITH COREF FILE: /root/conll2012_coref_pred_011.root
CONLL MENTION PREDICTED WITH COREF FILE: /srv/conll2012_coref_pred_012.srv
CONLL MENTION PREDICTED WITH COREF FILE: /proc/conll2012_coref_pred_013.proc
CONLL MENTION PREDICTED WITH COREF FILE: /sys/conll2012_coref_pred_014.sys
CONLL MENTION PREDICTED WITH COREF FILE: /run/conll2012_coref_pred_015.run
Processing 5 trees
Processing 12 trees
Processing 0 trees
Processing 8 trees
Processing 3 trees
Processing 10 trees
Processing 7 trees
Processing 4 trees
Processing 9 trees
Processing 6 trees
Processing 11 trees
Processing 2 trees
Processing 13 trees
Processing 14 trees
Adjusting begin char offset from 0 to 12
Adjusting begin char offset from 25 to 36
Adjusting begin char offset from 50 to 63
Adjusting begin char offset from 75 to 88
Adjusting begin char offset from 100 to 112
Adjusting begin char offset from 125 to 139
Adjusting begin char offset from 150 to 164
Adjusting begin char offset from 175 to 187
Adjusting begin char offset from 200 to 213
Adjusting begin char offset from 225 to 238
Adjusting begin char offset from 250 to 264
Adjusting begin char offset from 275 to 289
Adjusting begin char offset from 300 to 313
Adjusting begin char offset from 325 to 338
Adjusting begin char offset from 350 to 363
CONLL MENTION WITH COREF EVAL FILE: /home/user/data/coref_eval.txt
CONLL MENTION WITH COREF EVAL FILE: C:\Users\user\Documents\coref_eval.csv
CONLL MENTION WITH COREF EVAL FILE: https://www.example.com/data/coref_eval.json
CONLL MENTION WITH COREF EVAL FILE: /mnt/cdrom/coref_eval.dat
CONLL MENTION WITH COREF EVAL FILE: /tmp/coref_eval.log
CONLL MENTION WITH COREF EVAL FILE: /usr/local/bin/coref_eval.sh
CONLL MENTION WITH COREF EVAL FILE: /dev/null
CONLL MENTION WITH COREF EVAL FILE: s3://bucket-name/coref_eval.parquet
CONLL MENTION WITH COREF EVAL FILE: ftp://user:password@host/coref_eval.xml
CONLL MENTION WITH COREF EVAL FILE: file:///var/log/coref_eval.gz
CONLL MENTION WITH COREF EVAL FILE: hdfs://namenode:port/coref_eval.avro
CONLL MENTION WITH COREF EVAL FILE: smb://server/share/coref_eval.xls
CONLL MENTION WITH COREF EVAL FILE: nfs://server:/export/coref_eval.db
CONLL MENTION WITH COREF EVAL FILE: gcs://bucket-name/coref_eval.tfrecord
CONLL MENTION WITH COREF EVAL FILE: ssh://user@host:/home/user/coref_eval.md
Adjusting end char offset from 12 to 0
Adjusting end char offset from 27 to 0
Adjusting end char offset from 9 to 0
Adjusting end char offset from 15 to 0
Adjusting end char offset from 21 to 0
Adjusting end char offset from 18 to 0
Adjusting end char offset from 24 to 0
Adjusting end char offset from 30 to 0
Adjusting end char offset from 6 to 0
Adjusting end char offset from 3 to 0
Adjusting end char offset from 33 to 0
Adjusting end char offset from 36 to 0
Adjusting end char offset from 39 to 0
Adjusting end char offset from 42 to 0
Adjusting end char offset from 45 to 0
Warning: No rule found for motherCat (first char: m)
Warning: No rule found for fatherCat (first char: f)
Warning: No rule found for kitten (first char: k)
Warning: No rule found for catnip (first char: c)
Warning: No rule found for whiskers (first char: w)
Warning: No rule found for meow (first char: m)
Warning: No rule found for purr (first char: p)
Warning: No rule found for scratch (first char: s)
Warning: No rule found for litterBox (first char: l)
Warning: No rule found for furBall (first char: f)
Warning: No rule found for mouse (first char: m)
Warning: No rule found for yarn (first char: y)
Warning: No rule found for nap (first char: n)
Warning: No rule found for tuna (first char: t)
Warning: No rule found for milk (first char: m)
Finished processing 100 trees
Finished processing 50 trees
Finished processing 75 trees
Finished processing 25 trees
Finished processing 150 trees
Finished processing 200 trees
Finished processing 125 trees
Finished processing 175 trees
Finished processing 80 trees
Finished processing 120 trees
Finished processing 60 trees
Finished processing 40 trees
Finished processing 140 trees
Finished processing 160 trees
Finished processing 180 trees
CONLL EVAL SUMMARY (Before COREF)
CONLL EVAL SUMMARY (Before COREF)
CONLL EVAL SUMMARY (Before COREF)
CONLL EVAL SUMMARY (Before COREF)
CONLL EVAL SUMMARY (Before COREF)
CONLL EVAL SUMMARY (Before COREF)
CONLL EVAL SUMMARY (Before COREF)
CONLL EVAL SUMMARY (Before COREF)
CONLL EVAL SUMMARY (Before COREF)
CONLL EVAL SUMMARY (Before COREF)
CONLL EVAL SUMMARY (Before COREF)
CONLL EVAL SUMMARY (Before COREF)
CONLL EVAL SUMMARY (Before COREF)
CONLL EVAL SUMMARY (Before COREF)
CONLL EVAL SUMMARY (Before COREF)
CONLL EVAL SUMMARY (After COREF)
CONLL EVAL SUMMARY (After COREF)
CONLL EVAL SUMMARY (After COREF)
CONLL EVAL SUMMARY (After COREF)
CONLL EVAL SUMMARY (After COREF)
CONLL EVAL SUMMARY (After COREF)
CONLL EVAL SUMMARY (After COREF)
CONLL EVAL SUMMARY (After COREF)
CONLL EVAL SUMMARY (After COREF)
CONLL EVAL SUMMARY (After COREF)
CONLL EVAL SUMMARY (After COREF)
CONLL EVAL SUMMARY (After COREF)
CONLL EVAL SUMMARY (After COREF)
CONLL EVAL SUMMARY (After COREF)
CONLL EVAL SUMMARY (After COREF)
Invalid encoding 'UTF-9'
Invalid encoding 'ASCII-8'
Invalid encoding 'ISO-8859-0'
Invalid encoding 'EBCDIC'
Invalid encoding 'UTF-16LE'
Invalid encoding 'UTF-32BE'
Invalid encoding 'CP1252'
Invalid encoding 'KOI8-R'
Invalid encoding 'GB18030'
Invalid encoding 'Shift-JIS'
Invalid encoding 'ISO-2022-JP'
Invalid encoding 'Big5'
Invalid encoding 'UTF-7'
Invalid encoding 'Windows-1251'
Invalid encoding 'ISO-8859-15'
Loading serialized DVParser from /home/user/models/base.dvp
Loading serialized DVParser from C:\Program Files\models\base.dvp
Loading serialized DVParser from /opt/models/base.dvp
Loading serialized DVParser from /mnt/usb/models/base.dvp
Loading serialized DVParser from /var/lib/models/base.dvp
Loading serialized DVParser from /tmp/models/base.dvp
Loading serialized DVParser from /usr/local/models/base.dvp
Loading serialized DVParser from /models/base.dvp
Loading serialized DVParser from models\base.dvp
Loading serialized DVParser from D:\models\base.dvp
Loading serialized DVParser from E:\models\base.dvp
Loading serialized DVParser from F:\models\base.dvp
Loading serialized DVParser from G:\models\base.dvp
Loading serialized DVParser from H:\models\base.dvp
Loading serialized DVParser from I:\models\base.dvp
File not found : "input.txt"
File not found : "config.ini"
File not found : "image.jpg"
File not found : "data.csv"
File not found : "script.py"
File not found : "report.docx"
File not found : "music.mp3"
File not found : "video.mp4"
File not found : "archive.zip"
File not found : "log.txt"
File not found : "index.html"
File not found : "style.css"
File not found : "icon.png"
File not found : "readme.md"
File not found : "license.txt"
Chunk end offset: 1024, Source text begin offset 0
Chunk end offset: 2048, Source text begin offset 1024
Chunk end offset: 3072, Source text begin offset 2048
Chunk end offset: 4096, Source text begin offset 3072
Chunk end offset: 5120, Source text begin offset 4096
Chunk end offset: 6144, Source text begin offset 5120
Chunk end offset: 7168, Source text begin offset 6144
Chunk end offset: 8192, Source text begin offset 7168
Chunk end offset: 9216, Source text begin offset 8192
Chunk end offset: 10240, Source text begin offset 9216
Chunk end offset: 11264, Source text begin offset 10240
Chunk end offset: 12288, Source text begin offset 11264
Chunk end offset: 13312, Source text begin offset 12288
Chunk end offset: 14336, Source text begin offset 13312
Chunk end offset: 15360, Source text begin offset 14336
IO error scanning file "/home/user/documents/report.pdf"
IO error scanning file "C:\Windows\System32\drivers\etc\hosts"
IO error scanning file "/usr/local/bin/python3"
IO error scanning file "D:\Games\Steam\steamapps\common\Half-Life 2\hl2.exe"
IO error scanning file "/etc/passwd"
IO error scanning file "E:\Music\Taylor Swift\folklore\cardigan.mp3"
IO error scanning file "/var/log/syslog"
IO error scanning file "F:\Photos\2021-10-23\IMG_1234.JPG"
IO error scanning file "/opt/apache-tomcat-9.0.54/webapps/ROOT/index.jsp"
IO error scanning file "G:\Videos\Movies\Inception (2010)\Inception.mkv"
IO error scanning file "/dev/sda1"
IO error scanning file "H:\Books\Harry Potter and the Philosopher's Stone.pdf"
IO error scanning file "/tmp/foo.txt"
IO error scanning file "I:\Software\Microsoft Office 2019\setup.exe"
IO error scanning file "/home/user/.bashrc"
Adjusting end char offset from 12 to 15
Adjusting end char offset from 25 to 28
Adjusting end char offset from 37 to 40
Adjusting end char offset from 49 to 52
Adjusting end char offset from 63 to 66
Adjusting end char offset from 78 to 81
Adjusting end char offset from 92 to 95
Adjusting end char offset from 105 to 108
Adjusting end char offset from 118 to 121
Adjusting end char offset from 132 to 135
Adjusting end char offset from 145 to 148
Adjusting end char offset from 159 to 162
Adjusting end char offset from 173 to 176
Adjusting end char offset from 186 to 189
Adjusting end char offset from 199 to 202
Reading in trees from /data/testTreebankPath
Reading in trees from C:\Users\test\testTreebankPath
Reading in trees from /home/test/testTreebankPath
Reading in trees from testTreebankPath.txt
Reading in trees from testTreebankPath.csv
Reading in trees from testTreebankPath.json
Reading in trees from https://www.example.com/testTreebankPath
Reading in trees from s3://bucket/testTreebankPath
Reading in trees from hdfs://test/testTreebankPath
Reading in trees from ftp://test@test.com/testTreebankPath
Reading in trees from /tmp/testTreebankPath
Reading in trees from /var/log/testTreebankPath
Reading in trees from /opt/test/testTreebankPath
Reading in trees from /dev/null/testTreebankPath
Reading in trees from /etc/testTreebankPath
Unexpected exception:
Unexpected exception:
Unexpected exception:
Unexpected exception:
Unexpected exception:
Unexpected exception:
Unexpected exception:
Unexpected exception:
Unexpected exception:
Unexpected exception:
Unexpected exception:
Unexpected exception:
Unexpected exception:
Unexpected exception:
Unexpected exception:
WARNING: Only 5/10 chunks found. Check if offsets are sorted/nonoverlapping
WARNING: Only 12/15 chunks found. Check if offsets are sorted/nonoverlapping
WARNING: Only 8/9 chunks found. Check if offsets are sorted/nonoverlapping
WARNING: Only 3/7 chunks found. Check if offsets are sorted/nonoverlapping
WARNING: Only 6/11 chunks found. Check if offsets are sorted/nonoverlapping
WARNING: Only 9/12 chunks found. Check if offsets are sorted/nonoverlapping
WARNING: Only 4/8 chunks found. Check if offsets are sorted/nonoverlapping
WARNING: Only 7/13 chunks found. Check if offsets are sorted/nonoverlapping
WARNING: Only 10/14 chunks found. Check if offsets are sorted/nonoverlapping
WARNING: Only 2/6 chunks found. Check if offsets are sorted/nonoverlapping
WARNING: Only 11/16 chunks found. Check if offsets are sorted/nonoverlapping
WARNING: Only 13/17 chunks found. Check if offsets are sorted/nonoverlapping
WARNING: Only 14/18 chunks found. Check if offsets are sorted/nonoverlapping
WARNING: Only 15/19 chunks found. Check if offsets are sorted/nonoverlapping
Filtering on testTreebankFilter=English
Filtering on testTreebankFilter=Chinese
Filtering on testTreebankFilter=Arabic
Filtering on testTreebankFilter=Spanish
Filtering on testTreebankFilter=French
Filtering on testTreebankFilter=German
Filtering on testTreebankFilter=Russian
Filtering on testTreebankFilter=Hindi
Filtering on testTreebankFilter=Japanese
Filtering on testTreebankFilter=Korean
Filtering on testTreebankFilter=Turkish
Filtering on testTreebankFilter=Swedish
Filtering on testTreebankFilter=Dutch
Filtering on testTreebankFilter=Portuguese
Filtering on testTreebankFilter=Italian
Usage : java NegraLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraPennLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraPennLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraPennLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraPennLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraPennLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraPennLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraPennLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraPennLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraPennLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraPennLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraPennLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraPennLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraPennLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraPennLexer [ --encoding <name> ] <inputfile(s)>
Usage : java NegraPennLexer [ --encoding <name> ] <inputfile(s)>
CleanXMLAnnotator: xmlElementsToProcess=book
CleanXMLAnnotator: xmlElementsToProcess=article
CleanXMLAnnotator: xmlElementsToProcess=person
CleanXMLAnnotator: xmlElementsToProcess=movie
CleanXMLAnnotator: xmlElementsToProcess=product
CleanXMLAnnotator: xmlElementsToProcess=recipe
CleanXMLAnnotator: xmlElementsToProcess=event
CleanXMLAnnotator: xmlElementsToProcess=review
CleanXMLAnnotator: xmlElementsToProcess=game
CleanXMLAnnotator: xmlElementsToProcess=company
CleanXMLAnnotator: xmlElementsToProcess=song
CleanXMLAnnotator: xmlElementsToProcess=animal
CleanXMLAnnotator: xmlElementsToProcess=place
CleanXMLAnnotator: xmlElementsToProcess=vehicle
CleanXMLAnnotator: xmlElementsToProcess=plant
dateTags=2021-10-23
dateTags=2023-08-18
dateTags=2020-12-31
dateTags=2022-02-29
dateTags=2019-07-04
dateTags=2024-01-01
dateTags=2018-05-06
dateTags=2025-03-21
dateTags=2017-09-15
dateTags=2026-06-30
dateTags=2016-11-11
dateTags=2027-04-14
dateTags=2015-01-25
dateTags=2028-10-31
dateTags=2014-08-08
##### Warning -- no NP material here!
##### Warning -- no NP material here!
##### Warning -- no NP material here!
##### Warning -- no NP material here!
##### Warning -- no NP material here!
##### Warning -- no NP material here!
##### Warning -- no NP material here!
##### Warning -- no NP material here!
##### Warning -- no NP material here!
##### Warning -- no NP material here!
##### Warning -- no NP material here!
##### Warning -- no NP material here!
##### Warning -- no NP material here!
##### Warning -- no NP material here!
##### Warning -- no NP material here!
Found 12 .txt files. Continuing execution.
Found 0 .jpg files. Continuing execution.
Found 3 .docx files. Continuing execution.
Found 5 .pdf files. Continuing execution.
Found 7 .mp3 files. Continuing execution.
Found 4 .zip files. Continuing execution.
Found 9 .html files. Continuing execution.
Found 2 .pptx files. Continuing execution.
Found 6 .mp4 files. Continuing execution.
Found 8 .xml files. Continuing execution.
Found 10 .json files. Continuing execution.
Found 11 .py files. Continuing execution.
Found 13 .java files. Continuing execution.
Found 14 .exe files. Continuing execution.
Read in 100 trees for testing
Read in 25 trees for testing
Read in 50 trees for testing
Read in 75 trees for testing
Read in 10 trees for testing
Read in 40 trees for testing
Read in 60 trees for testing
Read in 80 trees for testing
Read in 20 trees for testing
Read in 30 trees for testing
Read in 70 trees for testing
Read in 90 trees for testing
Read in 15 trees for testing
Read in 45 trees for testing
Read in 55 trees for testing
#### inserted NP in PP
#### inserted NP in PP
#### inserted NP in PP
#### inserted NP in PP
#### inserted NP in PP
#### inserted NP in PP
#### inserted NP in PP
#### inserted NP in PP
#### inserted NP in PP
#### inserted NP in PP
#### inserted NP in PP
#### inserted NP in PP
#### inserted NP in PP
#### inserted NP in PP
#### inserted NP in PP
singleSentenceTags=[NOUN, VERB, ADJ, NOUN]
singleSentenceTags=[PRON, VERB, ADV, ADJ]
singleSentenceTags=[DET, NOUN, VERB, DET, NOUN]
singleSentenceTags=[ADV, VERB, PRON, ADP, NOUN]
singleSentenceTags=[NOUN, ADP, DET, ADJ, NOUN]
singleSentenceTags=[VERB, DET, NOUN, CONJ, VERB]
singleSentenceTags=[ADJ, NOUN, VERB, PRON]
singleSentenceTags=[PRON, VERB, DET, ADJ, ADJ, NOUN]
singleSentenceTags=[DET, ADJ, NOUN, VERB, ADV]
singleSentenceTags=[NOUN, VERB, ADP, DET, NOUN]
singleSentenceTags=[VERB, PRON, CONJ, VERB]
singleSentenceTags=[ADP, DET, NOUN, PRON, VERB]
singleSentenceTags=[ADV, ADJ, NOUN]
singleSentenceTags=[PRON, VERB]
singleSentenceTags=[NOUN]
NOT EQUAL AFTER UNTRANSFORMATION!!!
NOT EQUAL AFTER UNTRANSFORMATION!!!
NOT EQUAL AFTER UNTRANSFORMATION!!!
NOT EQUAL AFTER UNTRANSFORMATION!!!
NOT EQUAL AFTER UNTRANSFORMATION!!!
NOT EQUAL AFTER UNTRANSFORMATION!!!
NOT EQUAL AFTER UNTRANSFORMATION!!!
NOT EQUAL AFTER UNTRANSFORMATION!!!
NOT EQUAL AFTER UNTRANSFORMATION!!!
NOT EQUAL AFTER UNTRANSFORMATION!!!
NOT EQUAL AFTER UNTRANSFORMATION!!!
NOT EQUAL AFTER UNTRANSFORMATION!!!
NOT EQUAL AFTER UNTRANSFORMATION!!!
NOT EQUAL AFTER UNTRANSFORMATION!!!
NOT EQUAL AFTER UNTRANSFORMATION!!!
Still waiting... 5 minutes have passed.
Still waiting... 12 minutes have passed.
Still waiting... 3 minutes have passed.
Still waiting... 8 minutes have passed.
Still waiting... 10 minutes have passed.
Still waiting... 7 minutes have passed.
Still waiting... 9 minutes have passed.
Still waiting... 4 minutes have passed.
Still waiting... 6 minutes have passed.
Still waiting... 11 minutes have passed.
Still waiting... 2 minutes have passed.
Still waiting... 15 minutes have passed.
Still waiting... 13 minutes have passed.
Still waiting... 14 minutes have passed.
Loading lexparser from: englishPCFG.ser.gz
Loading lexparser from: englishFactored.ser.gz
Loading lexparser from: chineseFactored.ser.gz
Loading lexparser from: germanPCFG.ser.gz
Loading lexparser from: frenchFactored.ser.gz
Loading lexparser from: spanishPCFG.ser.gz
Loading lexparser from: arabicFactored.ser.gz
Loading lexparser from: hindiPCFG.ser.gz
Loading lexparser from: englishRNN.ser.gz
Loading lexparser from: englishUD.ser.gz
Loading lexparser from: englishBiLSTM.ser.gz
Loading lexparser from: englishBiLSTMchar.ser.gz
Loading lexparser from: englishBiLSTMcharFactored.ser.gz
Loading lexparser from: englishBiLSTMcharFactoredUD.ser.gz
Loading lexparser from: englishBiLSTMcharFactoredUD2.ser.gz
ChineseCollinizer: all children of root deleted; returning null
ChineseCollinizer: all children of noun deleted; returning null
ChineseCollinizer: all children of verb deleted; returning null
ChineseCollinizer: all children of adj deleted; returning null
ChineseCollinizer: all children of adv deleted; returning null
ChineseCollinizer: all children of prep deleted; returning null
ChineseCollinizer: all children of conj deleted; returning null
ChineseCollinizer: all children of pron deleted; returning null
ChineseCollinizer: all children of num deleted; returning null
ChineseCollinizer: all children of det deleted; returning null
ChineseCollinizer: all children of name deleted; returning null
ChineseCollinizer: all children of date deleted; returning null
ChineseCollinizer: all children of time deleted; returning null
ChineseCollinizer: all children of loc deleted; returning null
ChineseCollinizer: all children of sent deleted; returning null
LexicalizedParser weight 0.75: labeled NP tag NNP
LexicalizedParser weight 0.63: labeled VP tag VBD
LexicalizedParser weight 0.82: labeled ADJP tag JJ
LexicalizedParser weight 0.91: labeled S tag IN
LexicalizedParser weight 0.67: labeled PP tag TO
LexicalizedParser weight 0.76: labeled NP tag DT
LexicalizedParser weight 0.58: labeled VP tag VBZ
LexicalizedParser weight 0.69: labeled ADVP tag RB
LexicalizedParser weight 0.85: labeled SBAR tag WHNP
LexicalizedParser weight 0.73: labeled NP tag PRP
LexicalizedParser weight 0.61: labeled VP tag VBN
LexicalizedParser weight 0.79: labeled ADJP tag JJR
LexicalizedParser weight 0.88: labeled S tag CC
LexicalizedParser weight 0.65: labeled PP tag IN
LexicalizedParser weight 0.71: labeled NP tag NN
CleanXML: starting tokens: 0
CleanXML: starting tokens: 1
CleanXML: starting tokens: 2
CleanXML: starting tokens: 3
CleanXML: starting tokens: 4
CleanXML: starting tokens: 5
CleanXML: starting tokens: 6
CleanXML: starting tokens: 7
CleanXML: starting tokens: 8
CleanXML: starting tokens: 9
CleanXML: starting tokens: 10
CleanXML: starting tokens: 11
CleanXML: starting tokens: 12
CleanXML: starting tokens: 13
CleanXML: starting tokens: 14
-model : DVModel to load
-model : DVModel to load
-model : DVModel to load
-model : DVModel to load
-model : DVModel to load
-model : DVModel to load
-model : DVModel to load
-model : DVModel to load
-model : DVModel to load
-model : DVModel to load
-model : DVModel to load
-model : DVModel to load
-model : DVModel to load
-model : DVModel to load
-model : DVModel to load
=============SIEVE OPTIMIZATION START ====================
=============SIEVE OPTIMIZATION START ====================
=============SIEVE OPTIMIZATION START ====================
=============SIEVE OPTIMIZATION START ====================
=============SIEVE OPTIMIZATION START ====================
=============SIEVE OPTIMIZATION START ====================
=============SIEVE OPTIMIZATION START ====================
=============SIEVE OPTIMIZATION START ====================
=============SIEVE OPTIMIZATION START ====================
=============SIEVE OPTIMIZATION START ====================
=============SIEVE OPTIMIZATION START ====================
=============SIEVE OPTIMIZATION START ====================
=============SIEVE OPTIMIZATION START ====================
=============SIEVE OPTIMIZATION START ====================
=============SIEVE OPTIMIZATION START ====================
-output : where to dump the matrices
-output : where to dump the matrices
-output : where to dump the matrices
-output : where to dump the matrices
-output : where to dump the matrices
-output : where to dump the matrices
-output : where to dump the matrices
-output : where to dump the matrices
-output : where to dump the matrices
-output : where to dump the matrices
-output : where to dump the matrices
-output : where to dump the matrices
-output : where to dump the matrices
-output : where to dump the matrices
-output : where to dump the matrices
Optimize sieves using score: accuracy
Optimize sieves using score: precision
Optimize sieves using score: recall
Optimize sieves using score: f1
Optimize sieves using score: auc
Optimize sieves using score: mae
Optimize sieves using score: mse
Optimize sieves using score: rmse
Optimize sieves using score: r2
Optimize sieves using score: ndcg
Optimize sieves using score: map
Optimize sieves using score: mrr
Optimize sieves using score: hit_rate
Optimize sieves using score: coverage
Optimize sieves using score: diversity
Unknown argument -v
Unknown argument --help
Unknown argument /f
Unknown argument 42
Unknown argument foo
Unknown argument -d
Unknown argument --version
Unknown argument /r
Unknown argument 3.14
Unknown argument bar
Unknown argument -h
Unknown argument --verbose
Unknown argument /n
Unknown argument 0
Unknown argument baz
CleanXML: token begins as "the"
CleanXML: token begins as "a"
CleanXML: token begins as "xml"
CleanXML: token begins as "name"
CleanXML: token begins as "value"
CleanXML: token begins as "attribute"
CleanXML: token begins as "tag"
CleanXML: token begins as "end"
CleanXML: token begins as "start"
CleanXML: token begins as "text"
CleanXML: token begins as "comment"
CleanXML: token begins as "CDATA"
CleanXML: token begins as "DOCTYPE"
CleanXML: token begins as "entity"
CleanXML: token begins as "processing"
*** Optimizing Sieve ordering for pass 1 ***
*** Optimizing Sieve ordering for pass 5 ***
*** Optimizing Sieve ordering for pass 10 ***
*** Optimizing Sieve ordering for pass 3 ***
*** Optimizing Sieve ordering for pass 7 ***
*** Optimizing Sieve ordering for pass 2 ***
*** Optimizing Sieve ordering for pass 9 ***
*** Optimizing Sieve ordering for pass 4 ***
*** Optimizing Sieve ordering for pass 6 ***
*** Optimizing Sieve ordering for pass 8 ***
*** Optimizing Sieve ordering for pass 11 ***
*** Optimizing Sieve ordering for pass 12 ***
*** Optimizing Sieve ordering for pass 13 ***
*** Optimizing Sieve ordering for pass 14 ***
*** Optimizing Sieve ordering for pass 15 ***
norm=0.5
norm=1.2
norm=-0.3
norm=0.8
norm=1.5
norm=-0.7
norm=0.2
norm=1.7
norm=-0.4
norm=0.9
norm=1.3
norm=-0.6
norm=0.4
norm=1.8
norm=-0.5
Matched 12 chinese year vectors
Matched 5 chinese year vectors
Matched 9 chinese year vectors
Matched 7 chinese year vectors
Matched 10 chinese year vectors
Matched 8 chinese year vectors
Matched 6 chinese year vectors
Matched 11 chinese year vectors
Matched 4 chinese year vectors
Matched 3 chinese year vectors
Matched 2 chinese year vectors
Matched 15 chinese year vectors
Matched 14 chinese year vectors
Matched 13 chinese year vectors
usage: ChineseEnglishWordMap [-all] [-dictPath path] [-encoding enc_string] inputFile
usage: ChineseEnglishWordMap [-all] [-dictPath path] [-encoding enc_string] inputFile
usage: ChineseEnglishWordMap [-all] [-dictPath path] [-encoding enc_string] inputFile
usage: ChineseEnglishWordMap [-all] [-dictPath path] [-encoding enc_string] inputFile
usage: ChineseEnglishWordMap [-all] [-dictPath path] [-encoding enc_string] inputFile
usage: ChineseEnglishWordMap [-all] [-dictPath path] [-encoding enc_string] inputFile
usage: ChineseEnglishWordMap [-all] [-dictPath path] [-encoding enc_string] inputFile
usage: ChineseEnglishWordMap [-all] [-dictPath path] [-encoding enc_string] inputFile
usage: ChineseEnglishWordMap [-all] [-dictPath path] [-encoding enc_string] inputFile
usage: ChineseEnglishWordMap [-all] [-dictPath path] [-encoding enc_string] inputFile
usage: ChineseEnglishWordMap [-all] [-dictPath path] [-encoding enc_string] inputFile
usage: ChineseEnglishWordMap [-all] [-dictPath path] [-encoding enc_string] inputFile
usage: ChineseEnglishWordMap [-all] [-dictPath path] [-encoding enc_string] inputFile
usage: ChineseEnglishWordMap [-all] [-dictPath path] [-encoding enc_string] inputFile
usage: ChineseEnglishWordMap [-all] [-dictPath path] [-encoding enc_string] inputFile
Matched 5 chinese number vectors
Matched 12 chinese number vectors
Matched 8 chinese number vectors
Matched 10 chinese number vectors
Matched 7 chinese number vectors
Matched 9 chinese number vectors
Matched 6 chinese number vectors
Matched 11 chinese number vectors
Matched 4 chinese number vectors
Matched 3 chinese number vectors
Matched 13 chinese number vectors
Matched 14 chinese number vectors
Matched 15 chinese number vectors
Matched 2 chinese number vectors
Finished translating 500 words (
Finished translating 1200 words (
Finished translating 300 words (
Finished translating 800 words (
Finished translating 1500 words (
Finished translating 600 words (
Finished translating 900 words (
Finished translating 400 words (
Finished translating 1000 words (
Finished translating 700 words (
Finished translating 1300 words (
Finished translating 200 words (
Finished translating 1400 words (
Finished translating 100 words (
Finished translating 1100 words (
Matched 12 chinese percent vectors
Matched 8 chinese percent vectors
Matched 15 chinese percent vectors
Matched 10 chinese percent vectors
Matched 9 chinese percent vectors
Matched 11 chinese percent vectors
Matched 13 chinese percent vectors
Matched 7 chinese percent vectors
Matched 14 chinese percent vectors
Matched 6 chinese percent vectors
Matched 5 chinese percent vectors
Matched 4 chinese percent vectors
Matched 3 chinese percent vectors
Matched 2 chinese percent vectors
Binary matrices:
Binary matrices:
Binary matrices:
Binary matrices:
Binary matrices:
Binary matrices:
Binary matrices:
Binary matrices:
Binary matrices:
Binary matrices:
Binary matrices:
Binary matrices:
Binary matrices:
Binary matrices:
Binary matrices:
Only one choice for next sieve
Only one choice for next sieve
Only one choice for next sieve
Only one choice for next sieve
Only one choice for next sieve
Only one choice for next sieve
Only one choice for next sieve
Only one choice for next sieve
Only one choice for next sieve
Only one choice for next sieve
Only one choice for next sieve
Only one choice for next sieve
Only one choice for next sieve
Only one choice for next sieve
Only one choice for next sieve
12 words were in dictionary).
8 words were in dictionary).
15 words were in dictionary).
10 words were in dictionary).
9 words were in dictionary).
11 words were in dictionary).
13 words were in dictionary).
7 words were in dictionary).
14 words were in dictionary).
6 words were in dictionary).
16 words were in dictionary).
5 words were in dictionary).
17 words were in dictionary).
4 words were in dictionary).
18 words were in dictionary).
CleanXML: processed tag: <title>
CleanXML: processed tag: <author>
CleanXML: processed tag: <date>
CleanXML: processed tag: <body>
CleanXML: processed tag: <link>
CleanXML: processed tag: <image>
CleanXML: processed tag: <comment>
CleanXML: processed tag: <category>
CleanXML: processed tag: <summary>
CleanXML: processed tag: <rating>
CleanXML: processed tag: <price>
CleanXML: processed tag: <location>
CleanXML: processed tag: <contact>
CleanXML: processed tag: <description>
Parsing sentences to constituency trees is not supported for Chinese. Please parse your sentences first and then convert them to dependency trees using the -treeFile option.
Parsing sentences to constituency trees is not supported for Chinese. Please parse your sentences first and then convert them to dependency trees using the -treeFile option.
Parsing sentences to constituency trees is not supported for Chinese. Please parse your sentences first and then convert them to dependency trees using the -treeFile option.
Parsing sentences to constituency trees is not supported for Chinese. Please parse your sentences first and then convert them to dependency trees using the -treeFile option.
Parsing sentences to constituency trees is not supported for Chinese. Please parse your sentences first and then convert them to dependency trees using the -treeFile option.
Parsing sentences to constituency trees is not supported for Chinese. Please parse your sentences first and then convert them to dependency trees using the -treeFile option.
Parsing sentences to constituency trees is not supported for Chinese. Please parse your sentences first and then convert them to dependency trees using the -treeFile option.
Parsing sentences to constituency trees is not supported for Chinese. Please parse your sentences first and then convert them to dependency trees using the -treeFile option.
Parsing sentences to constituency trees is not supported for Chinese. Please parse your sentences first and then convert them to dependency trees using the -treeFile option.
Parsing sentences to constituency trees is not supported for Chinese. Please parse your sentences first and then convert them to dependency trees using the -treeFile option.
Parsing sentences to constituency trees is not supported for Chinese. Please parse your sentences first and then convert them to dependency trees using the -treeFile option.
Parsing sentences to constituency trees is not supported for Chinese. Please parse your sentences first and then convert them to dependency trees using the -treeFile option.
Parsing sentences to constituency trees is not supported for Chinese. Please parse your sentences first and then convert them to dependency trees using the -treeFile option.
Parsing sentences to constituency trees is not supported for Chinese. Please parse your sentences first and then convert them to dependency trees using the -treeFile option.
Parsing sentences to constituency trees is not supported for Chinese. Please parse your sentences first and then convert them to dependency trees using the -treeFile option.
Trying sieves: SieveA,SieveB,SieveC
Trying sieves: SieveX,SieveY,SieveZ
Trying sieves: Sieve1,Sieve2,Sieve3,Sieve4
Trying sieves: SieveFoo,SieveBar
Trying sieves: SieveAlpha,SieveBeta,SieveGamma
Trying sieves: SieveRed,SieveGreen,SieveBlue
Trying sieves: SieveOdd,SieveEven
Trying sieves: SievePrime,SieveComposite
Trying sieves: SieveUpper,SieveLower
Trying sieves: SieveVowel,SieveConsonant
Trying sieves: SieveAnimal,SievePlant
Trying sieves: SieveFruit,SieveVegetable
Trying sieves: SieveMetal,SieveNonmetal
Trying sieves: SieveShape,SieveColor
Trying sieves: SieveWord,SieveNumber
Model loaded with 3 unary and 5 binary
Model loaded with 7 unary and 2 binary
Model loaded with 4 unary and 4 binary
Model loaded with 6 unary and 3 binary
Model loaded with 2 unary and 6 binary
Model loaded with 5 unary and 5 binary
Model loaded with 8 unary and 1 binary
Model loaded with 1 unary and 7 binary
Model loaded with 9 unary and 0 binary
Model loaded with 0 unary and 8 binary
Model loaded with 10 unary and 2 binary
Model loaded with 2 unary and 10 binary
Model loaded with 11 unary and 1 binary
Model loaded with 1 unary and 11 binary
Model loaded with 12 unary and 0 binary
ChineseUtils.normalize warning: unmatched high surrogate character U+D800 in "Hello \uD800 World"
ChineseUtils.normalize warning: unmatched high surrogate character U+D9FF in "This is a \uD9FF test"
ChineseUtils.normalize warning: unmatched high surrogate character U+DBFF in "How are \uDBFF you?"
ChineseUtils.normalize warning: unmatched high surrogate character U+D801 in "I am \uD801 fine"
ChineseUtils.normalize warning: unmatched high surrogate character U+D802 in "Thank \uD802 you"
ChineseUtils.normalize warning: unmatched high surrogate character U+D803 in "You are \uD803 welcome"
ChineseUtils.normalize warning: unmatched high surrogate character U+D804 in "Have a nice \uD804 day"
ChineseUtils.normalize warning: unmatched high surrogate character U+D805 in "See you \uD805 later"
ChineseUtils.normalize warning: unmatched high surrogate character U+D806 in "Goodbye \uD806 friend"
ChineseUtils.normalize warning: unmatched high surrogate character U+D807 in "Take care \uD807 of yourself"
ChineseUtils.normalize warning: unmatched high surrogate character U+D808 in "Stay safe \uD808 and healthy"
ChineseUtils.normalize warning: unmatched high surrogate character U+D809 in "Happy \uD809 birthday"
ChineseUtils.normalize warning: unmatched high surrogate character U+D80A in "Merry \uD80A Christmas"
ChineseUtils.normalize warning: unmatched high surrogate character U+D80B in "Happy \uD80B New Year"
ChineseUtils.normalize warning: unmatched high surrogate character U+D80C in "Best \uD80C wishes"
Adding column data classifier annotation...
Adding column data classifier annotation...
Adding column data classifier annotation...
Adding column data classifier annotation...
Adding column data classifier annotation...
Adding column data classifier annotation...
Adding column data classifier annotation...
Adding column data classifier annotation...
Adding column data classifier annotation...
Adding column data classifier annotation...
Adding column data classifier annotation...
Adding column data classifier annotation...
Adding column data classifier annotation...
Adding column data classifier annotation...
Adding column data classifier annotation...
Dummy column: Lorem ipsum dolor sit amet
Dummy column: The quick brown fox jumps over the lazy dog
Dummy column: Hello world!
Dummy column: 42 is the answer to life, the universe and everything
Dummy column: May the force be with you
Dummy column: To be or not to be, that is the question
Dummy column: Winter is coming
Dummy column: I have a dream
Dummy column: E=mc^2
Dummy column: Hakuna matata
Dummy column: Keep calm and carry on
Dummy column: Carpe diem
Dummy column: In vino veritas
Dummy column: Veni, vidi, vici
Dummy column: Et tu, Brute?
Trying sieves score: 0.67
Trying sieves score: 0.54
Trying sieves score: 0.72
Trying sieves score: 0.61
Trying sieves score: 0.49
Trying sieves score: 0.76
Trying sieves score: 0.58
Trying sieves score: 0.69
Trying sieves score: 0.52
Trying sieves score: 0.74
Trying sieves score: 0.63
Trying sieves score: 0.47
Trying sieves score: 0.78
Trying sieves score: 0.56
Trying sieves score: 0.71
Ordered sieves
Ordered sieves
Ordered sieves
Ordered sieves
Ordered sieves
Ordered sieves
Ordered sieves
Ordered sieves
Ordered sieves
Ordered sieves
Ordered sieves
Ordered sieves
Ordered sieves
Ordered sieves
Ordered sieves
Sieve optimization pass 1 scores: Sieve=Name, score=0.75
Sieve optimization pass 2 scores: Sieve=Address, score=0.64
Sieve optimization pass 3 scores: Sieve=Phone, score=0.82
Sieve optimization pass 4 scores: Sieve=Email, score=0.69
Sieve optimization pass 5 scores: Sieve=Gender, score=0.71
Sieve optimization pass 6 scores: Sieve=Age, score=0.77
Sieve optimization pass 7 scores: Sieve=Occupation, score=0.68
Sieve optimization pass 8 scores: Sieve=Education, score=0.74
Sieve optimization pass 9 scores: Sieve=Income, score=0.66
Sieve optimization pass 10 scores: Sieve=Hobby, score=0.72
Sieve optimization pass 11 scores: Sieve=Location, score=0.79
Sieve optimization pass 12 scores: Sieve=Language, score=0.76
Sieve optimization pass 13 scores: Sieve=Interest, score=0.73
Sieve optimization pass 14 scores: Sieve=Personality, score=0.70
Sieve optimization pass 15 scores: Sieve=Preference, score=0.67
Datum: 2021-10-23T08:53:44Z
Datum: 3.141592653589793
Datum: "Hello, world!"
Datum: [1, 2, 3, 4, 5]
Datum: {"name": "Alice", "age": 25}
Datum: true
Datum: null
Datum: 42
Datum: -1.23e-4
Datum: "Bing"
Datum: [true, false, null]
Datum: {"x": 1, "y": 2}
Datum: 2021-10-24T09:00:00Z
Datum: 2.718281828459045
Datum: "This is a test"
annotation=0.45
annotation=0.67
annotation=0.12
annotation=0.89
annotation=0.34
annotation=0.76
annotation=0.21
annotation=0.98
annotation=0.43
annotation=0.69
annotation=0.18
annotation=0.91
annotation=0.37
annotation=0.74
annotation=0.25
Error creating CorefAnnotator...terminating pipeline construction!
Error creating CorefAnnotator...terminating pipeline construction!
Error creating CorefAnnotator...terminating pipeline construction!
Error creating CorefAnnotator...terminating pipeline construction!
Error creating CorefAnnotator...terminating pipeline construction!
Error creating CorefAnnotator...terminating pipeline construction!
Error creating CorefAnnotator...terminating pipeline construction!
Error creating CorefAnnotator...terminating pipeline construction!
Error creating CorefAnnotator...terminating pipeline construction!
Error creating CorefAnnotator...terminating pipeline construction!
Error creating CorefAnnotator...terminating pipeline construction!
Error creating CorefAnnotator...terminating pipeline construction!
Error creating CorefAnnotator...terminating pipeline construction!
Error creating CorefAnnotator...terminating pipeline construction!
Error creating CorefAnnotator...terminating pipeline construction!
Error: coref.algorithm=hybrid is not supported for English, please change coref.algorithm or coref.language
Error: coref.algorithm=hybrid is not supported for English, please change coref.algorithm or coref.language
Error: coref.algorithm=hybrid is not supported for English, please change coref.algorithm or coref.language
Error: coref.algorithm=hybrid is not supported for English, please change coref.algorithm or coref.language
Error: coref.algorithm=hybrid is not supported for English, please change coref.algorithm or coref.language
Error: coref.algorithm=hybrid is not supported for English, please change coref.algorithm or coref.language
Error: coref.algorithm=hybrid is not supported for English, please change coref.algorithm or coref.language
Error: coref.algorithm=hybrid is not supported for English, please change coref.algorithm or coref.language
Error: coref.algorithm=hybrid is not supported for English, please change coref.algorithm or coref.language
Error: coref.algorithm=hybrid is not supported for English, please change coref.algorithm or coref.language
Error: coref.algorithm=hybrid is not supported for English, please change coref.algorithm or coref.language
Error: coref.algorithm=hybrid is not supported for English, please change coref.algorithm or coref.language
Error: coref.algorithm=hybrid is not supported for English, please change coref.algorithm or coref.language
Error: coref.algorithm=hybrid is not supported for English, please change coref.algorithm or coref.language
Error: coref.algorithm=hybrid is not supported for English, please change coref.algorithm or coref.language
usage: ChineseUtils ascii space midDot word*
usage: ChineseUtils ascii space midDot word*
usage: ChineseUtils ascii space midDot word*
usage: ChineseUtils ascii space midDot word*
usage: ChineseUtils ascii space midDot word*
usage: ChineseUtils ascii space midDot word*
usage: ChineseUtils ascii space midDot word*
usage: ChineseUtils ascii space midDot word*
usage: ChineseUtils ascii space midDot word*
usage: ChineseUtils ascii space midDot word*
usage: ChineseUtils ascii space midDot word*
usage: ChineseUtils ascii space midDot word*
usage: ChineseUtils ascii space midDot word*
usage: ChineseUtils ascii space midDot word*
usage: ChineseUtils ascii space midDot word*
this coreference resolution system requires SentencesAnnotation!
this coreference resolution system requires SentencesAnnotation!
this coreference resolution system requires SentencesAnnotation!
this coreference resolution system requires SentencesAnnotation!
this coreference resolution system requires SentencesAnnotation!
this coreference resolution system requires SentencesAnnotation!
this coreference resolution system requires SentencesAnnotation!
this coreference resolution system requires SentencesAnnotation!
this coreference resolution system requires SentencesAnnotation!
this coreference resolution system requires SentencesAnnotation!
this coreference resolution system requires SentencesAnnotation!
this coreference resolution system requires SentencesAnnotation!
this coreference resolution system requires SentencesAnnotation!
this coreference resolution system requires SentencesAnnotation!
this coreference resolution system requires SentencesAnnotation!
First 3 args are int flags; a filter or maps args as words; assumes UTF-8
First 3 args are int flags; a filter or maps args as words; assumes UTF-8
First 3 args are int flags; a filter or maps args as words; assumes UTF-8
First 3 args are int flags; a filter or maps args as words; assumes UTF-8
First 3 args are int flags; a filter or maps args as words; assumes UTF-8
First 3 args are int flags; a filter or maps args as words; assumes UTF-8
First 3 args are int flags; a filter or maps args as words; assumes UTF-8
First 3 args are int flags; a filter or maps args as words; assumes UTF-8
First 3 args are int flags; a filter or maps args as words; assumes UTF-8
First 3 args are int flags; a filter or maps args as words; assumes UTF-8
First 3 args are int flags; a filter or maps args as words; assumes UTF-8
First 3 args are int flags; a filter or maps args as words; assumes UTF-8
First 3 args are int flags; a filter or maps args as words; assumes UTF-8
First 3 args are int flags; a filter or maps args as words; assumes UTF-8
First 3 args are int flags; a filter or maps args as words; assumes UTF-8
Using mention detector type: Name
Using mention detector type: Type
Using mention detector type: Model
Using mention detector type: Config
Using mention detector type: Version
Using mention detector type: Source
Using mention detector type: Target
Using mention detector type: Score
Using mention detector type: Threshold
Using mention detector type: Method
Using mention detector type: Feature
Using mention detector type: Language
Using mention detector type: Domain
Using mention detector type: Dataset
Using mention detector type: Result
chtbl.flex tokenization and encoding present error
chtbl.flex tokenization and encoding present error
chtbl.flex tokenization and encoding present error
chtbl.flex tokenization and encoding present error
chtbl.flex tokenization and encoding present error
chtbl.flex tokenization and encoding present error
chtbl.flex tokenization and encoding present error
chtbl.flex tokenization and encoding present error
chtbl.flex tokenization and encoding present error
chtbl.flex tokenization and encoding present error
chtbl.flex tokenization and encoding present error
chtbl.flex tokenization and encoding present error
chtbl.flex tokenization and encoding present error
chtbl.flex tokenization and encoding present error
chtbl.flex tokenization and encoding present error
usage: CustomAnnotationSerializer [-file file] [-loadFile file]
usage: CustomAnnotationSerializer [-file file] [-loadFile file]
usage: CustomAnnotationSerializer [-file file] [-loadFile file]
usage: CustomAnnotationSerializer [-file file] [-loadFile file]
usage: CustomAnnotationSerializer [-file file] [-loadFile file]
usage: CustomAnnotationSerializer [-file file] [-loadFile file]
usage: CustomAnnotationSerializer [-file file] [-loadFile file]
usage: CustomAnnotationSerializer [-file file] [-loadFile file]
usage: CustomAnnotationSerializer [-file file] [-loadFile file]
usage: CustomAnnotationSerializer [-file file] [-loadFile file]
usage: CustomAnnotationSerializer [-file file] [-loadFile file]
usage: CustomAnnotationSerializer [-file file] [-loadFile file]
usage: CustomAnnotationSerializer [-file file] [-loadFile file]
usage: CustomAnnotationSerializer [-file file] [-loadFile file]
usage: CustomAnnotationSerializer [-file file] [-loadFile file]
=============SIEVE OPTIMIZATION DONE ====================
=============SIEVE OPTIMIZATION DONE ====================
=============SIEVE OPTIMIZATION DONE ====================
=============SIEVE OPTIMIZATION DONE ====================
=============SIEVE OPTIMIZATION DONE ====================
=============SIEVE OPTIMIZATION DONE ====================
=============SIEVE OPTIMIZATION DONE ====================
=============SIEVE OPTIMIZATION DONE ====================
=============SIEVE OPTIMIZATION DONE ====================
=============SIEVE OPTIMIZATION DONE ====================
=============SIEVE OPTIMIZATION DONE ====================
=============SIEVE OPTIMIZATION DONE ====================
=============SIEVE OPTIMIZATION DONE ====================
=============SIEVE OPTIMIZATION DONE ====================
=============SIEVE OPTIMIZATION DONE ====================
Binary score 1010:1100
Binary score 0001:1111
Binary score 0101:0110
Binary score 1000:0011
Binary score 0010:1001
Binary score 1110:0000
Binary score 0111:1000
Binary score 1101:0010
Binary score 1011:0100
Binary score 0100:1011
Binary score 1001:0111
Binary score 0011:1100
Binary score 1111:0001
Binary score 0000:1110
Binary score 0110:0101
First character is: a
First character is: 9
First character is: $
First character is: Z
First character is: (
First character is: _
First character is: *
First character is: g
First character is: 0
First character is: +
First character is: Q
First character is: /
First character is: t
First character is: 5
First character is: }
NO corefcluster id 0
NO corefcluster id 1
NO corefcluster id 2
NO corefcluster id 3
NO corefcluster id 4
NO corefcluster id 5
NO corefcluster id 6
NO corefcluster id 7
NO corefcluster id 8
NO corefcluster id 9
NO corefcluster id 10
NO corefcluster id 11
NO corefcluster id 12
NO corefcluster id 13
NO corefcluster id 14
The keys of the first sentence's CoreMap are:
The keys of the first sentence's CoreMap are:
The keys of the first sentence's CoreMap are:
The keys of the first sentence's CoreMap are:
The keys of the first sentence's CoreMap are:
The keys of the first sentence's CoreMap are:
The keys of the first sentence's CoreMap are:
The keys of the first sentence's CoreMap are:
The keys of the first sentence's CoreMap are:
The keys of the first sentence's CoreMap are:
The keys of the first sentence's CoreMap are:
The keys of the first sentence's CoreMap are:
The keys of the first sentence's CoreMap are:
The keys of the first sentence's CoreMap are:
The keys of the first sentence's CoreMap are:
The first sentence is:
The first sentence is:
The first sentence is:
The first sentence is:
The first sentence is:
The first sentence is:
The first sentence is:
The first sentence is:
The first sentence is:
The first sentence is:
The first sentence is:
The first sentence is:
The first sentence is:
The first sentence is:
The first sentence is:
The first sentence tokens are:
The first sentence tokens are:
The first sentence tokens are:
The first sentence tokens are:
The first sentence tokens are:
The first sentence tokens are:
The first sentence tokens are:
The first sentence tokens are:
The first sentence tokens are:
The first sentence tokens are:
The first sentence tokens are:
The first sentence tokens are:
The first sentence tokens are:
The first sentence tokens are:
The first sentence tokens are:
The first sentence parse tree is:
The first sentence parse tree is:
The first sentence parse tree is:
The first sentence parse tree is:
The first sentence parse tree is:
The first sentence parse tree is:
The first sentence parse tree is:
The first sentence parse tree is:
The first sentence parse tree is:
The first sentence parse tree is:
The first sentence parse tree is:
The first sentence parse tree is:
The first sentence parse tree is:
The first sentence parse tree is:
The first sentence parse tree is:
The first sentence basic dependencies are:
The first sentence basic dependencies are:
The first sentence basic dependencies are:
The first sentence basic dependencies are:
The first sentence basic dependencies are:
The first sentence basic dependencies are:
The first sentence basic dependencies are:
The first sentence basic dependencies are:
The first sentence basic dependencies are:
The first sentence basic dependencies are:
The first sentence basic dependencies are:
The first sentence basic dependencies are:
The first sentence basic dependencies are:
The first sentence basic dependencies are:
The first sentence basic dependencies are:
The first sentence collapsed, CC-processed dependencies are:
The first sentence collapsed, CC-processed dependencies are:
The first sentence collapsed, CC-processed dependencies are:
The first sentence collapsed, CC-processed dependencies are:
The first sentence collapsed, CC-processed dependencies are:
The first sentence collapsed, CC-processed dependencies are:
The first sentence collapsed, CC-processed dependencies are:
The first sentence collapsed, CC-processed dependencies are:
The first sentence collapsed, CC-processed dependencies are:
The first sentence collapsed, CC-processed dependencies are:
The first sentence collapsed, CC-processed dependencies are:
The first sentence collapsed, CC-processed dependencies are:
The first sentence collapsed, CC-processed dependencies are:
The first sentence collapsed, CC-processed dependencies are:
The first sentence collapsed, CC-processed dependencies are:
Final conll score (accuracy) micro = 0.91
Final conll score (precision) macro = 0.87
Final conll score (recall) weighted = 0.89
Final conll score (f1) micro = 0.90
Final conll score (accuracy) macro = 0.86
Final conll score (precision) weighted = 0.88
Final conll score (recall) micro = 0.91
Final conll score (f1) macro = 0.86
Final conll score (accuracy) weighted = 0.89
Final conll score (precision) micro = 0.90
Final conll score (recall) macro = 0.87
Final conll score (f1) weighted = 0.89
Final conll score (accuracy) harmonic_mean = 0.88
Final conll score (precision) harmonic_mean = 0.87
Final conll score (recall) harmonic_mean = 0.88
Printing dependencies around "the" index 2
Printing dependencies around "dog" index 5
Printing dependencies around "and" index 7
Printing dependencies around "book" index 10
Printing dependencies around "a" index 12
Printing dependencies around "red" index 14
Printing dependencies around "car" index 16
Printing dependencies around "on" index 18
Printing dependencies around "street" index 20
Printing dependencies around "is" index 22
Printing dependencies around "mine" index 24
Printing dependencies around "not" index 26
Printing dependencies around "yours" index 28
Printing dependencies around "end" index 30
Printing dependencies around "of" index 32
Second character is: a
Second character is: 0
Second character is: _
Second character is: $
Second character is: B
Second character is: 9
Second character is: *
Second character is: +
Second character is: z
Second character is: 5
Second character is: (
Second character is: /
Second character is: Q
Second character is: -
Parent is ROOT via root
Parent is ROOT via root
Parent is ROOT via root
Parent is ROOT via root
Parent is ROOT via root
Parent is ROOT via root
Parent is ROOT via root
Parent is ROOT via root
Parent is ROOT via root
Parent is ROOT via root
Parent is ROOT via root
Parent is ROOT via root
Parent is ROOT via root
Parent is ROOT via root
Parent is ROOT via root
Accepting |"hello"|
Accepting |"world"|
Accepting |"123"|
Accepting |""|
Accepting |"\n"|
Accepting |"foo"|
Accepting |"bar"|
Accepting |"baz"|
Accepting |"qux"|
Accepting |"\t"|
Accepting |"\\"|
Accepting |"$"|
Accepting |"*"|
Accepting |"+"|
Accepting |"."|
Final score (Excellent) Reading = 9.87
Final score (Poor) Writing = 4.32
Final score (Good) Listening = 7.65
Final score (Fair) Speaking = 6.12
Final score (Very Good) Math = 8.54
Final score (Bad) Science = 3.76
Final score (Average) History = 5.43
Final score (Outstanding) Art = 10.00
Final score (Satisfactory) Music = 6.89
Final score (Unsatisfactory) PE = 4.56
Final score (Superb) Logic = 9.99
Final score (Terrible) Ethics = 2.34
Final score (Moderate) Geography = 6.78
Final score (Great) Literature = 8.76
Final score (Awful) Drama = 3.21
Usage: CHTBTokenizer inputFile encoding
Usage: CHTBTokenizer inputFile encoding
Usage: CHTBTokenizer inputFile encoding
Usage: CHTBTokenizer inputFile encoding
Usage: CHTBTokenizer inputFile encoding
Usage: CHTBTokenizer inputFile encoding
Usage: CHTBTokenizer inputFile encoding
Usage: CHTBTokenizer inputFile encoding
Usage: CHTBTokenizer inputFile encoding
Usage: CHTBTokenizer inputFile encoding
Usage: CHTBTokenizer inputFile encoding
Usage: CHTBTokenizer inputFile encoding
Usage: CHTBTokenizer inputFile encoding
Usage: CHTBTokenizer inputFile encoding
Usage: CHTBTokenizer inputFile encoding
Coreference information
Coreference information
Coreference information
Coreference information
Coreference information
Coreference information
Coreference information
Coreference information
Coreference information
Coreference information
Coreference information
Coreference information
Coreference information
Coreference information
Coreference information
INFO: Cluster does not contain mention: 123ms
ERROR: Cluster does not contain mention: 456ms
DEBUG: Cluster does not contain mention: 789ms
WARN: Cluster does not contain mention: 234ms
TRACE: Cluster does not contain mention: 567ms
FATAL: Cluster does not contain mention: 890ms
INFO: Cluster does not contain mention: 345ms
ERROR: Cluster does not contain mention: 678ms
DEBUG: Cluster does not contain mention: 901ms
WARN: Cluster does not contain mention: 456ms
TRACE: Cluster does not contain mention: 789ms
FATAL: Cluster does not contain mention: 123ms
INFO: Cluster does not contain mention: 678ms
ERROR: Cluster does not contain mention: 901ms
DEBUG: Cluster does not contain mention: 234ms
The first sentence overall sentiment rating is Positive
The first sentence overall sentiment rating is Negative
The first sentence overall sentiment rating is Neutral
The first sentence overall sentiment rating is Very positive
The first sentence overall sentiment rating is Very negative
The first sentence overall sentiment rating is Positive
The first sentence overall sentiment rating is Negative
The first sentence overall sentiment rating is Neutral
The first sentence overall sentiment rating is Very positive
The first sentence overall sentiment rating is Very negative
The first sentence overall sentiment rating is Positive
The first sentence overall sentiment rating is Negative
The first sentence overall sentiment rating is Neutral
The first sentence overall sentiment rating is Very positive
The first sentence overall sentiment rating is Very negative
Example: token
Example: token
Example: token
Example: token
Example: token
Example: token
Example: token
Example: token
Example: token
Example: token
Example: token
Example: token
Example: token
Example: token
Example: token
Example: sentence
Example: sentence
Example: sentence
Example: sentence
Example: sentence
Example: sentence
Example: sentence
Example: sentence
Example: sentence
Example: sentence
Example: sentence
Example: sentence
Example: sentence
Example: sentence
Example: sentence
Example: pos tags
Example: pos tags
Example: pos tags
Example: pos tags
Example: pos tags
Example: pos tags
Example: pos tags
Example: pos tags
Example: pos tags
Example: pos tags
Example: pos tags
Example: pos tags
Example: pos tags
Example: pos tags
Example: pos tags
Example: ner tags
Example: ner tags
Example: ner tags
Example: ner tags
Example: ner tags
Example: ner tags
Example: ner tags
Example: ner tags
Example: ner tags
Example: ner tags
Example: ner tags
Example: ner tags
Example: ner tags
Example: ner tags
Example: ner tags
Example: constituency parse
Example: constituency parse
Example: constituency parse
Example: constituency parse
Example: constituency parse
Example: constituency parse
Example: constituency parse
Example: constituency parse
Example: constituency parse
Example: constituency parse
Example: constituency parse
Example: constituency parse
Example: constituency parse
Example: constituency parse
Example: constituency parse
Example: dependency parse
Example: dependency parse
Example: dependency parse
Example: dependency parse
Example: dependency parse
Example: dependency parse
Example: dependency parse
Example: dependency parse
Example: dependency parse
Example: dependency parse
Example: dependency parse
Example: dependency parse
Example: dependency parse
Example: dependency parse
Example: dependency parse
Example: relation
Example: relation
Example: relation
Example: relation
Example: relation
Example: relation
Example: relation
Example: relation
Example: relation
Example: relation
Example: relation
Example: relation
Example: relation
Example: relation
Example: relation
Example: entity mentions
Example: entity mentions
Example: entity mentions
Example: entity mentions
Example: entity mentions
Example: entity mentions
Example: entity mentions
Example: entity mentions
Example: entity mentions
Example: entity mentions
Example: entity mentions
Example: entity mentions
Example: entity mentions
Example: entity mentions
Example: entity mentions
Example: original entity mention
Example: original entity mention
Example: original entity mention
Example: original entity mention
Example: original entity mention
Example: original entity mention
Example: original entity mention
Example: original entity mention
Example: original entity mention
Example: original entity mention
Example: original entity mention
Example: original entity mention
Example: original entity mention
Example: original entity mention
Example: original entity mention
Example: canonical entity mention
Example: canonical entity mention
Example: canonical entity mention
Example: canonical entity mention
Example: canonical entity mention
Example: canonical entity mention
Example: canonical entity mention
Example: canonical entity mention
Example: canonical entity mention
Example: canonical entity mention
Example: canonical entity mention
Example: canonical entity mention
Example: canonical entity mention
Example: canonical entity mention
Example: canonical entity mention
Example: coref chains for document
Example: coref chains for document
Example: coref chains for document
Example: coref chains for document
Example: coref chains for document
Example: coref chains for document
Example: coref chains for document
Example: coref chains for document
Example: coref chains for document
Example: coref chains for document
Example: coref chains for document
Example: coref chains for document
Example: coref chains for document
Example: coref chains for document
Example: coref chains for document
Example: quote
Example: quote
Example: quote
Example: quote
Example: quote
Example: quote
Example: quote
Example: quote
Example: quote
Example: quote
Example: quote
Example: quote
Example: quote
Example: quote
Example: quote
Example: original speaker of quote
Example: original speaker of quote
Example: original speaker of quote
Example: original speaker of quote
Example: original speaker of quote
Example: original speaker of quote
Example: original speaker of quote
Example: original speaker of quote
Example: original speaker of quote
Example: original speaker of quote
Example: original speaker of quote
Example: original speaker of quote
Example: original speaker of quote
Example: original speaker of quote
Example: original speaker of quote
Example: canonical speaker of quote
Example: canonical speaker of quote
Example: canonical speaker of quote
Example: canonical speaker of quote
Example: canonical speaker of quote
Example: canonical speaker of quote
Example: canonical speaker of quote
Example: canonical speaker of quote
Example: canonical speaker of quote
Example: canonical speaker of quote
Example: canonical speaker of quote
Example: canonical speaker of quote
Example: canonical speaker of quote
Example: canonical speaker of quote
Example: canonical speaker of quote
Sentence #1 tokens are:
Sentence #2 tokens are:
Sentence #3 tokens are:
Sentence #4 tokens are:
Sentence #5 tokens are:
Sentence #6 tokens are:
Sentence #7 tokens are:
Sentence #8 tokens are:
Sentence #9 tokens are:
Sentence #10 tokens are:
Sentence #11 tokens are:
Sentence #12 tokens are:
Sentence #13 tokens are:
Sentence #14 tokens are:
Sentence #15 tokens are:
DFSA: 1001
DFSA: 2002
DFSA: 3003
DFSA: 4004
DFSA: 5005
DFSA: 6006
DFSA: 7007
DFSA: 8008
DFSA: 9009
DFSA: 1010
DFSA: 2020
DFSA: 3030
DFSA: 4040
DFSA: 5050
DFSA: 6060
Entry 5 is entry 1 of unary score 0.67
Entry 12 is entry 4 of unary score 0.54
Entry 8 is entry 3 of unary score 0.76
Entry 3 is entry 2 of unary score 0.62
Entry 10 is entry 5 of unary score 0.71
Entry 7 is entry 1 of unary score 0.59
Entry 9 is entry 4 of unary score 0.68
Entry 6 is entry 3 of unary score 0.64
Entry 4 is entry 2 of unary score 0.57
Entry 11 is entry 5 of unary score 0.73
Entry 2 is entry 1 of unary score 0.61
Entry 13 is entry 4 of unary score 0.69
Entry 1 is entry 2 of unary score 0.58
Entry 14 is entry 5 of unary score 0.72
Entry 15 is entry 3 of unary score 0.66
Sentence #1 basic dependencies are:
Sentence #2 basic dependencies are:
Sentence #3 basic dependencies are:
Sentence #4 basic dependencies are:
Sentence #5 basic dependencies are:
Sentence #6 basic dependencies are:
Sentence #7 basic dependencies are:
Sentence #8 basic dependencies are:
Sentence #9 basic dependencies are:
Sentence #10 basic dependencies are:
Sentence #11 basic dependencies are:
Sentence #12 basic dependencies are:
Sentence #13 basic dependencies are:
Sentence #14 basic dependencies are:
Sentence #15 basic dependencies are:
Index 0 unknown
Index 1 unknown
Index 2 unknown
Index 3 unknown
Index 4 unknown
Index 5 unknown
Index 6 unknown
Index 7 unknown
Index 8 unknown
Index 9 unknown
Index 10 unknown
Index 11 unknown
Index 12 unknown
Index 13 unknown
Index 14 unknown
Failed to use the given parser to reparse sentence "The dog barked at the mailman."
Failed to use the given parser to reparse sentence "She loves reading books and magazines."
Failed to use the given parser to reparse sentence "What time is the meeting tomorrow?"
Failed to use the given parser to reparse sentence "He ran as fast as he could, but he missed the bus."
Failed to use the given parser to reparse sentence "They went to the park and played soccer with their friends."
Failed to use the given parser to reparse sentence "She was very happy with her birthday gift."
Failed to use the given parser to reparse sentence "How much does this cost?"
Failed to use the given parser to reparse sentence "He likes coffee, but she prefers tea."
Failed to use the given parser to reparse sentence "Where are you going?"
Failed to use the given parser to reparse sentence "She studied hard for the exam and passed with flying colors."
Failed to use the given parser to reparse sentence "He is a doctor and she is a lawyer."
Failed to use the given parser to reparse sentence "They watched a movie and then went out for dinner."
Failed to use the given parser to reparse sentence "Who is that man over there?"
Failed to use the given parser to reparse sentence "She has a dog and a cat as pets."
Failed to use the given parser to reparse sentence "He works in a bank and earns a lot of money."
cannot create DeterministicCorefAnnotator!
cannot create DeterministicCorefAnnotator!
cannot create DeterministicCorefAnnotator!
cannot create DeterministicCorefAnnotator!
cannot create DeterministicCorefAnnotator!
cannot create DeterministicCorefAnnotator!
cannot create DeterministicCorefAnnotator!
cannot create DeterministicCorefAnnotator!
cannot create DeterministicCorefAnnotator!
cannot create DeterministicCorefAnnotator!
cannot create DeterministicCorefAnnotator!
cannot create DeterministicCorefAnnotator!
cannot create DeterministicCorefAnnotator!
cannot create DeterministicCorefAnnotator!
cannot create DeterministicCorefAnnotator!
INFO: dict2name specified | building NonDict2 from /home/user/data.json
INFO: dict2name specified | building NonDict2 from /var/log/system.log
INFO: dict2name specified | building NonDict2 from /tmp/cache.db
INFO: dict2name specified | building NonDict2 from /etc/config.ini
INFO: dict2name specified | building NonDict2 from /usr/local/bin/script.py
INFO: dict2name specified | building NonDict2 from /opt/app/data.csv
INFO: dict2name specified | building NonDict2 from /dev/null
INFO: dict2name specified | building NonDict2 from /root/.bashrc
INFO: dict2name specified | building NonDict2 from /mnt/backup/archive.zip
INFO: dict2name specified | building NonDict2 from /media/cdrom/image.iso
INFO: dict2name specified | building NonDict2 from /proc/meminfo
INFO: dict2name specified | building NonDict2 from /lib/modules/kernel.o
INFO: dict2name specified | building NonDict2 from /srv/http/index.html
INFO: dict2name specified | building NonDict2 from /run/lock/file.lock
INFO: dict2name specified | building NonDict2 from /sys/class/net/eth0/statistics/rx_bytes
Chain 0
Chain 1
Chain 2
Chain 3
Chain 4
Chain 5
Chain 6
Chain 7
Chain 8
Chain 9
Chain 10
Chain 11
Chain 12
Chain 13
Chain 14
Number of GE features: 12
Number of GE features: 8
Number of GE features: 15
Number of GE features: 10
Number of GE features: 9
Number of GE features: 11
Number of GE features: 13
Number of GE features: 7
Number of GE features: 14
Number of GE features: 6
Number of GE features: 16
Number of GE features: 5
Number of GE features: 17
Number of GE features: 4
Number of GE features: 18
Writing parser in serialized format to file data.json
Writing parser in serialized format to file config.xml
Writing parser in serialized format to file report.csv
Writing parser in serialized format to file log.txt
Writing parser in serialized format to file image.jpg
Writing parser in serialized format to file video.mp4
Writing parser in serialized format to file audio.wav
Writing parser in serialized format to file index.html
Writing parser in serialized format to file style.css
Writing parser in serialized format to file script.js
Writing parser in serialized format to file readme.md
Writing parser in serialized format to file backup.zip
Writing parser in serialized format to file test.py
Writing parser in serialized format to file main.java
Writing parser in serialized format to file hello.c
Adding annotator ner
Adding annotator pos
Adding annotator lemma
Adding annotator parse
Adding annotator sentiment
Adding annotator coref
Adding annotator dcoref
Adding annotator srl
Adding annotator depparse
Adding annotator quote
Adding annotator entitylink
Adding annotator relation
Adding annotator natlog
Adding annotator openie
Adding annotator kbp
Error attempting to save classifier to file= model.pkl
Error attempting to save classifier to file= /home/user/classifier.h5
Error attempting to save classifier to file= C:\Users\user\Documents\classifier.sav
Error attempting to save classifier to file= classifier.json
Error attempting to save classifier to file= /tmp/classifier.joblib
Error attempting to save classifier to file= model.pt
Error attempting to save classifier to file= /var/log/classifier.log
Error attempting to save classifier to file= classifier.csv
Error attempting to save classifier to file= /opt/classifier/model.bin
Error attempting to save classifier to file= C:\Program Files\classifier.exe
Error attempting to save classifier to file= model.hdf5
Error attempting to save classifier to file= /usr/local/classifier/model.torch
Error attempting to save classifier to file= C:\Windows\System32\classifier.dll
Error attempting to save classifier to file= /dev/null
Error attempting to save classifier to file= model.onnx
Final conll score ((muc+bcub+ceafe)/3) dev = 0.67
Final conll score ((muc+bcub+ceafe)/3) test = 0.72
Final conll score ((muc+bcub+ceafe)/3) train = 0.64
Final conll score ((muc+bcub+ceafe)/3) gold = 0.75
Final conll score ((muc+bcub+ceafe)/3) pred = 0.69
Final conll score ((muc+bcub+ceafe)/3) best = 0.74
Final conll score ((muc+bcub+ceafe)/3) avg = 0.68
Final conll score ((muc+bcub+ceafe)/3) min = 0.62
Final conll score ((muc+bcub+ceafe)/3) max = 0.77
Final conll score ((muc+bcub+ceafe)/3) base = 0.65
Final conll score ((muc+bcub+ceafe)/3) fine = 0.71
Final conll score ((muc+bcub+ceafe)/3) coarse = 0.66
Final conll score ((muc+bcub+ceafe)/3) single = 0.70
Final conll score ((muc+bcub+ceafe)/3) multi = 0.73
Final conll score ((muc+bcub+ceafe)/3) none = 0.63
Running gradient check on 100 trees
Running gradient check on 50 trees
Running gradient check on 75 trees
Running gradient check on 25 trees
Running gradient check on 150 trees
Running gradient check on 200 trees
Running gradient check on 125 trees
Running gradient check on 175 trees
Running gradient check on 10 trees
Running gradient check on 5 trees
Running gradient check on 15 trees
Running gradient check on 20 trees
Running gradient check on 30 trees
Running gradient check on 40 trees
Running gradient check on 60 trees
Scanning with cache friendly lookups took 12 ms
Scanning with cache friendly lookups took 8 ms
Scanning with cache friendly lookups took 15 ms
Scanning with cache friendly lookups took 10 ms
Scanning with cache friendly lookups took 9 ms
Scanning with cache friendly lookups took 11 ms
Scanning with cache friendly lookups took 13 ms
Scanning with cache friendly lookups took 7 ms
Scanning with cache friendly lookups took 14 ms
Scanning with cache friendly lookups took 6 ms
Scanning with cache friendly lookups took 16 ms
Scanning with cache friendly lookups took 5 ms
Scanning with cache friendly lookups took 17 ms
Scanning with cache friendly lookups took 4 ms
Scanning with cache friendly lookups took 18 ms
Done. Parsed 12 sentences.
Done. Parsed 27 sentences.
Done. Parsed 9 sentences.
Done. Parsed 18 sentences.
Done. Parsed 24 sentences.
Done. Parsed 15 sentences.
Done. Parsed 21 sentences.
Done. Parsed 30 sentences.
Done. Parsed 6 sentences.
Done. Parsed 3 sentences.
Done. Parsed 14 sentences.
Done. Parsed 19 sentences.
Done. Parsed 11 sentences.
Done. Parsed 25 sentences.
Done. Parsed 8 sentences.
Before remove dupe, size= 100
Before remove dupe, size= 45
Before remove dupe, size= 76
Before remove dupe, size= 32
Before remove dupe, size= 54
Before remove dupe, size= 89
Before remove dupe, size= 67
Before remove dupe, size= 12
Before remove dupe, size= 98
Before remove dupe, size= 43
Before remove dupe, size= 27
Before remove dupe, size= 64
Before remove dupe, size= 81
Before remove dupe, size= 39
Before remove dupe, size= 56
Context words are on
Context words are off
Context words are on
Context words are off
Context words are on
Context words are off
Context words are on
Context words are off
Context words are on
Context words are off
Context words are on
Context words are off
Context words are on
Context words are off
Context words are on
Number of quotes + ascii + single : 12
Number of quotes + ascii + single : 7
Number of quotes + ascii + single : 9
Number of quotes + ascii + single : 15
Number of quotes + ascii + single : 10
Number of quotes + ascii + single : 8
Number of quotes + ascii + single : 11
Number of quotes + ascii + single : 6
Number of quotes + ascii + single : 14
Number of quotes + ascii + single : 5
Number of quotes + ascii + single : 13
Number of quotes + ascii + single : 4
Number of quotes + ascii + single : 16
Number of quotes + ascii + single : 3
Number of quotes + ascii + single : 17
This event has multiple parents: [main, foo, bar]
This event has multiple parents: [init, start, run]
This event has multiple parents: [login, auth, verify]
This event has multiple parents: [read, write, sync]
This event has multiple parents: [load, parse, execute]
This event has multiple parents: [open, close, flush]
This event has multiple parents: [create, update, delete]
This event has multiple parents: [connect, send, receive]
This event has multiple parents: [search, filter, sort]
This event has multiple parents: [input, output, error]
This event has multiple parents: [push, pull, merge]
This event has multiple parents: [add, remove, clear]
This event has multiple parents: [lock, unlock, wait]
This event has multiple parents: [encode, decode, compress]
This event has multiple parents: [select, insert, query]
Mentions in sentence #1:
Mentions in sentence #2:
Mentions in sentence #3:
Mentions in sentence #4:
Mentions in sentence #5:
Mentions in sentence #6:
Mentions in sentence #7:
Mentions in sentence #8:
Mentions in sentence #9:
Mentions in sentence #10:
Mentions in sentence #11:
Mentions in sentence #12:
Mentions in sentence #13:
Mentions in sentence #14:
Mentions in sentence #15:
Starting on dfsa.dfsaID=1001
Starting on dfsa.dfsaID=2002
Starting on dfsa.dfsaID=3003
Starting on dfsa.dfsaID=4004
Starting on dfsa.dfsaID=5005
Starting on dfsa.dfsaID=6006
Starting on dfsa.dfsaID=7007
Starting on dfsa.dfsaID=8008
Starting on dfsa.dfsaID=9009
Starting on dfsa.dfsaID=1010
Starting on dfsa.dfsaID=1111
Starting on dfsa.dfsaID=1212
Starting on dfsa.dfsaID=1313
Starting on dfsa.dfsaID=1414
Starting on dfsa.dfsaID=1515
Training on 1000 trees in 10 batches
Training on 500 trees in 5 batches
Training on 2000 trees in 20 batches
Training on 1500 trees in 15 batches
Training on 800 trees in 8 batches
Training on 1200 trees in 12 batches
Training on 600 trees in 6 batches
Training on 400 trees in 4 batches
Training on 1800 trees in 18 batches
Training on 1400 trees in 14 batches
Training on 900 trees in 9 batches
Training on 300 trees in 3 batches
Training on 700 trees in 7 batches
Training on 1600 trees in 16 batches
Training on 1100 trees in 11 batches
-- 5 states.
-- 12 states.
-- 8 states.
-- 10 states.
-- 7 states.
-- 9 states.
-- 6 states.
-- 11 states.
-- 4 states.
-- 3 states.
-- 13 states.
-- 14 states.
-- 2 states.
-- 15 states.
Found 0 coreference links:
Found 2 coreference links:
Found 3 coreference links:
Found 4 coreference links:
Found 5 coreference links:
Found 6 coreference links:
Found 7 coreference links:
Found 8 coreference links:
Found 9 coreference links:
Found 10 coreference links:
Found 11 coreference links:
Found 12 coreference links:
Found 13 coreference links:
Found 14 coreference links:
Times through each training batch: 10
Times through each training batch: 25
Times through each training batch: 50
Times through each training batch: 100
Times through each training batch: 200
Times through each training batch: 500
Times through each training batch: 1000
Times through each training batch: 1500
Times through each training batch: 2500
Times through each training batch: 5000
Times through each training batch: 7500
Times through each training batch: 10000
Times through each training batch: 15000
Times through each training batch: 20000
Times through each training batch: 30000
QN iterations per batch: 5
QN iterations per batch: 10
QN iterations per batch: 3
QN iterations per batch: 7
QN iterations per batch: 4
QN iterations per batch: 8
QN iterations per batch: 6
QN iterations per batch: 9
QN iterations per batch: 2
QN iterations per batch: 12
QN iterations per batch: 11
QN iterations per batch: 1
QN iterations per batch: 13
QN iterations per batch: 14
QN iterations per batch: 15
All pairs marked: 1634978653123
All pairs marked: 1634978654124
All pairs marked: 1634978655125
All pairs marked: 1634978656126
All pairs marked: 1634978657127
All pairs marked: 1634978658128
All pairs marked: 1634978659129
All pairs marked: 1634978660130
All pairs marked: 1634978661131
All pairs marked: 1634978662132
All pairs marked: 1634978663133
All pairs marked: 1634978664134
All pairs marked: 1634978665135
All pairs marked: 1634978666136
All pairs marked: 1634978667137
Iteration 1 batch 32
Iteration 2 batch 64
Iteration 3 batch 128
Iteration 4 batch 256
Iteration 5 batch 512
Iteration 6 batch 1024
Iteration 7 batch 2048
Iteration 8 batch 4096
Iteration 9 batch 8192
Iteration 10 batch 16384
Iteration 11 batch 32768
Iteration 12 batch 65536
Iteration 13 batch 131072
Iteration 14 batch 262144
Iteration 15 batch 524288
DocDate mapping file failed to match against 1001
DocDate mapping file failed to match against 2002
DocDate mapping file failed to match against 3003
DocDate mapping file failed to match against 4004
DocDate mapping file failed to match against 5005
DocDate mapping file failed to match against 6006
DocDate mapping file failed to match against 7007
DocDate mapping file failed to match against 8008
DocDate mapping file failed to match against 9009
DocDate mapping file failed to match against 1010
DocDate mapping file failed to match against 1111
DocDate mapping file failed to match against 1212
DocDate mapping file failed to match against 1313
DocDate mapping file failed to match against 1414
DocDate mapping file failed to match against 1515
Finished iteration 1 batch 32; total training time 120 ms
Finished iteration 2 batch 64; total training time 240 ms
Finished iteration 3 batch 128; total training time 480 ms
Finished iteration 4 batch 256; total training time 960 ms
Finished iteration 5 batch 512; total training time 1920 ms
Finished iteration 6 batch 1024; total training time 3840 ms
Finished iteration 7 batch 2048; total training time 7680 ms
Finished iteration 8 batch 4096; total training time 15360 ms
Finished iteration 9 batch 8192; total training time 30720 ms
Finished iteration 10 batch 16384; total training time 61440 ms
Finished iteration 11 batch 32768; total training time 122880 ms
Finished iteration 12 batch 65536; total training time 245760 ms
Finished iteration 13 batch 131072; total training time 491520 ms
Finished iteration 14 batch 262144; total training time 983040 ms
Finished iteration 15 batch 524288; total training time 1966080 ms
cannot create HybridCorefAnnotator!
cannot create HybridCorefAnnotator!
cannot create HybridCorefAnnotator!
cannot create HybridCorefAnnotator!
cannot create HybridCorefAnnotator!
cannot create HybridCorefAnnotator!
cannot create HybridCorefAnnotator!
cannot create HybridCorefAnnotator!
cannot create HybridCorefAnnotator!
cannot create HybridCorefAnnotator!
cannot create HybridCorefAnnotator!
cannot create HybridCorefAnnotator!
cannot create HybridCorefAnnotator!
cannot create HybridCorefAnnotator!
cannot create HybridCorefAnnotator!
Finished 10 total batches, running evaluation cycle
Finished 25 total batches, running evaluation cycle
Finished 5 total batches, running evaluation cycle
Finished 15 total batches, running evaluation cycle
Finished 20 total batches, running evaluation cycle
Finished 30 total batches, running evaluation cycle
Finished 8 total batches, running evaluation cycle
Finished 12 total batches, running evaluation cycle
Finished 18 total batches, running evaluation cycle
Finished 22 total batches, running evaluation cycle
Finished 6 total batches, running evaluation cycle
Finished 14 total batches, running evaluation cycle
Finished 16 total batches, running evaluation cycle
Finished 24 total batches, running evaluation cycle
Finished 28 total batches, running evaluation cycle
Best label f1 on dev set so far: 0.76
Best label f1 on dev set so far: 0.81
Best label f1 on dev set so far: 0.74
Best label f1 on dev set so far: 0.79
Best label f1 on dev set so far: 0.77
Best label f1 on dev set so far: 0.82
Best label f1 on dev set so far: 0.75
Best label f1 on dev set so far: 0.80
Best label f1 on dev set so far: 0.78
Best label f1 on dev set so far: 0.83
Best label f1 on dev set so far: 0.73
Best label f1 on dev set so far: 0.84
Best label f1 on dev set so far: 0.72
Best label f1 on dev set so far: 0.85
Best label f1 on dev set so far: 0.71
Max training time exceeded, exiting
Max training time exceeded, exiting
Max training time exceeded, exiting
Max training time exceeded, exiting
Max training time exceeded, exiting
Max training time exceeded, exiting
Max training time exceeded, exiting
Max training time exceeded, exiting
Max training time exceeded, exiting
Max training time exceeded, exiting
Max training time exceeded, exiting
Max training time exceeded, exiting
Max training time exceeded, exiting
Max training time exceeded, exiting
Max training time exceeded, exiting
Initialized: 1634986353000
Initialized: 1634986354001
Initialized: 1634986355002
Initialized: 1634986356003
Initialized: 1634986357004
Initialized: 1634986358005
Initialized: 1634986359006
Initialized: 1634986360007
Initialized: 1634986361008
Initialized: 1634986362009
Initialized: 1634986363010
Initialized: 1634986364011
Initialized: 1634986365012
Initialized: 1634986366013
Initialized: 1634986367014
batch cost: 0.56
batch cost: 1.23
batch cost: 0.87
batch cost: 1.45
batch cost: 0.32
batch cost: 0.99
batch cost: 1.67
batch cost: 0.74
batch cost: 1.12
batch cost: 0.41
batch cost: 1.38
batch cost: 0.63
batch cost: 1.54
batch cost: 0.29
batch cost: 0.92
basic spanish coref results
basic spanish coref results
basic spanish coref results
basic spanish coref results
basic spanish coref results
basic spanish coref results
basic spanish coref results
basic spanish coref results
basic spanish coref results
basic spanish coref results
basic spanish coref results
basic spanish coref results
basic spanish coref results
basic spanish coref results
basic spanish coref results
mapped: @John to: John Smith
mapped: #NYC to: New York City
mapped: $AAPL to: Apple Inc.
mapped: @Jane to: Jane Doe
mapped: #COVID19 to: coronavirus disease 2019
mapped: $TSLA to: Tesla Inc.
mapped: @Jack to: Jack Jones
mapped: #AI to: artificial intelligence
mapped: $AMZN to: Amazon.com Inc.
mapped: @Mary to: Mary Lee
mapped: #NBA to: National Basketball Association
mapped: $MSFT to: Microsoft Corporation
mapped: @Bob to: Bob Green
mapped: #ML to: machine learning
mapped: $GOOG to: Google LLC
Random seed set to 42
Random seed set to 123
Random seed set to 0
Random seed set to -1
Random seed set to 9999
Random seed set to 2021
Random seed set to 314159
Random seed set to 271828
Random seed set to 666
Random seed set to 7777
Random seed set to 101010
Random seed set to 98765
Random seed set to 2468
Random seed set to 13579
Random seed set to 11111
But I do have blocks for:
But I do have blocks for:
But I do have blocks for:
But I do have blocks for:
But I do have blocks for:
But I do have blocks for:
But I do have blocks for:
But I do have blocks for:
But I do have blocks for:
But I do have blocks for:
But I do have blocks for:
But I do have blocks for:
But I do have blocks for:
But I do have blocks for:
But I do have blocks for:
Finding lemmas ...
Finding lemmas ...
Finding lemmas ...
Finding lemmas ...
Finding lemmas ...
Finding lemmas ...
Finding lemmas ...
Finding lemmas ...
Finding lemmas ...
Finding lemmas ...
Finding lemmas ...
Finding lemmas ...
Finding lemmas ...
Finding lemmas ...
Finding lemmas ...
Starting minimizer test...
Starting minimizer test...
Starting minimizer test...
Starting minimizer test...
Starting minimizer test...
Starting minimizer test...
Starting minimizer test...
Starting minimizer test...
Starting minimizer test...
Starting minimizer test...
Starting minimizer test...
Starting minimizer test...
Starting minimizer test...
Starting minimizer test...
Starting minimizer test...
Random seed not set, using randomly chosen seed of 42
Random seed not set, using randomly chosen seed of 123456789
Random seed not set, using randomly chosen seed of 314159265
Random seed not set, using randomly chosen seed of 271828182
Random seed not set, using randomly chosen seed of 161803398
Random seed not set, using randomly chosen seed of 141421356
Random seed not set, using randomly chosen seed of 173205080
Random seed not set, using randomly chosen seed of 223606797
Random seed not set, using randomly chosen seed of 244948974
Random seed not set, using randomly chosen seed of 316227766
Random seed not set, using randomly chosen seed of 387298334
Random seed not set, using randomly chosen seed of 458257569
Random seed not set, using randomly chosen seed of 529150262
Random seed not set, using randomly chosen seed of 600000000
Random seed not set, using randomly chosen seed of 670820393
Minimized from 56 to 12
Minimized from 43 to 9
Minimized from 67 to 14
Minimized from 49 to 10
Minimized from 72 to 15
Minimized from 52 to 11
Minimized from 59 to 13
Minimized from 46 to 8
Minimized from 64 to 16
Minimized from 51 to 7
Minimized from 55 to 17
Minimized from 44 to 6
Minimized from 68 to 18
Minimized from 48 to 5
Minimized from 71 to 19
Adding NER Combiner annotation ...
Adding NER Combiner annotation ...
Adding NER Combiner annotation ...
Adding NER Combiner annotation ...
Adding NER Combiner annotation ...
Adding NER Combiner annotation ...
Adding NER Combiner annotation ...
Adding NER Combiner annotation ...
Adding NER Combiner annotation ...
Adding NER Combiner annotation ...
Adding NER Combiner annotation ...
Adding NER Combiner annotation ...
Adding NER Combiner annotation ...
Adding NER Combiner annotation ...
Adding NER Combiner annotation ...
Adding number annotation ...
Adding number annotation ...
Adding number annotation ...
Adding number annotation ...
Adding number annotation ...
Adding number annotation ...
Adding number annotation ...
Adding number annotation ...
Adding number annotation ...
Adding number annotation ...
Adding number annotation ...
Adding number annotation ...
Adding number annotation ...
Adding number annotation ...
Adding number annotation ...
Word vector file: /home/user/op.lexOptions.wordVectorFile
Word vector file: C:\Users\user\op.lexOptions.wordVectorFile
Word vector file: /opt/op.lexOptions.wordVectorFile
Word vector file: /usr/local/share/op.lexOptions.wordVectorFile
Word vector file: /tmp/op.lexOptions.wordVectorFile
Word vector file: /var/log/op.lexOptions.wordVectorFile
Word vector file: /mnt/op.lexOptions.wordVectorFile
Word vector file: /dev/op.lexOptions.wordVectorFile
Word vector file: /etc/op.lexOptions.wordVectorFile
Word vector file: /media/op.lexOptions.wordVectorFile
Word vector file: /run/op.lexOptions.wordVectorFile
Word vector file: /srv/op.lexOptions.wordVectorFile
Word vector file: /sys/op.lexOptions.wordVectorFile
Word vector file: /proc/op.lexOptions.wordVectorFile
Word vector file: /root/op.lexOptions.wordVectorFile
Equal? true
Equal? false
Equal? true
Equal? false
Equal? false
Equal? true
Equal? false
Equal? true
Equal? false
Equal? true
Equal? false
Equal? true
Equal? false
Equal? true
Equal? false
Done creating random graph
Done creating random graph
Done creating random graph
Done creating random graph
Done creating random graph
Done creating random graph
Done creating random graph
Done creating random graph
Done creating random graph
Done creating random graph
Done creating random graph
Done creating random graph
Done creating random graph
Done creating random graph
Done creating random graph
Done quasi-determinizing
Done quasi-determinizing
Done quasi-determinizing
Done quasi-determinizing
Done quasi-determinizing
Done quasi-determinizing
Done quasi-determinizing
Done quasi-determinizing
Done quasi-determinizing
Done quasi-determinizing
Done quasi-determinizing
Done quasi-determinizing
Done quasi-determinizing
Done quasi-determinizing
Done quasi-determinizing
No outbound arcs from node.
No outbound arcs from node.
No outbound arcs from node.
No outbound arcs from node.
No outbound arcs from node.
No outbound arcs from node.
No outbound arcs from node.
No outbound arcs from node.
No outbound arcs from node.
No outbound arcs from node.
No outbound arcs from node.
No outbound arcs from node.
No outbound arcs from node.
No outbound arcs from node.
No outbound arcs from node.
Size of word vectors: 300
Size of word vectors: 100
Size of word vectors: 50
Size of word vectors: 200
Size of word vectors: 150
Size of word vectors: 250
Size of word vectors: 400
Size of word vectors: 500
Size of word vectors: 350
Size of word vectors: 450
Size of word vectors: 600
Size of word vectors: 700
Size of word vectors: 800
Size of word vectors: 750
Size of word vectors: 650
Number of hypothesis trees to train against: 10
Number of hypothesis trees to train against: 25
Number of hypothesis trees to train against: 5
Number of hypothesis trees to train against: 15
Number of hypothesis trees to train against: 20
Number of hypothesis trees to train against: 8
Number of hypothesis trees to train against: 12
Number of hypothesis trees to train against: 18
Number of hypothesis trees to train against: 7
Number of hypothesis trees to train against: 14
Number of hypothesis trees to train against: 9
Number of hypothesis trees to train against: 16
Number of hypothesis trees to train against: 6
Number of hypothesis trees to train against: 13
Number of hypothesis trees to train against: 11
Loading Parser Model [C:\Users\Bing\Documents\parser1.pkl] ...
Loading Parser Model [/home/bing/parser2.h5] ...
Loading Parser Model [https://bing.com/models/parser3.pt] ...
Loading Parser Model [D:\Bing\Projects\parser4.bin] ...
Loading Parser Model [E:\Bing\Data\parser5.json] ...
Loading Parser Model [C:\Bing\Models\parser6.pickle] ...
Loading Parser Model [/usr/local/bin/parser7.hdf5] ...
Loading Parser Model [https://bing.com/data/parser8.torch] ...
Loading Parser Model [F:\Bing\Documents\parser9.pkl] ...
Loading Parser Model [/home/bing/models/parser10.h5] ...
Loading Parser Model [https://bing.com/projects/parser11.pt] ...
Loading Parser Model [G:\Bing\Projects\parser12.bin] ...
Loading Parser Model [H:\Bing\Data\parser13.json] ...
Loading Parser Model [C:\Bing\Data\parser14.pickle] ...
Loading Parser Model [/usr/local/bin/parser15.hdf5] ...
Number of trees in one batch: 32
Number of trees in one batch: 64
Number of trees in one batch: 16
Number of trees in one batch: 48
Number of trees in one batch: 24
Number of trees in one batch: 40
Number of trees in one batch: 56
Number of trees in one batch: 8
Number of trees in one batch: 36
Number of trees in one batch: 12
Number of trees in one batch: 28
Number of trees in one batch: 20
Number of trees in one batch: 44
Number of trees in one batch: 4
Number of trees in one batch: 52
Splitting into 3 lists with weights [0.2, 0.5, 0.3]
Splitting into 5 lists with weights [0.1, 0.1, 0.4, 0.2, 0.2]
Splitting into 2 lists with weights [0.6, 0.4]
Splitting into 4 lists with weights [0.25, 0.25, 0.25, 0.25]
Splitting into 6 lists with weights [0.05, 0.15, 0.2, 0.25, 0.15, 0.2]
Splitting into 7 lists with weights [0.1, 0.05, 0.15, 0.3, 0.1, 0.2, 0.1]
Splitting into 8 lists with weights [0.05, 0.05, 0.1, 0.15, 0.2, 0.15, 0.1, 0.2]
Splitting into 9 lists with weights [0.05, 0.05, 0.05, 0.1, 0.15, 0.2, 0.15, 0.1, 0.15]
Splitting into 10 lists with weights [0.05, 0.05, 0.05, 0.05, 0.1, 0.15, 0.15, 0.15, 0.1, 0.1]
Splitting into 11 lists with weights [0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.15, 0.15, 0.1 , .10]
Loading event extraction model from EventBERT ...
Loading event extraction model from OpenIE ...
Loading event extraction model from ACE2005 ...
Loading event extraction model from DyGIE++ ...
Loading event extraction model from ELMo ...
Loading event extraction model from BERT ...
Loading event extraction model from GPT-3 ...
Loading event extraction model from T5 ...
Loading event extraction model from RoBERTa ...
Loading event extraction model from SpanBERT ...
Loading event extraction model from ERNIE ...
Loading event extraction model from XLNet ...
Loading event extraction model from Transformer-XL ...
Loading event extraction model from ALBERT ...
Loading event extraction model from BART ...
annotation= 0.45
annotation= 0.67
annotation= 0.32
annotation= 0.54
annotation= 0.76
annotation= 0.29
annotation= 0.61
annotation= 0.43
annotation= 0.58
annotation= 0.72
annotation= 0.34
annotation= 0.69
annotation= 0.51
annotation= 0.64
annotation= 0.37
Unary score 0.56
Unary score 0.91
Unary score 0.67
Unary score 0.43
Unary score 0.78
Unary score 0.62
Unary score 0.85
Unary score 0.49
Unary score 0.72
Unary score 0.94
Unary score 0.58
Unary score 0.81
Unary score 0.69
Unary score 0.46
Unary score 0.76
added feature with key fK1 has support 5
added feature with key fK2 has support 3
added feature with key fK3 has support 7
added feature with key fK4 has support 4
added feature with key fK5 has support 6
added feature with key fK6 has support 2
added feature with key fK7 has support 8
added feature with key fK8 has support 9
added feature with key fK9 has support 10
added feature with key fK10 has support 1
added feature with key fK11 has support 11
added feature with key fK12 has support 12
added feature with key fK13 has support 13
added feature with key fK14 has support 14
added feature with key fK15 has support 15
directed path nodes btw 1 and 3 is [2, 4, 5, 6]
directed path nodes btw 1 and 3 is [2, 7, 8, 9]
directed path nodes btw 1 and 3 is [4, 5, 6, 7]
directed path nodes btw 1 and 3 is [4, 8, 9, 10]
directed path nodes btw 1 and 3 is [5, 6, 7, 8]
directed path nodes btw 1 and 3 is [5, 9, 10, 11]
directed path nodes btw 1 and 3 is [6, 7, 8, 9]
directed path nodes btw 1 and 3 is [6, 10, 11, 12]
directed path nodes btw 1 and 3 is [7, 8, 9, 10]
directed path nodes btw 1 and 3 is [7, 11, 12, 13]
directed path nodes btw 1 and 3 is [8, 9, 10, 11]
directed path nodes btw 1 and 3 is [8, 12, 13, 14]
directed path nodes btw 1 and 3 is [9,10 ,11 ,12]
error running HeidelTime on this doc: doc_001
error running HeidelTime on this doc: news_2023_10_19
error running HeidelTime on this doc: report_123
error running HeidelTime on this doc: article_456
error running HeidelTime on this doc: blog_789
error running HeidelTime on this doc: tweet_1011
error running HeidelTime on this doc: email_1213
error running HeidelTime on this doc: review_1415
error running HeidelTime on this doc: transcript_1617
error running HeidelTime on this doc: essay_1819
error running HeidelTime on this doc: story_2021
error running HeidelTime on this doc: poem_2223
error running HeidelTime on this doc: speech_2425
error running HeidelTime on this doc: letter_2627
error running HeidelTime on this doc: diary_2829
No block found for: o
No block found for: ooo
No block found for: oooooo
No block found for: oooooooo
No block found for: oooooooooo
No block found for: oooooooooooo
No block found for: oooooooooooooo
No block found for: oooooooooooooooo
No block found for: oooooooooooooooooo
No block found for: oooooooooooooooooooo
No block found for: oooooooooooooooooooooo
No block found for: oooooooooooooooooooooooo
No block found for: oooooooooooooooooooooooooo
No block found for: oooooooooooooooooooooooooooo
No block found for: oooooooooooooooooooooooooooooo
MODE : what mode for dependencies. basic, collapsed, or ccprocessed. To get 'noncollapsed', use basic with extras
basic : what mode for dependencies. basic, collapsed, or ccprocessed. To get 'noncollapsed', use basic with extras
collapsed : what mode for dependencies. basic, collapsed, or ccprocessed. To get 'noncollapsed', use basic with extras
ccprocessed : what mode for dependencies. basic, collapsed, or ccprocessed. To get 'noncollapsed', use basic with extras
noncollapsed : what mode for dependencies. basic, collapsed, or ccprocessed. To get 'noncollapsed', use basic with extras
default : what mode for dependencies. basic, collapsed, or ccprocessed. To get 'noncollapsed', use basic with extras
enhanced : what mode for dependencies. basic, collapsed, or ccprocessed. To get 'noncollapsed', use basic with extras
custom : what mode for dependencies. basic, collapsed, or ccprocessed. To get 'noncollapsed', use basic with extras
fast : what mode for dependencies. basic, collapsed, or ccprocessed. To get 'noncollapsed', use basic with extras
accurate : what mode for dependencies. basic, collapsed, or ccprocessed. To get 'noncollapsed', use basic with extras
hybrid : what mode for dependencies. basic, collapsed, or ccprocessed. To get 'noncollapsed', use basic with extras
simple : what mode for dependencies. basic, collapsed, or ccprocessed. To get 'noncollapsed', use basic with extras
complex : what mode for dependencies. basic, collapsed, or ccprocessed. To get 'noncollapsed', use basic with extras
minimal : what mode for dependencies. basic, collapsed, or ccprocessed. To get 'noncollapsed', use basic with extras
maximal : what mode for dependencies. basic, collapsed, or ccprocessed. To get 'noncollapsed', use basic with extras
optimal : what mode for dependencies. basic, collapsed, or ccprocessed. To get 'noncollapsed', use basic with extras
0 0.0
1 0.5
2 0.25
3 0.75
4 0.2
5 0.8
6 0.16666666666666666
7 0.8571428571428571
8 0.125
9 0.8888888888888888
10 0.1
11 0.9090909090909091
12 0.08333333333333333
13 0.9230769230769231
14 0.07142857142857142
True Positive: dog
True Positive: cat
True Positive: car
True Positive: person
True Positive: flower
True Positive: bird
True Positive: bike
True Positive: book
True Positive: chair
True Positive: apple
True Positive: tree
True Positive: house
True Positive: plane
True Positive: boat
True Positive: hat
Dropped relation mention due to failed argument mapping: rm(John, president)
Dropped relation mention due to failed argument mapping: rm(London, capital)
Dropped relation mention due to failed argument mapping: rm(apple, fruit)
Dropped relation mention due to failed argument mapping: rm(China, country)
Dropped relation mention due to failed argument mapping: rm(Mona Lisa, painting)
Dropped relation mention due to failed argument mapping: rm(Earth, planet)
Dropped relation mention due to failed argument mapping: rm(Pi, number)
Dropped relation mention due to failed argument mapping: rm(Bing, search engine)
Dropped relation mention due to failed argument mapping: rm(COVID-19, virus)
Dropped relation mention due to failed argument mapping: rm(Shakespeare, writer)
Dropped relation mention due to failed argument mapping: rm(Nile, river)
Dropped relation mention due to failed argument mapping: rm(Tesla, car)
Dropped relation mention due to failed argument mapping: rm(Einstein, physicist)
Dropped relation mention due to failed argument mapping: rm(sunflower, plant)
Dropped relation mention due to failed argument mapping: rm(soccer, sport)
Using backends: [web, db, cache]
Using backends: [api, storage, queue]
Using backends: [auth, email, search]
Using backends: [web, api, auth]
Using backends: [db, storage, email]
Using backends: [cache, queue, search]
Using backends: [web, storage, search]
Using backends: [api, db, cache]
Using backends: [auth, web, queue]
Using backends: [email, api, search]
Using backends: [web, email, cache]
Using backends: [api, auth, storage]
Using backends: [db, queue, search]
Using backends: [cache, web, email]
Using backends: [search, db, auth]
-- 0 states.
-- 1 states.
-- 2 states.
-- 3 states.
-- 4 states.
-- 5 states.
-- 6 states.
-- 7 states.
-- 8 states.
-- 9 states.
-- 10 states.
-- 11 states.
-- 12 states.
-- 13 states.
-- 14 states.
scoref-trainOn epoch: 1 / 10 , document: 50 / 100
scoref-trainOn epoch: 2 / 10 , document: 25 / 100
scoref-trainOn epoch: 3 / 10 , document: 75 / 100
scoref-trainOn epoch: 4 / 10 , document: 100 / 100
scoref-trainOn epoch: 5 / 10 , document: 10 / 100
scoref-trainOn epoch: 6 / 10 , document: 40 / 100
scoref-trainOn epoch: 7 / 10 , document: 90 / 100
scoref-trainOn epoch: 8 / 10 , document: 20 / 100
scoref-trainOn epoch: 9 / 10 , document: 60 / 100
scoref-trainOn epoch: 10 / 10 , document: 80 / 100
scoref-trainOn epoch: 1 / 5 , document: 20 / 50
scoref-trainOn epoch: 2 / 5 , document: 40 / 50
scoref-trainOn epoch: 3 / 5 , document: 10 / 50
scoref-trainOn epoch: 4 / 5 , document: 30 / 50
lambda 1 too big: 0.95
lambda 2 too big: 0.87
lambda 3 too big: 0.92
lambda 4 too big: 0.89
lambda 5 too big: 0.91
lambda 6 too big: 0.94
lambda 7 too big: 0.88
lambda 8 too big: 0.93
lambda 9 too big: 0.90
lambda 10 too big: 0.96
lambda 11 too big: 0.86
lambda 12 too big: 0.97
lambda 13 too big: 0.85
lambda 14 too big: 0.98
lambda 15 too big: 0.84
load IO Exception: java.io.FileNotFoundException: File not found
load IO Exception: java.io.IOException: Stream closed
load IO Exception: java.net.SocketTimeoutException: Read timed out
load IO Exception: java.lang.NullPointerException: Attempt to invoke virtual method on a null object reference
load IO Exception: java.io.EOFException: End of input at line 1 column 1 path $
load IO Exception: java.net.MalformedURLException: no protocol
load IO Exception: java.io.InterruptedIOException: thread interrupted
load IO Exception: java.io.SyncFailedException: sync failed
load IO Exception: java.net.UnknownHostException: Unable to resolve host
load IO Exception: java.io.UTFDataFormatException: malformed input around byte 10
load IO Exception: java.io.InvalidClassException: local class incompatible
load IO Exception: java.net.ConnectException: Connection refused
load IO Exception: java.io.NotSerializableException: Object not serializable
load IO Exception: java.io.CharConversionException: Invalid UTF-8 sequence
load IO Exception: java.io.OptionalDataException
Number of raw states: 0
Number of raw states: 1
Number of raw states: 2
Number of raw states: 3
Number of raw states: 4
Number of raw states: 5
Number of raw states: 6
Number of raw states: 7
Number of raw states: 8
Number of raw states: 9
Number of raw states: 10
Number of raw states: 11
Number of raw states: 12
Number of raw states: 13
Number of raw states: 14
sentenceEndingTags= [., !, ?]
sentenceEndingTags= [., ?, ;]
sentenceEndingTags= [., !, ...]
sentenceEndingTags= [., ?, :]
sentenceEndingTags= [., !, -]
sentenceEndingTags= [., ?, ']
sentenceEndingTags= [., !, "]
sentenceEndingTags= [., ?, *]
sentenceEndingTags= [., !, /]
sentenceEndingTags= [., ?, +]
sentenceEndingTags= [., !, =]
sentenceEndingTags= [., ?, %]
sentenceEndingTags= [., !, &]
sentenceEndingTags= [., ?, #]
sentenceEndingTags= [., !, @]
directed path edges btw 1 and 3 is [2, 4, 5, 3]
directed path edges btw 1 and 3 is [2, 6, 7, 3]
directed path edges btw 1 and 3 is [2, 8, 9, 3]
directed path edges btw 1 and 3 is [2, 10, 11, 3]
directed path edges btw 1 and 3 is [2, 12, 13, 3]
directed path edges btw 1 and 3 is [2, 14, 15, 3]
directed path edges btw 1 and 3 is [2, 16, 17, 3]
directed path edges btw 1 and 3 is [2, 18, 19, 3]
directed path edges btw 1 and 3 is [2, 20, 21, 3]
directed path edges btw 1 and 3 is [2, 22, 23, 3]
directed path edges btw 1 and 3 is [2, 24, 25, 3]
directed path edges btw 1 and 3 is [2, 26, 27, 3]
directed path edges btw 1 and 3 is [2, 28, 29, 3]
directed path edges btw 1 and 3 is [2,30 ,31 ,3 ]
to tmpT
to tmpT + 1
to tmpT * 2
to tmpT / 2
to tmpT - 1
to tmpT ^ 2
to tmpT % 2
to tmpT & 1
to tmpT | 1
to tmpT << 1
to tmpT >> 1
to tmpT ~ 1
to tmpT == 1
to tmpT != 1
to tmpT < 1
Unknown option: -v
Unknown option: --help
Unknown option: /f
Unknown option: -debug
Unknown option: --version
Unknown option: /c
Unknown option: -h
Unknown option: --force
Unknown option: /r
Unknown option: -quiet
Unknown option: --verbose
Unknown option: /s
Unknown option: -log
Unknown option: --config
Unknown option: /d
weights have dimension 3
weights have dimension 5
weights have dimension 2
weights have dimension 4
weights have dimension 6
weights have dimension 7
weights have dimension 8
weights have dimension 9
weights have dimension 10
weights have dimension 11
weights have dimension 12
weights have dimension 13
weights have dimension 14
weights have dimension 15
weights have dimension 16
xValues is null: fK[0] is empty
xValues is null: fK[1] has invalid format
xValues is null: fK[2] is out of range
xValues is null: fK[3] is negative
xValues is null: fK[4] is not a number
xValues is null: fK[5] is missing
xValues is null: fK[6] has wrong type
xValues is null: fK[7] is too large
xValues is null: fK[8] is zero
xValues is null: fK[9] has duplicate values
xValues is null: fK[10] has inconsistent units
xValues is null: fK[11] has incorrect decimal places
xValues is null: fK[12] has trailing spaces
xValues is null: fK[13] has leading zeros
xValues is null: fK[14] has special characters
Loading clause splitter from serializedModel ... done (0.12s)
Loading clause splitter from serializedModel ... done (0.09s)
Loading clause splitter from serializedModel ... done (0.15s)
Loading clause splitter from serializedModel ... done (0.11s)
Loading clause splitter from serializedModel ... done (0.10s)
Loading clause splitter from serializedModel ... done (0.13s)
Loading clause splitter from serializedModel ... done (0.08s)
Loading clause splitter from serializedModel ... done (0.14s)
Loading clause splitter from serializedModel ... done (0.07s)
Loading clause splitter from serializedModel ... done (0.16s)
Loading clause splitter from serializedModel ... done (0.12s)
Loading clause splitter from serializedModel ... done (0.10s)
Loading clause splitter from serializedModel ... done (0.09s)
Loading clause splitter from serializedModel ... done (0.13s)
10 examples completed
3 examples completed
7 examples completed
12 examples completed
5 examples completed
9 examples completed
4 examples completed
8 examples completed
11 examples completed
6 examples completed
2 examples completed
14 examples completed
15 examples completed
13 examples completed
CC speaker conversion: 0.65
CC speaker conversion: 0.91
CC speaker conversion: 0.78
CC speaker conversion: 0.54
CC speaker conversion: 0.87
CC speaker conversion: 0.72
CC speaker conversion: 0.96
CC speaker conversion: 0.59
CC speaker conversion: 0.83
CC speaker conversion: 0.69
CC speaker conversion: 0.94
CC speaker conversion: 0.62
CC speaker conversion: 0.76
CC speaker conversion: 0.51
CC speaker conversion: 0.89
tagBins: 0
tagBins: 1
tagBins: 2
tagBins: 3
tagBins: 4
tagBins: 5
tagBins: 6
tagBins: 7
tagBins: 8
tagBins: 9
tagBins: 10
tagBins: 11
tagBins: 12
tagBins: 13
tagBins: 14
mergeDocuments: Using classifier # 0 for PERSON
mergeDocuments: Using classifier # 1 for LOCATION
mergeDocuments: Using classifier # 2 for ORGANIZATION
mergeDocuments: Using classifier # 3 for DATE
mergeDocuments: Using classifier # 4 for MONEY
mergeDocuments: Using classifier # 5 for PERCENT
mergeDocuments: Using classifier # 6 for TIME
mergeDocuments: Using classifier # 7 for QUANTITY
mergeDocuments: Using classifier # 8 for ORDINAL
mergeDocuments: Using classifier # 9 for CARDINAL
mergeDocuments: Using classifier # 10 for EVENT
mergeDocuments: Using classifier # 11 for PRODUCT
mergeDocuments: Using classifier # 12 for LANGUAGE
mergeDocuments: Using classifier # 13 for WORK_OF_ART
mergeDocuments: Using classifier # 14 for LAW
allChildren is: [null, null, null, null]
allChildren is: [Node@4f3f5b24, Node@6f75e721, Node@3cd1a2f1, Node@7a81197d]
allChildren is: [null, Node@5c647e05, null, Node@33909752]
allChildren is: [Node@2b193f2d, null, Node@4b1210ee, null]
allChildren is: [null, null, Node@6d311334, Node@682a0b20]
allChildren is: [Node@7d4991ad, Node@28d93b30, null, null]
allChildren is: [Node@4e25154f, Node@4ee285c6, Node@66d33a88, Node@3b9a45b3]
allChildren is: [null, Node@2f92e0f4, Node@5f184fc6, null]
allChildren is: [Node@1b6d3586, null, null, Node@4554617c]
allChildren is: [Node@74a14482, Node@1540e19d, Node@677327b6, null]
allChildren is: [null, null, null, Node@14ae5a5]
allChildren is: [Node@7cdbc5d9, null, null, null]
allChildren is: [null, Node@6e0be858, null, null]
allChildren is: [null, null, Node@61bbe9ba, null]
allChildren is: [Node@610455d6, Node@511d50c0, Node@60e53b93, Node@5e2de80c]
Mentions in sentence # 1 :
Mentions in sentence # 2 :
Mentions in sentence # 3 :
Mentions in sentence # 4 :
Mentions in sentence # 5 :
Mentions in sentence # 6 :
Mentions in sentence # 7 :
Mentions in sentence # 8 :
Mentions in sentence # 9 :
Mentions in sentence # 10 :
Mentions in sentence # 11 :
Mentions in sentence # 12 :
Mentions in sentence # 13 :
Mentions in sentence # 14 :
Mentions in sentence # 15 :
Ignoring | yytext() |
Ignoring | yytext()+1 |
Ignoring | yytext()-1 |
Ignoring | yytext()*2 |
Ignoring | yytext()/2 |
Ignoring | yytext()^2 |
Ignoring | yytext().length() |
Ignoring | yytext().charAt(0) |
Ignoring | yytext().substring(0,1) |
Ignoring | yytext().reverse() |
Ignoring | yytext().toUpperCase() |
Ignoring | yytext().toLowerCase() |
Ignoring | yytext().trim() |
Ignoring | yytext().replace('a','b') |
Ignoring | yytext().concat("abc") |
Finished iteration 1 batch 32 ; total training time 120 ms
Finished iteration 2 batch 64 ; total training time 240 ms
Finished iteration 3 batch 128 ; total training time 480 ms
Finished iteration 4 batch 256 ; total training time 960 ms
Finished iteration 5 batch 512 ; total training time 1920 ms
Finished iteration 6 batch 1024 ; total training time 3840 ms
Finished iteration 7 batch 2048 ; total training time 7680 ms
Finished iteration 8 batch 4096 ; total training time 15360 ms
Finished iteration 9 batch 8192 ; total training time 30720 ms
Finished iteration 10 batch 16384 ; total training time 61440 ms
Finished iteration 11 batch 32768 ; total training time 122880 ms
Finished iteration 12 batch 65536 ; total training time 245760 ms
Finished iteration 13 batch 131072 ; total training time 491520 ms
Finished iteration 14 batch 262144 ; total training time 983040 ms
Finished iteration 15 batch 524288 ; total training time 1966080 ms
model precision at recall 0.5 0.76
model precision at recall 0.6 0.71
model precision at recall 0.7 0.65
model precision at recall 0.8 0.58
model precision at recall 0.9 0.51
model precision at recall 1.0 0.45
model precision at recall 0.4 0.81
model precision at recall 0.3 0.85
model precision at recall 0.2 0.89
model precision at recall 0.1 0.92
model precision at recall 0.55 0.74
model precision at recall 0.65 0.68
model precision at recall 0.75 0.62
model precision at recall 0.85 0.55
model precision at recall 0.95 0.48
Minimized from 10 to 5
Minimized from 12 to 6
Minimized from 8 to 4
Minimized from 15 to 7
Minimized from 9 to 3
Minimized from 11 to 5
Minimized from 13 to 6
Minimized from 7 to 3
Minimized from 14 to 7
Minimized from 16 to 8
Minimized from 18 to 9
Minimized from 20 to 10
Minimized from 17 to 8
Minimized from 19 to 9
Minimized from 21 to 10
DEBUG For label PERSON : Number of gold entities is 50 , Precision is 90.00 , Recall is 80.00 , F1 is 84.85
INFO For label LOCATION : Number of gold entities is 40 , Precision is 85.71 , Recall is 87.50 , F1 is 86.60
TRACE For label ORGANIZATION : Number of gold entities is 30 , Precision is 92.31 , Recall is 76.67 , F1 is 83.72
ERROR For label DATE : Number of gold entities is 20 , Precision is 66.67 , Recall is 70.00 , F1 is 68.29
WARN For label TIME : Number of gold entities is 10 , Precision is 50.00 , Recall is 40.00 , F1 is 44.44
DEBUG For label MONEY : Number of gold entities is 15 , Precision is 75.00 , Recall is 66.67 , F1 is 70.59
INFO For label PERCENT : Number of gold entities is 12 , Precision is 83.33 , Recall is 75.00 , F1 is 78.95
TRACE For label QUANTITY : Number of gold entities is 8 , Precision is 100.00 , Recall is 62.50 , F1 is 76.92
ERROR For label ORDINAL : Number of gold entities is 6 , Precision is 60.00 , Recall is 50.00 , F1 is 54.55
WARN For label CARDINAL : Number of gold entities is 18 , Precision is 72.22 , Recall is 77.78 , F1 is 74.93
DEBUG For label EVENT : Number of gold entities is 4 , Precision is 50.00 , Recall is 25.00 , F1 is 33.33
INFO For label ARTIFACT : Number of gold entities is 5 , Precision is 80.00 , Recall is 80.00 , F1 is 80.00
TRACE For label LANGUAGE : Number of gold entities is 3 , Precision is 66.67 , Recall is 33.33 , F1 is 44.44
ERROR For label LAW : Number of gold entities is 2 , Precision is 0.00 , Recall is 0.00 , F1 is NaN
ERROR For label LAW : Number of gold entities is 2 , Precision is 0.00 , Recall is 0.00 , F1 is NaN
xSize is 10
xSize is 5.6
xSize is 0
xSize is -3
xSize is 100
xSize is 12.34
xSize is 8.9
xSize is 1
xSize is 50
xSize is 7.5
xSize is 25.4
xSize is 3.14
xSize is -10
xSize is 9.8
xSize is 6.7
CleanXML: ignoring tokens because matchDepth= 0 ; xmlTagMatcher= null
CleanXML: ignoring tokens because matchDepth= 1 ; xmlTagMatcher= <name>
CleanXML: ignoring tokens because matchDepth= 2 ; xmlTagMatcher= <title>
CleanXML: ignoring tokens because matchDepth= 3 ; xmlTagMatcher= <author>
CleanXML: ignoring tokens because matchDepth= 4 ; xmlTagMatcher= <publisher>
CleanXML: ignoring tokens because matchDepth= 5 ; xmlTagMatcher= <year>
CleanXML: ignoring tokens because matchDepth= 6 ; xmlTagMatcher= <isbn>
CleanXML: ignoring tokens because matchDepth= 7 ; xmlTagMatcher= <price>
CleanXML: ignoring tokens because matchDepth= 8 ; xmlTagMatcher= <category>
CleanXML: ignoring tokens because matchDepth= 9 ; xmlTagMatcher= <description>
CleanXML: ignoring tokens because matchDepth= 10 ; xmlTagMatcher= </book>
CleanXML: ignoring tokens because matchDepth= -1 ; xmlTagMatcher= <?xml version="1.0"?>
CleanXML: ignoring tokens because matchDepth= -2 ; xmlTagMatcher= <!DOCTYPE booklist SYSTEM "booklist.dtd">
CleanXML: ignoring tokens because matchDepth= -3 ; xmlTagMatcher= <booklist>
CleanXML: ignoring tokens because matchDepth= -4 ; xmlTagMatcher= <book id="b001">
normalizedNumberString: normalizing 3.14159
normalizedNumberString: normalizing -0.5
normalizedNumberString: normalizing 1.23e4
normalizedNumberString: normalizing 0
normalizedNumberString: normalizing 6.022e23
normalizedNumberString: normalizing -1.618
normalizedNumberString: normalizing 2/3
normalizedNumberString: normalizing 9.81
normalizedNumberString: normalizing 1.414
normalizedNumberString: normalizing -3.14e-2
normalizedNumberString: normalizing 0.333
normalizedNumberString: normalizing 4.6692
normalizedNumberString: normalizing -7.77e-3
normalizedNumberString: normalizing 5/4
normalizedNumberString: normalizing 2.718
ySize is 10
ySize is 15.5
ySize is 0
ySize is -3
ySize is 8.9
ySize is 12.3
ySize is 20
ySize is 5.4
ySize is 9.8
ySize is 7.2
ySize is 18.6
ySize is 13.7
ySize is 6.1
ySize is 11.4
ySize is 14.9
Failed to parse gold.yieldHasWord()
Failed to parse gold.yieldHasWord(0.5)
Failed to parse gold.yieldHasWord("gold")
Failed to parse gold.yieldHasWord(true)
Failed to parse gold.yieldHasWord(null)
Failed to parse gold.yieldHasWord([])
Failed to parse gold.yieldHasWord([1,2,3])
Failed to parse gold.yieldHasWord({})
Failed to parse gold.yieldHasWord({key: value})
Failed to parse gold.yieldHasWord(1+2)
Failed to parse gold.yieldHasWord(gold)
Failed to parse gold.yieldHasWord(yield)
Failed to parse gold.yieldHasWord(word)
Failed to parse gold.yieldHasWord("yield")
Failed to parse gold.yieldHasWord("word")
There are 12 subtrees in the set of trees
There are 7 subtrees in the set of trees
There are 9 subtrees in the set of trees
There are 4 subtrees in the set of trees
There are 10 subtrees in the set of trees
There are 6 subtrees in the set of trees
There are 8 subtrees in the set of trees
There are 5 subtrees in the set of trees
There are 11 subtrees in the set of trees
There are 3 subtrees in the set of trees
There are 13 subtrees in the set of trees
There are 2 subtrees in the set of trees
There are 14 subtrees in the set of trees
There are 1 subtrees in the set of trees
There are 15 subtrees in the set of trees
There are 5 categories to compact.
There are 12 categories to compact.
There are 0 categories to compact.
There are 8 categories to compact.
There are 3 categories to compact.
There are 10 categories to compact.
There are 6 categories to compact.
There are 9 categories to compact.
There are 4 categories to compact.
There are 7 categories to compact.
There are 1 categories to compact.
There are 2 categories to compact.
There are 11 categories to compact.
There are 13 categories to compact.
There are 14 categories to compact.
Done filtering rules; 12 binary matrices, 8 unary matrices, 100 word vectors
Done filtering rules; 15 binary matrices, 10 unary matrices, 120 word vectors
Done filtering rules; 9 binary matrices, 6 unary matrices, 80 word vectors
Done filtering rules; 10 binary matrices, 7 unary matrices, 90 word vectors
Done filtering rules; 11 binary matrices, 9 unary matrices, 110 word vectors
Done filtering rules; 13 binary matrices, 11 unary matrices, 130 word vectors
Done filtering rules; 14 binary matrices, 12 unary matrices, 140 word vectors
Done filtering rules; 16 binary matrices, 13 unary matrices, 150 word vectors
Done filtering rules; 17 binary matrices, 14 unary matrices, 160 word vectors
Done filtering rules; 18 binary matrices, 15 unary matrices, 170 word vectors
Done filtering rules; 19 binary matrices, 16 unary matrices, 180 word vectors
Done filtering rules; 20 binary matrices, 17 unary matrices, 190 word vectors
Done filtering rules; 21 binary matrices, 18 unary matrices, 200 word vectors
Done filtering rules; 22 binary matrices, 19 unary matrices, 210 word vectors
Done filtering rules; 23 binary matrices, 20 unary matrices, 220 word vectors
real 1/2 + 1/2 = -0.6931471805599453
real 1/2 + 1/2 = -0.6931471805599454
real 1/2 + 1/2 = -0.6931471805599455
real 1/2 + 1/2 = -0.6931471805599456
real 1/2 + 1/2 = -0.6931471805599457
real 1/2 + 1/2 = -0.6931471805599458
real 1/2 + 1/2 = -0.6931471805599459
real 1/2 + 1/2 = -0.693147180559946
real 1/2 + 1/2 = -0.6931471805599461
real 1/2 + 1/2 = -0.6931471805599462
real 1/2 + 1/2 = -0.6931471805599463
real 1/2 + 1/2 = -0.6931471805599464
real 1/2 + 1/2 = -0.6931471805599465
real 1/2 + 1/2 = -0.6931471805599466
real 1/2 + 1/2 = -0.6931471805599467
Answer is: [1, 2, 3]
Answer is: [true, false, true]
Answer is: [null, "hello", 42]
Answer is: [[1, 2], [3, 4], [5, 6]]
Answer is: ["apple", "banana", "cherry"]
Answer is: [0.5, 0.75, 0.25]
Answer is: ["a", "b", "c", "d"]
Answer is: [null, null, null]
Answer is: [1, 4, 9, 16]
Answer is: [false, false, false]
Answer is: ["cat", "dog", "fish"]
Answer is: [[], [], []]
Answer is: [10, 20, 30, 40]
Answer is: [true, true, true]
Answer is: ["x", "y", "z"]
Training on 1000 trees in 50 batches
Training on 500 trees in 10 batches
Training on 2000 trees in 100 batches
Training on 750 trees in 25 batches
Training on 1500 trees in 75 batches
Training on 250 trees in 5 batches
Training on 1250 trees in 50 batches
Training on 3000 trees in 150 batches
Training on 600 trees in 20 batches
Training on 1800 trees in 90 batches
Training on 400 trees in 8 batches
Training on 900 trees in 30 batches
Training on 2100 trees in 105 batches
Training on 800 trees in 40 batches
Training on 1200 trees in 60 batches
Found 0 coreference links:
Found 2 coreference links:
Found 3 coreference links:
Found 4 coreference links:
Found 5 coreference links:
Found 6 coreference links:
Found 7 coreference links:
Found 8 coreference links:
Found 9 coreference links:
Found 10 coreference links:
Found 11 coreference links:
Found 12 coreference links:
Found 13 coreference links:
Found 14 coreference links:
total positive 12 total negative 3 total 15
total positive 8 total negative 5 total 13
total positive 10 total negative 7 total 17
total positive 9 total negative 4 total 13
total positive 11 total negative 6 total 17
total positive 7 total negative 8 total 15
total positive 6 total negative 9 total 15
total positive 5 total negative 10 total 15
total positive 4 total negative 11 total 15
total positive 3 total negative 12 total 15
total positive 14 total negative 1 total 15
total positive 13 total negative 2 total 15
total positive 15 total negative 0 total 15
total positive 0 total negative 15 total 15
total positive -1 total negative -2 total -3
DepParser accepted tagging: 5 | NNP , got score 0.95
DepParser accepted tagging: 12 | VBZ , got score 0.87
DepParser accepted tagging: 3 | DT , got score 0.92
DepParser accepted tagging: 7 | JJ , got score 0.89
DepParser accepted tagging: 9 | NN , got score 0.94
DepParser accepted tagging: 15 | IN , got score 0.86
DepParser accepted tagging: 2 | PRP , got score 0.91
DepParser accepted tagging: 10 | VBD , got score 0.88
DepParser accepted tagging: 6 | CC , got score 0.90
DepParser accepted tagging: 8 | RB , got score 0.93
DepParser accepted tagging: 4 | NNS , got score 0.96
DepParser accepted tagging: 11 | VBN , got score 0.85
DepParser accepted tagging: 1 | WRB , got score 0.90
DepParser accepted tagging: 13 | TO , got score 0.89
DepParser accepted tagging: 14 | VB , got score 0.86
Mention tag: quote.get(QuoteAttributionAnnotator.MentionSieveAnnotation.class)
Mention tag: "He"
Mention tag: "She"
Mention tag: "They"
Mention tag: "It"
Mention tag: "We"
Mention tag: "You"
Mention tag: "I"
Mention tag: "Harry"
Mention tag: "Alice"
Mention tag: "The president"
Mention tag: "The teacher"
Mention tag: "The dog"
Mention tag: "The company"
Mention tag: "The book"
Mention tag: "The movie"
Different partition functions for tree 0 :
Different partition functions for tree 1 :
Different partition functions for tree 2 :
Different partition functions for tree 3 :
Different partition functions for tree 4 :
Different partition functions for tree 5 :
Different partition functions for tree 6 :
Different partition functions for tree 7 :
Different partition functions for tree 8 :
Different partition functions for tree 9 :
Different partition functions for tree 10 :
Different partition functions for tree 11 :
Different partition functions for tree 12 :
Different partition functions for tree 13 :
Different partition functions for tree 14 :
NB CF: 12 data items
NB CF: 8 data items
NB CF: 15 data items
NB CF: 10 data items
NB CF: 9 data items
NB CF: 11 data items
NB CF: 7 data items
NB CF: 13 data items
NB CF: 14 data items
NB CF: 6 data items
NB CF: 16 data items
NB CF: 5 data items
NB CF: 4 data items
NB CF: 3 data items
NB CF: 2 data items
normalizeDate: 2021-10-21 to 21/10/2021
normalizeDate: 2020-02-29 to 29/02/2020
normalizeDate: 2023-01-01 to 01/01/2023
normalizeDate: 2019-12-31 to 31/12/2019
normalizeDate: 2022-04-15 to 15/04/2022
normalizeDate: 2018-07-04 to 04/07/2018
normalizeDate: 2024-06-30 to 30/06/2024
normalizeDate: 2017-09-18 to 18/09/2017
normalizeDate: 2025-08-25 to 25/08/2025
normalizeDate: 2016-05-12 to 12/05/2016
normalizeDate: 2026-03-27 to 27/03/2026
normalizeDate: 2015-11-06 to 06/11/2015
normalizeDate: 2027-02-14 to 14/02/2027
normalizeDate: 2014-10-10 to 10/10/2014
normalizeDate: 2028-05-05 to 05/05/2028
Average weight: 65.4 kg ; std dev: 8.7 kg
Average weight: 72.1 kg ; std dev: 9.2 kg
Average weight: 68.9 kg ; std dev: 7.6 kg
Average weight: 70.3 kg ; std dev: 8.1 kg
Average weight: 66.7 kg ; std dev: 7.9 kg
Average weight: 64.2 kg ; std dev: 8.4 kg
Average weight: 69.5 kg ; std dev: 9.0 kg
Average weight: 67.3 kg ; std dev: 8.3 kg
Average weight: 71.6 kg ; std dev: 9.4 kg
Average weight: 63.8 kg ; std dev: 7.8 kg
Average weight: 65.9 kg ; std dev: 8.5 kg
Average weight: 73.2 kg ; std dev: 9.6 kg
Average weight: 68.4 kg ; std dev: 7.7 kg
Average weight: 70.8 kg ; std dev: 8.2 kg
Average weight: 66.1 kg ; std dev: 7.5 kg
Index 0 unknown
Index 1 unknown
Index 2 unknown
Index 3 unknown
Index 4 unknown
Index 5 unknown
Index 6 unknown
Index 7 unknown
Index 8 unknown
Index 9 unknown
Index 10 unknown
Index 11 unknown
Index 12 unknown
Index 13 unknown
Index 14 unknown
Total 12 15 20 80.00 60.00 68.57
Total 9 10 18 90.00 50.00 64.29
Total 15 18 25 83.33 60.00 69.57
Total 10 12 16 83.33 62.50 71.43
Total 8 9 14 88.89 57.14 69.23
Total 11 13 19 84.62 57.89 68.75
Total 13 16 22 81.25 59.09 68.42
Total 14 17 24 82.35 58.33 68.57
Total 7 8 13 87.50 53.85 66.67
Total 6 7 11 85.71 54.55 66.67
Total 16 20 28 80.00 57.14 66.67
language is now included as part of the tokenize annotator by default
caseSensitive is now included as part of the tokenize annotator by default
splitChars is now included as part of the tokenize annotator by default
splitChars is now included as part of the tokenize annotator by default
targetPattern is now included as part of the tokenize annotator by default
minLength is now included as part of the tokenize annotator by default
maxLength is now included as part of the tokenize annotator by default
exceptions is now included as part of the tokenize annotator by default
includeDefaults is now included as part of the tokenize annotator by default
compositeTokens is now included as part of the tokenize annotator by default
explodeSentences is now included as part of the tokenize annotator by default
explodeTokens is now included as part of the tokenize annotator by default
outputAnnotatorType is now included as part of the tokenize annotator by default
outputAnnotationCol is now included as part of the tokenize annotator by default
inputCols is now included as part of the tokenize annotator by default
outputCol is now included as part of the tokenize annotator by default
##Scorer is Messi
##Scorer is Ronaldo
##Scorer is Lewandowski
##Scorer is Mbappe
##Scorer is Salah
##Scorer is Kane
##Scorer is Benzema
##Scorer is Haaland
##Scorer is Lukaku
##Scorer is Neymar
##Scorer is Griezmann
##Scorer is Suarez
##Scorer is Vardy
##Scorer is Son
##Scorer is Sterling
Taking 3 out of 10 slices of node features for training
Taking 5 out of 12 slices of node features for training
Taking 7 out of 15 slices of node features for training
Taking 2 out of 8 slices of node features for training
Taking 4 out of 9 slices of node features for training
Taking 6 out of 11 slices of node features for training
Taking 8 out of 16 slices of node features for training
Taking 9 out of 18 slices of node features for training
Taking 10 out of 20 slices of node features for training
Taking 1 out of 6 slices of node features for training
Taking 12 out of 24 slices of node features for training
Taking 14 out of 28 slices of node features for training
Taking 16 out of 32 slices of node features for training
Taking 18 out of 36 slices of node features for training
Taking 20 out of 40 slices of node features for training
Macro-averaged F1: 0.76
Macro-averaged F1: 0.82
Macro-averaged F1: 0.69
Macro-averaged F1: 0.74
Macro-averaged F1: 0.79
Macro-averaged F1: 0.71
Macro-averaged F1: 0.77
Macro-averaged F1: 0.68
Macro-averaged F1: 0.73
Macro-averaged F1: 0.81
Macro-averaged F1: 0.75
Macro-averaged F1: 0.70
Macro-averaged F1: 0.78
Macro-averaged F1: 0.72
Macro-averaged F1: 0.80
ABORTING: Input file and output is the same - report.pdf
ABORTING: Input file and output is the same - image.jpg
ABORTING: Input file and output is the same - data.csv
ABORTING: Input file and output is the same - video.mp4
ABORTING: Input file and output is the same - music.mp3
ABORTING: Input file and output is the same - document.docx
ABORTING: Input file and output is the same - presentation.pptx
ABORTING: Input file and output is the same - spreadsheet.xlsx
ABORTING: Input file and output is the same - archive.zip
ABORTING: Input file and output is the same - code.py
ABORTING: Input file and output is the same - script.sh
ABORTING: Input file and output is the same - config.ini
ABORTING: Input file and output is the same - log.txt
ABORTING: Input file and output is the same - database.db
ABORTING: Input file and output is the same - website.html
Approximate color label accuracy: 0.95
Approximate shape label accuracy: 0.87
Approximate size label accuracy: 0.92
Approximate texture label accuracy: 0.89
Approximate material label accuracy: 0.94
Approximate category label accuracy: 0.91
Approximate brand label accuracy: 0.88
Approximate price label accuracy: 0.93
Approximate rating label accuracy: 0.90
Approximate sentiment label accuracy: 0.96
Approximate gender label accuracy: 0.97
Approximate age label accuracy: 0.86
Approximate location label accuracy: 0.98
Approximate time label accuracy: 0.99
Approximate emotion label accuracy: 0.85
Filtered dev trees: 0
Filtered dev trees: 1
Filtered dev trees: 2
Filtered dev trees: 3
Filtered dev trees: 4
Filtered dev trees: 5
Filtered dev trees: 6
Filtered dev trees: 7
Filtered dev trees: 8
Filtered dev trees: 9
Filtered dev trees: 10
Filtered dev trees: 11
Filtered dev trees: 12
Filtered dev trees: 13
Filtered dev trees: 14
Processing 5 trees
Processing 12 trees
Processing 8 trees
Processing 10 trees
Processing 7 trees
Processing 9 trees
Processing 6 trees
Processing 11 trees
Processing 4 trees
Processing 13 trees
Processing 3 trees
Processing 14 trees
Processing 2 trees
Processing 15 trees
Running the baseline results for tagger 0 1
Running the baseline results for tagger 1 2
Running the baseline results for tagger 2 3
Running the baseline results for tagger 3 4
Running the baseline results for tagger 4 5
Running the baseline results for tagger 5 6
Running the baseline results for tagger 6 7
Running the baseline results for tagger 7 8
Running the baseline results for tagger 8 9
Running the baseline results for tagger 9 10
Running the baseline results for tagger 10 11
Running the baseline results for tagger 11 12
Running the baseline results for tagger 12 13
Running the baseline results for tagger 13 14
Running the baseline results for tagger 14 15
DatabaseManager : Skipping malformed parameter in dbconfig.txt (line 12)
Server : Skipping malformed parameter in server.properties (line 8)
Logger : Skipping malformed parameter in log4j.xml (line 15)
Calculator : Skipping malformed parameter in calc.ini (line 3)
Parser : Skipping malformed parameter in parser.json (line 7)
FileManager : Skipping malformed parameter in file.conf (line 10)
Scanner : Skipping malformed parameter in scan.cfg (line 5)
Encoder : Skipping malformed parameter in encode.yaml (line 9)
Decoder : Skipping malformed parameter in decode.toml (line 6)
Compressor : Skipping malformed parameter in compress.hcl (line 11)
Transformer : Skipping malformed parameter in transform.xml (line 4)
Validator : Skipping malformed parameter in validate.properties (line 13)
Generator : Skipping malformed parameter in generate.ini (line 2)
Filter : Skipping malformed parameter in filter.json (line 14)
graph is {nodes: 5, edges: 7, directed: false}
graph is {nodes: 3, edges: 4, directed: true}
graph is {nodes: 8, edges: 12, directed: false}
graph is {nodes: 6, edges: 9, directed: true}
graph is {nodes: 4, edges: 6, directed: false}
graph is {nodes: 7, edges: 10, directed: true}
graph is {nodes: 9, edges: 14, directed: false}
graph is {nodes: 10, edges: 15, directed: true}
graph is {nodes: 11, edges: 16, directed: false}
graph is {nodes: 12, edges: 18, directed: true}
graph is {nodes: 13, edges: 20, directed: false}
graph is {nodes: 14, edges: 21, directed: true}
graph is {nodes: 15, edges: 23, directed: false}
graph is {nodes: 16, edges: 24, directed:true}
graph is {nodes :17 ,edges :26 ,directed :false }
node A : 192.168.0.1
node B : 192.168.0.2
node C : 192.168.0.3
node D : 192.168.0.4
node E : 192.168.0.5
node F : 192.168.0.6
node G : 192.168.0.7
node H : 192.168.0.8
node I : 192.168.0.9
node J : 192.168.0.10
node K : 192.168.0.11
node L : 192.168.0.12
node M : 192.168.0.13
node N : 192.168.0.14
node O : 192.168.0.15
data[dataIndex][j][k].length < 3
data[dataIndex][j][k].length > data[dataIndex][j].length
data[dataIndex][j][k].length == 0
data[dataIndex][j][k].length != data[dataIndex][j].length
data[dataIndex][j][k].length >= 2
data[dataIndex][j][k].length <= data[dataIndex][j].length
data[dataIndex][j][k].length + 1
data[dataIndex][j][k].length - data[dataIndex][j].length
data[dataIndex][j][k].length * 2
data[dataIndex][j][k].length / data[dataIndex][j].length
data[dataIndex][j][k].length % 2
data[dataIndex][j][k].length ** 2
data[dataIndex][j][k].length // 2
data[dataIndex][j][k].length & 1
data[dataIndex][j][k].length | data[dataIndex][j].length
@@ Position 1 :
@@ Position 2 :
@@ Position 3 :
@@ Position 4 :
@@ Position 5 :
@@ Position 6 :
@@ Position 7 :
@@ Position 8 :
@@ Position 9 :
@@ Position 10 :
@@ Position 11 :
@@ Position 12 :
@@ Position 13 :
@@ Position 14 :
@@ Position 15 :
EVENT # 1 : NullPointerException
EVENT # 2 : OutOfMemoryError
EVENT # 3 : ArrayIndexOutOfBoundsException
EVENT # 4 : FileNotFoundException
EVENT # 5 : ArithmeticException
EVENT # 6 : IOException
EVENT # 7 : ClassNotFoundException
EVENT # 8 : NumberFormatException
EVENT # 9 : IllegalArgumentException
EVENT # 10 : InterruptedException
EVENT # 11 : SocketException
EVENT # 12 : SQLException
EVENT # 13 : NoSuchMethodException
EVENT # 14 : SecurityException
EVENT # 15 : AssertionError
no rule for rule1
no rule for rule1_0
no rule for rule1_1
no rule for rule1_2
no rule for rule1_3
no rule for rule1_4
no rule for rule1_5
no rule for rule1_6
no rule for rule1_7
no rule for rule1_8
no rule for rule1_9
no rule for rule1_a
no rule for rule1_b
no rule for rule1_c
no rule for rule1_d
Number of hypothesis trees to train against: 10
Number of hypothesis trees to train against: 25
Number of hypothesis trees to train against: 5
Number of hypothesis trees to train against: 15
Number of hypothesis trees to train against: 20
Number of hypothesis trees to train against: 8
Number of hypothesis trees to train against: 12
Number of hypothesis trees to train against: 18
Number of hypothesis trees to train against: 7
Number of hypothesis trees to train against: 22
Number of hypothesis trees to train against: 9
Number of hypothesis trees to train against: 13
Number of hypothesis trees to train against: 17
Number of hypothesis trees to train against: 6
Number of hypothesis trees to train against: 21
Constituents: [0, 1, 2, 3]
Constituents: [4, 5, 6]
Constituents: [7, 8, 9, 10, 11]
Constituents: [12]
Constituents: [13, 14, 15, 16]
Constituents: [17, 18]
Constituents: [19, 20, 21]
Constituents: [22, 23, 24, 25]
Constituents: [26, 27]
Constituents: [28, 29, 30]
Constituents: [31, 32, 33, 34]
Constituents: [35]
Constituents: [36, 37, 38]
Constituents: [39, 40]
Constituents: [41, 42]
LL: -0.34
LL: 1.56
LL: -2.78
LL: 0.87
LL: -1.23
LL: 2.45
LL: -3.67
LL: 1.09
LL: -0.56
LL: 2.98
LL: -4.32
LL: 0.65
LL: -1.89
LL: 3.76
LL: -5.43
Document 1 : doc001
Document 2 : doc002
Document 3 : doc003
Document 4 : doc004
Document 5 : doc005
Document 6 : doc006
Document 7 : doc007
Document 8 : doc008
Document 9 : doc009
Document 10 : doc010
Document 11 : doc011
Document 12 : doc012
Document 13 : doc013
Document 14 : doc014
Document 15 : doc015
Extractor 0 annotating dataset.
Extractor 1 annotating dataset.
Extractor 2 annotating dataset.
Extractor 3 annotating dataset.
Extractor 4 annotating dataset.
Extractor 5 annotating dataset.
Extractor 6 annotating dataset.
Extractor 7 annotating dataset.
Extractor 8 annotating dataset.
Extractor 9 annotating dataset.
Extractor 10 annotating dataset.
Extractor 11 annotating dataset.
Extractor 12 annotating dataset.
Extractor 13 annotating dataset.
Extractor 14 annotating dataset.
p-value: 0.034
p-value: 0.001
p-value: 0.056
p-value: 0.019
p-value: 0.087
p-value: 0.042
p-value: 0.002
p-value: 0.064
p-value: 0.028
p-value: 0.091
p-value: 0.049
p-value: 0.003
p-value: 0.073
p-value: 0.037
p-value: 0.098
Combo: 0.97
Combo: 0.85
Combo: 0.92
Combo: 0.88
Combo: 0.94
Combo: 0.91
Combo: 0.86
Combo: 0.95
Combo: 0.89
Combo: 0.90
Combo: 0.96
Combo: 0.93
Combo: 0.87
Combo: 0.98
Combo: 0.84
windowSize= 800x600
windowSize= 1024x768
windowSize= 1280x720
windowSize= 1366x768
windowSize= 1440x900
windowSize= 1600x900
windowSize= 1680x1050
windowSize= 1920x1080
windowSize= 2560x1440
windowSize= 3840x2160
windowSize= 640x480
windowSize= 720x480
windowSize= 800x480
windowSize= 854x480
windowSize= 960x540
SemanticGraphParser warning: NullPointerException
SemanticGraphParser warning: InvalidSyntaxException
SemanticGraphParser warning: OutOfMemoryError
SemanticGraphParser warning: IOException
SemanticGraphParser warning: ParseException
SemanticGraphParser warning: UnsupportedEncodingException
SemanticGraphParser warning: ClassCastException
SemanticGraphParser warning: ArrayIndexOutOfBoundsException
SemanticGraphParser warning: FileNotFoundException
SemanticGraphParser warning: NumberFormatException
SemanticGraphParser warning: AssertionError
SemanticGraphParser warning: StackOverflowError
SemanticGraphParser warning: IllegalArgumentException
SemanticGraphParser warning: IllegalStateException
SemanticGraphParser warning: TimeoutException
Loading relation extraction model from bert-base-cased ...
Loading relation extraction model from spanbert-large-cased ...
Loading relation extraction model from roberta-base ...
Loading relation extraction model from xlnet-base-cased ...
Loading relation extraction model from albert-xxlarge-v2 ...
Loading relation extraction model from distilbert-base-uncased ...
Loading relation extraction model from gpt2-medium ...
Loading relation extraction model from t5-small ...
Loading relation extraction model from electra-large-discriminator ...
Loading relation extraction model from longformer-base-4096 ...
Loading relation extraction model from bigbird-base-uncased ...
Loading relation extraction model from bart-large-cnn ...
Loading relation extraction model from reformer-crime-and-punishment ...
Loading relation extraction model from deberta-v2-xlarge ...
Loading relation extraction model from layoutlmv2-base-uncased ...
Done! ( 12 trees)
Done! ( 5 trees)
Done! ( 8 trees)
Done! ( 10 trees)
Done! ( 7 trees)
Done! ( 9 trees)
Done! ( 11 trees)
Done! ( 6 trees)
Done! ( 4 trees)
Done! ( 13 trees)
Done! ( 14 trees)
Done! ( 3 trees)
Done! ( 15 trees)
Done! ( 2 trees)
undirected edges btw 1 and 3 is 0
undirected edges btw 1 and 3 is 1
undirected edges btw 1 and 3 is 2
undirected edges btw 1 and 3 is 3
undirected edges btw 1 and 3 is 4
undirected edges btw 1 and 3 is 5
undirected edges btw 1 and 3 is 6
undirected edges btw 1 and 3 is 7
undirected edges btw 1 and 3 is 8
undirected edges btw 1 and 3 is 9
undirected edges btw 1 and 3 is -1
undirected edges btw 1 and 3 is -2
undirected edges btw 1 and 3 is -3
undirected edges btw 1 and 3 is -4
undirected edges btw 1 and 3 is -5
normalizingTime: 0.5s
normalizingTime: 1.2s
normalizingTime: 0.8s
normalizingTime: 1.5s
normalizingTime: 0.3s
normalizingTime: 1.7s
normalizingTime: 0.9s
normalizingTime: 1.3s
normalizingTime: 0.6s
normalizingTime: 1.8s
normalizingTime: 0.4s
normalizingTime: 1.6s
normalizingTime: 0.7s
normalizingTime: 2.0s
normalizingTime: 1.4s
Manipulating: newCollocationChild(1, 2, 3)
Manipulating: newCollocationChild(4, 5, 6)
Manipulating: newCollocationChild(7, 8, 9)
Manipulating: newCollocationChild(10, 11, 12)
Manipulating: newCollocationChild(13, 14, 15)
Manipulating: newCollocationChild(16, 17, 18)
Manipulating: newCollocationChild(19, 20, 21)
Manipulating: newCollocationChild(22, 23, 24)
Manipulating: newCollocationChild(25, 26, 27)
Manipulating: newCollocationChild(28, 29, 30)
Manipulating: newCollocationChild(31, 32, 33)
Manipulating: newCollocationChild(34, 35, 36)
Manipulating: newCollocationChild(37, 38, 39)
Manipulating: newCollocationChild(40, 41, 42)
Manipulating: newCollocationChild(43, 44, 45)
Head preterminal is null: com.example.MyClass@3a71f4dd
Head preterminal is null: org.apache.commons.lang3.StringUtils@5e9f23b4
Head preterminal is null: java.util.ArrayList@7a81197d
Head preterminal is null: java.lang.String@6e8cf4c6
Head preterminal is null: java.lang.Integer@1b28cdfa
Head preterminal is null: java.util.HashMap@4f023edb
Head preterminal is null: java.io.File@3c679bde
Head preterminal is null: java.net.URL@5c647e05
Head preterminal is null: java.util.Date@6bc7c054
Head preterminal is null: java.lang.Boolean@7ea987ac
Head preterminal is null: java.math.BigDecimal@2b193f2d
Head preterminal is null: java.util.regex.Pattern@4a574795
Head preterminal is null: java.lang.Thread@7d4991ad
Head preterminal is null: java.nio.ByteBuffer@1c20c684
Head preterminal is null: javax.swing.JFrame@6d311334
Saving serialized model to model_1.h5
Saving serialized model to /home/user/models/model_2.pkl
Saving serialized model to C:\Users\user\Documents\model_3.pt
Saving serialized model to /tmp/model_4.json
Saving serialized model to model_5.sav
Saving serialized model to /var/log/models/model_6.bin
Saving serialized model to D:\Models\model_7.onnx
Saving serialized model to /usr/local/share/models/model_8.joblib
Saving serialized model to model_9.tflite
Saving serialized model to /opt/models/model_10.npz
Saving serialized model to E:\model_11.dat
Saving serialized model to /etc/models/model_12.xml
Saving serialized model to model_13.keras
Saving serialized model to /dev/null/model_14.txt
Saving serialized model to F:\model_15.mlmodel
Read in 10 trees for testing
Read in 25 trees for testing
Read in 15 trees for testing
Read in 30 trees for testing
Read in 12 trees for testing
Read in 20 trees for testing
Read in 18 trees for testing
Read in 27 trees for testing
Read in 14 trees for testing
Read in 22 trees for testing
Read in 16 trees for testing
Read in 24 trees for testing
Read in 13 trees for testing
Read in 28 trees for testing
Read in 19 trees for testing
Gold Mention: "He said" CORRECT
Gold Mention: "she replied" CORRECT
Gold Mention: "the president announced" CORRECT
Gold Mention: "they shouted" CORRECT
Gold Mention: "I asked" CORRECT
Gold Mention: "you whispered" CORRECT
Gold Mention: "we agreed" CORRECT
Gold Mention: "she exclaimed" CORRECT
Gold Mention: "he muttered" CORRECT
Gold Mention: "they argued" CORRECT
Gold Mention: "she confessed" CORRECT
Gold Mention: "he admitted" CORRECT
Gold Mention: "they reported" CORRECT
Gold Mention: "I suggested" CORRECT
Gold Mention: "you agreed" CORRECT
##sigma = 0.5 -> average Score: 75.6
##sigma = 0.8 -> average Score: 82.3
##sigma = 1.2 -> average Score: 88.7
##sigma = 0.3 -> average Score: 69.4
##sigma = 1.5 -> average Score: 91.2
##sigma = 0.7 -> average Score: 79.8
##sigma = 1.0 -> average Score: 85.4
##sigma = 0.4 -> average Score: 72.9
##sigma = 1.3 -> average Score: 89.6
##sigma = 0.6 -> average Score: 77.1
##sigma = 0.9 -> average Score: 83.5
##sigma = 1.4 -> average Score: 90.4
##sigma = 0.2 -> average Score: 66.8
##sigma = 1.6 -> average Score: 92.7
##sigma = 1.1 -> average Score: 87.2
minimaldebug Saving the learned patterns for label 0 in patternsOutputFileLabel0
minimaldebug Saving the learned patterns for label 1 in patternsOutputFileLabel1
minimaldebug Saving the learned patterns for label 2 in patternsOutputFileLabel2
minimaldebug Saving the learned patterns for label 3 in patternsOutputFileLabel3
minimaldebug Saving the learned patterns for label 4 in patternsOutputFileLabel4
minimaldebug Saving the learned patterns for label 5 in patternsOutputFileLabel5
minimaldebug Saving the learned patterns for label 6 in patternsOutputFileLabel6
minimaldebug Saving the learned patterns for label 7 in patternsOutputFileLabel7
minimaldebug Saving the learned patterns for label 8 in patternsOutputFileLabel8
minimaldebug Saving the learned patterns for label 9 in patternsOutputFileLabel9
minimaldebug Saving the learned patterns for label 10 in patternsOutputFileLabel10
minimaldebug Saving the learned patterns for label 11 in patternsOutputFileLabel11
minimaldebug Saving the learned patterns for label 12 in patternsOutputFileLabel12
minimaldebug Saving the learned patterns for label 13 in patternsOutputFileLabel13
minimaldebug Saving the learned patterns for label 14 in patternsOutputFileLabel14
Featurized 100 data tokens [done].
Featurized 256 data tokens [done].
Featurized 512 data tokens [done].
Featurized 1024 data tokens [done].
Featurized 2048 data tokens [done].
Featurized 4096 data tokens [done].
Featurized 8192 data tokens [done].
Featurized 16384 data tokens [done].
Featurized 32768 data tokens [done].
Featurized 65536 data tokens [done].
Featurized 131072 data tokens [done].
Featurized 262144 data tokens [done].
Featurized 524288 data tokens [done].
Featurized 1048576 data tokens [done].
Featurized 2097152 data tokens [done].
in CRFClassifierEvaluator.interpretCmdOutput(), cannot find FB1 score in output: [INFO] 0 of 0 correct (0.00%)
in CRFClassifierEvaluator.interpretCmdOutput(), cannot find FB1 score in output: [ERROR] java.lang.NullPointerException
in CRFClassifierEvaluator.interpretCmdOutput(), cannot find FB1 score in output: [WARN] No test data found
in CRFClassifierEvaluator.interpretCmdOutput(), cannot find FB1 score in output: [DEBUG] CRFClassifierEvaluator.evaluate() finished in 12.34 seconds
in CRFClassifierEvaluator.interpretCmdOutput(), cannot find FB1 score in output: [INFO] Precision: 0.76 Recall: 0.82
in CRFClassifierEvaluator.interpretCmdOutput(), cannot find FB1 score in output: [ERROR] java.io.FileNotFoundException: test.txt (No such file or directory)
in CRFClassifierEvaluator.interpretCmdOutput(), cannot find FB1 score in output: [WARN] Invalid label format
in CRFClassifierEvaluator.interpretCmdOutput(), cannot find FB1 score in output: [DEBUG] CRFClassifierEvaluator.evaluate() started with parameters: model=crf.model, test=test.txt
in CRFClassifierEvaluator.interpretCmdOutput(), cannot find FB1 score in output: [INFO] 100 of 200 correct (50.00%)
in CRFClassifierEvaluator.interpretCmdOutput(), cannot find FB1 score in output: [ERROR] java.lang.OutOfMemoryError
in CRFClassifierEvaluator.interpretCmdOutput(), cannot find FB1 score in output: [WARN] Low confidence score for some labels
in CRFClassifierEvaluator.interpretCmdOutput(), cannot find FB1 score in output: [DEBUG] CRFClassifierEvaluator.loadModel() loaded model from crf.model
in CRFClassifierEvaluator.interpretCmdOutput(), cannot find FB1 score in output: [INFO] 150 of 300 correct (50.00%)
in CRFClassifierEvaluator.interpretCmdOutput(), cannot find FB1 score in output: [ERROR] java.lang.IllegalArgumentException: Invalid argument for CRFClassifier
in CRFClassifierEvaluator.interpretCmdOutput(), cannot find FB1 score in output: [WARN] Missing label for some tokens
Training a parser from treebank dir: /home/user/data/PTB
Training a parser from treebank dir: C:\Users\user\Documents\UD_English
Training a parser from treebank dir: /mnt/treebanks/Chinese_Treebank_5.1
Training a parser from treebank dir: /Users/user/Desktop/treebank
Training a parser from treebank dir: D:\data\Arabic_Treebank
Training a parser from treebank dir: /home/user/data/UD_French
Training a parser from treebank dir: C:\Users\user\Documents\PTB3
Training a parser from treebank dir: /mnt/treebanks/UD_German
Training a parser from treebank dir: /Users/user/Desktop/UD_Spanish
Training a parser from treebank dir: D:\data\UD_Italian
Training a parser from treebank dir: /home/user/data/UD_Portuguese
Training a parser from treebank dir: C:\Users\user\Documents\UD_Russian
Training a parser from treebank dir: /mnt/treebanks/UD_Turkish
Training a parser from treebank dir: /Users/user/Desktop/UD_Japanese
Training a parser from treebank dir: D:\data\UD_Korean
Hashed 16 histories.
Hashed 32 histories.
Hashed 64 histories.
Hashed 128 histories.
Hashed 256 histories.
Hashed 512 histories.
Hashed 1024 histories.
Hashed 2048 histories.
Hashed 4096 histories.
Hashed 8192 histories.
Hashed 16384 histories.
Hashed 32768 histories.
Hashed 65536 histories.
Hashed 131072 histories.
Hashed 262144 histories.
Failed to open file in writeToDOTfile: config.txt
Failed to open file in writeToDOTfile: graph.dot
Failed to open file in writeToDOTfile: output.csv
Failed to open file in writeToDOTfile: data.json
Failed to open file in writeToDOTfile: report.pdf
Failed to open file in writeToDOTfile: log.txt
Failed to open file in writeToDOTfile: index.html
Failed to open file in writeToDOTfile: style.css
Failed to open file in writeToDOTfile: script.js
Failed to open file in writeToDOTfile: image.png
Failed to open file in writeToDOTfile: audio.mp3
Failed to open file in writeToDOTfile: video.mp4
Failed to open file in writeToDOTfile: document.docx
Failed to open file in writeToDOTfile: presentation.pptx
Failed to open file in writeToDOTfile: spreadsheet.xlsx
Redwood.DBG reading from ser file f1.ser
Redwood.DBG reading from ser file f2.ser
Redwood.DBG reading from ser file f3.ser
Redwood.DBG reading from ser file f4.ser
Redwood.DBG reading from ser file f5.ser
Redwood.DBG reading from ser file f6.ser
Redwood.DBG reading from ser file f7.ser
Redwood.DBG reading from ser file f8.ser
Redwood.DBG reading from ser file f9.ser
Redwood.DBG reading from ser file f10.ser
Redwood.DBG reading from ser file f11.ser
Redwood.DBG reading from ser file f12.ser
Redwood.DBG reading from ser file f13.ser
Redwood.DBG reading from ser file f14.ser
Redwood.DBG reading from ser file f15.ser
(logs) -Inf + -7 = -Inf (logAdd(Double.NEGATIVE_INFINITY, -7.0))
(logs) -Inf + -7 = NaN (logAdd(Double.NEGATIVE_INFINITY, -7.0))
(logs) -Inf + -7 = Double.NEGATIVE_INFINITY (logAdd(Double.NEGATIVE_INFINITY, -7.0))
(logs) -Inf + -7 = undefined (logAdd(Double.NEGATIVE_INFINITY, -7.0))
(logs) -Inf + -7 = null (logAdd(Double.NEGATIVE_INFINITY, -7.0))
(logs) -Inf + -7 = error (logAdd(Double.NEGATIVE_INFINITY, -7.0))
(logs) -Inf + -7 = exception (logAdd(Double.NEGATIVE_INFINITY, -7.0))
(logs) -Inf + -7 = invalid (logAdd(Double.NEGATIVE_INFINITY, -7.0))
(logs) -Inf + -7 = not a number (logAdd(Double.NEGATIVE_INFINITY, -7.0))
(logs) -Inf + -7 = negative infinity (logAdd(Double.NEGATIVE_INFINITY, -7.0))
(logs) -Inf + -7 = indeterminate (logAdd(Double.NEGATIVE_INFINITY, -7.0))
(logs) -Inf + -7 = uncomputable (logAdd(Double.NEGATIVE_INFINITY, -7.0))
(logs) -Inf + -7 = unknown (logAdd(Double.NEGATIVE_INFINITY, -7.0))
(logs) -Inf + -7 = unsupported (logAdd(Double.NEGATIVE_INFINITY, -7.0))
(logs) -Inf + -7 = illegal (logAdd(Double.NEGATIVE_INFINITY, -7.0))
Done. Loaded 100 sentences.
Done. Loaded 256 sentences.
Done. Loaded 50 sentences.
Done. Loaded 500 sentences.
Done. Loaded 75 sentences.
Done. Loaded 300 sentences.
Done. Loaded 120 sentences.
Done. Loaded 400 sentences.
Done. Loaded 80 sentences.
Done. Loaded 150 sentences.
Done. Loaded 200 sentences.
Done. Loaded 60 sentences.
Done. Loaded 250 sentences.
Done. Loaded 40 sentences.
Done. Loaded 350 sentences.
Entry 0 is entry 1 of unary transform negate
Entry 5 is entry 6 of unary transform square
Entry 10 is entry 11 of unary transform sqrt
Entry 15 is entry 16 of unary transform abs
Entry 20 is entry 21 of unary transform sin
Entry 25 is entry 26 of unary transform cos
Entry 30 is entry 31 of unary transform tan
Entry 35 is entry 36 of unary transform log
Entry 40 is entry 41 of unary transform exp
Entry 45 is entry 46 of unary transform ceil
Entry 50 is entry 51 of unary transform floor
Entry 55 is entry 56 of unary transform round
Entry 60 is entry 61 of unary transform sign
Entry 65 is entry 66 of unary transform reciprocal
Entry 70 is entry 71 of unary transform factorial
Linear classifier with 5 f(x,y) features
Linear classifier with 10 f(x,y) features
Linear classifier with 3 f(x,y) features
Linear classifier with 7 f(x,y) features
Linear classifier with 12 f(x,y) features
Linear classifier with 4 f(x,y) features
Linear classifier with 8 f(x,y) features
Linear classifier with 6 f(x,y) features
Linear classifier with 9 f(x,y) features
Linear classifier with 11 f(x,y) features
Linear classifier with 2 f(x,y) features
Linear classifier with 15 f(x,y) features
Linear classifier with 13 f(x,y) features
Linear classifier with 14 f(x,y) features
Linear classifier with 16 f(x,y) features
Training treebank does not exist! /home/user/data/train.conll
Training treebank does not exist! C:\Users\user\Documents\train.txt
Training treebank does not exist! /mnt/train/train.json
Training treebank does not exist! /Users/user/Desktop/train.csv
Training treebank does not exist! /var/lib/train/train.xml
Training treebank does not exist! /opt/data/train/train.tsv
Training treebank does not exist! C:\Program Files\train\train.dat
Training treebank does not exist! /tmp/train/train.pkl
Training treebank does not exist! /home/user/train/train.hdf5
Training treebank does not exist! /data/train/train.parquet
Training treebank does not exist! /usr/local/share/train/train.yaml
Training treebank does not exist! C:\temp\train\train.ini
Training treebank does not exist! /dev/shm/train/train.bin
Training treebank does not exist! /root/data/train/train.npz
Training treebank does not exist! C:\Windows\Temp\train\train.mat
Creating lucene index at /home/user1/indexes/product
Creating lucene index at C:\Users\user2\Documents\index\customer
Creating lucene index at /var/lib/lucene/indexes/news
Creating lucene index at /opt/lucene/indexes/blog
Creating lucene index at D:\lucene\index\book
Creating lucene index at /tmp/lucene/indexes/test
Creating lucene index at /mnt/lucene/indexes/review
Creating lucene index at C:\lucene\index\movie
Creating lucene index at /usr/local/lucene/indexes/wiki
Creating lucene index at E:\index\lucene\recipe
Creating lucene index at /data/lucene/indexes/twitter
Creating lucene index at F:\lucene\index\music
Creating lucene index at /root/lucene/indexes/email
Creating lucene index at G:\index\lucene\game
Creating lucene index at /media/lucene/indexes/photo
MINIMIM BATCH SIZE ON THIS MACHINE: 32
MINIMIM BATCH SIZE ON THIS MACHINE: 40
MINIMIM BATCH SIZE ON THIS MACHINE: 24
MINIMIM BATCH SIZE ON THIS MACHINE: 48
MINIMIM BATCH SIZE ON THIS MACHINE: 36
MINIMIM BATCH SIZE ON THIS MACHINE: 28
MINIMIM BATCH SIZE ON THIS MACHINE: 44
MINIMIM BATCH SIZE ON THIS MACHINE: 20
MINIMIM BATCH SIZE ON THIS MACHINE: 16
MINIMIM BATCH SIZE ON THIS MACHINE: 56
MINIMIM BATCH SIZE ON THIS MACHINE: 52
MINIMIM BATCH SIZE ON THIS MACHINE: 12
MINIMIM BATCH SIZE ON THIS MACHINE: 8
MINIMIM BATCH SIZE ON THIS MACHINE: 60
MINIMIM BATCH SIZE ON THIS MACHINE: 64
ERROR: failed to save model to path: /home/user/models/model_1.h5
ERROR: failed to save model to path: C:\Users\user\Documents\models\model_2.pkl
ERROR: failed to save model to path: /mnt/s3/models/model_3.pt
ERROR: failed to save model to path: /tmp/models/model_4.joblib
ERROR: failed to save model to path: /data/user/models/model_5.onnx
ERROR: failed to save model to path: /var/lib/models/model_6.tflite
ERROR: failed to save model to path: /opt/models/model_7.sav
ERROR: failed to save model to path: /Users/user/Desktop/models/model_8.json
ERROR: failed to save model to path: D:\models\model_9.bin
ERROR: failed to save model to path: /usr/local/models/model_10.npz
ERROR: failed to save model to path: /media/user/models/model_11.dill
ERROR: failed to save model to path: /root/models/model_12.pickle
ERROR: failed to save model to path: E:\models\model_13.xml
ERROR: failed to save model to path: /etc/models/model_14.yaml
ERROR: failed to save model to path: /dev/models/model_15.mat
FOUND TEMPORALS: [Monday, 9:00 AM]
FOUND TEMPORALS: [2023-10-26]
FOUND TEMPORALS: [last week]
FOUND TEMPORALS: [two hours ago]
FOUND TEMPORALS: [January 1st, 2021]
FOUND TEMPORALS: [every day]
FOUND TEMPORALS: [next month]
FOUND TEMPORALS: [tomorrow morning]
FOUND TEMPORALS: [three days later]
FOUND TEMPORALS: [from 10:00 to 11:00]
FOUND TEMPORALS: [the day before yesterday]
FOUND TEMPORALS: [this weekend]
FOUND TEMPORALS: [Friday night]
FOUND TEMPORALS: [2022-12-31]
FOUND TEMPORALS: [in a few minutes]
Feature level 0 is not supported in ChineseWordFeatureExtractor.
Feature level 1 is not supported in ChineseWordFeatureExtractor.
Feature level 2 is not supported in ChineseWordFeatureExtractor.
Feature level 3 is not supported in ChineseWordFeatureExtractor.
Feature level 4 is not supported in ChineseWordFeatureExtractor.
Feature level 5 is not supported in ChineseWordFeatureExtractor.
Feature level 6 is not supported in ChineseWordFeatureExtractor.
Feature level 7 is not supported in ChineseWordFeatureExtractor.
Feature level 8 is not supported in ChineseWordFeatureExtractor.
Feature level 9 is not supported in ChineseWordFeatureExtractor.
Feature level 10 is not supported in ChineseWordFeatureExtractor.
Feature level A is not supported in ChineseWordFeatureExtractor.
Feature level B is not supported in ChineseWordFeatureExtractor.
Feature level C is not supported in ChineseWordFeatureExtractor.
Feature level D is not supported in ChineseWordFeatureExtractor.
doc.allGoldMentions.size() is 0
doc.allGoldMentions.size() is 1
doc.allGoldMentions.size() is 2
doc.allGoldMentions.size() is 3
doc.allGoldMentions.size() is 4
doc.allGoldMentions.size() is 5
doc.allGoldMentions.size() is 6
doc.allGoldMentions.size() is 7
doc.allGoldMentions.size() is 8
doc.allGoldMentions.size() is 9
doc.allGoldMentions.size() is 10
doc.allGoldMentions.size() is 11
doc.allGoldMentions.size() is 12
doc.allGoldMentions.size() is 13
doc.allGoldMentions.size() is 14
0 t= 12.34
1 t= 9.87
2 t= 15.67
3 t= 10.45
4 t= 13.89
5 t= 11.23
6 t= 14.56
7 t= 8.90
8 t= 16.78
9 t= 10.12
10 t= 14.32
11 t= 9.54
12 t= 15.43
13 t= 11.76
14 t= 13.21
Initialized: 1634908169000 - time
Initialized: 1634908170000 - time
Initialized: 1634908171000 - time
Initialized: 1634908172000 - time
Initialized: 1634908173000 - time
Initialized: 1634908174000 - time
Initialized: 1634908175000 - time
Initialized: 1634908176000 - time
Initialized: 1634908177000 - time
Initialized: 1634908178000 - time
Initialized: 1634908179000 - time
Initialized: 1634908180000 - time
Initialized: 1634908181000 - time
Initialized: 1634908182000 - time
Initialized: 1634908183000 - time
Redwood.DBG Negative Words: not:0.12;bad:0.09;fail:0.07;error:0.05;wrong:0.04
Redwood.DBG Negative Words: no:0.15;poor:0.08;reject:0.06;crash:0.04;miss:0.03
Redwood.DBG Negative Words: never:0.11;worse:0.09;break:0.07;bug:0.05;lose:0.04
Redwood.DBG Negative Words: none:0.13;awful:0.08;cancel:0.06;corrupt:0.04;ignore:0.03
Redwood.DBG Negative Words: nothing:0.12;terrible:0.09;abort:0.07;faulty:0.05;damage:0.04
Redwood.DBG Negative Words: hardly:0.11;better:0.08;stop:0.06;invalid:0.04;delay:0.03
Redwood.DBG Negative Words: rarely:0.13;sadly:0.09;suspend:0.07;incomplete:0.05;conflict:0.04
Redwood.DBG Negative Words: seldom:0.12;lousy:0.08;kill:0.06;unstable:0.04;violate:0.03
Redwood.DBG Negative Words: barely:0.11;miserable:0.09;close:0.07;malfunctioning:0.05;affect:0.04
Redwood.DBG Negative Words: unfortunately:0.13;dreadful:0.08;terminate:0.06;defective:0.04;harmful:
com.example.app.MainActivity : Error reading XML from input stream.
org.apache.commons.io.IOUtils : Error reading XML from input stream.
javax.xml.parsers.DocumentBuilder : Error reading XML from input stream.
com.google.gson.Gson : Error reading XML from input stream.
org.springframework.beans.factory.xml.XmlBeanFactory : Error reading XML from input stream.
com.fasterxml.jackson.dataformat.xml.XmlMapper : Error reading XML from input stream.
org.w3c.dom.Document : Error reading XML from input stream.
java.io.BufferedInputStream : Error reading XML from input stream.
org.xml.sax.InputSource : Error reading XML from input stream.
android.util.Xml : Error reading XML from input stream.
javax.xml.transform.stream.StreamSource : Error reading XML from input stream.
org.dom4j.io.SAXReader : Error reading XML from input stream.
net.sf.saxon.s9api.XdmNode : Error reading XML from input stream.
org.jdom2.input.SAXBuilder : Error reading XML from input stream.
org.apache.xerces.parsers.DOMParser : Error reading XML from input stream.
Class not found: args[0 + 1]
Class not found: args[2 + 1]
Class not found: args[4 + 1]
Class not found: args[6 + 1]
Class not found: args[8 + 1]
Class not found: args[10 + 1]
Class not found: args[12 + 1]
Class not found: args[14 + 1]
Class not found: args[16 + 1]
Class not found: args[18 + 1]
Class not found: args[20 + 1]
Class not found: args[22 + 1]
Class not found: args[24 + 1]
Class not found: args[26 + 1]
Class not found: args[28 + 1]
Optimize sieves using score: accuracy
Optimize sieves using score: precision
Optimize sieves using score: recall
Optimize sieves using score: f1
Optimize sieves using score: auc
Optimize sieves using score: mae
Optimize sieves using score: mse
Optimize sieves using score: rmse
Optimize sieves using score: r2
Optimize sieves using score: ndcg
Optimize sieves using score: map
Optimize sieves using score: mrr
Optimize sieves using score: hit_rate
Optimize sieves using score: coverage
Optimize sieves using score: diversity
after exact minimization graph has 12 arcs and 8 nodes.
after exact minimization graph has 9 arcs and 6 nodes.
after exact minimization graph has 15 arcs and 10 nodes.
after exact minimization graph has 11 arcs and 7 nodes.
after exact minimization graph has 10 arcs and 9 nodes.
after exact minimization graph has 13 arcs and 11 nodes.
after exact minimization graph has 8 arcs and 5 nodes.
after exact minimization graph has 14 arcs and 12 nodes.
after exact minimization graph has 16 arcs and 13 nodes.
after exact minimization graph has 7 arcs and 4 nodes.
after exact minimization graph has 17 arcs and 14 nodes.
after exact minimization graph has 18 arcs and 15 nodes.
after exact minimization graph has 19 arcs and 16 nodes.
after exact minimization graph has 20 arcs and 17 nodes.
after exact minimization graph has 21 arcs and 18 nodes.
directed path edges btw 1 and 5 is [2, 4, 6]
directed path edges btw 1 and 5 is [3, 7, 9]
directed path edges btw 1 and 5 is [2, 8, 10]
directed path edges btw 1 and 5 is [3, 6, 11]
directed path edges btw 1 and 5 is [4, 7, 12]
directed path edges btw 1 and 5 is [2, 9, 13]
directed path edges btw 1 and 5 is [3, 10, 14]
directed path edges btw 1 and 5 is [4, 8, 15]
directed path edges btw 1 and 5 is [2, 11, 16]
directed path edges btw 1 and 5 is [3, 12, 17]
directed path edges btw 1 and 5 is [4, 9, 18]
directed path edges btw 1 and 5 is [2, 13, 19]
directed path edges btw 1 and 5 is [3, 14, 20]
directed path edges btw 1 and 5 is [4, 10, 21]
directed path edges btw 1 and 5 is [2, 15, 22]
WARNING: unmatched quote of type single found at index 17 in text segment: He said 'I'm fine
WARNING: unmatched quote of type double found at index 23 in text segment: She replied "Thank you
WARNING: unmatched quote of type triple found at index 12 in text segment: The code is '''print("Hello")
WARNING: unmatched quote of type single found at index 9 in text segment: It's a 'nice day
WARNING: unmatched quote of type double found at index 15 in text segment: The title is "The Catcher
WARNING: unmatched quote of type triple found at index 18 in text segment: The comment is '''This is a
WARNING: unmatched quote of type single found at index 21 in text segment: He asked 'What's your name?
WARNING: unmatched quote of type double found at index 19 in text segment: She said "I love you
WARNING: unmatched quote of type triple found at index 14 in text segment: The function is '''def add(x,y):
WARNING: unmatched quote of type single found at index 11 in text segment: It was a 'great movie
WARNING: unmatched quote of type double found at index 13 in text segment: The song is "Let it be
WARNING: unmatched quote of type triple found at index 16 in text segment: The query is '''select * from
WARNING: unmatched quote of type single found at index 25 in text segment: He wrote 'To be or not to be
WARNING: unmatched quote of type double found at index 27 in text segment: She sang "I will always love you
WARNING: unmatched quote of type triple found at index 20 in text segment: The script is '''import numpy as
numFeatures = 5
numFeatures = 12
numFeatures = 8
numFeatures = 10
numFeatures = 7
numFeatures = 9
numFeatures = 6
numFeatures = 11
numFeatures = 4
numFeatures = 13
numFeatures = 3
numFeatures = 14
numFeatures = 2
numFeatures = 15
numFeatures = 1
#feattags: 0
#feattags: 1
#feattags: 2
#feattags: 3
#feattags: 4
#feattags: 5
#feattags: 6
#feattags: 7
#feattags: 8
#feattags: 9
#feattags: 10
#feattags: 11
#feattags: 12
#feattags: 13
#feattags: 14
Warning: no unknown word model in place! Giving the combination apple NN zero probability.
Warning: no unknown word model in place! Giving the combination quickly RB zero probability.
Warning: no unknown word model in place! Giving the combination hello UH zero probability.
Warning: no unknown word model in place! Giving the combination yesterday IN zero probability.
Warning: no unknown word model in place! Giving the combination awesome JJ zero probability.
Warning: no unknown word model in place! Giving the combination pizza NN zero probability.
Warning: no unknown word model in place! Giving the combination because IN zero probability.
Warning: no unknown word model in place! Giving the combination maybe RB zero probability.
Warning: no unknown word model in place! Giving the combination happy JJ zero probability.
Warning: no unknown word model in place! Giving the combination internet NN zero probability.
Warning: no unknown word model in place! Giving the combination however RB zero probability.
Warning: no unknown word model in place! Giving the combination sorry UH zero probability.
Warning: no unknown word model in place! Giving the combination tomorrow IN zero probability.
Warning: no unknown word model in place! Giving the combination boring JJ zero probability.
Warning: no unknown word model in place! Giving the combination coffee NN zero probability.
minimaldebug Number of seed words for label A is 12
minimaldebug Number of seed words for label B is 9
minimaldebug Number of seed words for label C is 10
minimaldebug Number of seed words for label D is 11
minimaldebug Number of seed words for label E is 8
minimaldebug Number of seed words for label F is 13
minimaldebug Number of seed words for label G is 7
minimaldebug Number of seed words for label H is 14
minimaldebug Number of seed words for label I is 6
minimaldebug Number of seed words for label J is 15
minimaldebug Number of seed words for label K is 5
minimaldebug Number of seed words for label L is 16
minimaldebug Number of seed words for label M is 4
minimaldebug Number of seed words for label N is 17
minimaldebug Number of seed words for label O is 3
#lemmaTagPairs: 12
#lemmaTagPairs: 9
#lemmaTagPairs: 15
#lemmaTagPairs: 11
#lemmaTagPairs: 10
#lemmaTagPairs: 13
#lemmaTagPairs: 8
#lemmaTagPairs: 14
#lemmaTagPairs: 7
#lemmaTagPairs: 16
#lemmaTagPairs: 6
#lemmaTagPairs: 17
#lemmaTagPairs: 5
#lemmaTagPairs: 18
#lemmaTagPairs: 4
Time to combine CRFClassifier: 0.12 seconds
Time to combine CRFClassifier: 0.08 seconds
Time to combine CRFClassifier: 0.15 seconds
Time to combine CRFClassifier: 0.11 seconds
Time to combine CRFClassifier: 0.09 seconds
Time to combine CRFClassifier: 0.13 seconds
Time to combine CRFClassifier: 0.10 seconds
Time to combine CRFClassifier: 0.07 seconds
Time to combine CRFClassifier: 0.14 seconds
Time to combine CRFClassifier: 0.16 seconds
Time to combine CRFClassifier: 0.06 seconds
Time to combine CRFClassifier: 0.17 seconds
Time to combine CRFClassifier: 0.05 seconds
Time to combine CRFClassifier: 0.18 seconds
Time to combine CRFClassifier: 0.04 seconds
# minWordsLockTags = 5
# minWordsLockTags = 10
# minWordsLockTags = 7
# minWordsLockTags = 8
# minWordsLockTags = 6
# minWordsLockTags = 9
# minWordsLockTags = 4
# minWordsLockTags = 3
# minWordsLockTags = 12
# minWordsLockTags = 11
# minWordsLockTags = 15
# minWordsLockTags = 13
# minWordsLockTags = 14
# minWordsLockTags = 2
# minWordsLockTags = 1
Speaker Tag: John
Speaker Tag: Alice
Speaker Tag: Bob
Speaker Tag: Mary
Speaker Tag: David
Speaker Tag: Emma
Speaker Tag: James
Speaker Tag: Anna
Speaker Tag: Mark
Speaker Tag: Lisa
Speaker Tag: Tom
Speaker Tag: Sarah
Speaker Tag: Jack
Speaker Tag: Kate
Speaker Tag: Eric
numValues 0 12
numValues 1 9
numValues 2 15
numValues 3 11
numValues 4 13
numValues 5 10
numValues 6 14
numValues 7 8
numValues 8 16
numValues 9 7
numValues 10 17
numValues 11 6
numValues 12 18
numValues 13 5
numValues 14 19
Quantifiable: Processing entity string "hello"
Quantifiable: Processing entity string "world"
Quantifiable: Processing entity string "Bing"
Quantifiable: Processing entity string "search"
Quantifiable: Processing entity string "log"
Quantifiable: Processing entity string "data"
Quantifiable: Processing entity string "simulator"
Quantifiable: Processing entity string "template"
Quantifiable: Processing entity string "parameter"
Quantifiable: Processing entity string "output"
Quantifiable: Processing entity string "example"
Quantifiable: Processing entity string "entity"
Quantifiable: Processing entity string "string"
Quantifiable: Processing entity string "quantifiable"
Quantifiable: Processing entity string "processing"
line: 1
line: 2
line: 3
line: 4
line: 5
line: 6
line: 7
line: 8
line: 9
line: 10
line: 11
line: 12
line: 13
line: 14
line: 15
Base directory: /home
Base directory: /home/user
Base directory: /home/root
Base directory: /home/bing
Base directory: /home/data
Base directory: /home/logs
Base directory: /home/config
Base directory: /home/temp
Base directory: /home/cache
Base directory: /home/backup
Base directory: /home/admin
Base directory: /home/test
Base directory: /home/docs
Base directory: /home/images
Base directory: /home/videos
After UCPTransformer: 0.34
After UCPTransformer: 0.56
After UCPTransformer: 0.78
After UCPTransformer: 0.91
After UCPTransformer: 0.45
After UCPTransformer: 0.67
After UCPTransformer: 0.89
After UCPTransformer: 0.12
After UCPTransformer: 0.23
After UCPTransformer: 0.44
After UCPTransformer: 0.65
After UCPTransformer: 0.87
After UCPTransformer: 0.98
After UCPTransformer: 0.36
After UCPTransformer: 0.59
GOT | name|age|gender
GOT | id|score|grade
GOT | title|author|year
GOT | product|price|quantity
GOT | city|country|population
GOT | date|time|event
GOT | email|subject|body
GOT | username|password|role
GOT | color|shape|size
GOT | animal|sound|habitat
GOT | movie|genre|rating
GOT | food|calories|nutrition
GOT | sport|team|player
GOT | word|meaning|synonym
GOT | song|artist|album
Encoding set to UTF-8
Encoding set to ASCII
Encoding set to ISO-8859-1
Encoding set to GBK
Encoding set to UTF-16
Encoding set to Big5
Encoding set to Shift-JIS
Encoding set to EUC-KR
Encoding set to Windows-1252
Encoding set to CP437
Encoding set to UTF-32
Encoding set to KOI8-R
Encoding set to ISO-2022-JP
Encoding set to Base64
Encoding set to Hex
Problem: 87 vs. 92 on /home/user1/file1.txt
Problem: 76 vs. 81 on /var/log/syslog
Problem: 95 vs. 89 on /etc/passwd
Problem: 84 vs. 86 on /home/user2/file2.txt
Problem: 91 vs. 93 on /var/log/messages
Problem: 88 vs. 90 on /etc/hosts
Problem: 79 vs. 83 on /home/user3/file3.txt
Problem: 82 vs. 85 on /var/log/kern.log
Problem: 94 vs. 96 on /etc/shadow
Problem: 80 vs. 78 on /home/user4/file4.txt
Problem: 86 vs. 88 on /var/log/auth.log
Problem: 90 vs. 92 on /etc/fstab
Problem: 77 vs. 75 on /home/user5/file5.txt
Problem: 83 vs. 87 on /var/log/dmesg
Problem: 93 vs. 91 on /etc/crontab
DocDate regex failed to match against 5f3a7c9b
DocDate regex failed to match against 9d4e1e23
DocDate regex failed to match against 6b8da4f0
DocDate regex failed to match against 3c2f9a6d
DocDate regex failed to match against 0a1b4c8e
DocDate regex failed to match against 7e6f3d2a
DocDate regex failed to match against 4d5c2b18
DocDate regex failed to match against 1a3b9f7c
DocDate regex failed to match against 8e5d6c4a
DocDate regex failed to match against 2b4a1f9d
DocDate regex failed to match against 9f7c3d6b
DocDate regex failed to match against 6c4a8e5d
DocDate regex failed to match against 3d6b9f7c
DocDate regex failed to match against 0e5d2b4a
DocDate regex failed to match against 1f9d3c6b
Using properties file: config.properties
Using properties file: settings.properties
Using properties file: database.properties
Using properties file: user.properties
Using properties file: log4j.properties
Using properties file: app.properties
Using properties file: security.properties
Using properties file: network.properties
Using properties file: resources.properties
Using properties file: test.properties
Using properties file: backup.properties
Using properties file: custom.properties
Using properties file: system.properties
Using properties file: project.properties
Using properties file: environment.properties
Determined head (case 2) for 0.5 is: 0.25
Determined head (case 2) for 1.0 is: 0.5
Determined head (case 2) for 0.75 is: 0.375
Determined head (case 2) for 1.5 is: 0.75
Determined head (case 2) for 2.0 is: 1.0
Determined head (case 2) for 3.0 is: 1.5
Determined head (case 2) for 4.0 is: 2.0
Determined head (case 2) for 5.0 is: 2.5
Determined head (case 2) for 6.0 is: 3.0
Determined head (case 2) for 7.0 is: 3.5
Determined head (case 2) for 8.0 is: 4.0
Determined head (case 2) for 9.0 is: 4.5
Determined head (case 2) for -1.0 is: -0.5
Determined head (case 2) for -2.0 is: -1.0
Determined head (case 2) for -3.0 is: -1.5
##in Cross Validate, folds = 5
##in Cross Validate, folds = 10
##in Cross Validate, folds = 3
##in Cross Validate, folds = 7
##in Cross Validate, folds = 4
##in Cross Validate, folds = 8
##in Cross Validate, folds = 6
##in Cross Validate, folds = 9
##in Cross Validate, folds = 2
##in Cross Validate, folds = 12
##in Cross Validate, folds = 15
##in Cross Validate, folds = 11
##in Cross Validate, folds = 13
##in Cross Validate, folds = 14
##in Cross Validate, folds = 16
To visit: Paris
To visit: New York
To visit: Tokyo
To visit: Beijing
To visit: London
To visit: Sydney
To visit: Rome
To visit: Berlin
To visit: Cairo
To visit: Rio de Janeiro
To visit: Toronto
To visit: Seoul
To visit: Amsterdam
To visit: Istanbul
To visit: Mexico City
DBG Unlabeled Words: the:0.23;of:0.18;and:0.15;to:0.12;a:0.09
DBG Unlabeled Words: in:0.21;for:0.17;is:0.14;on:0.11;that:0.08
DBG Unlabeled Words: with:0.19;as:0.16;by:0.13;from:0.10;it:0.07
DBG Unlabeled Words: be:0.22;at:0.18;or:0.15;an:0.12;this:0.09
DBG Unlabeled Words: are:0.20;not:0.17;but:0.14;have:0.11;he:0.08
DBG Unlabeled Words: was:0.24;his:0.19;they:0.16;has:0.13;she:0.10
DBG Unlabeled Words: you:0.23;her:0.18;we:0.15;can:0.12;him:0.09
DBG Unlabeled Words: their:0.21;will:0.17;if:0.14;all:0.11;one:0.08
DBG Unlabeled Words: no:0.19;so:0.16;out:0.13;up:0.10;me:0.07
DBG Unlabeled Words: do:0.22;i'm:0.18;said:0.15;says:0.12;say:0.09
DBG Unlabeled Words: my:0.20;i've:0.17;i'll.:014;i'd.:011;i.:008
Using NEWGENE threshold: 0.5
Using NEWGENE threshold: 0.75
Using NEWGENE threshold: 0.25
Using NEWGENE threshold: 0.6
Using NEWGENE threshold: 0.8
Using NEWGENE threshold: 0.4
Using NEWGENE threshold: 0.7
Using NEWGENE threshold: 0.3
Using NEWGENE threshold: 0.9
Using NEWGENE threshold: 0.1
Using NEWGENE threshold: 0.55
Using NEWGENE threshold: 0.65
Using NEWGENE threshold: 0.45
Using NEWGENE threshold: 0.85
Using NEWGENE threshold: 0.35
Using multi word rules from /home/user/rules.txt
Using multi word rules from C:\Users\user\rules.csv
Using multi word rules from /opt/rules/rules.json
Using multi word rules from /var/log/rules/rules.log
Using multi word rules from /tmp/rules.tmp
Using multi word rules from https://example.com/rules.xml
Using multi word rules from /etc/rules.conf
Using multi word rules from /usr/local/bin/rules.sh
Using multi word rules from /dev/null
Using multi word rules from s3://bucket/rules.parquet
Using multi word rules from ftp://user:pass@host/rules.ini
Using multi word rules from file:///rules.dat
Using multi word rules from hdfs://namenode:port/rules.avro
Using multi word rules from smb://user:pass@server/share/rules.xlsx
Using multi word rules from ssh://user@host:/rules.md
Line 17 of data.csv is emptyUTF-8
Line 3 of report.txt is emptyUTF-8
Line 25 of config.ini is emptyUTF-8
Line 12 of index.html is emptyUTF-8
Line 8 of script.py is emptyUTF-8
Line 1 of readme.md is emptyUTF-8
Line 14 of log.txt is emptyUTF-8
Line 21 of main.java is emptyUTF-8
Line 5 of style.css is emptyUTF-8
Line 9 of test.json is emptyUTF-8
Line 18 of image.png is emptyUTF-8
Line 6 of music.mp3 is emptyUTF-8
Line 10 of video.mp4 is emptyUTF-8
Line 15 of document.docx is emptyUTF-8
Line 7 of presentation.pptx is emptyUTF-8
com.example.Foo : Skipping past whacked out header (0xdeadbeef)
org.apache.Bar : Skipping past whacked out header (0xbadcafe)
net.example.Baz : Skipping past whacked out header (0xfeedface)
com.example.Qux : Skipping past whacked out header (0xcafebabe)
org.apache.Quux : Skipping past whacked out header (0xdeadcode)
net.example.Corge : Skipping past whacked out header (0xbadc0de)
com.example.Grault : Skipping past whacked out header (0xfeedbeef)
org.apache.Garply : Skipping past whacked out header (0xcafed00d)
net.example.Waldo : Skipping past whacked out header (0xdeadbabe)
com.example.Fred : Skipping past whacked out header (0xbadfeed)
org.apache.Plugh : Skipping past whacked out header (0xfeedc0de)
net.example.Xyzzy : Skipping past whacked out header (0xdeadc0de)
com.example.Thud : Skipping past whacked out header (0xbadbad)
org.apache.FooBar : Skipping past whacked out header (0xfeedbad)
net.example.BazQux : Skipping past whacked out header (0xdeedbeef)
FALSE POSITIVE: m.toString() = "java.lang.NullPointerException"
FALSE POSITIVE: m.toString() = "java.lang.IllegalArgumentException: Invalid argument"
FALSE POSITIVE: m.toString() = "java.lang.ClassCastException: Cannot cast java.lang.String to java.lang.Integer"
FALSE POSITIVE: m.toString() = "java.lang.ArrayIndexOutOfBoundsException: Index 5 out of bounds for length 4"
FALSE POSITIVE: m.toString() = "java.lang.ArithmeticException: / by zero"
FALSE POSITIVE: m.toString() = "java.lang.NumberFormatException: For input string: \"abc\""
FALSE POSITIVE: m.toString() = "java.lang.OutOfMemoryError: Java heap space"
FALSE POSITIVE: m.toString() = "java.lang.StackOverflowError"
FALSE POSITIVE: m.toString() = "java.lang.UnsupportedOperationException: Not implemented"
FALSE POSITIVE: m.toString() = "java.lang.SecurityException: Access denied"
FALSE POSITIVE: m.toString() = "java.lang.NoSuchMethodError: No such method found"
FALSE POSITIVE: m.toString() = "java.lang.NoSuchFieldError: No such field found"
FALSE POSITIVE: m.toString() = "java.lang.NoClassDefFoundError: No class definition found"
FALSE POSITIVE: m.toString() = "java.lang.VerifyError: Verification failed"
FALSE POSITIVE: m.toString() = "java.lang.LinkageError: Linkage failed"
PERCENTAGE OF TRAIN: 0.6
PERCENTAGE OF TRAIN: 0.75
PERCENTAGE OF TRAIN: 0.9
PERCENTAGE OF TRAIN: 0.3
PERCENTAGE OF TRAIN: 0.45
PERCENTAGE OF TRAIN: 0.8
PERCENTAGE OF TRAIN: 0.25
PERCENTAGE OF TRAIN: 0.7
PERCENTAGE OF TRAIN: 0.5
PERCENTAGE OF TRAIN: 0.4
PERCENTAGE OF TRAIN: 0.85
PERCENTAGE OF TRAIN: 0.65
PERCENTAGE OF TRAIN: 0.35
PERCENTAGE OF TRAIN: 0.55
PERCENTAGE OF TRAIN: 0.95
retained 0
retained 1
retained 2
retained 3
retained 4
retained 5
retained 6
retained 7
retained 8
retained 9
retained 10
retained 11
retained 12
retained 13
retained 14
Sentence # 1 basic dependencies are:
Sentence # 5 basic dependencies are:
Sentence # 10 basic dependencies are:
Sentence # 15 basic dependencies are:
Sentence # 20 basic dependencies are:
Sentence # 25 basic dependencies are:
Sentence # 30 basic dependencies are:
Sentence # 35 basic dependencies are:
Sentence # 40 basic dependencies are:
Sentence # 45 basic dependencies are:
Sentence # 50 basic dependencies are:
Sentence # 55 basic dependencies are:
Sentence # 60 basic dependencies are:
Sentence # 65 basic dependencies are:
Sentence # 70 basic dependencies are:
combinationMode: 0
combinationMode: 1
combinationMode: 2
combinationMode: 3
combinationMode: 4
combinationMode: 5
combinationMode: 6
combinationMode: 7
combinationMode: 8
combinationMode: 9
combinationMode: 10
combinationMode: 11
combinationMode: 12
combinationMode: 13
combinationMode: 14
Collected collocations: [big, apple]
Collected collocations: [red, herring]
Collected collocations: [hot, dog]
Collected collocations: [black, hole]
Collected collocations: [green, thumb]
Collected collocations: [blue, moon]
Collected collocations: [white, lie]
Collected collocations: [cold, turkey]
Collected collocations: [dark, horse]
Collected collocations: [sweet, tooth]
Collected collocations: [high, tide]
Collected collocations: [hard, cheese]
Collected collocations: [dry, spell]
Collected collocations: [deep, sleep]
Collected collocations: [fast, food]
Using following SUTime rules: edu/stanford/nlp/models/sutime/english.sutime.txt
Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt
Using following SUTime rules: edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
Using following SUTime rules: edu/stanford/nlp/models/sutime/spanish.sutime.txt
Using following SUTime rules: edu/stanford/nlp/models/sutime/french.sutime.txt
Using following SUTime rules: edu/stanford/nlp/models/sutime/german.sutime.txt
Using following SUTime rules: edu/stanford/nlp/models/sutime/chinese.sutime.txt
Using following SUTime rules: edu/stanford/nlp/models/sutime/japanese.sutime.txt
Using following SUTime rules: edu/stanford/nlp/models/sutime/italian.sutime.txt
Using following SUTime rules: edu/stanford/nlp/models/sutime/portuguese.sutime.txt
Using following SUTime rules: edu/stanford/nlp/models/sutime/arabic.sutime.txt
Using following SUTime rules: edu/stanford/nlp/models/sutime/russian.sutime.txt
Using following SUTime rules: edu/stanford/nlp/models/sutime/dutch.sutime.txt
Using following SUTime rules: edu/stanford/nlp/models/sutime/swedish.sutime.txt
Using following SUTime rules: edu/stanford/nlp/models/sutime/custom.sutime.txt
checking in kid
checking in kid 1
checking in kid 2
checking in kid 3
checking in kid 4
checking in kid 5
checking in kid 6
checking in kid 7
checking in kid 8
checking in kid 9
checking in kid 10
checking in kid A
checking in kid B
checking in kid C
checking in kid D
Found units: 0x12345678
Found units: 0x87654321
Found units: 0xABCDEF01
Found units: 0x10FEDCBA
Found units: 0x55555555
Found units: 0xAAAAAAAA
Found units: 0xDEADBEEF
Found units: 0xFACEB00C
Found units: 0xCAFEBA11
Found units: 0xBADDECAF
Found units: 0x13371337
Found units: 0x42424242
Found units: 0x80000000
Found units: 0xFFFFFFFF
Found units: 0x00000000
# minFeatureThresh = 0.1
# minFeatureThresh = 0.25
# minFeatureThresh = 0.5
# minFeatureThresh = 0.75
# minFeatureThresh = 0.9
# minFeatureThresh = 0.05
# minFeatureThresh = 0.15
# minFeatureThresh = 0.35
# minFeatureThresh = 0.45
# minFeatureThresh = 0.55
# minFeatureThresh = 0.65
# minFeatureThresh = 0.85
# minFeatureThresh = 0.95
# minFeatureThresh = 0.2
# minFeatureThresh = 0.3
#words: 12
#words: 27
#words: 0
#words: 9
#words: 34
#words: 15
#words: 21
#words: 6
#words: 18
#words: 3
#words: 29
#words: 10
#words: 24
#words: 7
#words: 31
vpContainsParticiple examining kid
vpContainsParticiple examining kids
vpContainsParticiple examining a kid
vpContainsParticiple examining the kid
vpContainsParticiple examining some kids
vpContainsParticiple examining any kid
vpContainsParticiple examining no kid
vpContainsParticiple examining one kid
vpContainsParticiple examining two kids
vpContainsParticiple examining three kids
vpContainsParticiple examining four kids
vpContainsParticiple examining five kids
vpContainsParticiple examining six kids
vpContainsParticiple examining seven kids
vpContainsParticiple examining eight kids
undirected nodes btw 1 and 3 is 5
undirected nodes btw 1 and 3 is 7
undirected nodes btw 1 and 3 is 4
undirected nodes btw 1 and 3 is 6
undirected nodes btw 1 and 3 is 8
undirected nodes btw 1 and 3 is 9
undirected nodes btw 1 and 3 is 10
undirected nodes btw 1 and 3 is 11
undirected nodes btw 1 and 3 is 12
undirected nodes btw 1 and 3 is 13
undirected nodes btw 1 and 3 is 14
undirected nodes btw 1 and 3 is 15
undirected nodes btw 1 and 3 is 16
undirected nodes btw 1 and 3 is 17
undirected nodes btw 1 and 3 is 18
QN iterations per batch: 5
QN iterations per batch: 10
QN iterations per batch: 3
QN iterations per batch: 7
QN iterations per batch: 4
QN iterations per batch: 8
QN iterations per batch: 6
QN iterations per batch: 9
QN iterations per batch: 2
QN iterations per batch: 11
QN iterations per batch: 12
QN iterations per batch: 1
QN iterations per batch: 13
QN iterations per batch: 14
QN iterations per batch: 15
Graph is [V={1, 2, 3}, E={(1,2), (2,3), (3,1)}]
Graph is [V={a, b, c, d}, E={(a,b), (b,c), (c,d), (d,a)}]
Graph is [V={x, y, z}, E={(x,y, 5), (y,z, 3), (z,x, 4)}]
Graph is [V={p, q, r, s}, E={(p,q), (q,r), (r,s)}]
Graph is [V={m, n, o}, E={(m,n, 2), (n,o, 4), (o,m, 6)}]
Graph is [V={u, v, w}, E={(u,v), (v,w), (w,u)}]
Graph is [V={h, i, j, k}, E={(h,i, 7), (i,j, 8), (j,k, 9), (k,h, 10)}]
Graph is [V={e, f, g}, E={(e,f), (f,g), (g,e)}]
Graph is [V={t1, t2, t3}, E={(t1,t2, 11), (t2,t3, 12), (t3,t1, 13)}]
Graph is [V={s1, s2, s3}, E={(s1,s2), (s2,s3), (s3,s1)}]
Graph is [V={r1, r2, r3}, E={(r1,r2, 14), (r2,r3, 15), (r3,r1, 16)}]
Graph is [V={q1, q2}, E={(q1,q2)}]
Graph is [V={p1}, E={}]
Graph is [V={}, E={}]
# rareWordThresh = 0.01
# rareWordThresh = 0.05
# rareWordThresh = 0.1
# rareWordThresh = 0.2
# rareWordThresh = 0.3
# rareWordThresh = 0.4
# rareWordThresh = 0.5
# rareWordThresh = 0.6
# rareWordThresh = 0.7
# rareWordThresh = 0.8
# rareWordThresh = 0.9
# rareWordThresh = 1.0
# rareWordThresh = 1.5
# rareWordThresh = 2.0
# rareWordThresh = 2.5
set delta to smth deltaL=0.01
set delta to smth deltaL=0.05
set delta to smth deltaL=-0.02
set delta to smth deltaL=0.1
set delta to smth deltaL=0.03
set delta to smth deltaL=-0.04
set delta to smth deltaL=0.07
set delta to smth deltaL=-0.01
set delta to smth deltaL=0.08
set delta to smth deltaL=-0.03
set delta to smth deltaL=0.02
set delta to smth deltaL=-0.05
set delta to smth deltaL=0.06
set delta to smth deltaL=-0.06
set delta to smth deltaL=0.04
Using mention detector type: mdName
Using mention detector type: StanfordCoreNLP
Using mention detector type: Spacy
Using mention detector type: OpenNLP
Using mention detector type: AllenNLP
Using mention detector type: Flair
Using mention detector type: Polyglot
Using mention detector type: NLTK
Using mention detector type: TextBlob
Using mention detector type: Gensim
Using mention detector type: FastText
Using mention detector type: BERT
Using mention detector type: RoBERTa
Using mention detector type: XLNet
Using mention detector type: ALBERT
Using mention detector type: TransformerXL
Using unknown percent vector for Chinese words: [0.1]
Using unknown percent vector for Chinese words: [0.05]
Using unknown percent vector for Chinese words: [0.2]
Using unknown percent vector for Chinese words: [0.15]
Using unknown percent vector for Chinese words: [0.25]
Using unknown percent vector for Chinese words: [0.3]
Using unknown percent vector for Chinese words: [0.35]
Using unknown percent vector for Chinese words: [0.4]
Using unknown percent vector for Chinese words: [0.45]
Using unknown percent vector for Chinese words: [0.5]
Using unknown percent vector for Chinese words: [0.55]
Using unknown percent vector for Chinese words: [0.6]
Using unknown percent vector for Chinese words: [0.65]
Using unknown percent vector for Chinese words: [0.7]
Using unknown percent vector for Chinese words: [0.75]
Discovered better point: -0.34
Discovered better point: -0.27
Discovered better point: -0.41
Discovered better point: -0.29
Discovered better point: -0.36
Discovered better point: -0.32
Discovered better point: -0.39
Discovered better point: -0.31
Discovered better point: -0.35
Discovered better point: -0.28
Discovered better point: -0.37
Discovered better point: -0.33
Discovered better point: -0.38
Discovered better point: -0.30
Discovered better point: -0.40
##time elapsed: 12 milliseconds.
##time elapsed: 34 milliseconds.
##time elapsed: 56 milliseconds.
##time elapsed: 78 milliseconds.
##time elapsed: 90 milliseconds.
##time elapsed: 102 milliseconds.
##time elapsed: 114 milliseconds.
##time elapsed: 136 milliseconds.
##time elapsed: 158 milliseconds.
##time elapsed: 180 milliseconds.
##time elapsed: 202 milliseconds.
##time elapsed: 224 milliseconds.
##time elapsed: 246 milliseconds.
##time elapsed: 268 milliseconds.
##time elapsed: 290 milliseconds.
No merge: c is ' 0 '
No merge: c is ' 1 '
No merge: c is ' a '
No merge: c is ' b '
No merge: c is ' x '
No merge: c is ' y '
No merge: c is ' z '
No merge: c is ' * '
No merge: c is ' ? '
No merge: c is ' ! '
No merge: c is ' @ '
No merge: c is ' # '
No merge: c is ' $ '
No merge: c is ' % '
No merge: c is ' & '
Checking Document 4567 span ( 12 , 34 )
Checking Document 8910 span ( 56 , 78 )
Checking Document 1123 span ( 90 , 112 )
Checking Document 4456 span ( 134 , 156 )
Checking Document 7789 span ( 178 , 200 )
Checking Document 9900 span ( 222 , 244 )
Checking Document 1234 span ( 266 , 288 )
Checking Document 5678 span ( 310 , 332 )
Checking Document 9012 span ( 354 , 376 )
Checking Document 3456 span ( 398 , 420 )
Checking Document 7890 span ( 442 , 464 )
Checking Document 1122 span ( 486 , 508 )
Checking Document 3344 span ( 530 , 552 )
Checking Document 5566 span ( 574 , 596 )
Checking Document 7788 span ( 618 , 640 )
# curWordMinFeatureThresh = 0.1
# curWordMinFeatureThresh = 0.05
# curWordMinFeatureThresh = 0.2
# curWordMinFeatureThresh = 0.15
# curWordMinFeatureThresh = 0.25
# curWordMinFeatureThresh = 0.3
# curWordMinFeatureThresh = 0.35
# curWordMinFeatureThresh = 0.4
# curWordMinFeatureThresh = 0.45
# curWordMinFeatureThresh = 0.5
# curWordMinFeatureThresh = 0.55
# curWordMinFeatureThresh = 0.6
# curWordMinFeatureThresh = 0.65
# curWordMinFeatureThresh = 0.7
# curWordMinFeatureThresh = 0.75
Non-zero parameters: 12 / 100 ( 12.0 %)
Non-zero parameters: 23 / 150 ( 15.3 %)
Non-zero parameters: 34 / 200 ( 17.0 %)
Non-zero parameters: 45 / 250 ( 18.0 %)
Non-zero parameters: 56 / 300 ( 18.7 %)
Non-zero parameters: 67 / 350 ( 19.1 %)
Non-zero parameters: 78 / 400 ( 19.5 %)
Non-zero parameters: 89 / 450 ( 19.8 %)
Non-zero parameters: 100 / 500 ( 20.0 %)
Non-zero parameters: 111 / 550 ( 20.2 %)
Non-zero parameters: 122 / 600 ( 20.3 %)
Non-zero parameters: 133 / 650 ( 20.5 %)
Non-zero parameters: 144 / 700 ( 20.6 %)
Non-zero parameters: 155 / 750 ( 20.7 %)
Non-zero parameters: 166 / 800 ( 20.8 %)
Parsing of sentence ran out of memory (length= 23 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 47 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 12 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 36 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 29 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 41 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 18 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 32 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 25 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 39 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 15 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 28 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 21 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 34 ). Will ignore and try to continue.
Parsing of sentence ran out of memory (length= 17 ). Will ignore and try to continue.
Binary score 1010 : 1111
Binary score 0101 : 1001
Binary score 1100 : 0011
Binary score 0110 : 0100
Binary score 1001 : 1010
Binary score 0011 : 1100
Binary score 1110 : 0111
Binary score 0001 : 0000
Binary score 1011 : 1000
Binary score 0100 : 0010
Binary score 1101 : 1110
Binary score 0111 : 0101
Binary score 1000 : 1011
Binary score 0010 : 1101
Binary score 1111 : 0001
Warning: unparseable date 2023-10-32
Warning: unparseable date 2023-02-29
Warning: unparseable date 2023-13-01
Warning: unparseable date 2023-04-31
Warning: unparseable date 2023-06-31
Warning: unparseable date 2023-09-31
Warning: unparseable date 2023-11-31
Warning: unparseable date 2023-01-00
Warning: unparseable date 2023-03-00
Warning: unparseable date 2023-05-00
Warning: unparseable date 2023-07-00
Warning: unparseable date 2023-08-00
Warning: unparseable date 2023-10-00
Warning: unparseable date 2023-12-00
Warning: unparseable date 2023/10/26
-> average Score: 78.5
-> average Score: 63.2
-> average Score: 85.9
-> average Score: 72.4
-> average Score: 90.1
-> average Score: 66.7
-> average Score: 81.3
-> average Score: 74.6
-> average Score: 69.8
-> average Score: 87.4
-> average Score: 76.2
-> average Score: 82.7
-> average Score: 79.3
-> average Score: 68.5
-> average Score: 84.6
0 . Compacted grammar for NP from 12 arcs to 4 arcs.
1 . Compacted grammar for VP from 8 arcs to 3 arcs.
2 . Compacted grammar for S from 15 arcs to 5 arcs.
3 . Compacted grammar for PP from 6 arcs to 2 arcs.
4 . Compacted grammar for ADJP from 9 arcs to 3 arcs.
5 . Compacted grammar for ADVP from 7 arcs to 2 arcs.
6 . Compacted grammar for NNP from 10 arcs to 4 arcs.
7 . Compacted grammar for VBD from 11 arcs to 4 arcs.
9 . Compacted grammar for JJ from 8 arcs to 3 arcs.
10 . Compacted grammar for RB from 6 arcs to 2 arcs.
11 . Compacted grammar for IN from 7 arcs to 2 arcs.
12 . Compacted grammar for NN from 9 arcs to 3 arcs.
13 . Compacted grammar for PRP from 6 arcs to 2 arcs.
xSize [num Phi templates] = 10 ; ySize [num classes] = 5
xSize [num Phi templates] = 12 ; ySize [num classes] = 7
xSize [num Phi templates] = 8 ; ySize [num classes] = 4
xSize [num Phi templates] = 9 ; ySize [num classes] = 6
xSize [num Phi templates] = 11 ; ySize [num classes] = 8
xSize [num Phi templates] = 7 ; ySize [num classes] = 3
xSize [num Phi templates] = 13 ; ySize [num classes] = 9
xSize [num Phi templates] = 6 ; ySize [num classes] = 2
xSize [num Phi templates] = 14 ; ySize [num classes] = 10
xSize [num Phi templates] = 5 ; ySize [num classes] = 1
xSize [num Phi templates] = 15 ; ySize [num classes] = 11
xSize [num Phi templates] = 4 ; ySize [num classes] = 0
xSize [num Phi templates] = 16 ; ySize [num classes] = 12
xSize [num Phi templates] = 3 ; ySize [num classes] = -1
xSize [num Phi templates] = 17 ; ySize [num classes] = 13
Unary transform 0x7f8a
Unary transform 0x9c3b
Unary transform 0x4d6e
Unary transform 0x1a2f
Unary transform 0x8b4c
Unary transform 0x6e9d
Unary transform 0x3f7e
Unary transform 0x5c1b
Unary transform 0x2d4a
Unary transform 0xb9fd
Unary transform 0xec6e
Unary transform 0xcf3b
Unary transform 0xad8a
Unary transform 0xfe5d
Unary transform 0xd9ec
WARNING: CollinsDependency.extractFromTree() could not find root for: (S (NP (DT The) (NN dog)) (VP (VBD barked)))
WARNING: CollinsDependency.extractFromTree() could not find root for: (NP (PRP$ My) (NN brother) (CC and) (PRP I))
WARNING: CollinsDependency.extractFromTree() could not find root for: (VP (VBZ likes) (NP (NN pizza)))
WARNING: CollinsDependency.extractFromTree() could not find root for: (ADJP (JJ happy) (CC but) (JJ tired))
WARNING: CollinsDependency.extractFromTree() could not find root for: (PP (IN from) (NP (DT the) (NN store)))
WARNING: CollinsDependency.extractFromTree() could not find root for: (SINV (VBP Do) (NP you) (VP (VB know) (NP him)))
WARNING: CollinsDependency.extractFromTree() could not find root for: (SBARQ (WHNP (WP What)) (SQ (VBP are) (NP you) (VP doing)))
WARNING: CollinsDependency.extractFromTree() could not find root for: (FRAG (RB No) (. .))
WARNING: CollinsDependency.extractFromTree() could not find root for: (ADVP (RB very) (RB well))
WARNING: CollinsDependency.extractFromTree() could not find root for: (INTJ (UH Wow))
WARNING: CollinsDependency.extractFromTree() could not find root for: (QP ($ $) (CD 100))
WARNING: CollinsDependency.extractFromTree() could not find root for: ()
WARNING: CollinsDependency.extractFromTree() could not find root for: null
WARNING: CollinsDependency.extractFromTree() could not find root for: [Error]
WARNING: CollinsDependency.extractFromTree() could not find root for: undefined
Parsing of sentence failed. Will ignore and continue: The quick brown fox jumps over the lazy dog.
Parsing of sentence failed. Will ignore and continue: She sells seashells by the seashore.
Parsing of sentence failed. Will ignore and continue: How much wood would a woodchuck chuck if a woodchuck could chuck wood?
Parsing of sentence failed. Will ignore and continue: Peter Piper picked a peck of pickled peppers.
Parsing of sentence failed. Will ignore and continue: I saw Susie sitting in a shoeshine shop.
Parsing of sentence failed. Will ignore and continue: He threw three free throws.
Parsing of sentence failed. Will ignore and continue: Betty bought a bit of butter but the butter was bitter.
Parsing of sentence failed. Will ignore and continue: A skunk sat on a stump and thunk the stump stunk, but the stump thunk the skunk stunk.
Parsing of sentence failed. Will ignore and continue: How can a clam cram in a clean cream can?
Parsing of sentence failed. Will ignore and continue: Fuzzy Wuzzy was a bear. Fuzzy Wuzzy had no hair. Fuzzy Wuzzy wasn't fuzzy, was he?
Parsing of sentence failed. Will ignore and continue: Can you can a can as a canner can can a can?
Parsing of sentence failed. Will ignore and continue: Six sick hicks nick six slick bricks with picks and sticks.
Parsing of sentence failed. Will ignore and continue: I wish to wish the wish you wish to wish, but if you wish the wish the witch wishes, I won't wish the wish you wish to wish.
Parsing of sentence failed. Will ignore and continue: If two witches were watching two watches, which witch would watch which watch?
Parsing of sentence failed. Will ignore and continue: I slit the sheet, the sheet I slit, and on the slitted sheet I sit.
# text = Hello world
# text = This is a test
# text = How are you?
# text = I like Bing
# text = What is your name?
# text = Today is Friday
# text = Happy Halloween
# text = This is a long sentence with many words and punctuation marks.
# text = 42 is the answer to everything
# text = Lorem ipsum dolor sit amet
# text = Python is a programming language
# text = Bing is better than Google
# text = This is a question?
# text = This is an exclamation!
# text = This is a quote "To be or not to be"
Processing file C:\Users\Alice\Documents\report.docx ... writing to report_final.pdf
Processing file /home/bob/Pictures/image.jpg ... writing to image_edited.png
Processing file D:\Music\song.mp3 ... writing to song_converted.wav
Processing file /var/log/syslog ... writing to syslog_backup.txt
Processing file E:\Games\save.dat ... writing to save_copy.dat
Processing file /tmp/test.txt ... writing to test_output.txt
Processing file C:\Windows\system32\config ... writing to config_backup.bin
Processing file /usr/local/bin/hello ... writing to hello_copy
Processing file F:\Videos\movie.mkv ... writing to movie_compressed.mp4
Processing file /etc/passwd ... writing to passwd_copy.txt
Processing file G:\Books\book.pdf ... writing to book_extracted.txt
Processing file /opt/java/bin/java ... writing to java_copy
Processing file H:\Data\data.csv ... writing to data_analyzed.xlsx
Processing file /root/.bashrc ... writing to bashrc_modified.sh
Processing file I:\Software\setup.exe ... writing to setup_log.txt
Number of quotes + unicode - single : 3
Number of quotes + unicode - single : 7
Number of quotes + unicode - single : 2
Number of quotes + unicode - single : 5
Number of quotes + unicode - single : 4
Number of quotes + unicode - single : 6
Number of quotes + unicode - single : 1
Number of quotes + unicode - single : 8
Number of quotes + unicode - single : 9
Number of quotes + unicode - single : 10
Number of quotes + unicode - single : 11
Number of quotes + unicode - single : 12
Number of quotes + unicode - single : 13
Number of quotes + unicode - single : 14
Number of quotes + unicode - single : 15
Running 4 threads of tagger 1
Running 8 threads of tagger 1
Running 16 threads of tagger 1
Running 2 threads of tagger 1
Running 12 threads of tagger 1
Running 6 threads of tagger 1
Running 10 threads of tagger 1
Running 20 threads of tagger 1
Running 3 threads of tagger 1
Running 9 threads of tagger 1
Running 18 threads of tagger 1
Running 5 threads of tagger 1
Running 15 threads of tagger 1
Running 7 threads of tagger 1
Running 14 threads of tagger 1
Total time for StanfordCoreNLP pipeline: 1.23 sec.
Total time for StanfordCoreNLP pipeline: 0.87 sec.
Total time for StanfordCoreNLP pipeline: 2.34 sec.
Total time for StanfordCoreNLP pipeline: 1.56 sec.
Total time for StanfordCoreNLP pipeline: 1.78 sec.
Total time for StanfordCoreNLP pipeline: 0.65 sec.
Total time for StanfordCoreNLP pipeline: 1.98 sec.
Total time for StanfordCoreNLP pipeline: 1.45 sec.
Total time for StanfordCoreNLP pipeline: 2.12 sec.
Total time for StanfordCoreNLP pipeline: 0.94 sec.
Total time for StanfordCoreNLP pipeline: 1.67 sec.
Total time for StanfordCoreNLP pipeline: 1.89 sec.
Total time for StanfordCoreNLP pipeline: 2.56 sec.
Total time for StanfordCoreNLP pipeline: 0.76 sec.
Total time for StanfordCoreNLP pipeline: 1.34 sec.
Couldn't instantiate GrammarCompactor: java.lang.NullPointerException
Couldn't instantiate GrammarCompactor: java.lang.ClassNotFoundException
Couldn't instantiate GrammarCompactor: java.lang.IllegalArgumentException
Couldn't instantiate GrammarCompactor: java.io.IOException
Couldn't instantiate GrammarCompactor: java.lang.OutOfMemoryError
Couldn't instantiate GrammarCompactor: java.lang.SecurityException
Couldn't instantiate GrammarCompactor: java.lang.NoSuchMethodError
Couldn't instantiate GrammarCompactor: java.lang.UnsupportedOperationException
Couldn't instantiate GrammarCompactor: java.lang.NoClassDefFoundError
Couldn't instantiate GrammarCompactor: java.lang.StackOverflowError
Couldn't instantiate GrammarCompactor: java.lang.IncompatibleClassChangeError
Couldn't instantiate GrammarCompactor: java.lang.LinkageError
Couldn't instantiate GrammarCompactor: java.lang.VerifyError
Couldn't instantiate GrammarCompactor: java.lang.InternalError
Couldn't instantiate GrammarCompactor: java.lang.ExceptionInInitializerError
Sighan2005DocRandW: using normalization file norm_table.txt UTF-8
Sighan2005DocRandW: using normalization file norm_table_1.csv UTF-8
Sighan2005DocRandW: using normalization file norm_table_2.xls UTF-8
Sighan2005DocRandW: using normalization file norm_table_3.xlsx UTF-8
Sighan2005DocRandW: using normalization file norm_table_4.xml UTF-8
Sighan2005DocRandW: using normalization file norm_table_5.json UTF-8
Sighan2005DocRandW: using normalization file norm_table_6.yaml UTF-8
Sighan2005DocRandW: using normalization file norm_table_7.ini UTF-8
Sighan2005DocRandW: using normalization file norm_table_8.conf UTF-8
Sighan2005DocRandW: using normalization file norm_table_9.properties UTF-8
Sighan2005DocRandW: using normalization file norm_table_a.dat UTF-8
Sighan2005DocRandW: using normalization file norm_table_b.bin UTF-8
Sighan2005DocRandW: using normalization file norm_table_c.hex UTF-8
Sighan2005DocRandW: using normalization file norm_table_d.log UTF-8
Sighan2005DocRandW: using normalization file norm_table_e.bak UTF-8
Error loading | https://www.bing.com/ |
Error loading | http://www.example.com/ |
Error loading | ftp://ftp.microsoft.com/ |
Error loading | https://en.wikipedia.org/wiki/Main_Page |
Error loading | https://www.amazon.com/ |
Error loading | https://www.youtube.com/watch?v=12345678 |
Error loading | https://www.facebook.com/ |
Error loading | https://www.instagram.com/p/ABCDEFGH/ |
Error loading | https://www.reddit.com/r/AskReddit/ |
Error loading | https://www.netflix.com/title/98765432 |
Error loading | https://www.spotify.com/us/ |
Error loading | https://www.nytimes.com/2023/10/28/world/europe/brexit-deal.html |
Error loading | https://www.nasa.gov/image-feature/goddard/2023/hubble-captures-stunning-spiral-galaxy |
Error loading | https://www.imdb.com/title/tt1234567/ |
Error loading | https://www.google.com/search?q=simulated+logs |
INFO: flags.usePk=false | building NonDict2 from /home/user/data.csv
INFO: flags.usePk=false | building NonDict2 from /var/log/messages
INFO: flags.usePk=false | building NonDict2 from /tmp/test.txt
INFO: flags.usePk=false | building NonDict2 from /etc/passwd
INFO: flags.usePk=false | building NonDict2 from /opt/app/config.json
INFO: flags.usePk=false | building NonDict2 from /dev/null
INFO: flags.usePk=false | building NonDict2 from /usr/local/bin/script.sh
INFO: flags.usePk=false | building NonDict2 from /mnt/backup/archive.zip
INFO: flags.usePk=false | building NonDict2 from /root/.bashrc
INFO: flags.usePk=false | building NonDict2 from /media/cdrom/image.iso
INFO: flags.usePk=false | building NonDict2 from /proc/cpuinfo
INFO: flags.usePk=false | building NonDict2 from /sys/devices/system/node/node0/meminfo
INFO: flags.usePk=false | building NonDict2 from /run/lock/lockfile
INFO: flags.usePk=false | building NonDict2 from /srv/http/index.html
INFO: flags.usePk=false | building NonDict2 from /lib/modules/5.10.0-9-amd64/kernel/drivers/net/ethernet/intel/e1000e/e1000e.ko
pearson: 0.95
pearson: -0.67
pearson: 0.12
pearson: 0.76
pearson: -0.34
pearson: 0.43
pearson: -0.89
pearson: 0.27
pearson: -0.51
pearson: 0.64
pearson: -0.18
pearson: 0.38
pearson: -0.72
pearson: 0.49
pearson: -0.06
Number of trees in one batch: 32
Number of trees in one batch: 64
Number of trees in one batch: 128
Number of trees in one batch: 256
Number of trees in one batch: 512
Number of trees in one batch: 16
Number of trees in one batch: 48
Number of trees in one batch: 96
Number of trees in one batch: 192
Number of trees in one batch: 384
Number of trees in one batch: 24
Number of trees in one batch: 72
Number of trees in one batch: 144
Number of trees in one batch: 288
Number of trees in one batch: 576
after adapting, weights size= 256
after adapting, weights size= 512
after adapting, weights size= 1024
after adapting, weights size= 2048
after adapting, weights size= 4096
after adapting, weights size= 8192
after adapting, weights size= 16384
after adapting, weights size= 32768
after adapting, weights size= 65536
after adapting, weights size= 131072
after adapting, weights size= 262144
after adapting, weights size= 524288
after adapting, weights size= 1048576
after adapting, weights size= 2097152
after adapting, weights size= 4194304
The sum of all empirical expectations is 0.56
The sum of all empirical expectations is 1.23
The sum of all empirical expectations is -0.34
The sum of all empirical expectations is 2.78
The sum of all empirical expectations is -1.45
The sum of all empirical expectations is 0.89
The sum of all empirical expectations is 1.67
The sum of all empirical expectations is -2.56
The sum of all empirical expectations is 3.45
The sum of all empirical expectations is -0.67
The sum of all empirical expectations is 1.34
The sum of all empirical expectations is -1.23
The sum of all empirical expectations is 2.34
The sum of all empirical expectations is -3.45
The sum of all empirical expectations is 4.56
Left norm (0.5) 0.25
Left norm (1.0) 1.0
Left norm (0.8) 0.64
Left norm (0.2) 0.04
Left norm (0.6) 0.36
Left norm (0.9) 0.81
Left norm (0.3) 0.09
Left norm (0.7) 0.49
Left norm (1.2) 1.44
Left norm (0.4) 0.16
Left norm (1.1) 1.21
Left norm (1.3) 1.69
Left norm (1.4) 1.96
Left norm (1.5) 2.25
Left norm (1.6) 2.56
Tuning selected smoothUnseen 0.3 smoothSeen 0.7 at 0.95
Tuning selected smoothUnseen 0.4 smoothSeen 0.6 at 0.92
Tuning selected smoothUnseen 0.5 smoothSeen 0.5 at 0.9
Tuning selected smoothUnseen 0.6 smoothSeen 0.4 at 0.88
Tuning selected smoothUnseen 0.7 smoothSeen 0.3 at 0.85
Tuning selected smoothUnseen 0.8 smoothSeen 0.2 at 0.83
Tuning selected smoothUnseen 0.9 smoothSeen 0.1 at 0.8
Tuning selected smoothUnseen 1.0 smoothSeen 0.0 at 0.78
Tuning selected smoothUnseen 0.2 smoothSeen 0.8 at 0.97
Tuning selected smoothUnseen 0.1 smoothSeen 0.9 at 1.0
Tuning selected smoothUnseen 0.25 smoothSeen 0.75 at 0.96
Tuning selected smoothUnseen 0.35 smoothSeen 0.65 at 0.93
Tuning selected smoothUnseen 0.45 smoothSeen 0.55 at 0.91
Tuning selected smoothUnseen 0.55 smoothSeen 0.45 at 0.89
Tuning selected smoothUnseen 0.65 smoothSeen 0.35 at 0.86
Finished loading pipeline. Current memory usage: 123 mb
Finished loading pipeline. Current memory usage: 156 mb
Finished loading pipeline. Current memory usage: 98 mb
Finished loading pipeline. Current memory usage: 176 mb
Finished loading pipeline. Current memory usage: 142 mb
Finished loading pipeline. Current memory usage: 105 mb
Finished loading pipeline. Current memory usage: 189 mb
Finished loading pipeline. Current memory usage: 131 mb
Finished loading pipeline. Current memory usage: 112 mb
Finished loading pipeline. Current memory usage: 167 mb
Finished loading pipeline. Current memory usage: 145 mb
Finished loading pipeline. Current memory usage: 102 mb
Finished loading pipeline. Current memory usage: 174 mb
Finished loading pipeline. Current memory usage: 128 mb
Finished loading pipeline. Current memory usage: 115 mb
Parse exception while reading config.json
Parse exception while reading data.csv
Parse exception while reading report.docx
Parse exception while reading image.png
Parse exception while reading log.txt
Parse exception while reading index.html
Parse exception while reading script.py
Parse exception while reading video.mp4
Parse exception while reading audio.mp3
Parse exception while reading archive.zip
Parse exception while reading settings.ini
Parse exception while reading database.db
Parse exception while reading readme.md
Parse exception while reading license.txt
Parse exception while reading code.java
determineHead for t.value()=0
determineHead for t.value()=1
determineHead for t.value()=2
determineHead for t.value()=3
determineHead for t.value()=4
determineHead for t.value()=5
determineHead for t.value()=6
determineHead for t.value()=7
determineHead for t.value()=8
determineHead for t.value()=9
determineHead for t.value()=10
determineHead for t.value()=-1
determineHead for t.value()=-2
determineHead for t.value()=-3
determineHead for t.value()=-4
This clique for tree: 12
This clique for tree: 27
This clique for tree: 9
This clique for tree: 18
This clique for tree: 15
This clique for tree: 24
This clique for tree: 21
This clique for tree: 30
This clique for tree: 6
This clique for tree: 33
This clique for tree: 3
This clique for tree: 36
This clique for tree: 0
This clique for tree: 39
This clique for tree: 42
done. Output: [Sentence #1 (5 tokens): This is a test., Sentence #2 (4 tokens): It works well.]
done. Output: [Sentence #1 (6 tokens): How are you today?, Sentence #2 (3 tokens): I'm fine.]
done. Output: [Sentence #1 (7 tokens): The weather is nice., Sentence #2 (5 tokens): Let's go outside.]
done. Output: [Sentence #1 (8 tokens): She loves reading books., Sentence #2 (6 tokens): He prefers watching movies.]
done. Output: [Sentence #1 (9 tokens): They are going to the park., Sentence #2 (7 tokens): We can join them later.]
done. Output: [Sentence #1 (10 tokens): He bought a new car yesterday., Sentence #2 (8 tokens): She was very surprised by it.]
done. Output: [Sentence #1 (11 tokens): I have a lot of homework to do., Sentence #2 (9 tokens): You should help me with it.]
done. Output: [Sentence #1 (12 tokens): She has a beautiful voice and sings well., Sentence #2 (10 tokens): He plays the guitar and writes songs.]
done. Output: [Sentence #1 (13 tokens): They have been married for ten years now., Sentence #2 (11 tokens): They have two children and a dog.]
done. Output: [Sentence #1 (14 tokens): He is studying computer science at the university., Sentence #2 (12 tokens): She is working as a software engineer.]
done. Output: [Sentence #1 (15 tokens): She likes to travel around the world and learn new things., Sentence #2 (13 tokens): He enjoys staying at home and playing games.]
done. Output: [Sentence #1 (16 tokens): He is a famous actor and has many fans., Sentence #2 (14 tokens): She is a journalist and interviews celebrities.]
done. Output: [Sentence #1 (17 tokens): She is a vegetarian and cares about animal rights., Sentence #2 (15 tokens): He is a meat lover and likes to barbecue.]
done. Output: [Sentence #1 (18 tokens): He is a doctor and saves many lives every day., Sentence #2 (16 tokens): She is a teacher and educates many students every day.]
done. Output: [Sentence #1 (19 tokens): She is a lawyer and defends her clients in court., Sentence #2 (17 tokens): He is a policeman and catches criminals on the street.]
#wordTagPairs: 12
#wordTagPairs: 9
#wordTagPairs: 15
#wordTagPairs: 11
#wordTagPairs: 10
#wordTagPairs: 13
#wordTagPairs: 14
#wordTagPairs: 8
#wordTagPairs: 7
#wordTagPairs: 16
#wordTagPairs: 6
#wordTagPairs: 5
#wordTagPairs: 4
#wordTagPairs: 3
#wordTagPairs: 17
Using unknown word vector for numbers: [0.5, 0.3, 0.1]
Using unknown word vector for numbers: [0.2, 0.4, 0.6]
Using unknown word vector for numbers: [0.7, 0.8, 0.9]
Using unknown word vector for numbers: [0.1, 0.2, 0.3]
Using unknown word vector for numbers: [0.9, 0.7, 0.5]
Using unknown word vector for numbers: [0.3, 0.6, 0.9]
Using unknown word vector for numbers: [0.4, 0.5, 0.7]
Using unknown word vector for numbers: [0.8, 0.6, 0.4]
Using unknown word vector for numbers: [0.6, 0.9, 0.3]
Using unknown word vector for numbers: [0.5, 0.7, 0.8]
Using unknown word vector for numbers: [0.4, 0.8, 0.2]
Using unknown word vector for numbers: [0.3, 0.5, 0.6]
Using unknown word vector for numbers: [0.7, 0.4, 0.1]
Using unknown word vector for numbers: [0.6, 0.3, 0.8]
Using unknown word vector for numbers: [0.8, 0.1, 0.4]
Positive Words: the;0.23;and;0.19;good;0.15;great;0.12;happy;0.09
Positive Words: a;0.21;to;0.18;nice;0.14;well;0.11;satisfied;0.08
Positive Words: of;0.22;in;0.17;excellent;0.13;splendid;0.10;joyful;0.07
Positive Words: for;0.20;on;0.16;awesome;0.12;brilliant;0.09;delighted;0.06
Positive Words: with;0.19;at;0.15;sweet;0.11;magnificent;0.08;jubilant;0.05
Positive Words: by;0.18;from;0.14;nicey nicey nicey nicey nicey nicey nicey nicey nicey nicey nicey nicey nicey nicey nicey nicey nicey nicey nicey nicey nicey nicey ; 1
Positive Words: as;0.17;is;0.13;cute;0.10;dazzling;0.07;radiant;0.04
Positive Words: be;0.16;if, 1
Positive Words: are, 1
Positive Words: was, 1
Positive Words: were, 1
Positive Words: have, 1
Positive Words: has, 1
Positive Words: had, 1
Positive Words: do, 1
ChineseQuantifiableEntityNormalizer.processEntity: 0.5
ChineseQuantifiableEntityNormalizer.processEntity: 1.0
ChineseQuantifiableEntityNormalizer.processEntity: 0.75
ChineseQuantifiableEntityNormalizer.processEntity: 0.25
ChineseQuantifiableEntityNormalizer.processEntity: 0.8
ChineseQuantifiableEntityNormalizer.processEntity: 0.6
ChineseQuantifiableEntityNormalizer.processEntity: 0.9
ChineseQuantifiableEntityNormalizer.processEntity: 0.4
ChineseQuantifiableEntityNormalizer.processEntity: 0.7
ChineseQuantifiableEntityNormalizer.processEntity: 0.3
ChineseQuantifiableEntityNormalizer.processEntity: 0.2
ChineseQuantifiableEntityNormalizer.processEntity: 1.5
ChineseQuantifiableEntityNormalizer.processEntity: 2.0
ChineseQuantifiableEntityNormalizer.processEntity: 1.25
ChineseQuantifiableEntityNormalizer.processEntity: 1.75
regionEndPattern= \n
regionEndPattern= \r\n
regionEndPattern= \t
regionEndPattern= \s
regionEndPattern= \.
regionEndPattern= \?
regionEndPattern= \!
regionEndPattern= \;
regionEndPattern= \,
regionEndPattern= \"
regionEndPattern= \'
regionEndPattern= \(
regionEndPattern= \)
regionEndPattern= \[
regionEndPattern= \]
numFeatures: orig1= 12 , orig2= 8 , combined= 20
numFeatures: orig1= 10 , orig2= 10 , combined= 20
numFeatures: orig1= 9 , orig2= 11 , combined= 20
numFeatures: orig1= 11 , orig2= 9 , combined= 20
numFeatures: orig1= 8 , orig2= 12 , combined= 20
numFeatures: orig1= 7 , orig2= 13 , combined= 20
numFeatures: orig1= 13 , orig2= 7 , combined= 20
numFeatures: orig1= 6 , orig2= 14 , combined= 20
numFeatures: orig1= 14 , orig2= 6 , combined= 20
numFeatures: orig1= 5 , orig2= 15 , combined= 20
numFeatures: orig1= 15 , orig2= 5 , combined= 20
numFeatures: orig1= 4 , orig2= 16 , combined= 20
numFeatures: orig1= 16 , orig2= 4 , combined= 20
numFeatures: orig1= 3 , orig2= 17 , combined= 20
numFeatures: orig1= 17 , orig2= 3 , combined= 20
MAP: [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
MAP: [[true, false], [false, true]]
MAP: [[a, b, c], [d, e, f], [g, h, i]]
MAP: [[10.5, 20.3], [30.7, 40.9]]
MAP: [[null, null], [null, null]]
MAP: [[red, green, blue], [yellow, cyan, magenta]]
MAP: [[apple, banana], [orange, pear]]
MAP: [[0.1, 0.2], [0.3, 0.4]]
MAP: [[x, y], [z, w]]
MAP: [[cat, dog], [bird, fish]]
MAP: [[A, B], [C, D]]
MAP: [[1.0, 2.0], [3.0, 4.0]]
MAP: [[one, two], [three, four]]
MAP: [[hello, world], [foo, bar]]
MAP: [[-1, -2], [-3, -4]]
Updating annotators from tokenize,ssplit,pos to tokenize,ssplit,pos,lemma
Updating annotators from tokenize,ssplit to tokenize,ssplit,pos,ner
Updating annotators from tokenize,ssplit,pos,ner,parse to tokenize,ssplit,pos,ner
Updating annotators from tokenize to tokenize,ssplit,pos,lemma,ner,parse
Updating annotators from tokenize,ssplit,pos,lemma,ner to tokenize,ssplit,pos
Updating annotators from tokenize,ssplit,pos,ner to tokenize,ssplit,pos,lemma
Updating annotators from tokenize,ssplit,pos to tokenize
Updating annotators from tokenize,ssplit,pos,lemma to tokenize,ssplit,pos
Updating annotators from tokenize,ssplit,pos,ner to tokenize
Updating annotators from tokenize to tokenize,ssplit
Updating annotators from tokenize,ssplit,pos to tokenize,ssplit,pos,sentiment
Updating annotators from tokenize to tokenize,ssplit,pos,sentiment
Updating annotators from tokenize,ssplit,pos,sentiment to tokenize
Updating annotators from tokenize to tokenize,ssplit,pos,sentiment,dcoref
Updating annotators from tokenize,ssplit,pos,sentiment,dcoref to tokenize
Max weight: 75 kg min weight: 50 kg
Max weight: 120 lbs min weight: 80 lbs
Max weight: 100 kg min weight: 60 kg
Max weight: 150 lbs min weight: 100 lbs
Max weight: 80 kg min weight: 55 kg
Max weight: 90 kg min weight: 65 kg
Max weight: 200 lbs min weight: 120 lbs
Max weight: 70 kg min weight: 45 kg
Max weight: 110 kg min weight: 70 kg
Max weight: 180 lbs min weight: 110 lbs
Max weight: 85 kg min weight: 58 kg
Max weight: 95 kg min weight: 68 kg
Max weight: 170 lbs min weight: 90 lbs
Max weight: 65 kg min weight: 40 kg
Max weight: 105 kg min weight: 75 kg
Serializing relation extraction model to REModel_1 ...
Serializing relation extraction model to BertRE ...
Serializing relation extraction model to OpenRE ...
Serializing relation extraction model to RelationNet ...
Serializing relation extraction model to REModel_2 ...
Serializing relation extraction model to TransformerRE ...
Serializing relation extraction model to GraphRE ...
Serializing relation extraction model to REModel_3 ...
Serializing relation extraction model to BiLSTMRE ...
Serializing relation extraction model to GCNRE ...
Serializing relation extraction model to REModel_4 ...
Serializing relation extraction model to CapsuleRE ...
Serializing relation extraction model to AttentionRE ...
Serializing relation extraction model to REModel_5 ...
Serializing relation extraction model to CNNRE ...
#4: Changing normalized NER from PERSON to LOCATION at index 5
#4: Changing normalized NER from ORGANIZATION to DATE at index 12
#4: Changing normalized NER from MISC to MONEY at index 9
#4: Changing normalized NER from LOCATION to PERSON at index 3
#4: Changing normalized NER from DATE to ORGANIZATION at index 7
#4: Changing normalized NER from MONEY to MISC at index 10
#4: Changing normalized NER from PERSON to MONEY at index 8
#4: Changing normalized NER from ORGANIZATION to LOCATION at index 6
#4: Changing normalized NER from MISC to DATE at index 11
#4: Changing normalized NER from LOCATION to MISC at index 4
#4: Changing normalized NER from DATE to PERSON at index 2
#4: Changing normalized NER from MONEY to ORGANIZATION at index 1
#4: Changing normalized NER from PERSON to ORGANIZATION at index 13
#4: Changing normalized NER from ORGANIZATION to PERSON at index 14
#4: Changing normalized NER from MISC to LOCATION at index 15
Build: 1.0.0
Build: 2.3.4
Build: 3.2.1
Build: 4.5.6
Build: 5.4.3
Build: 6.7.8
Build: 7.6.5
Build: 8.9.10
Build: 9.8.7
Build: 10.11.12
Build: 11.10.9
Build: 12.13.14
Build: 13.12.11
Build: 14.15.16
Build: 15.14.13
(logs) -7 + -11 = -18.0
(logs) -7 + -11 = logAdd(-7.0, -11.0)
(logs) -7 + -11 = logAdd(-7, -11)
(logs) -7 + -11 = logAdd(-7.00, -11.00)
(logs) -7 + -11 = logAdd(-7, -11.0)
(logs) -7 + -11 = logAdd(-7.0, -11)
(logs) -7 + -11 = logAdd(-7.000, -11.000)
(logs) -7 + -11 = logAdd(-07.0, -011.0)
(logs) -7 + -11 = logAdd(-007.0, -011.0)
(logs) -7 + -11 = logAdd(-007.00, -011.00)
(logs) -7 + -11 = logAdd(-007, -011)
(logs) -7 + -11 = logAdd(-007, -011.0)
(logs) -7 + -11 = logAdd(-007.0, -011)
(logs) -7 + -11 = logAdd(-07, -011)
(logs) -7 + -11 = logAdd(-07, -011.0)
numFeatures: 12
numFeatures: 8
numFeatures: 10
numFeatures: 9
numFeatures: 11
numFeatures: 7
numFeatures: 13
numFeatures: 6
numFeatures: 14
numFeatures: 5
numFeatures: 15
numFeatures: 4
numFeatures: 16
numFeatures: 3
numFeatures: 17
Collapsing matchingColl_1
Collapsing matchingColl_2
Collapsing matchingColl_3
Collapsing matchingColl_4
Collapsing matchingColl_5
Collapsing matchingColl_6
Collapsing matchingColl_7
Collapsing matchingColl_8
Collapsing matchingColl_9
Collapsing matchingColl_10
Collapsing matchingColl_11
Collapsing matchingColl_12
Collapsing matchingColl_13
Collapsing matchingColl_14
Collapsing matchingColl_15
Time rules file is not specified: using default rules at /usr/local/bin/rules.txt
Time rules file is not specified: using default rules at C:\Program Files\Grammar\default.rules
Time rules file is not specified: using default rules at /home/user/.config/grammar/rules
Time rules file is not specified: using default rules at /opt/grammar/rules/default.rules
Time rules file is not specified: using default rules at /var/lib/grammar/rules.dat
Time rules file is not specified: using default rules at /etc/grammar/rules.conf
Time rules file is not specified: using default rules at /Users/user/Desktop/rules.txt
Time rules file is not specified: using default rules at D:\Grammar\Rules\default.rules
Time rules file is not specified: using default rules at /tmp/grammar/rules.tmp
Time rules file is not specified: using default rules at /mnt/grammar/rules/default.rules
Time rules file is not specified: using default rules at C:\Users\user\AppData\Local\Grammar\rules.ini
Time rules file is not specified: using default rules at /root/.grammar/rules
Time rules file is not specified: using default rules at /Volumes/GRAMMAR/Rules/default.rules
Time rules file is not specified: using default rules at /dev/null
Time rules file is not specified: using default rules at /grammar/rules/default.rules
Accepting | "hello" |
Accepting | "world" |
Accepting | "123" |
Accepting | "foo" |
Accepting | "bar" |
Accepting | "" |
Accepting | "bing" |
Accepting | "chat" |
Accepting | "mode" |
Accepting | "test" |
Accepting | "log" |
Accepting | "simulator" |
Accepting | "awesome" |
Accepting | "cool" |
Accepting | "fun" |
com.example.app.Main : Cannot open file config.properties
com.example.app.utils.Logger : Cannot open file log.txt
com.example.app.model.User : Cannot open file user.json
com.example.app.service.AuthService : Cannot open file auth.key
com.example.app.controller.HomeController : Cannot open file index.html
com.example.app.dao.ProductDao : Cannot open file products.db
com.example.app.exception.CustomException : Cannot open file error.log
com.example.app.config.AppConfig : Cannot open file app.properties
com.example.app.view.ProductView : Cannot open file product.jsp
com.example.app.listener.SessionListener : Cannot open file session.dat
com.example.app.test.TestRunner : Cannot open file test.xml
com.example.app.validator.InputValidator : Cannot open file rules.txt
com.example.app.converter.JsonConverter : Cannot open file schema.json
com.example.app.filter.SecurityFilter : Cannot open file security.policy
com.example.app.helper.FileHelper : Cannot open file temp.txt
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 5
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 17
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 23
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 42
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 64
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 81
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 97
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 101
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 115
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 132
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 149
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 168
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 184
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 201
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 217
possible tags for apple : [fruit, red, sweet]
possible tags for dog : [animal, pet, loyal]
possible tags for book : [object, read, learn]
possible tags for sun : [star, bright, hot]
possible tags for car : [vehicle, drive, transport]
possible tags for pizza : [food, cheese, delicious]
possible tags for music : [sound, listen, enjoy]
possible tags for tree : [plant, green, grow]
possible tags for water : [liquid, drink, wet]
possible tags for love : [emotion, feel, care]
possible tags for computer : [machine, use, program]
possible tags for soccer : [sport, play, kick]
possible tags for bird : [animal, fly, sing]
possible tags for cake : [food, bake, sweet]
possible tags for moon : [satellite, night, glow]
tokenPatternsToDiscard= [a-zA-Z]+
tokenPatternsToDiscard= \d+
tokenPatternsToDiscard= \s+
tokenPatternsToDiscard= [\.,;:?!]
tokenPatternsToDiscard= \w+
tokenPatternsToDiscard= \W+
tokenPatternsToDiscard= [^a-zA-Z0-9]
tokenPatternsToDiscard= [\[\]\(\)\{\}]
tokenPatternsToDiscard= [aeiou]
tokenPatternsToDiscard= [^aeiou]
tokenPatternsToDiscard= [A-Z][a-z]+
tokenPatternsToDiscard= \b\w{1,2}\b
tokenPatternsToDiscard= \w{5,}
tokenPatternsToDiscard= [A-Z]+
tokenPatternsToDiscard= [a-z]+
Finished processing 10 trees
Finished processing 15 trees
Finished processing 8 trees
Finished processing 12 trees
Finished processing 9 trees
Finished processing 11 trees
Finished processing 13 trees
Finished processing 7 trees
Finished processing 14 trees
Finished processing 6 trees
Finished processing 16 trees
Finished processing 5 trees
Finished processing 17 trees
Finished processing 4 trees
Finished processing 18 trees
Correcting error: "33" under PU tag; tag changed to CD: subtree 0x00000001
Correcting error: "33" under PU tag; tag changed to CD: subtree 0x00000002
Correcting error: "33" under PU tag; tag changed to CD: subtree 0x00000003
Correcting error: "33" under PU tag; tag changed to CD: subtree 0x00000004
Correcting error: "33" under PU tag; tag changed to CD: subtree 0x00000005
Correcting error: "33" under PU tag; tag changed to CD: subtree 0x00000006
Correcting error: "33" under PU tag; tag changed to CD: subtree 0x00000007
Correcting error: "33" under PU tag; tag changed to CD: subtree 0x00000008
Correcting error: "33" under PU tag; tag changed to CD: subtree 0x00000009
Correcting error: "33" under PU tag; tag changed to CD: subtree 0x0000000A
Correcting error: "33" under PU tag; tag changed to CD: subtree 0x0000000B
Correcting error: "33" under PU tag; tag changed to CD: subtree 0x0000000C
Correcting error: "33" under PU tag; tag changed to CD: subtree 0x0000000D
Correcting error: "33" under PU tag; tag changed to CD: subtree 0x0000000E
Correcting error: "33" under PU tag; tag changed to CD: subtree 0x0000000F
Found unmatching line: 23
Found unmatching line: 56
Found unmatching line: 78
Found unmatching line: 101
Found unmatching line: 124
Found unmatching line: 147
Found unmatching line: 169
Found unmatching line: 192
Found unmatching line: 215
Found unmatching line: 238
Found unmatching line: 261
Found unmatching line: 284
Found unmatching line: 307
Found unmatching line: 330
Found unmatching line: 353
Found number and: apple
Found number and: banana
Found number and: carrot
Found number and: dog
Found number and: elephant
Found number and: fish
Found number and: grape
Found number and: hat
Found number and: ice
Found number and: jam
Found number and: kite
Found number and: lemon
Found number and: mouse
Found number and: nut
Found number and: orange
Best label f1 on dev set so far: 0.76
Best label f1 on dev set so far: 0.81
Best label f1 on dev set so far: 0.79
Best label f1 on dev set so far: 0.77
Best label f1 on dev set so far: 0.83
Best label f1 on dev set so far: 0.78
Best label f1 on dev set so far: 0.80
Best label f1 on dev set so far: 0.82
Best label f1 on dev set so far: 0.75
Best label f1 on dev set so far: 0.84
Best label f1 on dev set so far: 0.74
Best label f1 on dev set so far: 0.85
Best label f1 on dev set so far: 0.73
Best label f1 on dev set so far: 0.86
Best label f1 on dev set so far: 0.72
newlineIsSentenceBreak= true
newlineIsSentenceBreak= false
newlineIsSentenceBreak= auto
newlineIsSentenceBreak= always
newlineIsSentenceBreak= never
newlineIsSentenceBreak= yes
newlineIsSentenceBreak= no
newlineIsSentenceBreak= 1
newlineIsSentenceBreak= 0
newlineIsSentenceBreak= on
newlineIsSentenceBreak= off
newlineIsSentenceBreak= default
newlineIsSentenceBreak= custom
newlineIsSentenceBreak= none
newlineIsSentenceBreak= all
newlineIsSentenceBreak= two
newlineIsSentenceBreak= never
newlineIsSentenceBreak= always
newlineIsSentenceBreak= auto
newlineIsSentenceBreak= one
newlineIsSentenceBreak= none
newlineIsSentenceBreak= true
newlineIsSentenceBreak= false
newlineIsSentenceBreak= yes
newlineIsSentenceBreak= no
newlineIsSentenceBreak= 0
newlineIsSentenceBreak= 1
newlineIsSentenceBreak= 2
newlineIsSentenceBreak= default
newlineIsSentenceBreak= custom
Gold: [0, 0, 0, 0, 0]
Gold: [1, 2, 3, 4, 5]
Gold: [5, 4, 3, 2, 1]
Gold: [2, 4, 6, 8, 10]
Gold: [10, 8, 6, 4, 2]
Gold: [3, 6, 9, 12, 15]
Gold: [15, 12, 9, 6, 3]
Gold: [4, 8, 12, 16, 20]
Gold: [20, 16, 12, 8, 4]
Gold: [5, 10, 15, 20, 25]
Gold: [25, 20, 15, 10, 5]
Gold: [6, 12, 18, 24, 30]
Gold: [30, 24, 18, 12, 6]
Gold: [7, 14, 21, 28, 35]
Gold: [35, 28, 21, 14 ,7]
Output_simulateExample_1: data.txt : 12 trees, 5 matched and printed
Output_simulateExample_2: report.docx : 8 trees, 3 matched and printed
Output_simulateExample_3: image.jpg : 4 trees, 1 matched and printed
Output_simulateExample_4: video.mp4 : 16 trees, 7 matched and printed
Output_simulateExample_5: music.mp3 : 10 trees, 4 matched and printed
Output_simulateExample_6: archive.zip : 20 trees, 9 matched and printed
Output_simulateExample_7: code.java : 6 trees, 2 matched and printed
Output_simulateExample_8: presentation.pptx : 14 trees, 6 matched and printed
Output_simulateExample_9: email.eml : 2 trees, 0 matched and printed
Output_simulateExample_10: document.pdf : 18 trees, 8 matched and printed
Output_simulateExample_11: spreadsheet.xlsx : 22 trees, 10 matched and printed
Output_simulateExample_12: graph.png : 5 trees, 2 matched and printed
Output_simulateExample_13: script.py : 7 trees, 3 matched and printed
Output_simulateExample_14: game.exe : 24 trees, 11 matched and printed
Output_simulateExample_15: book.epub : 9 trees, 4 matched and printed
Searching for resource: Alice ... found.
Searching for resource: Bob ... found.
Searching for resource: Charlie ... found.
Searching for resource: David ... found.
Searching for resource: Eve ... found.
Searching for resource: Frank ... found.
Searching for resource: Grace ... found.
Searching for resource: Harry ... found.
Searching for resource: Irene ... found.
Searching for resource: Jack ... found.
Searching for resource: Kelly ... found.
Searching for resource: Leo ... found.
Searching for resource: Mary ... found.
Searching for resource: Nick ... found.
Searching for resource: Olivia ... found.
Number of non-zero feature x,y pairs: xSize * ySize - numZeros"
Number of non-zero feature x,y pairs: 10 * 20 - 5"
Number of non-zero feature x,y pairs: 15 * 30 - 8"
Number of non-zero feature x,y pairs: 5 * 15 - 3"
Number of non-zero feature x,y pairs: 8 * 25 - 7"
Number of non-zero feature x,y pairs: 12 * 18 - 6"
Number of non-zero feature x,y pairs: 7 * 22 - 9"
Number of non-zero feature x,y pairs: 14 * 26 - 11"
Number of non-zero feature x,y pairs: 6 * 13 - 4"
Number of non-zero feature x,y pairs: 9 * 19 - 10"
Number of non-zero feature x,y pairs: 11 * 24 - 7"
Number of non-zero feature x,y pairs: 16 * 35 - 12"
Number of non-zero feature x,y pairs: 4 * 11 - 2"
Number of non-zero feature x,y pairs: 13 * 28 - 14"
Number of non-zero feature x,y pairs: 3 * 8 - 5"
Document Type: PDF
Document Type: DOCX
Document Type: TXT
Document Type: HTML
Document Type: XML
Document Type: JPEG
Document Type: PNG
Document Type: GIF
Document Type: MP3
Document Type: MP4
Document Type: ZIP
Document Type: RAR
Document Type: PPTX
Document Type: XLSX
Document Type: CSV
Matched 3 chinese number vectors
Matched 7 chinese number vectors
Matched 12 chinese number vectors
Matched 5 chinese number vectors
Matched 9 chinese number vectors
Matched 4 chinese number vectors
Matched 8 chinese number vectors
Matched 10 chinese number vectors
Matched 6 chinese number vectors
Matched 11 chinese number vectors
Matched 2 chinese number vectors
Matched 13 chinese number vectors
Matched 14 chinese number vectors
Matched 15 chinese number vectors
CleanXML: token added as: hello
CleanXML: token added as: 42
CleanXML: token added as: true
CleanXML: token added as: name
CleanXML: token added as: $
CleanXML: token added as: 3.14
CleanXML: token added as: null
CleanXML: token added as: foo
CleanXML: token added as: bar
CleanXML: token added as: baz
CleanXML: token added as: qux
CleanXML: token added as: xyzzy
CleanXML: token added as: plugh
CleanXML: token added as: thud
CleanXML: token added as: waldo
optimal precision at recall 0.5 0.76
optimal precision at recall 0.6 0.81
optimal precision at recall 0.7 0.85
optimal precision at recall 0.8 0.88
optimal precision at recall 0.9 0.91
optimal precision at recall 1.0 0.93
optimal precision at recall 0.4 0.72
optimal precision at recall 0.3 0.68
optimal precision at recall 0.2 0.64
optimal precision at recall 0.1 0.59
optimal precision at recall 0.55 0.78
optimal precision at recall 0.65 0.83
optimal precision at recall 0.75 0.86
optimal precision at recall 0.85 0.89
optimal precision at recall 0.95 0.92
timePattern matched groups: |12:34:56| |12| |34| |56|
timePattern matched groups: |23:59:59| |23| |59| |59|
timePattern matched groups: |00:00:00| |00| |00| |00|
timePattern matched groups: |09:15:30| |09| |15| |30|
timePattern matched groups: |18:45:12| |18| |45| |12|
timePattern matched groups: |06:30:24| |06| |30| |24|
timePattern matched groups: |15:22:48| |15| |22| |48|
timePattern matched groups: |03:07:21| |03| |07| |21|
timePattern matched groups: |21:54:36| |21| |54| |36|
timePattern matched groups: |11:11:11| |11| |11| |11|
timePattern matched groups: |16:32:08| |16| |32| |08|
timePattern matched groups: |04:20:42| |04| |20| |42|
timePattern matched groups: |19:19:19| |19| |19| |19|
timePattern matched groups: |08:16:32| |08| |16| |32|
timePattern matched groups: |14:28:56| |14| |28| |56|
Deleted (foo): bar
Deleted (name): Alice
Deleted (id): 1234
Deleted (file): report.docx
Deleted (key): secret
Deleted (node): 10.0.0.1
Deleted (user): admin
Deleted (email): bob@example.com
Deleted (record): 5678
Deleted (folder): backup
Deleted (item): book
Deleted (message): hello
Deleted (comment): spam
Deleted (product): iPhone
normalizedNumberString: normalizing 3.14159
normalizedNumberString: normalizing -0.001
normalizedNumberString: normalizing 1.23e+4
normalizedNumberString: normalizing 0.0
normalizedNumberString: normalizing 6.022e-23
normalizedNumberString: normalizing -9.81
normalizedNumberString: normalizing 2.71828
normalizedNumberString: normalizing 1.0e+6
normalizedNumberString: normalizing -1.41421
normalizedNumberString: normalizing 4.6692e-3
normalizedNumberString: normalizing 9.87654
normalizedNumberString: normalizing -3.333e+2
normalizedNumberString: normalizing 0.12345
normalizedNumberString: normalizing -7.89e-4
normalizedNumberString: normalizing 5.4321e+2
Index 0 is element 0 of binaryClassification "true, false"
Index 1 is element -1 of binaryClassification "false, true"
Index 2 is element -2 of binaryClassification "true, true"
Index 3 is element -3 of binaryClassification "false, false"
Index 4 is element -4 of binaryClassification "true, false"
Index 5 is element -5 of binaryClassification "false, true"
Index 6 is element -6 of binaryClassification "true, true"
Index 7 is element -7 of binaryClassification "false, false"
Index 8 is element -8 of binaryClassification "true, false"
Index 9 is element -9 of binaryClassification "false, true"
Index 10 is element -10 of binaryClassification "true, true"
Index 11 is element -11 of binaryClassification "false, false"
Index 12 is element -12 of binaryClassification "true, false"
Index 13 is element -13 of binaryClassification "false, true"
Index 14 is element -14 of binaryClassification "true, true"
Index 0 is element 0 of binaryClassification 1, 0
Index 1 is element -1 of binaryClassification 0, 1
Index 2 is element -2 of binaryClassification 1, 1
Index 3 is element -3 of binaryClassification 0, 0
Index 4 is element -4 of binaryClassification 1, 0
Index 5 is element -5 of binaryClassification 0, 1
Index 6 is element -6 of binaryClassification 1, 1
Index 7 is element -7 of binaryClassification 0, 0
Index 8 is element -8 of binaryClassification 1, 0
Index 9 is element -9 of binaryClassification 0, 1
Index 10 is element -10 of binaryClassification 1, 1
Index 11 is element -11 of binaryClassification 0, 0
Index 12 is element -12 of binaryClassification 1, 0
Index 13 is element -13 of binaryClassification 0, 1
Index 14 is element -14 of binaryClassification 1, 1
Sentence number: 10 ; length 9 ; correct: 7 ; wrong: 2 ; unknown wrong: 1
Sentence number: 5 ; length 12 ; correct: 4 ; wrong: 1 ; unknown wrong: 0
Sentence number: 8 ; length 10 ; correct: 6 ; wrong: 2 ; unknown wrong: 0
Sentence number: 12 ; length 8 ; correct: 9 ; wrong: 3 ; unknown wrong: 2
Sentence number: 7 ; length 11 ; correct: 5 ; wrong: 1 ; unknown wrong: 1
Sentence number: 9 ; length 10 ; correct: 8 ; wrong: 1 ; unknown wrong: 0
Sentence number: 6 ; length 13 ; correct: 4 ; wrong: 2 ; unknown wrong: 1
Sentence number: 11 ; length 9 ; correct: 10 ; wrong: 0 ; unknown wrong: 1
Sentence number: 4 ; length 14 ; correct: 3 ; wrong: 1 ; unknown wrong: 0
Sentence number: 13 ; length 7 ; correct: 11 ; wrong: 2 ; unknown wrong: 0
Sentence # 1 tokens are:
Sentence # 2 tokens are:
Sentence # 3 tokens are:
Sentence # 4 tokens are:
Sentence # 5 tokens are:
Sentence # 6 tokens are:
Sentence # 7 tokens are:
Sentence # 8 tokens are:
Sentence # 9 tokens are:
Sentence # 10 tokens are:
Sentence # 11 tokens are:
Sentence # 12 tokens are:
Sentence # 13 tokens are:
Sentence # 14 tokens are:
Sentence # 15 tokens are:
Labeling because of phrase "out of memory"
Labeling because of phrase "invalid input"
Labeling because of phrase "null pointer exception"
Labeling because of phrase "segmentation fault"
Labeling because of phrase "access denied"
Labeling because of phrase "connection timeout"
Labeling because of phrase "syntax error"
Labeling because of phrase "file not found"
Labeling because of phrase "index out of bounds"
Labeling because of phrase "division by zero"
Labeling because of phrase "stack overflow"
Labeling because of phrase "resource leak"
Labeling because of phrase "deadlock detected"
Labeling because of phrase "infinite loop"
Labeling because of phrase "unexpected end of file"
Predicted Mention: "He said" INCORRECT
Predicted Mention: "she replied" INCORRECT
Predicted Mention: "the president announced" INCORRECT
Predicted Mention: "they shouted" INCORRECT
Predicted Mention: "I asked" INCORRECT
Predicted Mention: "you whispered" INCORRECT
Predicted Mention: "we agreed" INCORRECT
Predicted Mention: "she exclaimed" INCORRECT
Predicted Mention: "he admitted" INCORRECT
Predicted Mention: "they argued" INCORRECT
Predicted Mention: "she suggested" INCORRECT
Predicted Mention: "he explained" INCORRECT
Predicted Mention: "they complained" INCORRECT
Predicted Mention: "I confessed" INCORRECT
Predicted Mention: "you demanded" INCORRECT
Using unknown year vector for Chinese words: [0.1, 0.2, 0.3, 0.4]
Using unknown year vector for Chinese words: [0.5, 0.6, 0.7, 0.8]
Using unknown year vector for Chinese words: [0.9, 1.0, 1.1, 1.2]
Using unknown year vector for Chinese words: [1.3, 1.4, 1.5, 1.6]
Using unknown year vector for Chinese words: [1.7, 1.8, 1.9, 2.0]
Using unknown year vector for Chinese words: [2.1, 2.2, 2.3, 2.4]
Using unknown year vector for Chinese words: [2.5, 2.6, 2.7, 2.8]
Using unknown year vector for Chinese words: [2.9, 3.0, 3.1, 3.2]
Using unknown year vector for Chinese words: [3.3, 3.4, 3.5, 3.6]
Using unknown year vector for Chinese words: [3.7, 3.8, 3.9, 4.0]
Using unknown year vector for Chinese words: [4.1, 4.2, 4.3, 4.4]
Using unknown year vector for Chinese words: [4.5, 4.6, 4.7, 4.8]
Using unknown year vector for Chinese words: [4.9, 5.0, 5.1, 5.2]
Using unknown year vector for Chinese words: [5.3, 5.4, 5.5, 5.6]
Using unknown year vector for Chinese words: [5.7, 5.8, 5.9, 6.0]
ChineseCollinizer: all children of root deleted; returning null
ChineseCollinizer: all children of noun deleted; returning null
ChineseCollinizer: all children of verb deleted; returning null
ChineseCollinizer: all children of adj deleted; returning null
ChineseCollinizer: all children of adv deleted; returning null
ChineseCollinizer: all children of prep deleted; returning null
ChineseCollinizer: all children of conj deleted; returning null
ChineseCollinizer: all children of pron deleted; returning null
ChineseCollinizer: all children of num deleted; returning null
ChineseCollinizer: all children of det deleted; returning null
ChineseCollinizer: all children of name deleted; returning null
ChineseCollinizer: all children of date deleted; returning null
ChineseCollinizer: all children of time deleted; returning null
ChineseCollinizer: all children of loc deleted; returning null
ChineseCollinizer: all children of temp deleted; returning null
featureIndex.size()= 0
featureIndex.size()= 1
featureIndex.size()= 2
featureIndex.size()= 3
featureIndex.size()= 4
featureIndex.size()= 5
featureIndex.size()= 6
featureIndex.size()= 7
featureIndex.size()= 8
featureIndex.size()= 9
featureIndex.size()= 10
featureIndex.size()= 11
featureIndex.size()= 12
featureIndex.size()= 13
featureIndex.size()= 14
[00001] 23 known extractions
[00002] 17 known extractions
[00003] 19 known extractions
[00004] 21 known extractions
[00005] 18 known extractions
[00006] 20 known extractions
[00007] 22 known extractions
[00008] 16 known extractions
[00009] 24 known extractions
[00010] 25 known extractions
[00011] 15 known extractions
[00012] 26 known extractions
[00013] 14 known extractions
[00014] 27 known extractions
[00015] 13 known extractions
Reading data using FileReader
Reading data using BufferedReader
Reading data using Scanner
Reading data using InputStreamReader
Reading data using DataInputStream
Reading data using ObjectInputStream
Reading data using FileInputStream
Reading data using RandomAccessFile
Reading data using FileChannel
Reading data using FileReaderAndWriter
Reading data using BufferedInputStream
Reading data using StreamTokenizer
Reading data using LineNumberReader
Reading data using PushbackReader
Reading data using FilterReader
Sieve optimization pass 1 scores: Sieve= Name , score= 0.95
Sieve optimization pass 2 scores: Sieve= Address , score= 0.87
Sieve optimization pass 3 scores: Sieve= Phone , score= 0.92
Sieve optimization pass 4 scores: Sieve= Email , score= 0.89
Sieve optimization pass 5 scores: Sieve= Gender , score= 0.98
Sieve optimization pass 6 scores: Sieve= Age , score= 0.94
Sieve optimization pass 7 scores: Sieve= Occupation , score= 0.86
Sieve optimization pass 8 scores: Sieve= Education , score= 0.91
Sieve optimization pass 9 scores: Sieve= Income , score= 0.88
Sieve optimization pass 10 scores: Sieve= Marital Status , score= 0.93
Sieve optimization pass 11 scores: Sieve= Hobbies , score= 0.85
Sieve optimization pass 12 scores: Sieve= Interests , score= 0.90
Sieve optimization pass 13 scores: Sieve= Preferences , score= 0.96
Sieve optimization pass 14 scores: Sieve= Personality , score= 0.97
Sieve optimization pass 15 scores: Sieve= Behavior , score= 0.99
Current train options: [express, local, regional]
Current train options: [bullet, commuter, intercity]
Current train options: [metro, light rail, tram]
Current train options: [high-speed, maglev, hyperloop]
Current train options: [freight, passenger, cargo]
Current train options: [diesel, electric, steam]
Current train options: [monorail, subway, elevated]
Current train options: [standard, premium, deluxe]
Current train options: [transit, shuttle, loop]
Current train options: [narrow gauge, broad gauge, standard gauge]
Current train options: [single track, double track, quadruple track]
Current train options: [direct, connecting, transfer]
Current train options: [daytime, overnight, sleeper]
Current train options: [reserved, unreserved, flexible]
Current train options: [first class, second class, third class]
Starting Ssurgeon log, at /home/user/logs/ssurgeon.log date= Monday, October 23, 2023
Starting Ssurgeon log, at C:\Users\user\Documents\logs\ssurgeon.log date= Monday, October 23, 2023
Starting Ssurgeon log, at /var/log/ssurgeon/ssurgeon.log date= Monday, October 23, 2023
Starting Ssurgeon log, at /tmp/logs/ssurgeon.log date= Monday, October 23, 2023
Starting Ssurgeon log, at /Users/user/Desktop/logs/ssurgeon.log date= Monday, October 23, 2023
Starting Ssurgeon log, at D:\logs\ssurgeon.log date= Monday, October 23, 2023
Starting Ssurgeon log, at /opt/ssurgeon/logs/ssurgeon.log date= Monday, October 23, 2023
Starting Ssurgeon log, at /mnt/logs/ssurgeon.log date= Monday, October 23, 2023
Starting Ssurgeon log, at /data/logs/ssurgeon.log date= Monday, October 23, 2023
Starting Ssurgeon log, at /logs/ssurgeon.log date= Monday, October 23, 2023
Starting Ssurgeon log, at E:\ssurgeon\logs\ssurgeon.log date= Monday, October 23, 2023
Starting Ssurgeon log, at /home/user/.ssurgeon/logs/ssurgeon.log date= Monday, October 23, 2023
Starting Ssurgeon log, at /usr/local/ssurgeon/logs/ssurgeon.log date= Monday, October 23, 2023
Starting Ssurgeon log, at /root/logs/ssurgeon.log date= Monday, October 23, 2023
Starting Ssurgeon log, at F:\logs\ssurgeon\ssurgeon.log date= Monday, October 23, 2023
debug-clusterCluster ID: 1 Numbers: 3 Genders: male, female, female animacies: animate, animate, animate
debug-clusterCluster ID: 2 Numbers: 2 Genders: male, male animacies: animate, inanimate
debug-clusterCluster ID: 3 Numbers: 4 Genders: female, female, male, male animacies: inanimate, inanimate, inanimate, inanimate
debug-clusterCluster ID: 4 Numbers: 1 Genders: female animacies: animate
debug-clusterCluster ID: 5 Numbers: 5 Genders: male, male, female, female, female animacies: animate, animate, animate, inanimate, inanimate
debug-clusterCluster ID: 6 Numbers: 2 Genders: female, male animacies: inanimate, animate
debug-clusterCluster ID: 7 Numbers: 3 Genders: male, male, female animacies: inanimate, inanimate, animate
debug-clusterCluster ID: 8 Numbers: 4 Genders: female, male, female, male animacies: animate, inanimate, animate, inanimate
debug-clusterCluster ID: 9 Numbers: 1 Genders: male animacies: inanimate
debug-clusterCluster ID: 10 Numbers: 5 Genders: female, female, female, male, male animacies: inanimate, animate, inanimate, animate, animate
debug-clusterCluster ID: 11 Numbers: 2 Genders: male, female animacies: animate, animate
debug-clusterCluster ID: 12 Numbers: 3 Genders: female, female, male animacies: inanimate, animate, inanimate
debug-clusterCluster ID: 13 Numbers: 4 Genders: male, male, male, female animacies: animate, animate, inanimate, animate
debug-clusterCluster ID: 14 Numbers: 1 Genders: female animacies: inanimate
Connected component: cc1
Connected component: cc2
Connected component: cc3
Connected component: cc4
Connected component: cc5
Connected component: cc6
Connected component: cc7
Connected component: cc8
Connected component: cc9
Connected component: cc10
Connected component: cc11
Connected component: cc12
Connected component: cc13
Connected component: cc14
Connected component: cc15
adding seedw to seed
adding seedw+1 to seed
adding seedw*2 to seed
adding seedw-1 to seed
adding seedw/2 to seed
adding seedw^2 to seed
adding seedw%2 to seed
adding seedw*seedw to seed
adding seedw+seedw to seed
adding seedw-seedw to seed
adding seedw/seedw to seed
adding sqrt(seedw) to seed
adding log(seedw) to seed
adding sin(seedw) to seed
adding cos(seedw) to seed
Starting on dfsa.dfsaID=1001
Starting on dfsa.dfsaID=2002
Starting on dfsa.dfsaID=3003
Starting on dfsa.dfsaID=4004
Starting on dfsa.dfsaID=5005
Starting on dfsa.dfsaID=6006
Starting on dfsa.dfsaID=7007
Starting on dfsa.dfsaID=8008
Starting on dfsa.dfsaID=9009
Starting on dfsa.dfsaID=1010
Starting on dfsa.dfsaID=1111
Starting on dfsa.dfsaID=1212
Starting on dfsa.dfsaID=1313
Starting on dfsa.dfsaID=1414
Starting on dfsa.dfsaID=1515
lambda f too big lambda[f]
lambda g too big lambda[g]
lambda h too big lambda[h]
lambda x too big lambda[x]
lambda y too big lambda[y]
lambda z too big lambda[z]
lambda a too big lambda[a]
lambda b too big lambda[b]
lambda c too big lambda[c]
lambda d too big lambda[d]
lambda e too big lambda[e]
lambda i too big lambda[i]
lambda j too big lambda[j]
lambda k too big lambda[k]
lambda l too big lambda[l]
Not checking IPv6 address against blockList: 192.168.1.100"
Not checking IPv6 address against blockList: 10.0.0.2"
Not checking IPv6 address against blockList: 172.16.0.55"
Not checking IPv6 address against blockList: 192.168.0.1"
Not checking IPv6 address against blockList: 192.168.2.10"
Not checking IPv6 address against blockList: 10.0.0.15"
Not checking IPv6 address against blockList: 172.16.5.30"
Not checking IPv6 address against blockList: 192.168.3.77"
Not checking IPv6 address against blockList: 10.0.0.7"
Not checking IPv6 address against blockList: 172.16.2.42"
Not checking IPv6 address against blockList: 192.168.5.123"
Not checking IPv6 address against blockList: 10.0.0.31"
Not checking IPv6 address against blockList: 172.16.8.19"
Not checking IPv6 address against blockList: 192.168.4.8"
Not checking IPv6 address against blockList: 10.0.0.88"
Time to export features: 1.23 seconds
Time to export features: 2.75 seconds
Time to export features: 0.56 seconds
Time to export features: 3.14 seconds
Time to export features: 1.67 seconds
Time to export features: 2.34 seconds
Time to export features: 0.89 seconds
Time to export features: 4.12 seconds
Time to export features: 1.45 seconds
Time to export features: 2.98 seconds
Time to export features: 0.67 seconds
Time to export features: 3.56 seconds
Time to export features: 1.78 seconds
Time to export features: 2.45 seconds
Time to export features: 0.98 seconds
Serializing event extraction model to EventExtractionModel.pkl ...
Serializing event extraction model to EventExtractor.h5 ...
Serializing event extraction model to EEModel.pt ...
Serializing event extraction model to EventModel.bin ...
Serializing event extraction model to EventExtraction.sav ...
Serializing event extraction model to EEModel.joblib ...
Serializing event extraction model to EventExtractor.onnx ...
Serializing event extraction model to EventExtractionModel.torchscript ...
Serializing event extraction model to EEModel.tf ...
Serializing event extraction model to EventModel.npz ...
Serializing event extraction model to EventExtractionModel.dill ...
Serializing event extraction model to EEModel.pickle ...
Serializing event extraction model to EventExtractor.json ...
Serializing event extraction model to EventExtractionModel.xml ...
Serializing event extraction model to EEModel.mat ...
Mention # 0 : John Smith
Mention # 1 : New York City
Mention # 2 : Apple Inc.
Mention # 3 : Barack Obama
Mention # 4 : Harry Potter
Mention # 5 : Amazon.com
Mention # 6 : Netflix
Mention # 7 : Elon Musk
Mention # 8 : COVID-19
Mention # 9 : NASA
Mention # 10 : Bitcoin
Mention # 11 : Taylor Swift
Mention # 12 : Facebook
Mention # 13 : LeBron James
Mention # 14 : Starbucks
probability: 0.5
probability: 0.01
probability: 0.99
probability: 0.25
probability: 0.75
probability: 0.1
probability: 0.9
probability: 0.4
probability: 0.6
probability: 0.2
probability: 0.8
probability: 0.3
probability: 0.7
probability: 0.12
probability: 0.88
Second character is: a
Second character is: 0
Second character is: _
Second character is: B
Second character is: 9
Second character is: $
Second character is: c
Second character is: 3
Second character is: *
Second character is: E
Second character is: 6
Second character is: +
Second character is: d
Second character is: 4
Second character is: F
com.example.Foo : Loaded 100 lines from foo.txt into MWE counter
com.example.Bar : Loaded 50 lines from bar.csv into MWE counter
com.example.Baz : Loaded 200 lines from baz.json into MWE counter
com.example.Qux : Loaded 150 lines from qux.xml into MWE counter
com.example.Quux : Loaded 75 lines from quux.dat into MWE counter
com.example.Corge : Loaded 125 lines from corge.log into MWE counter
com.example.Grault : Loaded 175 lines from grault.ini into MWE counter
com.example.Garply : Loaded 25 lines from garply.md into MWE counter
com.example.Waldo : Loaded 300 lines from waldo.html into MWE counter
com.example.Fred : Loaded 400 lines from fred.xlsx into MWE counter
com.example.Plugh : Loaded 350 lines from plugh.pdf into MWE counter
com.example.Xyzzy : Loaded 250 lines from xyzzy.docx into MWE counter
com.example.Thud : Loaded 275 lines from thud.pptx into MWE counter
com.example.FooBar : Loaded 325 lines from foobar.zip into MWE counter
com.example.BazQux : Loaded 225 lines from bazqux.rar into MWE counter
Reading file: /home/user/documents/report.pdf
Reading file: /var/log/syslog
Reading file: /etc/passwd
Reading file: /usr/bin/python
Reading file: /dev/null
Reading file: /tmp/test.txt
Reading file: /opt/app/config.ini
Reading file: /media/cdrom/install.sh
Reading file: /root/.bashrc
Reading file: /boot/grub/grub.cfg
Reading file: /proc/meminfo
Reading file: /lib/libc.so.6
Reading file: /mnt/backup/data.zip
Reading file: /srv/http/index.html
Reading file: /bin/ls
Determined head (case 1) for 0.5 is: 0
Determined head (case 1) for 0.75 is: 1
Determined head (case 1) for 0.25 is: -1
Determined head (case 1) for 0.9 is: 2
Determined head (case 1) for 0.1 is: -2
Determined head (case 1) for 0.6 is: 0
Determined head (case 1) for 0.8 is: 1
Determined head (case 1) for 0.2 is: -1
Determined head (case 1) for 0.95 is: 3
Determined head (case 1) for 0.05 is: -3
Determined head (case 1) for 0.55 is: -1
Determined head (case 1) for 0.85 is: 2
Determined head (case 1) for 0.15 is: -2
Determined head (case 1) for 0.99 is: 4
Determined head (case 1) for 0.01 is: -4
mapped: @John to: John Smith
mapped: #NYC to: New York City
mapped: $AAPL to: Apple Inc.
mapped: @Jane to: Jane Doe
mapped: #AI to: Artificial Intelligence
mapped: $AMZN to: Amazon.com
mapped: @Bob to: Bob Jones
mapped: #COVID to: Coronavirus Disease
mapped: $TSLA to: Tesla Inc.
mapped: @Alice to: Alice Cooper
mapped: #NBA to: National Basketball Association
mapped: $MSFT to: Microsoft Corporation
mapped: @Tom to: Tom Cruise
mapped: #LOTR to: Lord of the Rings
mapped: $GOOG to: Google LLC
NER : Entry doesn't have overwriteable types PERSON , but entry type is in noDefaultOverwriteLabels
POS : Entry doesn't have overwriteable types ADJ , but entry type is in noDefaultOverwriteLabels
COREF : Entry doesn't have overwriteable types PRON , but entry type is in noDefaultOverwriteLabels
DEP : Entry doesn't have overwriteable types VERB , but entry type is in noDefaultOverwriteLabels
SRL : Entry doesn't have overwriteable types ARG , but entry type is in noDefaultOverwriteLabels
QA : Entry doesn't have overwriteable types QUES , but entry type is in noDefaultOverwriteLabels
SUMM : Entry doesn't have overwriteable types SENT , but entry type is in noDefaultOverwriteLabels
TRANSL : Entry doesn't have overwriteable types LANG , but entry type is in noDefaultOverwriteLabels
SENTI : Entry doesn't have overwriteable types POL , but entry type is in noDefaultOverwriteLabels
TOPIC : Entry doesn't have overwriteable types CAT , but entry type is in noDefaultOverwriteLabels
ELMo : Entry doesn't have overwriteable types EMB , but entry type is in noDefaultOverwriteLabels
BERT : Entry doesn't have overwriteable types MASK , but entry type is in noDefaultOverwriteLabels
GPT-3 : Entry doesn't have overwriteable types GEN , but entry type is in noDefaultOverwriteLabels
SPACY : Entry doesn't have overwriteable types DOC , but entry type is in noDefaultOverwriteLabels
NLTK : Entry doesn't have overwriteable types TOK , but entry type is in noDefaultOverwriteLabels
Redwood.DBG Applying patterns to 5 sentences
Redwood.DBG Applying patterns to 12 sentences
Redwood.DBG Applying patterns to 8 sentences
Redwood.DBG Applying patterns to 10 sentences
Redwood.DBG Applying patterns to 7 sentences
Redwood.DBG Applying patterns to 9 sentences
Redwood.DBG Applying patterns to 6 sentences
Redwood.DBG Applying patterns to 11 sentences
Redwood.DBG Applying patterns to 4 sentences
Redwood.DBG Applying patterns to 3 sentences
Redwood.DBG Applying patterns to 13 sentences
Redwood.DBG Applying patterns to 14 sentences
Redwood.DBG Applying patterns to 15 sentences
Redwood.DBG Applying patterns to 2 sentences
Result: [0.1, 0.2, 0.3, 0.4]
Result: [0.5, 0.6, 0.7, 0.8]
Result: [0.9, 1.0, 1.1, 1.2]
Result: [1.3, 1.4, 1.5, 1.6]
Result: [1.7, 1.8, 1.9, 2.0]
Result: [0.2, 0.4, 0.6, 0.8]
Result: [0.3, 0.6, 0.9, 1.2]
Result: [0.4, 0.8, 1.2, 1.6]
Result: [0.5, 1.0, 1.5, 2.0]
Result: [0.6, 1.2, 1.8, 2.4]
Result: [0.7, 1.4, 2.1, 2.8]
Result: [0.8, 1.6, 2.4, 3.2]
Result: [0.9, 1.8, 2.7, 3.6]
Result: [1.0, 2.0, 3.0, 4.0]
Result: [1.1, 2.2, 3.3, 4.4]
Took too long parsing: 1000 words
Took too long parsing: 500 words
Took too long parsing: 200 words
Took too long parsing: 1500 words
Took too long parsing: 300 words
Took too long parsing: 800 words
Took too long parsing: 250 words
Took too long parsing: 1200 words
Took too long parsing: 400 words
Took too long parsing: 700 words
Took too long parsing: 350 words
Took too long parsing: 900 words
Took too long parsing: 450 words
Took too long parsing: 600 words
Took too long parsing: 100 words
Observed Value: [1, 2, 3, 4]
Observed Value: [5, 6, 7, 8]
Observed Value: [9, 10, 11, 12]
Observed Value: [13, 14, 15, 16]
Observed Value: [17, 18, 19, 20]
Observed Value: [21, 22, 23, 24]
Observed Value: [25, 26, 27, 28]
Observed Value: [29, 30, 31, 32]
Observed Value: [33, 34, 35, 36]
Observed Value: [37, 38, 39, 40]
Observed Value: [41, 42, 43, 44]
Observed Value: [45, 46, 47, 48]
Observed Value: [49, 50, 51, 52]
Observed Value: [53, 54, 55, 56]
Observed Value: [57, 58, 59, 60]
train.conllu : a CoNLL-U file of dependency trees to process
test.conllu : a CoNLL-U file of dependency trees to process
dev.conllu : a CoNLL-U file of dependency trees to process
en_ewt-ud-train.conllu : a CoNLL-U file of dependency trees to process
fr_gsd-ud-test.conllu : a CoNLL-U file of dependency trees to process
zh_gsd-ud-dev.conllu : a CoNLL-U file of dependency trees to process
es_ancora-ud-train.conllu : a CoNLL-U file of dependency trees to process
de_hdt-ud-test.conllu : a CoNLL-U file of dependency trees to process
ar_padt-ud-dev.conllu : a CoNLL-U file of dependency trees to process
hi_hdtb-ud-train.conllu : a CoNLL-U file of dependency trees to process
ru_syntagrus-ud-test.conllu : a CoNLL-U file of dependency trees to process
ja_gsd-ud-dev.conllu : a CoNLL-U file of dependency trees to process
tr_imst-ud-train.conllu : a CoNLL-U file of dependency trees to process
sv_talbanken-ud-test.conllu : a CoNLL-U file of dependency trees to process
pt_bosque-ud-dev.conllu : a CoNLL-U file of dependency trees to process
TagAffixDetector: building TagAffixDetector from /home/user/cc.txt and /home/user/ad.txt
TagAffixDetector: building TagAffixDetector from /var/log/cc.log and /var/log/ad.log
TagAffixDetector: building TagAffixDetector from /opt/cc/cc.conf and /opt/ad/ad.conf
TagAffixDetector: building TagAffixDetector from /tmp/cc.tmp and /tmp/ad.tmp
TagAffixDetector: building TagAffixDetector from /etc/cc/cc.ini and /etc/ad/ad.ini
TagAffixDetector: building TagAffixDetector from /usr/local/cc/cc.jar and /usr/local/ad/ad.jar
TagAffixDetector: building TagAffixDetector from /data/cc/cc.csv and /data/ad/ad.csv
TagAffixDetector: building TagAffixDetector from /dev/cc/cc.bin and /dev/ad/ad.bin
TagAffixDetector: building TagAffixDetector from /media/cc/cc.iso and /media/ad/ad.iso
TagAffixDetector: building TagAffixDetector from /mnt/cc/cc.img and /mnt/ad/ad.img
TagAffixDetector: building TagAffixDetector from /proc/cc/cc.pid and /proc/ad/ad.pid
TagAffixDetector: building TagAffixDetector from /root/cc/cc.key and /root/ad/ad.key
TagAffixDetector: building TagAffixDetector from /run/cc/cc.sock and /run/ad/ad.sock
TagAffixDetector: building TagAffixDetector from /srv/cc/cc.html and /srv/ad/ad.html
TagAffixDetector: building TagAffixDetector from /sys/cc/cc.ko and /sys/ad/ad.ko
Redwood.DBG Removing 12 patterns that do not meet minUnlabPhraseSupportForPat requirement of >= 0.5
Redwood.DBG Removing 8 patterns that do not meet minUnlabPhraseSupportForPat requirement of >= 0.7
Redwood.DBG Removing 15 patterns that do not meet minUnlabPhraseSupportForPat requirement of >= 0.6
Redwood.DBG Removing 10 patterns that do not meet minUnlabPhraseSupportForPat requirement of >= 0.8
Redwood.DBG Removing 9 patterns that do not meet minUnlabPhraseSupportForPat requirement of >= 0.9
Redwood.DBG Removing 11 patterns that do not meet minUnlabPhraseSupportForPat requirement of >= 0.4
Redwood.DBG Removing 13 patterns that do not meet minUnlabPhraseSupportForPat requirement of >= 0.3
Redwood.DBG Removing 7 patterns that do not meet minUnlabPhraseSupportForPat requirement of >= 1.0
Redwood.DBG Removing 14 patterns that do not meet minUnlabPhraseSupportForPat requirement of >= 0.2
Redwood.DBG Removing 6 patterns that do not meet minUnlabPhraseSupportForPat requirement of >= 1.1
Redwood.DBG Removing 16 patterns that do not meet minUnlabPhraseSupportForPat requirement of >= 0.1
Redwood.DBG Removing 5 patterns that do not meet minUnlabPhraseSupportForPat requirement of >= 1.2
Redwood.DBG Removing 17 patterns that do not meet minUnlabPhraseSupportForPat requirement of >= 0.05
Redwood.DBG Removing 4 patterns that do not meet minUnlabPhraseSupportForPat requirement of >= 1.3
Redwood.DBG Removing 18 patterns that do not meet minUnlabPhraseSupportForPat requirement of >= 0.01
Deleting META tree that should be XML metadata in chtb_5200.df: child ChineseTreebankLanguagePack.ENCODING
Deleting META tree that should be XML metadata in chtb_5200.df: root ChineseTreebankLanguagePack.UTF8
Deleting META tree that should be XML metadata in chtb_5200.df: node ChineseTreebankLanguagePack.GBK
Deleting META tree that should be XML metadata in chtb_5200.df: leaf ChineseTreebankLanguagePack.BIG5
Deleting META tree that should be XML metadata in chtb_5200.df: parent ChineseTreebankLanguagePack.ASCII
Deleting META tree that should be XML metadata in chtb_5200.df: sibling ChineseTreebankLanguagePack.ISO8859
Deleting META tree that should be XML metadata in chtb_5200.df: ancestor ChineseTreebankLanguagePack.EUC
Deleting META tree that should be XML metadata in chtb_5200.df: descendant ChineseTreebankLanguagePack.SJIS
Deleting META tree that should be XML metadata in chtb_5200.df: cousin ChineseTreebankLanguagePack.KSC
Deleting META tree that should be XML metadata in chtb_5200.df: nephew ChineseTreebankLanguagePack.GB18030
Deleting META tree that should be XML metadata in chtb_5200.df: niece ChineseTreebankLanguagePack.CP936
Deleting META tree that should be XML metadata in chtb_5200.df: uncle ChineseTreebankLanguagePack.CP950
Deleting META tree that should be XML metadata in chtb_5200.df: aunt ChineseTreebankLanguagePack.CP1252
Deleting META tree that should be XML metadata in chtb_5200.df: grandparent ChineseTreebankLanguagePack.CP1251
Deleting META tree that should be XML metadata in chtb_5200.df: grandchild ChineseTreebankLanguagePack.CP1256
FALSE NEGATIVE: Method m = new Method("add", int.class, int.class)
FALSE NEGATIVE: Method m = new Method("print", String.class)
FALSE NEGATIVE: Method m = new Method("sort", List.class, Comparator.class)
FALSE NEGATIVE: Method m = new Method("equals", Object.class)
FALSE NEGATIVE: Method m = new Method("hashCode")
FALSE NEGATIVE: Method m = new Method("toString")
FALSE NEGATIVE: Method m = new Method("clone")
FALSE NEGATIVE: Method m = new Method("notify")
FALSE NEGATIVE: Method m = new Method("wait", long.class)
FALSE NEGATIVE: Method m = new Method("finalize")
FALSE NEGATIVE: Method m = new Method("append", StringBuilder.class, char.class)
FALSE NEGATIVE: Method m = new Method("reverse", StringBuilder.class)
FALSE NEGATIVE: Method m = new Method("charAt", String.class, int.class)
FALSE NEGATIVE: Method m = new Method("length", String.class)
FALSE NEGATIVE: Method m = new Method("substring", String.class, int.class, int.class)
incoming size 100 resulting 100
incoming size 50 resulting 48
incoming size 200 resulting 198
incoming size 75 resulting 73
incoming size 150 resulting 149
incoming size 25 resulting 24
incoming size 125 resulting 123
incoming size 175 resulting 174
incoming size 80 resulting 79
incoming size 40 resulting 39
incoming size 60 resulting 58
incoming size 140 resulting 138
incoming size 90 resulting 88
incoming size 30 resulting 29
incoming size 110 resulting 108
Lexicon log prob state(NP, 1) - the : -0.23
Lexicon log prob state(VP, 2) - ate : -0.12
Lexicon log prob state(NP, 3) - pizza : -0.34
Lexicon log prob state(PP, 4) - with : -0.45
Lexicon log prob state(NP, 5) - cheese : -0.56
Lexicon log prob state(END, 6) - . : -0.67
Lexicon log prob state(START, 0) - She : -0.11
Lexicon log prob state(VP, 1) - likes : -0.22
Lexicon log prob state(NP, 2) - dogs : -0.33
Lexicon log prob state(PP, 3) - more : -0.44
Lexicon log prob state(ADJP, 4) - than : -0.55
Lexicon log prob state(NP, 5) - cats : -0.66
Lexicon log prob state(END, 6) - . : -0.77
Lexicon log prob state(START, 0) - They : -0.21
Lexicon log prob state(VP, 1) - went : -0.32
Tokenizer tagged 345 words in 12 documents at 28.9 words per second.
Stemmer tagged 276 words in 15 documents at 23.0 words per second.
Tagger tagged 421 words in 18 documents at 35.1 words per second.
Parser tagged 312 words in 10 documents at 26.0 words per second.
Lemmatizer tagged 398 words in 14 documents at 33.2 words per second.
Chunker tagged 289 words in 11 documents at 24.1 words per second.
NER tagged 367 words in 13 documents at 30.6 words per second.
SentimentAnalyzer tagged 433 words in 16 documents at 36.1 words per second.
TopicModeler tagged 352 words in 12 documents at 29.3 words per second.
Summarizer tagged 411 words in 17 documents at 34.3 words per second.
Translator tagged 325 words in 11 documents at 27.1 words per second.
SpellChecker tagged 301 words in 10 documents at 25.1 words per second.
SynonymGenerator tagged 378 words in 14 documents at 31.5 words per second.
KeywordExtractor tagged 392 words in 15 documents at 32.7 words per second.
TextGenerator tagged 419 words in 18 documents at 35.0 words per second.
Serialized annotation saved in document.ser
Serialized annotation saved in image.ser
Serialized annotation saved in video.ser
Serialized annotation saved in audio.ser
Serialized annotation saved in report.ser
Serialized annotation saved in data.ser
Serialized annotation saved in model.ser
Serialized annotation saved in config.ser
Serialized annotation saved in test.ser
Serialized annotation saved in project.ser
Serialized annotation saved in user.ser
Serialized annotation saved in profile.ser
Serialized annotation saved in message.ser
Serialized annotation saved in comment.ser
Processing sentence: "Hello, world!"
Processing sentence: "The quick brown fox jumps over the lazy dog."
Processing sentence: "To be or not to be, that is the question."
Processing sentence: "I have a dream that one day this nation will rise up and live out the true meaning of its creed."
Processing sentence: "May the Force be with you."
Processing sentence: "In the beginning God created the heaven and the earth."
Processing sentence: "It was a bright cold day in April, and the clocks were striking thirteen."
Processing sentence: "All animals are equal, but some animals are more equal than others."
Processing sentence: "You can't handle the truth!"
Processing sentence: "Houston, we have a problem."
Processing sentence: "One small step for man, one giant leap for mankind."
Processing sentence: "Frankly, my dear, I don't give a damn."
Processing sentence: "Elementary, my dear Watson."
Processing sentence: "I'm sorry, Dave. I'm afraid I can't do that."
Processing sentence: "The answer to life, the universe and everything is 42."
com.example.MyClass : Skipping illegal parameter value in config.xml (line 23)
com.example.MyService : Skipping illegal parameter value in settings.json (line 12)
com.example.MyController : Skipping illegal parameter value in app.properties (line 34)
com.example.MyUtil : Skipping illegal parameter value in constants.java (line 56)
com.example.MyModel : Skipping illegal parameter value in schema.sql (line 45)
com.example.MyView : Skipping illegal parameter value in layout.xml (line 67)
com.example.MyTest : Skipping illegal parameter value in testdata.csv (line 89)
com.example.MyListener : Skipping illegal parameter value in events.log (line 78)
com.example.MyAdapter : Skipping illegal parameter value in adapter.ini (line 54)
com.example.MyException : Skipping illegal parameter value in error.txt (line 32)
com.example.MyFactory : Skipping illegal parameter value in factory.conf (line 21)
com.example.MyValidator : Skipping illegal parameter value in rules.xml (line 43)
com.example.MyConverter : Skipping illegal parameter value in format.json (line 65)
com.example.MyFilter : Skipping illegal parameter value in filter.properties (line 87)
com.example.Foo : Built from java.io.FileInputStream
com.example.Bar : Built from java.io.ByteArrayInputStream
com.example.Baz : Built from java.io.ObjectInputStream
com.example.Qux : Built from java.io.DataInputStream
com.example.Quux : Built from java.io.BufferedInputStream
com.example.Corge : Built from java.io.StringBufferInputStream
com.example.Grault : Built from java.io.SequenceInputStream
com.example.Garply : Built from java.io.PushbackInputStream
com.example.Waldo : Built from java.io.FilterInputStream
com.example.Fred : Built from java.io.PipedInputStream
com.example.Plugh : Built from java.io.LineNumberInputStream
com.example.Xyzzy : Built from java.io.CharArrayReader
com.example.Thud : Built from java.io.FileReader
com.example.FooBar : Built from java.io.InputStreamReader
com.example.FooBaz : Built from java.io.BufferedReader
Evaluating: [x + y = 5]
Evaluating: [sin(x) * cos(y)]
Evaluating: [a^2 + b^2 = c^2]
Evaluating: [log(x) + exp(y)]
Evaluating: [f(x) = x^3 - 2x + 1]
Evaluating: [g(x) = sqrt(x) + 1/x]
Evaluating: [h(x) = tan(x) / sec(x)]
Evaluating: [y = mx + b]
Evaluating: [z = ax^2 + bx + c]
Evaluating: [p = 2l + 2w]
Evaluating: [a = lw]
Evaluating: [v = lwh]
Evaluating: [r = (x - h)^2 + (y - k)^2]
Evaluating: [s = ut + 1/2at^2]
Evaluating: [v^2 = u^2 + 2as]
Parsing file: index.html
Parsing file: data.csv
Parsing file: report.pdf
Parsing file: image.jpg
Parsing file: script.py
Parsing file: config.ini
Parsing file: resume.docx
Parsing file: music.mp3
Parsing file: game.exe
Parsing file: logo.png
Parsing file: style.css
Parsing file: notes.txt
Parsing file: video.mp4
Parsing file: calendar.ics
Parsing file: backup.zip
Loading relation model from relationModel_1.json
Loading relation model from relationModel_2.pkl
Loading relation model from relationModel_3.h5
Loading relation model from relationModel_4.csv
Loading relation model from relationModel_5.xml
Loading relation model from relationModel_6.npz
Loading relation model from relationModel_7.mat
Loading relation model from relationModel_8.pth
Loading relation model from relationModel_9.onnx
Loading relation model from relationModel_10.joblib
Loading relation model from relationModel_11.npz
Loading relation model from relationModel_12.hdf5
Loading relation model from relationModel_13.torch
Loading relation model from relationModel_14.tf
Loading relation model from relationModel_15.bin
undirected nodes btw 2 and 4 is 3
undirected nodes btw 2 and 4 is 5
undirected nodes btw 2 and 4 is 4
undirected nodes btw 2 and 4 is 6
undirected nodes btw 2 and 4 is 7
undirected nodes btw 2 and 4 is 8
undirected nodes btw 2 and 4 is 9
undirected nodes btw 2 and 4 is 10
undirected nodes btw 2 and 4 is 11
undirected nodes btw 2 and 4 is 12
undirected nodes btw 2 and 4 is 13
undirected nodes btw 2 and 4 is 14
undirected nodes btw 2 and 4 is 15
undirected nodes btw 2 and 4 is 16
undirected nodes btw 2 and 4 is 17
_start BINARY_GRAMMAR
_end BINARY_GRAMMAR
_root BINARY_GRAMMAR
_expr BINARY_GRAMMAR
_term BINARY_GRAMMAR
_factor BINARY_GRAMMAR
_number BINARY_GRAMMAR
_operator BINARY_GRAMMAR
_plus BINARY_GRAMMAR
_minus BINARY_GRAMMAR
_times BINARY_GRAMMAR
_divide BINARY_GRAMMAR
_power BINARY_GRAMMAR
_paren BINARY_GRAMMAR
_digit BINARY_GRAMMAR
Loading lexparser from: englishPCFG.ser.gz
Loading lexparser from: englishFactored.ser.gz
Loading lexparser from: englishRNN.ser.gz
Loading lexparser from: chineseFactored.ser.gz
Loading lexparser from: germanPCFG.ser.gz
Loading lexparser from: frenchFactored.ser.gz
Loading lexparser from: spanishPCFG.ser.gz
Loading lexparser from: arabicFactored.ser.gz
Loading lexparser from: hindiPCFG.ser.gz
Loading lexparser from: japaneseFactored.ser.gz
Loading lexparser from: koreanRNN.ser.gz
Loading lexparser from: russianPCFG.ser.gz
Loading lexparser from: portugueseFactored.ser.gz
Loading lexparser from: italianPCFG.ser.gz
Loading lexparser from: turkishRNN.ser.gz
ConstantsAndVariables.minimaldebug Saving output in outputdir/2023-10-28_09-31-51
ConstantsAndVariables.minimaldebug Saving output in outputdir/2023-10-28_09-32-12
ConstantsAndVariables.minimaldebug Saving output in outputdir/2023-10-28_09-32-34
ConstantsAndVariables.minimaldebug Saving output in outputdir/2023-10-28_09-32-56
ConstantsAndVariables.minimaldebug Saving output in outputdir/2023-10-28_09-33-18
ConstantsAndVariables.minimaldebug Saving output in outputdir/2023-10-28_09-33-40
ConstantsAndVariables.minimaldebug Saving output in outputdir/2023-10-28_09-34-02
ConstantsAndVariables.minimaldebug Saving output in outputdir/2023-10-28_09-34-24
ConstantsAndVariables.minimaldebug Saving output in outputdir/2023-10-28_09-34-46
ConstantsAndVariables.minimaldebug Saving output in outputdir/2023-10-28_09-35-08
ConstantsAndVariables.minimaldebug Saving output in outputdir/2023-10-28_09-35-30
ConstantsAndVariables.minimaldebug Saving output in outputdir/2023-10-28_09-35-52
ConstantsAndVariables.minimaldebug Saving output in outputdir/2023-10-28_09-36-14
ConstantsAndVariables.minimaldebug Saving output in outputdir/2023-10-28_09-36-36
ConstantsAndVariables.minimaldebug Saving output in outputdir/2023-10-28_09-36-58
Determined head (case 3) for 0.5 is: 0.5
Determined head (case 3) for 1.0 is: 1.0
Determined head (case 3) for -0.25 is: -0.25
Determined head (case 3) for 2.5 is: 2.5
Determined head (case 3) for -1.75 is: -1.75
Determined head (case 3) for 3.14 is: 3.14
Determined head (case 3) for -2.718 is: -2.718
Determined head (case 3) for 4.2 is: 4.2
Determined head (case 3) for -3.6 is: -3.6
Determined head (case 3) for 5.0 is: 5.0
Determined head (case 3) for -4.5 is: -4.5
Determined head (case 3) for 6.28 is: 6.28
Determined head (case 3) for -5.12 is: -5.12
Determined head (case 3) for 7.77 is: 7.77
Determined head (case 3) for -6.66 is: -6.66
Error interpreting number range w : Invalid character 'w' in range expression
Error interpreting number range 1-5 : No error
Error interpreting number range 10-3 : Invalid range: lower bound is greater than upper bound
Error interpreting number range 0-0 : Empty range: lower bound is equal to upper bound
Error interpreting number range 1-10,15-20 : No error
Error interpreting number range 5-7,3-4 : Invalid range: overlapping intervals
Error interpreting number range -2--5 : Invalid character '-' in range expression
Error interpreting number range 1.5-2.5 : Invalid character '.' in range expression
Error interpreting number range a-z : No error
Error interpreting number range A-Z,0-9 : No error
Error interpreting number range a-A : Invalid range: lower bound is greater than upper bound
Error interpreting number range a-b,c-d : No error
Error interpreting number range a-c,e-g : No error
Error interpreting number range x-y,z-w : Invalid range: lower bound is greater than upper bound
Error interpreting number range 100-200 : No error
W matrix size: 10 x 10
W matrix size: 5 x 20
W matrix size: 15 x 15
W matrix size: 8 x 12
W matrix size: 12 x 8
W matrix size: 20 x 5
W matrix size: 16 x 16
W matrix size: 9 x 9
W matrix size: 6 x 18
W matrix size: 18 x 6
W matrix size: 14 x 14
W matrix size: 7 x 21
W matrix size: 21 x 7
W matrix size: 13 x 13
W matrix size: 11 x 11
Classifier wordshape: Uppercase
Classifier wordshape: Lowercase
Classifier wordshape: Mixedcase
Classifier wordshape: Numeric
Classifier wordshape: Alphanumeric
Classifier wordshape: Punctuation
Classifier wordshape: Hyphenated
Classifier wordshape: Capitalized
Classifier wordshape: Abbreviation
Classifier wordshape: Symbol
Classifier wordshape: Date
Classifier wordshape: Time
Classifier wordshape: URL
Classifier wordshape: Email
Classifier wordshape: Currency
Level.INFO Hello a fourth time from the class logger
Level.INFO Hello a fourth time from the class logger
Level.INFO Hello a fourth time from the class logger
Level.INFO Hello a fourth time from the class logger
Level.INFO Hello a fourth time from the class logger
Level.INFO Hello a fourth time from the class logger
Level.INFO Hello a fourth time from the class logger
Level.INFO Hello a fourth time from the class logger
Level.INFO Hello a fourth time from the class logger
Level.INFO Hello a fourth time from the class logger
Level.INFO Hello a fourth time from the class logger
Level.INFO Hello a fourth time from the class logger
Level.INFO Hello a fourth time from the class logger
Initial matrices scaled by: 0.5
Initial matrices scaled by: 1.0
Initial matrices scaled by: 0.75
Initial matrices scaled by: 1.5
Initial matrices scaled by: 0.25
Initial matrices scaled by: 2.0
Initial matrices scaled by: 0.1
Initial matrices scaled by: 0.9
Initial matrices scaled by: 1.2
Initial matrices scaled by: 0.6
Initial matrices scaled by: 1.8
Initial matrices scaled by: 0.3
Initial matrices scaled by: 1.4
Initial matrices scaled by: 0.7
Initial matrices scaled by: 1.6
Predicted Mention: "I'm not going to let you do this." Predictor: SpeakerSieve
Predicted Mention: "He said he would be back soon." Predictor: VerbSieve
Predicted Mention: "She smiled and nodded." Predictor: TripletSieve
Predicted Mention: "What do you think?" Predictor: PronounSieve
Predicted Mention: "It's a beautiful day." Predictor: DeterministicSieve
Predicted Mention: "They were arguing about something." Predictor: VerbSieve
Predicted Mention: "You're welcome." Predictor: SpeakerSieve
Predicted Mention: "He was very angry." Predictor: TripletSieve
Predicted Mention: "She asked me to help her." Predictor: VerbSieve
Predicted Mention: "It was a mistake." Predictor: DeterministicSieve
Predicted Mention: "They left without saying goodbye." Predictor: TripletSieve
Predicted Mention: "You can't be serious." Predictor: SpeakerSieve
Predicted Mention: "He gave her a hug." Predictor: VerbSieve
Predicted Mention: "She looked at him with curiosity." Predictor: TripletSieve
Predicted Mention: "What's your name?" Predictor: PronounSieve
Entry 5 is entry 1 of binary score 0 : 1
Entry 12 is entry 4 of binary score 1 : 0
Entry 8 is entry 2 of binary score 0 : 0
Entry 3 is entry 3 of binary score 1 : 1
Entry 10 is entry 5 of binary score 0 : 1
Entry 7 is entry 1 of binary score 1 : 0
Entry 9 is entry 3 of binary score 0 : 0
Entry 4 is entry 4 of binary score 1 : 1
Entry 6 is entry 2 of binary score 1 : 1
Entry 11 is entry 5 of binary score 1 : 0
Entry 2 is entry 2 of binary score 0 : 1
Entry 13 is entry 1 of binary score 0 : 0
Entry 14 is entry 4 of binary score 0 : 0
Entry 15 is entry 5 of binary score 0 : 0
Entry 1 is entry 3 of binary score 1 : 0
Match 0 at: (The/DT/0, dog/NN/1, barks/VBZ/2)
Match 1 at: (She/PRP/0, loves/VBZ/1, him/PRP/2)
Match 2 at: (It/PRP/0, is/VBZ/1, raining/VBG/2)
Match 3 at: (They/PRP/0, are/VBP/1, playing/VBG/2)
Match 4 at: (I/PRP/0, have/VBP/1, a/DT/2, book/NN/3)
Match 5 at: (You/PRP/0, can/MD/1, do/VB/2, it/PRP/3)
Match 6 at: (He/PRP/0, went/VBD/1, to/TO/2, the/DT/3, park/NN/4)
Match 7 at: (We/PRP/0, saw/VBD/1, a/DT/2, movie/NN/3)
Match 8 at: (She's/VBZ+PRP$/0, a/JJ+DT$/1, good/JJ+JJ$/2, friend/JJ+NN$/3)
Match 9 at: (What/WP$/0's/VBZ+POS$/1, your/JJ+PRP$/2, name/JJ+NN$/3)
Match 10 at: (This/JJ+DT$/0's/VBZ+POS$/1, the/JJ+DT$/2, best/JJ+JJ$/3, day/JJ+NN$/4)
Match 11 at: (Who/WP$/0's/VBZ+POS$/1, that/JJ+DT$/2, man/JJ+NN$/3)
Match 12 at: (Where/WRB$/0's/VBZ+POS$/1, the/JJ+DT$/2, nearest/JJ+JJ$/3, store/JJ+NN$/4)
Match 13 at: (When/WRB$/0's/VBZ+POS$/1, the/JJ+DT$/2, next/JJ+JJ$/3, game/JJ+NN$/4)
Match 14 at: (Why/WRB$/0's/VBZ+POS$/1, he/JJ+PRP$/2, so/JJ+RB$/3, sad/JJ+JJ$/4)
Changing normalized NER from PERSON to O
Changing normalized NER from LOCATION to O
Changing normalized NER from DATE to 2023-10-28
Changing normalized NER from MONEY to 100.00 USD
Changing normalized NER from ORGANIZATION to Microsoft
Changing normalized NER from DURATION to 2 hours
Changing normalized NER from PERCENT to 0.75
Changing normalized NER from TIME to 02:44:55
Changing normalized NER from NUMBER to 15
Changing normalized NER from ORDINAL to first
Changing normalized NER from MISC to O
Changing normalized NER from SET to every day
Changing normalized NER from EMAIL to bing@microsoft.com
Changing normalized NER from URL to https://www.bing.com/
Changing normalized NER from PHONE_NUMBER to +33 1 23 45 67 89
Normalizing decimal part: 0
Normalizing decimal part: 5
Normalizing decimal part: 12
Normalizing decimal part: 34
Normalizing decimal part: 78
Normalizing decimal part: 123
Normalizing decimal part: 456
Normalizing decimal part: 789
Normalizing decimal part: 1011
Normalizing decimal part: 1213
Normalizing decimal part: 1415
Normalizing decimal part: 1617
Normalizing decimal part: 1819
Normalizing decimal part: 2021
Normalizing decimal part: 2223
Initialized: 1635403346000
Initialized: 1635403351000
Initialized: 1635403354000
Initialized: 1635403357000
Initialized: 1635403360000
Initialized: 1635403363000
Initialized: 1635403366000
Initialized: 1635403369000
Initialized: 1635403372000
Initialized: 1635403375000
Initialized: 1635403378000
Initialized: 1635403381000
Initialized: 1635403384000
Initialized: 1635403387000
Initialized: 1635403390000
Attempting to load http://www.example.com/grammar.ser as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load C:\Users\Alice\Documents\grammar.ser as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load ftp://localhost/grammar.ser as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load /home/bob/grammar.ser as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load file:///tmp/grammar.ser as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load https://www.bing.com/grammar.ser as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load D:\Projects\grammar.ser as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load sftp://user@host/grammar.ser as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load jar:file:/lib/grammar.jar!/grammar.ser as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load smb://server/share/grammar.ser as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load resource:/org/example/grammar.ser as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load E:\Backup\grammar.ser as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load https://github.com/user/repo/raw/master/grammar.ser as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load file:/Users/charlie/Desktop/grammar.ser as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load data:text/plain;base64,Z3JhbW1hci5zZXI= as a serialized grammar caused error below, but this may just be because it's a text grammar!
Loading inverted index from /home/user/data/index
Loading inverted index from C:\Users\user\Documents\index
Loading inverted index from /mnt/sda1/index
Loading inverted index from /var/lib/elasticsearch/index
Loading inverted index from /tmp/index
Loading inverted index from /opt/index
Loading inverted index from D:\index
Loading inverted index from /usr/local/index
Loading inverted index from /dev/sdb2/index
Loading inverted index from E:\index
Loading inverted index from /media/user/USB/index
Loading inverted index from /root/index
Loading inverted index from F:\index
Loading inverted index from /etc/index
Loading inverted index from G:\index
Splitting 100 trees
Splitting 50 trees
Splitting 75 trees
Splitting 25 trees
Splitting 150 trees
Splitting 200 trees
Splitting 125 trees
Splitting 175 trees
Splitting 10 trees
Splitting 5 trees
Splitting 80 trees
Splitting 40 trees
Splitting 60 trees
Splitting 20 trees
Splitting 140 trees
C = 0.5
C = 1.0
C = 0.75
C = 0.25
C = 0.8
C = 0.6
C = 0.4
C = 0.9
C = 0.7
C = 0.3
C = 0.2
C = 1.2
C = 1.1
C = 0.1
C = 1.3
12 words were in dictionary).
8 words were in dictionary).
15 words were in dictionary).
10 words were in dictionary).
9 words were in dictionary).
11 words were in dictionary).
13 words were in dictionary).
7 words were in dictionary).
14 words were in dictionary).
6 words were in dictionary).
16 words were in dictionary).
5 words were in dictionary).
17 words were in dictionary).
4 words were in dictionary).
18 words were in dictionary).
output: 0.56
output: 0.67
output: 0.72
output: 0.61
output: 0.64
output: 0.69
output: 0.74
output: 0.59
output: 0.66
output: 0.71
output: 0.62
output: 0.65
output: 0.68
output: 0.75
output: 0.58
norm= 0.12
norm= -1.34
norm= 0.67
norm= -0.45
norm= 1.23
norm= -2.01
norm= 0.89
norm= -0.76
norm= 1.56
norm= -1.89
norm= 0.34
norm= -0.12
norm= 1.78
norm= -1.45
norm= 0.56
ANS (before comma norm): I'm going to the store UTF-8
ANS (before comma norm): She said, "Hello." UTF-8
ANS (before comma norm): He likes apples, bananas, and oranges. UTF-8
ANS (before comma norm): What time is it? UTF-8
ANS (before comma norm): It's a beautiful day. UTF-8
ANS (before comma norm): They went to the park, the museum, and the library. UTF-8
ANS (before comma norm): He asked, "Do you want to play?" UTF-8
ANS (before comma norm): She loves reading, writing, and painting. UTF-8
ANS (before comma norm): How are you? UTF-8
ANS (before comma norm): It's raining cats and dogs. UTF-8
ANS (before comma norm): He said, "Thank you." UTF-8
ANS (before comma norm): She likes chocolate, vanilla, and strawberry. UTF-8
ANS (before comma norm): Where are you from? UTF-8
ANS (before comma norm): It's a nice day. UTF-8
ANS (before comma norm): They watched a movie, ate popcorn, and drank soda. UTF-8
Quantifiable modifiers: previous: 0.5 ' ' 0.6 ' ' 0.7
Quantifiable modifiers: previous: 1.2 ' ' 1.1 ' ' 1.0
Quantifiable modifiers: previous: -0.3 ' ' -0.2 ' ' -0.1
Quantifiable modifiers: previous: 0.9 ' ' 0.8 ' ' 0.7
Quantifiable modifiers: previous: 0.4 ' ' 0.5 ' ' 0.6
Quantifiable modifiers: previous: -0.8 ' ' -0.7 ' ' -0.6
Quantifiable modifiers: previous: 1.5 ' ' 1.4 ' ' 1.3
Quantifiable modifiers: previous: -0.4 ' ' -0.3 ' ' -0.2
Quantifiable modifiers: previous: 1.8 ' ' 1.7 ' ' 1.6
Quantifiable modifiers: previous: -1.2 ' ' -1.1 ' ' -1.0
Quantifiable modifiers: previous: 2.1 ' ' 2.0 ' ' 1.9
Quantifiable modifiers: previous: -1.5 ' ' -1.4 ' ' -1.3
Quantifiable modifiers: previous: 2.4 ' ' 2.3 ' ' 2.2
Quantifiable modifiers: previous: -1.8 ' ' -1.7 ' ' -1.6
Quantifiable modifiers: previous: 2.7 ' ' 2.6 ' ' 2.5
com.example.service.UserService : Unable to access mapper type com.example.mapper.UserMapper
com.example.controller.BookController : Unable to access mapper type com.example.mapper.BookMapper
com.example.dao.ProductDao : Unable to access mapper type com.example.mapper.ProductMapper
com.example.service.OrderService : Unable to access mapper type com.example.mapper.OrderMapper
com.example.controller.CustomerController : Unable to access mapper type com.example.mapper.CustomerMapper
com.example.dao.CategoryDao : Unable to access mapper type com.example.mapper.CategoryMapper
com.example.service.ReviewService : Unable to access mapper type com.example.mapper.ReviewMapper
com.example.controller.CartController : Unable to access mapper type com.example.mapper.CartMapper
com.example.dao.InventoryDao : Unable to access mapper type com.example.mapper.InventoryMapper
com.example.service.PaymentService : Unable to access mapper type com.example.mapper.PaymentMapper
com.example.controller.AccountController : Unable to access mapper type com.example.mapper.AccountMapper
com.example.dao.AddressDao : Unable to access mapper type com.example.mapper.AddressMapper
com.example.service.ShippingService : Unable to access mapper type com.example.mapper.ShippingMapper
com.example.controller.ProductController : Unable to access mapper type com.example.mapper.ProductMapper
com.example.dao.UserDao : Unable to access mapper type com.example.mapper.UserMapper
Predicted Speaker: John Predictor: Deterministic
Predicted Speaker: Alice Predictor: Coreference
Predicted Speaker: Bob Predictor: Convolutional Neural Network
Predicted Speaker: Mary Predictor: Quotation
Predicted Speaker: David Predictor: SpeakerMatch
Predicted Speaker: Emma Predictor: SingleSpeaker
Predicted Speaker: James Predictor: Supervised
Predicted Speaker: Anna Predictor: TripletLoss
Predicted Speaker: George Predictor: Bayesian
Predicted Speaker: Lucy Predictor: DependencyParse
Predicted Speaker: Tom Predictor: Embedding
Predicted Speaker: Kate Predictor: FeatureExtraction
Predicted Speaker: Jack Predictor: GraphicalModel
Predicted Speaker: Lily Predictor: Heuristic
Predicted Speaker: Harry Predictor: LinearRegression
StanfordCoreNLPServer listening at http://localhost:9000
StanfordCoreNLPServer listening at https://192.168.1.100:8080
StanfordCoreNLPServer listening at http://corenlp.stanford.edu:80
StanfordCoreNLPServer listening at https://10.0.0.1:443
StanfordCoreNLPServer listening at http://127.0.0.1:9001
StanfordCoreNLPServer listening at https://172.16.0.2:8443
StanfordCoreNLPServer listening at http://[2001:db8::1]:8000
StanfordCoreNLPServer listening at https://[fe80::a00:27ff:fe4e:66a1]:9090
StanfordCoreNLPServer listening at http://example.com:8081
StanfordCoreNLPServer listening at https://subdomain.example.org:8444
StanfordCoreNLPServer listening at http://54.173.224.150:8008
StanfordCoreNLPServer listening at https://35.160.3.103:8088
StanfordCoreNLPServer listening at http://ec2-54-173-224-150.compute-1.amazonaws.com:9009
StanfordCoreNLPServer listening at https://ec2-35-160-3-103.us-west-2.compute.amazonaws.com:9099
StanfordCoreNLPServer listening at http://myserver.mydomain.com:8888
DocDate mapping file failed to match against 5f3a7c9b
DocDate mapping file failed to match against 8d4e2f6a
DocDate mapping file failed to match against 1a9c6e83
DocDate mapping file failed to match against 4b7d8f2c
DocDate mapping file failed to match against 9e5b4a1d
DocDate mapping file failed to match against 2c8d7e9f
DocDate mapping file failed to match against 7f6a3b8e
DocDate mapping file failed to match against 0d5c2a7d
DocDate mapping file failed to match against 3e9d8b6c
DocDate mapping file failed to match against 6b4e5c3b
DocDate mapping file failed to match against a3f2ed1a
DocDate mapping file failed to match against d2c1fe39
DocDate mapping file failed to match against e1b0df28
DocDate mapping file failed to match against b0a9ce17
DocDate mapping file failed to match against c9f8bd06
initial graph has 12 arcs and 8 nodes.
initial graph has 25 arcs and 10 nodes.
initial graph has 18 arcs and 9 nodes.
initial graph has 30 arcs and 12 nodes.
initial graph has 15 arcs and 7 nodes.
initial graph has 20 arcs and 11 nodes.
initial graph has 24 arcs and 13 nodes.
initial graph has 16 arcs and 6 nodes.
initial graph has 22 arcs and 14 nodes.
initial graph has 28 arcs and 15 nodes.
initial graph has 14 arcs and 5 nodes.
initial graph has 26 arcs and 16 nodes.
initial graph has 32 arcs and 17 nodes.
initial graph has 10 arcs and 4 nodes.
initial graph has 34 arcs and 18 nodes.
# rareWordMinFeatureThresh = 0.01
# rareWordMinFeatureThresh = 0.05
# rareWordMinFeatureThresh = 0.1
# rareWordMinFeatureThresh = 0.2
# rareWordMinFeatureThresh = 0.3
# rareWordMinFeatureThresh = 0.4
# rareWordMinFeatureThresh = 0.5
# rareWordMinFeatureThresh = 0.6
# rareWordMinFeatureThresh = 0.7
# rareWordMinFeatureThresh = 0.8
# rareWordMinFeatureThresh = 0.9
# rareWordMinFeatureThresh = 1.0
# rareWordMinFeatureThresh = 1.5
# rareWordMinFeatureThresh = 2.0
# rareWordMinFeatureThresh = 2.5
Creating partition # 1 using offsets [ 0 , 100 ) out of 500
Creating partition # 2 using offsets [ 100 , 200 ) out of 500
Creating partition # 3 using offsets [ 200 , 300 ) out of 500
Creating partition # 4 using offsets [ 300 , 400 ) out of 500
Creating partition # 5 using offsets [ 400 , 500 ) out of 500
Creating partition # 6 using offsets [ 0 , 50 ) out of 250
Creating partition # 7 using offsets [ 50 , 100 ) out of 250
Creating partition # 8 using offsets [ 100 , 150 ) out of 250
Creating partition # 9 using offsets [ 150 , 200 ) out of 250
Creating partition # 10 using offsets [ 200 , 250 ) out of 250
Creating partition # 11 using offsets [ 0 , 75 ) out of 375
Creating partition # 12 using offsets [ 75 , 150 ) out of 375
Creating partition # 13 using offsets [ 150 , 225 ) out of 375
Creating partition # 14 using offsets [ 225 , 300 ) out of 375
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: [The, quick, brown, fox, jumps, over, the, lazy, dog]
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: [This, is, a, test, sentence, for, the, simulation]
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: [Hello, world, this, is, Bing]
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: [How, are, you, today]
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: [I, like, to, eat, pizza]
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: [What, is, your, favorite, color]
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: [She, loves, to, read, books]
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: [He, plays, the, guitar]
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: [They, are, going, to, the, park]
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: [It's a beautiful day]
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: [Where are you from]
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: [Can you help me with something]
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: [Do you like music]
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: [Who is your favorite actor]
Parsing of sentence failed, possibly because of out of memory. Will ignore and continue: [Why is the sky blue]
Read in 5 sentences.
Read in 12 sentences.
Read in 8 sentences.
Read in 10 sentences.
Read in 7 sentences.
Read in 9 sentences.
Read in 6 sentences.
Read in 11 sentences.
Read in 4 sentences.
Read in 13 sentences.
Read in 3 sentences.
Read in 14 sentences.
Read in 2 sentences.
Read in 15 sentences.
Binomial p(X >= 3 ; 5 , 0.4 ) = 0.23
Binomial p(X >= 1 ; 10 , 0.8 ) = 0.99
Binomial p(X >= 4 ; 7 , 0.6 ) = 0.42
Binomial p(X >= 2 ; 8 , 0.3 ) = 0.86
Binomial p(X >= 5 ; 6 , 0.5 ) = 0.34
Binomial p(X >= 6 ; 9 , 0.7 ) = 0.27
Binomial p(X >= 7 ; 12 , 0.4 ) = 0.06
Binomial p(X >= 8 ; 15 , 0.2 ) = 0.01
Binomial p(X >= 9 ; 10 , 0.9 ) = 0.39
Binomial p(X >= 10 ; 20 , 0.5 ) = 0.12
Binomial p(X >= 11 ; 18 , 0.6 ) = 0.04
Binomial p(X >= 12 ; 25 , 0.3 ) = <0.01
Binomial p(X >= 13 ; 30 , 0.4 ) = <0.01
Binomial p(X >= 14 ; 35 , 0.2 ) = <0.01
Binomial p(X >= 15 ; 40 , 0.1 ) = <0.01
Training took 12 ms
Training took 17 ms
Training took 9 ms
Training took 15 ms
Training took 11 ms
Training took 14 ms
Training took 10 ms
Training took 13 ms
Training took 16 ms
Training took 8 ms
Training took 18 ms
Training took 7 ms
Training took 19 ms
Training took 6 ms
Training took 20 ms
Unknown word in position 4 : 'alot'
Unknown word in position 12 : 'wierd'
Unknown word in position 7 : 'definately'
Unknown word in position 9 : 'recieve'
Unknown word in position 3 : 'irregardless'
Unknown word in position 11 : 'supposably'
Unknown word in position 5 : 'congradulations'
Unknown word in position 8 : 'miniscule'
Unknown word in position 10 : 'seperate'
Unknown word in position 6 : 'realy'
Unknown word in position 13 : 'perogative'
Unknown word in position 14 : 'persue'
Unknown word in position 15 : 'ocassion'
Unknown word in position 16 : 'existance'
Unknown word in position 17 : 'embarass'
NOT ACCEPTED : /home/user/documents/file.txt
NOT ACCEPTED : /var/log/syslog
NOT ACCEPTED : /etc/passwd
NOT ACCEPTED : /usr/bin/python
NOT ACCEPTED : /dev/null
NOT ACCEPTED : /tmp/test.txt
NOT ACCEPTED : /opt/app/config.ini
NOT ACCEPTED : /root/.bashrc
NOT ACCEPTED : /media/cdrom/autorun.exe
NOT ACCEPTED : /proc/meminfo
NOT ACCEPTED : /boot/grub/grub.cfg
NOT ACCEPTED : /lib/modules/5.4.0-72-generic/kernel/drivers/net/ethernet/intel/e1000e/e1000e.ko
NOT ACCEPTED : /srv/http/index.html
NOT ACCEPTED : /mnt/usb/file.docx
NOT ACCEPTED : /run/user/1000/gvfs/sftp:host=example.com,user=alice/dir/file.jpg
After optimization neg (penalized) log cond likelihood: 0.123
After optimization neg (penalized) log cond likelihood: 0.456
After optimization neg (penalized) log cond likelihood: 0.789
After optimization neg (penalized) log cond likelihood: 0.234
After optimization neg (penalized) log cond likelihood: 0.567
After optimization neg (penalized) log cond likelihood: 0.891
After optimization neg (penalized) log cond likelihood: 0.345
After optimization neg (penalized) log cond likelihood: 0.678
After optimization neg (penalized) log cond likelihood: 0.912
After optimization neg (penalized) log cond likelihood: 0.456
After optimization neg (penalized) log cond likelihood: 0.789
After optimization neg (penalized) log cond likelihood: 0.123
After optimization neg (penalized) log cond likelihood: 0.567
After optimization neg (penalized) log cond likelihood: 0.891
After optimization neg (penalized) log cond likelihood: 0.234
Default encoding is: UTF-8
Default encoding is: ISO-8859-1
Default encoding is: ASCII
Default encoding is: GB2312
Default encoding is: Big5
Default encoding is: Shift-JIS
Default encoding is: EUC-KR
Default encoding is: UTF-16
Default encoding is: Windows-1252
Default encoding is: KOI8-R
Default encoding is: ISO-2022-JP
Default encoding is: CP437
Default encoding is: UTF-32
Default encoding is: ISO-8859-15
Default encoding is: CP1251
File or directory not found /home/user/Documents
File or directory not found /var/log/syslog
File or directory not found /etc/passwd
File or directory not found /usr/bin/python
File or directory not found /mnt/usb
File or directory not found /dev/sda1
File or directory not found /tmp/test.txt
File or directory not found /opt/java
File or directory not found /root/.bashrc
File or directory not found /lib/modules
File or directory not found /proc/meminfo
File or directory not found /boot/grub
File or directory not found /media/cdrom
File or directory not found /srv/http/index.html
File or directory not found /bin/ls
Redwood.DBG at the bottom
Redwood.DBG at the bottom
Redwood.DBG at the bottom
Redwood.DBG at the bottom
Redwood.DBG at the bottom
Redwood.DBG at the bottom
Redwood.DBG at the bottom
Redwood.DBG at the bottom
Redwood.DBG at the bottom
Redwood.DBG at the bottom
Redwood.DBG at the bottom
Redwood.DBG at the bottom
Redwood.DBG at the bottom
Find marked head method returned **NP** as head of **S**.
Find marked head method returned **VP** as head of **NP**.
Find marked head method returned **VBD** as head of **VP**.
Find marked head method returned **NN** as head of **NP**.
Find marked head method returned **DT** as head of **NP**.
Find marked head method returned **JJ** as head of **NP**.
Find marked head method returned **IN** as head of **PP**.
Find marked head method returned **NP** as head of **PP**.
Find marked head method returned **VBG** as head of **VP**.
Find marked head method returned **RB** as head of **ADVP**.
Find marked head method returned **ADVP** as head of **S**.
Find marked head method returned **PRP$** as head of **NP**.
Find marked head method returned **NNS** as head of **NP**.
Find marked head method returned **CC** as head of **NP**.
Find marked head method returned **MD** as head of **VP**.
1-tailed Fisher's exact( 12 ; 20 , 15 , 25 ) = 0.023
1-tailed Fisher's exact( 8 ; 18 , 10 , 30 ) = 0.056
1-tailed Fisher's exact( 5 ; 15 , 7 , 20 ) = 0.118
1-tailed Fisher's exact( 10 ; 22 , 12 , 28 ) = 0.037
1-tailed Fisher's exact( 7 ; 16 , 9 , 24 ) = 0.079
1-tailed Fisher's exact( 9 ; 19 , 11 , 26 ) = 0.048
1-tailed Fisher's exact( 6 ; 14 , 8 , 18 ) = 0.095
1-tailed Fisher's exact( 11 ; 21 , 13 , 23 ) = 0.029
1-tailed Fisher's exact( 4 ; 12 , 6 , 16 ) = 0.142
1-tailed Fisher's exact( 13 ; 23 , 15 , 27 ) = 0.021
1-tailed Fisher's exact( 3 ; 11 , 5 , 14 ) = 0.167
1-tailed Fisher's exact( 14 ; 24 , 16 , 29 ) = 0.018
1-tailed Fisher's exact( 2 ; 10 , 4 , 12 ) = 0.192
1-tailed Fisher's exact( 15 ; 25 , 17 , 31 ) = 0.016
Dataset construction took 12 ms
Dataset construction took 34 ms
Dataset construction took 27 ms
Dataset construction took 45 ms
Dataset construction took 18 ms
Dataset construction took 39 ms
Dataset construction took 23 ms
Dataset construction took 31 ms
Dataset construction took 42 ms
Dataset construction took 15 ms
Dataset construction took 36 ms
Dataset construction took 29 ms
Dataset construction took 48 ms
Dataset construction took 21 ms
Dataset construction took 33 ms
Using unknown number vector for Chinese words: [0.1, 0.2, 0.3, 0.4]
Using unknown number vector for Chinese words: [0.5, 0.6, 0.7, 0.8]
Using unknown number vector for Chinese words: [0.9, 1.0, 1.1, 1.2]
Using unknown number vector for Chinese words: [1.3, 1.4, 1.5, 1.6]
Using unknown number vector for Chinese words: [1.7, 1.8, 1.9, 2.0]
Using unknown number vector for Chinese words: [2.1, 2.2, 2.3, 2.4]
Using unknown number vector for Chinese words: [2.5, 2.6, 2.7, 2.8]
Using unknown number vector for Chinese words: [2.9, 3.0, 3.1, 3.2]
Using unknown number vector for Chinese words: [3.3, 3.4, 3.5, 3.6]
Using unknown number vector for Chinese words: [3.7, 3.8, 3.9, 4.0]
Using unknown number vector for Chinese words: [4.1, 4.2, 4.3, 4.4]
Using unknown number vector for Chinese words: [4.5, 4.6, 4.7, 4.8]
Using unknown number vector for Chinese words: [4.9, 5.0, 5.1, 5.2]
Using unknown number vector for Chinese words: [5.3, 5.4, 5.5, 5.6]
Using unknown number vector for Chinese words: [5.7, 5.8, 5.9, 6.0]
Running time 3 min 45 s
Running time 5 min 12 s
Running time 7 min 30 s
Running time 4 min 8 s
Running time 6 min 24 s
Running time 8 min 36 s
Running time 9 min 15 s
Running time 10 min 0 s
Running time 11 min 48 s
Running time 12 min 6 s
Running time 13 min 54 s
Running time 14 min 42 s
Running time 15 min 18 s
Running time 16 min 27 s
Running time 17 min 33 s
numDocuments: 12
numDocuments: 0
numDocuments: 34
numDocuments: 7
numDocuments: 21
numDocuments: 9
numDocuments: 15
numDocuments: 3
numDocuments: 27
numDocuments: 18
numDocuments: 6
numDocuments: 24
numDocuments: 10
numDocuments: 30
numDocuments: 4
annotating partition 0
annotating partition 1
annotating partition 2
annotating partition 3
annotating partition 4
annotating partition 5
annotating partition 6
annotating partition 7
annotating partition 8
annotating partition 9
annotating partition 10
annotating partition 11
annotating partition 12
annotating partition 13
annotating partition 14
Minimum batch size per CPU: 32
Minimum batch size per CPU: 64
Minimum batch size per CPU: 16
Minimum batch size per CPU: 48
Minimum batch size per CPU: 24
Minimum batch size per CPU: 40
Minimum batch size per CPU: 56
Minimum batch size per CPU: 8
Minimum batch size per CPU: 36
Minimum batch size per CPU: 12
Minimum batch size per CPU: 28
Minimum batch size per CPU: 20
Minimum batch size per CPU: 52
Minimum batch size per CPU: 44
Minimum batch size per CPU: 4
Setting bias for class Animal to 0.25
Setting bias for class Vehicle to -0.1
Setting bias for class Flower to 0.05
Setting bias for class Fruit to 0.15
Setting bias for class Planet to -0.05
Setting bias for class Book to 0.2
Setting bias for class Color to 0.1
Setting bias for class Sport to -0.15
Setting bias for class Music to 0.3
Setting bias for class Movie to 0.35
Setting bias for class Tree to 0.05
Setting bias for class Bird to 0.25
Setting bias for class Fish to -0.2
Setting bias for class Country to -0.1
Setting bias for class Food to 0.4
Output of model # 1 :
Output of model # 2 :
Output of model # 3 :
Output of model # 4 :
Output of model # 5 :
Output of model # 6 :
Output of model # 7 :
Output of model # 8 :
Output of model # 9 :
Output of model # 10 :
Output of model # 11 :
Output of model # 12 :
Output of model # 13 :
Output of model # 14 :
Output of model # 15 :
Used {REDAUX} to recognize "I'm" as "I am" ; quoteStyle= single ; probablyLeft= false
Used {REDAUX} to recognize "don't" as "do not" ; quoteStyle= double ; probablyLeft= false
Used {REDAUX} to recognize "can't" as "cannot" ; quoteStyle= single ; probablyLeft= false
Used {REDAUX} to recognize "it's" as "it is" ; quoteStyle= double ; probablyLeft= false
Used {REDAUX} to recognize "they're" as "they are" ; quoteStyle= single ; probablyLeft= false
Used {REDAUX} to recognize "we'll" as "we will" ; quoteStyle= double ; probablyLeft= false
Used {REDAUX} to recognize "he's" as "he is" ; quoteStyle= single ; probablyLeft= false
Used {REDAUX} to recognize "she's" as "she is" ; quoteStyle= double ; probablyLeft= false
Used {REDAUX} to recognize "you're" as "you are" ; quoteStyle= single ; probablyLeft= false
Used {REDAUX} to recognize "I'll" as "I will" ; quoteStyle= double ; probablyLeft= false
Used {REDAUX} to recognize "they've" as "they have" ; quoteStyle= single ; probablyLeft= false
Used {REDAUX} to recognize "we've" as "we have" ; quoteStyle= double ; probablyLeft= false
Used {REDAUX} to recognize "he'd" as "he would" ; quoteStyle= single ; probablyLeft= false
Used {REDAUX} to recognize "she'd" as "she would" ; quoteStyle= double ; probablyLeft= false
Used {REDAUX} to recognize "you'd" as "you would" ; quoteStyle= single ; probablyLeft= false
Assignment: [true, false, true, false, true]
Assignment: [false, false, false, false, false]
Assignment: [true, true, true, true, true]
Assignment: [false, true, false, true, false]
Assignment: [true, false, false, true, true]
Assignment: [false, false, true, true, false]
Assignment: [true, true, false, false, true]
Assignment: [false, true, true, false, false]
Assignment: [true, false, true, true, false]
Assignment: [false, true, false, false, true]
Assignment: [true, true, false, true, false]
Assignment: [false, false, true, false, true]
Assignment: [true, false, false, false, false]
Assignment: [false, true, true, true, true]
Assignment: [true, true, true, false, false]
before adapting, weights size= 256
before adapting, weights size= 512
before adapting, weights size= 1024
before adapting, weights size= 2048
before adapting, weights size= 4096
before adapting, weights size= 8192
before adapting, weights size= 16384
before adapting, weights size= 32768
before adapting, weights size= 65536
before adapting, weights size= 131072
before adapting, weights size= 262144
before adapting, weights size= 524288
before adapting, weights size= 1048576
before adapting, weights size= 2097152
before adapting, weights size= 4194304
Not hypothesizing a boundary at pos 5, since between two numeral characters (4 and 6).
Not hypothesizing a boundary at pos 12, since between two numeral characters (9 and 0).
Not hypothesizing a boundary at pos 8, since between two numeral characters (3 and 7).
Not hypothesizing a boundary at pos 10, since between two numeral characters (5 and 2).
Not hypothesizing a boundary at pos 7, since between two numeral characters (8 and 1).
Not hypothesizing a boundary at pos 9, since between two numeral characters (6 and 4).
Not hypothesizing a boundary at pos 11, since between two numeral characters (7 and 3).
Not hypothesizing a boundary at pos 6, since between two numeral characters (2 and 5).
Not hypothesizing a boundary at pos 13, since between two numeral characters (0 and 8).
Not hypothesizing a boundary at pos 4, since between two numeral characters (1 and 9).
Not hypothesizing a boundary at pos 14, since between two numeral characters (8 and 2).
Not hypothesizing a boundary at pos 3, since between two numeral characters (9 and 7).
Not hypothesizing a boundary at pos 15, since between two numeral characters (2 and 6).
Not hypothesizing a boundary at pos 2, since between two numeral characters (7 and 4).
Failed to get attributes from "title" , timeIndex 0
Failed to get attributes from "author" , timeIndex 1
Failed to get attributes from "content" , timeIndex 2
Failed to get attributes from "date" , timeIndex 3
Failed to get attributes from "url" , timeIndex 4
Failed to get attributes from "summary" , timeIndex 5
Failed to get attributes from "keywords" , timeIndex 6
Failed to get attributes from "comments" , timeIndex 7
Failed to get attributes from "rating" , timeIndex 8
Failed to get attributes from "category" , timeIndex 9
Failed to get attributes from "tags" , timeIndex 10
Failed to get attributes from "image" , timeIndex 11
Failed to get attributes from "source" , timeIndex 12
Failed to get attributes from "language" , timeIndex 13
Failed to get attributes from "format" , timeIndex 14
Correct: 0.97
Correct: 0.82
Correct: 0.91
Correct: 0.76
Correct: 0.88
Correct: 0.84
Correct: 0.95
Correct: 0.79
Correct: 0.92
Correct: 0.81
Correct: 0.86
Correct: 0.74
Correct: 0.94
Correct: 0.78
Correct: 0.89
Filtered training trees: 12
Filtered training trees: 9
Filtered training trees: 15
Filtered training trees: 11
Filtered training trees: 10
Filtered training trees: 8
Filtered training trees: 13
Filtered training trees: 14
Filtered training trees: 7
Filtered training trees: 6
Filtered training trees: 16
Filtered training trees: 5
Filtered training trees: 4
Filtered training trees: 3
Filtered training trees: 2
left over: [apple, banana, orange]
left over: [red, green, blue]
left over: [cat, dog, mouse]
left over: [one, two, three]
left over: [true, false, null]
left over: [a, b, c]
left over: [x, y, z]
left over: [java, python, c#]
left over: [rock, paper, scissors]
left over: [sun, moon, star]
left over: [north, south, east]
left over: [gold, silver, bronze]
left over: [fire, water, earth]
left over: [monday, tuesday, wednesday]
left over: [spring, summer, autumn]
fSize is 1024
fSize is 512
fSize is 2048
fSize is 256
fSize is 4096
fSize is 128
fSize is 8192
fSize is 64
fSize is 16384
fSize is 32
fSize is 32768
fSize is 16
fSize is 65536
fSize is 8
fSize is 131072
com.example.UserMapper : Mapper type com.example.User does not exist
org.apache.hadoop.mapreduce.Mapper : Mapper type org.apache.hadoop.mapreduce.Reducer does not exist
net.sf.json.JSONObject : Mapper type net.sf.json.JSONArray does not exist
java.util.HashMap : Mapper type java.util.List does not exist
javax.xml.bind.JAXBContext : Mapper type javax.xml.bind.Unmarshaller does not exist
org.springframework.jdbc.core.JdbcTemplate : Mapper type org.springframework.jdbc.core.RowMapper does not exist
com.google.gson.Gson : Mapper type com.google.gson.JsonElement does not exist
org.hibernate.SessionFactory : Mapper type org.hibernate.Query does not exist
java.io.ObjectInputStream : Mapper type java.io.Serializable does not exist
org.apache.commons.lang3.StringUtils : Mapper type org.apache.commons.lang3.StringEscapeUtils does not exist
java.lang.ClassLoader : Mapper type java.lang.Class does not exist
java.util.regex.Pattern : Mapper type java.util.regex.Matcher does not exist
javax.crypto.Cipher : Mapper type javax.crypto.SecretKey does not exist
java.net.URLClassLoader : Mapper type java.net.URL does not exist
java.text.SimpleDateFormat : Mapper type java.util.Date does not exist
A has length 10 and mean 5.4
A has length 8 and mean 6.25
A has length 12 and mean 4.83
A has length 9 and mean 5.67
A has length 11 and mean 5.09
A has length 7 and mean 6.71
A has length 13 and mean 4.54
A has length 6 and mean 7.17
A has length 14 and mean 4.29
A has length 5 and mean 7.6
A has length 15 and mean 4.07
A has length 4 and mean 8.0
A has length 16 and mean 3.88
A has length 3 and mean 8.33
A has length 17 and mean 3.71
Finished parsing 100 trees, getting 5 hypotheses each
Finished parsing 50 trees, getting 10 hypotheses each
Finished parsing 200 trees, getting 3 hypotheses each
Finished parsing 75 trees, getting 7 hypotheses each
Finished parsing 150 trees, getting 4 hypotheses each
Finished parsing 25 trees, getting 12 hypotheses each
Finished parsing 125 trees, getting 6 hypotheses each
Finished parsing 175 trees, getting 2 hypotheses each
Finished parsing 80 trees, getting 8 hypotheses each
Finished parsing 40 trees, getting 9 hypotheses each
Finished parsing 60 trees, getting 11 hypotheses each
Finished parsing 30 trees, getting 13 hypotheses each
Finished parsing 140 trees, getting 14 hypotheses each
Finished parsing 160 trees, getting 15 hypotheses each
Gradient check: converting 10 compressed trees
Gradient check: converting 25 compressed trees
Gradient check: converting 7 compressed trees
Gradient check: converting 18 compressed trees
Gradient check: converting 12 compressed trees
Gradient check: converting 22 compressed trees
Gradient check: converting 15 compressed trees
Gradient check: converting 9 compressed trees
Gradient check: converting 20 compressed trees
Gradient check: converting 13 compressed trees
Gradient check: converting 8 compressed trees
Gradient check: converting 16 compressed trees
Gradient check: converting 11 compressed trees
Gradient check: converting 24 compressed trees
Gradient check: converting 14 compressed trees
acc 0.95 opt 0.98 cwa 0.12 optcwa 0.08
acc 0.91 opt 0.94 cwa 0.15 optcwa 0.11
acc 0.89 opt 0.92 cwa 0.18 optcwa 0.14
acc 0.93 opt 0.96 cwa 0.10 optcwa 0.06
acc 0.88 opt 0.90 cwa 0.20 optcwa 0.16
acc 0.92 opt 0.95 cwa 0.13 optcwa 0.09
acc 0.90 opt 0.93 cwa 0.17 optcwa 0.13
acc 0.94 opt 0.97 cwa 0.11 optcwa 0.07
acc 0.87 opt 0.89 cwa 0.21 optcwa 0.17
acc 0.96 opt 1.00 cwa 0.09 optcwa 0.05
acc 0.86 opt 0.88 cwa 0.22 optcwa 0.18
acc 0.97 opt 1.00 cwa 0.08 optcwa 0.04
acc 0.85 opt 0.87 cwa 0.23 optcwa 0.19
acc 1.00 opt NA cwa NA optcwa NA
acc NA opt NA cwa NA optcwa NA
language can now be triggered as an option to tokenize rather than a separate annotator via language = true
lemma can now be triggered as an option to tokenize rather than a separate annotator via lemma = true
pos can now be triggered as an option to tokenize rather than a separate annotator via pos = true
ner can now be triggered as an option to tokenize rather than a separate annotator via ner = true
sentiment can now be triggered as an option to tokenize rather than a separate annotator via sentiment = true
parse can now be triggered as an option to tokenize rather than a separate annotator via parse = true
coref can now be triggered as an option to tokenize rather than a separate annotator via coref = true
relation can now be triggered as an option to tokenize rather than a separate annotator via relation = true
quote can now be triggered as an option to tokenize rather than a separate annotator via quote = true
dcoref can now be triggered as an option to tokenize rather than a separate annotator via dcoref = true
depparse can now be triggered as an option to tokenize rather than a separate annotator via depparse = true
natlog can now be triggered as an option to tokenize rather than a separate annotator via natlog = true
openie can now be triggered as an option to tokenize rather than a separate annotator via openie = true
kbp can now be triggered as an option to tokenize rather than a separate annotator via kbp = true
CleanXML: processed tag: <title>
CleanXML: processed tag: <author>
CleanXML: processed tag: <body>
CleanXML: processed tag: <date>
CleanXML: processed tag: <link>
CleanXML: processed tag: <image>
CleanXML: processed tag: <summary>
CleanXML: processed tag: <category>
CleanXML: processed tag: <comment>
CleanXML: processed tag: <rating>
CleanXML: processed tag: <id>
CleanXML: processed tag: <meta>
CleanXML: processed tag: <header>
CleanXML: processed tag: <footer>
CleanXML: processed tag: <script>
Number of features (Phi(X) types): 12
Number of features (Phi(X) types): 8
Number of features (Phi(X) types): 10
Number of features (Phi(X) types): 9
Number of features (Phi(X) types): 11
Number of features (Phi(X) types): 7
Number of features (Phi(X) types): 13
Number of features (Phi(X) types): 6
Number of features (Phi(X) types): 14
Number of features (Phi(X) types): 5
Number of features (Phi(X) types): 15
Number of features (Phi(X) types): 4
Number of features (Phi(X) types): 16
Number of features (Phi(X) types): 3
Number of features (Phi(X) types): 17
Serializing entity extraction model to ner_model ...
Serializing entity extraction model to sentiment_model ...
Serializing entity extraction model to relation_model ...
Serializing entity extraction model to topic_model ...
Serializing entity extraction model to qa_model ...
Serializing entity extraction model to pos_model ...
Serializing entity extraction model to chunk_model ...
Serializing entity extraction model to coref_model ...
Serializing entity extraction model to event_model ...
Serializing entity extraction model to keyword_model ...
Serializing entity extraction model to summary_model ...
Serializing entity extraction model to caption_model ...
Serializing entity extraction model to translation_model ...
Serializing entity extraction model to dialog_model ...
Serializing entity extraction model to style_model ...
correct 10 out of 10
correct 7 out of 12
correct 5 out of 8
correct 9 out of 15
correct 6 out of 10
correct 8 out of 9
correct 4 out of 7
correct 11 out of 14
correct 3 out of 5
correct 12 out of 16
correct 2 out of 4
correct 13 out of 18
correct 1 out of 3
correct 14 out of 20
correct 0 out of 2
Training will use 1 thread(s)
Training will use 4 thread(s)
Training will use 8 thread(s)
Training will use 16 thread(s)
Training will use 2 thread(s)
Training will use 6 thread(s)
Training will use 10 thread(s)
Training will use 12 thread(s)
Training will use 3 thread(s)
Training will use 5 thread(s)
Training will use 7 thread(s)
Training will use 9 thread(s)
Training will use 11 thread(s)
Training will use 13 thread(s)
Training will use 14 thread(s)
NUM NONZERO PARAMETERS: 5
NUM NONZERO PARAMETERS: 12
NUM NONZERO PARAMETERS: 0
NUM NONZERO PARAMETERS: 8
NUM NONZERO PARAMETERS: 3
NUM NONZERO PARAMETERS: 10
NUM NONZERO PARAMETERS: 1
NUM NONZERO PARAMETERS: 7
NUM NONZERO PARAMETERS: 4
NUM NONZERO PARAMETERS: 9
NUM NONZERO PARAMETERS: 6
NUM NONZERO PARAMETERS: 11
NUM NONZERO PARAMETERS: 2
NUM NONZERO PARAMETERS: 13
NUM NONZERO PARAMETERS: 14
Received bad output format in scenegraph ' image.jpg '
Received bad output format in scenegraph ' video.mp4 '
Received bad output format in scenegraph ' audio.wav '
Received bad output format in scenegraph ' text.txt '
Received bad output format in scenegraph ' model.obj '
Received bad output format in scenegraph ' animation.gif '
Received bad output format in scenegraph ' document.pdf '
Received bad output format in scenegraph ' spreadsheet.xlsx '
Received bad output format in scenegraph ' presentation.pptx '
Received bad output format in scenegraph ' archive.zip '
Received bad output format in scenegraph ' script.py '
Received bad output format in scenegraph ' executable.exe '
Received bad output format in scenegraph ' database.db '
Received bad output format in scenegraph ' webpage.html '
Received bad output format in scenegraph ' email.eml '
applyNumericClassifier: ncc.applyNumericClassifiers(0.5, 0.1, 0.9)
applyNumericClassifier: ncc.applyNumericClassifiers(0.3, 0.2, 0.8)
applyNumericClassifier: ncc.applyNumericClassifiers(0.4, 0.15, 0.85)
applyNumericClassifier: ncc.applyNumericClassifiers(0.6, 0.05, 0.95)
applyNumericClassifier: ncc.applyNumericClassifiers(0.7, 0.01, 0.99)
applyNumericClassifier: ncc.applyNumericClassifiers(0.2, 0.25, 0.75)
applyNumericClassifier: ncc.applyNumericClassifiers(0.1, 0.3, 0.7)
applyNumericClassifier: ncc.applyNumericClassifiers(0.8, 0.02, 0.98)
applyNumericClassifier: ncc.applyNumericClassifiers(0.9, 0.005, 0.995)
applyNumericClassifier: ncc.applyNumericClassifiers(1.0, 0.001, 1.001)
applyNumericClassifier: ncc.applyNumericClassifiers(1.1, -0.001, 1.101)
applyNumericClassifier: ncc.applyNumericClassifiers(1.2, -0.002, 1.202)
applyNumericClassifier: ncc.applyNumericClassifiers(1.3, -0.003, 1.303)
applyNumericClassifier: ncc.applyNumericClassifiers(1.4, -0.004, 1.404)
applyNumericClassifier: ncc.applyNumericClassifiers(1.5, -0.005, 1.505)
SUCCESS: wrote model to /home/user/model1.h5
SUCCESS: wrote model to /var/tmp/model2.pkl
SUCCESS: wrote model to /data/models/model3.pt
SUCCESS: wrote model to /mnt/model4.onnx
SUCCESS: wrote model to /usr/local/model5.json
SUCCESS: wrote model to /opt/models/model6.bin
SUCCESS: wrote model to /home/user/subdir/model7.hdf5
SUCCESS: wrote model to /tmp/model8.npz
SUCCESS: wrote model to /data/subdir/model9.torch
SUCCESS: wrote model to /mnt/subdir/model10.tf
SUCCESS: wrote model to /usr/local/subdir/model11.joblib
SUCCESS: wrote model to /opt/subdir/model12.nnet
SUCCESS: wrote model to /home/user/subsubdir/model13.keras
SUCCESS: wrote model to /tmp/subsubdir/model14.pickle
SUCCESS: wrote model to /data/subsubdir/model15.mat
Loading parser from serialized file https://example.com/parser.bin ... done [ 2.34 sec].
Loading parser from serialized file C:\Users\user\Documents\parser.dat ... done [ 1.87 sec].
Loading parser from serialized file /home/user/parser.pkl ... done [ 3.12 sec].
Loading parser from serialized file /tmp/parser.tmp ... done [ 2.56 sec].
Loading parser from serialized file https://example.net/parser.zip ... done [ 4.23 sec].
Loading parser from serialized file C:\Windows\Temp\parser.bak ... done [ 2.78 sec].
Loading parser from serialized file /usr/local/bin/parser ... done [ 1.45 sec].
Loading parser from serialized file https://example.org/parser.tar.gz ... done [ 5.67 sec].
Loading parser from serialized file C:\Program Files\parser.exe ... done [ 3.89 sec].
Loading parser from serialized file /opt/parser.jar ... done [ 2.98 sec].
Loading parser from serialized file https://example.com/parser.json ... done [ 3.45 sec].
Loading parser from serialized file C:\Users\user\Desktop\parser.txt ... done [ 1.23 sec].
Loading parser from serialized file /var/log/parser.log ... done [ 4.56 sec].
Loading parser from serialized file https://example.net/parser.xml ... done [ 4.12 sec].
Loading parser from serialized file C:\Temp\parser.tmp ... done [ 2.34 sec].
Reading embedding files word2vec.txt and glove.txt .
Reading embedding files bert.bin and elmo.hdf5 .
Reading embedding files fasttext.vec and wordnet.dat .
Reading embedding files gpt2.pkl and roberta.pt .
Reading embedding files skipgram.bin and cbow.bin .
Reading embedding files flair.npz and albert.npz .
Reading embedding files xlnet.pth and electra.pth .
Reading embedding files spacy.model and gensim.model .
Reading embedding files transformer.bin and lstm.hdf5 .
Reading embedding files vocab.txt and vectors.txt .
Reading embedding files w2v.bin and glv.bin .
Reading embedding files bpe.model and bpe.vocab .
Reading embedding files tfidf.vec and lda.vec .
Reading embedding files ngram.txt and bow.txt .
Reading embedding files glove.6B.300d.txt and word2vec.6B.300d.bin .
Number of labels: 5 [
Number of labels: 12 [
Number of labels: 8 [
Number of labels: 3 [
Number of labels: 10 [
Number of labels: 6 [
Number of labels: 9 [
Number of labels: 4 [
Number of labels: 7 [
Number of labels: 11 [
Number of labels: 2 [
Number of labels: 13 [
Number of labels: 1 [
Number of labels: 14 [
Number of labels: 15 [
after deleting 2, and then zero deg nodes [[1, 2], [3, 4], [5, 6]]
after deleting 2, and then zero deg nodes [[0, 1], [2, 3], [4, 5], [6, 7]]
after deleting 2, and then zero deg nodes [[1, 3], [2, 4], [5, 7]]
after deleting 2, and then zero deg nodes [[0, 2], [1, 3], [4, 6], [5, 7]]
after deleting 2, and then zero deg nodes [[0, 1], [3, 4], [5, 6]]
after deleting 2, and then zero deg nodes [[1, 2], [4, 5], [6, 7]]
after deleting 2, and then zero deg nodes [[0, 3], [1, 4], [2, 5], [6, 7]]
after deleting 2, and then zero deg nodes [[0, 4], [1, 5], [2, 6], [3, 7]]
after deleting 2, and then zero deg nodes [[0, 2], [3, 5], [4, 6]]
after deleting 2, and then zero deg nodes [[1, 3], [2, 5], [4, 7]]
after deleting 2, and then zero deg nodes [[0, 5], [1, 6], [2, 7]]
after deleting 2, and then zero deg nodes [[0, 6], [1, 7]]
after deleting 2, and then zero deg nodes [[0, 7]]
after deleting 2, and then zero deg nodes []
after deleting 2, and then zero deg nodes [[1]]
Threads: 4
Threads: 8
Threads: 16
Threads: 32
Threads: 64
Threads: 2
Threads: 1
Threads: 12
Threads: 24
Threads: 48
Threads: 6
Threads: 3
Threads: 10
Threads: 20
Threads: 40
Line 12 changed to postProcessed 'true'
Line 34 changed to postProcessed 'false'
Line 7 changed to postProcessed 'true'
Line 45 changed to postProcessed 'false'
Line 21 changed to postProcessed 'true'
Line 9 changed to postProcessed 'false'
Line 18 changed to postProcessed 'true'
Line 27 changed to postProcessed 'false'
Line 3 changed to postProcessed 'true'
Line 36 changed to postProcessed 'false'
Line 15 changed to postProcessed 'true'
Line 42 changed to postProcessed 'false'
Line 6 changed to postProcessed 'true'
Line 39 changed to postProcessed 'false'
Var Sizes: [3, 4, 5]
Var Sizes: [2, 2, 2]
Var Sizes: [1, 6, 8]
Var Sizes: [4, 4, 4]
Var Sizes: [5, 3, 7]
Var Sizes: [6, 2, 9]
Var Sizes: [3, 5, 6]
Var Sizes: [2, 4, 8]
Var Sizes: [4, 3, 9]
Var Sizes: [5, 5, 5]
Var Sizes: [6, 6, 6]
Var Sizes: [7, 3, 4]
Var Sizes: [8, 2, 5]
Var Sizes: [9, 1, 6]
Var Sizes: [10, 10, 10]
Only one child determines NOUN as head of VP
Only one child determines ADJ as head of NP
Only one child determines VBD as head of S
Only one child determines DT as head of QP
Only one child determines PRP as head of PP
Only one child determines RB as head of ADVP
Only one child determines JJ as head of ADJP
Only one child determines IN as head of SBAR
Only one child determines NN as head of NP
Only one child determines VBZ as head of VP
Only one child determines CD as head of QP
Only one child determines CC as head of CONJP
Only one child determines MD as head of VP
Only one child determines WDT as head of SBAR
Only one child determines VB as head of VP
Final Sieve Ordering: SieveA,SieveB,SieveC
Final Sieve Ordering: SieveX,SieveY,SieveZ
Final Sieve Ordering: Sieve1,Sieve2,Sieve3,Sieve4
Final Sieve Ordering: SieveM,SieveN
Final Sieve Ordering: SieveQ,SieveR,SieveS
Final Sieve Ordering: Sieve5,Sieve6
Final Sieve Ordering: SieveD,SieveE,SieveF
Final Sieve Ordering: Sieve7,Sieve8,Sieve9
Final Sieve Ordering: SieveG,SieveH
Final Sieve Ordering: SieveT,SieveU,SieveV
Final Sieve Ordering: SieveK,SieveL
Final Sieve Ordering: SieveW,SieveX
Final Sieve Ordering: SieveO,SieveP
Final Sieve Ordering: SieveJ
edit distance: 0
edit distance: 1
edit distance: 2
edit distance: 3
edit distance: 4
edit distance: 5
edit distance: 6
edit distance: 7
edit distance: 8
edit distance: 9
edit distance: 10
edit distance: 11
edit distance: 12
edit distance: 13
edit distance: 14
Warning: No rule found for motherCat (first char: m ')
Warning: No rule found for fatherCat (first char: f ')
Warning: No rule found for kitten (first char: k ')
Warning: No rule found for dog (first char: d ')
Warning: No rule found for mouse (first char: m ')
Warning: No rule found for cheese (first char: c ')
Warning: No rule found for milk (first char: m ')
Warning: No rule found for water (first char: w ')
Warning: No rule found for fish (first char: f ')
Warning: No rule found for bird (first char: b ')
Warning: No rule found for snake (first char: s ')
Warning: No rule found for frog (first char: f ')
Warning: No rule found for rabbit (first char: r ')
Warning: No rule found for carrot (first char: c ')
Warning: No rule found for grass (first char: g ')
after deleting 2->3 edge [[1, 2], [3, 4], [4, 5]]
after deleting 2->3 edge [[1, 3], [2, 4], [3, 5]]
after deleting 2->3 edge [[1, 2], [2, 5], [3, 4]]
after deleting 2->3 edge [[1, 4], [2, 3], [4, 5]]
after deleting 2->3 edge [[1, 5], [2, 4], [3, 5]]
after deleting 2->3 edge [[1, 3], [2, 5], [4, 5]]
after deleting 2->3 edge [[1, 4], [2, 5], [3, 4]]
after deleting 2->3 edge [[1, 5], [2, 3], [4, 5]]
after deleting 2->3 edge [[1, 2], [3, 5], [4, 5]]
after deleting 2->3 edge [[1, 5], [2, 5], [3, 4]]
after deleting 2->3 edge [[1, 4], [2, 4], [3, 5]]
after deleting 2->3 edge [[1, 3], [2, 4], [5, 5]]
after deleting 2->3 edge [[1, 4], [3, 4], [5, 5]]
after deleting 2->3 edge [[1, 5], [3, 5], [4, 4]]
after deleting 2->3 edge [[1, 2], [4, 4], [5, 5]]
Loading parser from text file data.txt ... done [ 0.34 sec].
Loading parser from text file https://example.com/parser.json ... done [ 1.23 sec].
Loading parser from text file config.ini ... done [ 0.12 sec].
Loading parser from text file /home/user/parser.py ... done [ 0.45 sec].
Loading parser from text file C:\Users\user\parser.bat ... done [ 0.67 sec].
Loading parser from text file parser.log ... done [ 0.25 sec].
Loading parser from text file https://github.com/user/parser.git ... done [ 2.34 sec].
Loading parser from text file /tmp/parser.tmp ... done [ 0.11 sec].
Loading parser from text file C:\Windows\System32\parser.dll ... done [ 1.56 sec].
Loading parser from text file /usr/local/bin/parser ... done [ 0.89 sec].
Loading parser from text file https://bing.com/parser.xml ... done [ 1.78 sec].
Loading parser from text file /var/log/parser.log ... done [ 0.32 sec].
Loading parser from text file C:\Program Files\parser\parser.exe ... done [ 1.23 sec].
Loading parser from text file /etc/parser.conf ... done [ 0.16 sec].
Loading parser from text file https://stackoverflow.com/questions/12345678/how-to-parse-a-text-file-in-java ... done [ 3.45 sec].
Serializing classifier to /home/user/model.pkl ... done.
Serializing classifier to C:\Users\user\Documents\model.h5 ... done.
Serializing classifier to /tmp/model.joblib ... done.
Serializing classifier to /data/classifier/model.pt ... done.
Serializing classifier to /var/lib/model.sav ... done.
Serializing classifier to /Users/user/Desktop/model.npz ... done.
Serializing classifier to /opt/model/model.bin ... done.
Serializing classifier to /mnt/model/model.json ... done.
Serializing classifier to /root/model/model.pickle ... done.
Serializing classifier to /dev/null ... done.
Serializing classifier to /media/user/USB/model.mat ... done.
Serializing classifier to /etc/model/model.xml ... done.
Serializing classifier to /usr/local/model/model.csv ... done.
Serializing classifier to /run/model/model.dat ... done.
Serializing classifier to /srv/model/model.yaml ... done.
Testing with batch size: 16
Testing with batch size: 32
Testing with batch size: 64
Testing with batch size: 128
Testing with batch size: 256
Testing with batch size: 512
Testing with batch size: 1024
Testing with batch size: 2048
Testing with batch size: 4096
Testing with batch size: 8192
Testing with batch size: 16384
Testing with batch size: 32768
Testing with batch size: 65536
Testing with batch size: 131072
Testing with batch size: 262144
Total norm 0.0
Total norm 1.0
Total norm 2.25
Total norm 4.0
Total norm 6.25
Total norm 9.0
Total norm 12.25
Total norm 16.0
Total norm 20.25
Total norm 25.0
Total norm 30.25
Total norm 36.0
Total norm 42.25
Total norm 49.0
Total norm 56.25
Weights: 12 negative; 8 positive; 0 zero.
Weights: 0 negative; 20 positive; 0 zero.
Weights: 5 negative; 10 positive; 5 zero.
Weights: 9 negative; 9 positive; 2 zero.
Weights: 3 negative; 15 positive; 2 zero.
Weights: 7 negative; 7 positive; 6 zero.
Weights: 10 negative; 6 positive; 4 zero.
Weights: 4 negative; 12 positive; 4 zero.
Weights: 6 negative; 8 positive; 6 zero.
Weights: 8 negative; 4 positive; 8 zero.
Weights: 11 negative; 5 positive; 4 zero.
Weights: 2 negative; 16 positive; 2 zero.
Weights: 1 negative; 19 positive; 0 zero.
Weights: 13 negative; 3 positive; 4 zero.
Weights: 14 negative; 2 positive; 4 zero.
MultiVector took 12 ms
MultiVector took 27 ms
MultiVector took 9 ms
MultiVector took 15 ms
MultiVector took 21 ms
MultiVector took 18 ms
MultiVector took 24 ms
MultiVector took 11 ms
MultiVector took 16 ms
MultiVector took 13 ms
MultiVector took 19 ms
MultiVector took 10 ms
MultiVector took 22 ms
MultiVector took 14 ms
MultiVector took 20 ms
Time: 0.0 sec.
Time: 0.1 sec.
Time: 0.2 sec.
Time: 0.3 sec.
Time: 0.4 sec.
Time: 0.5 sec.
Time: 0.6 sec.
Time: 0.7 sec.
Time: 0.8 sec.
Time: 0.9 sec.
Time: 1.0 sec.
Time: 1.1 sec.
Time: 1.2 sec.
Time: 1.3 sec.
Time: 1.4 sec.
Right norm (0.5) 0.25
Right norm (1.0) 1.0
Right norm (0.8) 0.64
Right norm (0.3) 0.09
Right norm (0.6) 0.36
Right norm (0.9) 0.81
Right norm (0.4) 0.16
Right norm (0.7) 0.49
Right norm (1.2) 1.44
Right norm (1.5) 2.25
Right norm (1.3) 1.69
Right norm (1.4) 1.96
Right norm (1.6) 2.56
Right norm (1.7) 2.89
Right norm (1.8) 3.24
Parsing 0 trees
Parsing 1 trees
Parsing 2 trees
Parsing 3 trees
Parsing 4 trees
Parsing 5 trees
Parsing 6 trees
Parsing 7 trees
Parsing 8 trees
Parsing 9 trees
Parsing 10 trees
Parsing 11 trees
Parsing 12 trees
Parsing 13 trees
Parsing 14 trees
com.example.service.FileService : Could not open split file /data/files/file1.part1
com.example.controller.UploadController : Could not open split file /tmp/uploads/file2.part3
com.example.util.FileUtils : Could not open split file /home/user/documents/file3.part2
com.example.dao.FileDao : Could not open split file /var/log/files/file4.part4
com.example.model.FileModel : Could not open split file /opt/apps/files/file5.part1
com.example.service.FileService : Could not open split file /data/files/file6.part2
com.example.controller.DownloadController : Could not open split file /tmp/downloads/file7.part3
com.example.util.FileUtils : Could not open split file /home/user/downloads/file8.part2
com.example.dao.FileDao : Could not open split file /var/log/files/file9.part4
com.example.model.FileModel : Could not open split file /opt/apps/files/file10.part1
com.example.service.FileService : Could not open split file /data/files/file11.part3
com.example.controller.UploadController : Could not open split file /tmp/uploads/file12.part4
com.example.util.FileUtils : Could not open split file /home/user/documents/file13.part1
com.example.dao.FileDao : Could not open split file /var/log/files/file14.part2
com.example.model.FileModel : Could not open split file /opt/apps/files/file15.part3
from: preProcessed UTF-8
from: preProcessed GB2312
from: preProcessed Big5
from: preProcessed GBK
from: preProcessed ISO-8859-1
from: preProcessed UTF-16
from: preProcessed ASCII
from: preProcessed EUC-CN
from: preProcessed EUC-TW
from: preProcessed Shift-JIS
from: preProcessed ISO-2022-CN
from: preProcessed ISO-2022-JP
from: preProcessed ISO-2022-KR
from: preProcessed Windows-1252
from: preProcessed Windows-1251
Filtered tree because of binary rule: NP , VP
Filtered tree because of binary rule: S , S
Filtered tree because of binary rule: DT , NN
Filtered tree because of binary rule: JJ , NNS
Filtered tree because of binary rule: RB , VB
Filtered tree because of binary rule: IN , PP
Filtered tree because of binary rule: VBD , NP
Filtered tree because of binary rule: PRP , VBP
Filtered tree because of binary rule: CC , S
Filtered tree because of binary rule: MD , VP
Filtered tree because of binary rule: CD , JJ
Filtered tree because of binary rule: NN , CC
Filtered tree because of binary rule: VBZ , ADJP
Filtered tree because of binary rule: TO , VP
Filtered tree because of binary rule: WRB , SBAR
Loading QuoteAttribution characters [characters.json]...
Loading QuoteAttribution characters [chars.csv]...
Loading QuoteAttribution characters [char_data.txt]...
Loading QuoteAttribution characters [characters.xml]...
Loading QuoteAttribution characters [char_list.dat]...
Loading QuoteAttribution characters [characters.pkl]...
Loading QuoteAttribution characters [chars.yaml]...
Loading QuoteAttribution characters [char_info.hdf5]...
Loading QuoteAttribution characters [characters.tsv]...
Loading QuoteAttribution characters [char_dict.json]...
Loading QuoteAttribution characters [chars.mat]...
Loading QuoteAttribution characters [char_table.db]...
Loading QuoteAttribution characters [characters.xlsx]...
Loading QuoteAttribution characters [char_set.ini]...
Loading QuoteAttribution characters [characters.parquet]...
stanfordXML = "&lt;root&gt;&lt;name&gt;Alice&lt;/name&gt;&lt;age&gt;25&lt;/age&gt;&lt;/root&gt;";
stanfordXML = "&lt;book&gt;&lt;title&gt;The Catcher in the Rye&lt;/title&gt;&lt;author&gt;J.D. Salinger&lt;/author&gt;&lt;/book&gt;";
stanfordXML = "&lt;person&gt;&lt;id&gt;1001&lt;/id&gt;&lt;name&gt;Bob&lt;/name&gt;&lt;gender&gt;male&lt;/gender&gt;&lt;/person&gt;";
stanfordXML = "&lt;order&gt;&lt;item&gt;iPhone 14&lt;/item&gt;&lt;price&gt;$999&lt;/price&gt;&lt;quantity&gt;1&lt;/quantity&gt;&lt;/order&gt;";
stanfordXML = "&lt;student&gt;&lt;name&gt;Charlie&lt;/name&gt;&lt;grade&gt;A&lt;/grade&gt;&lt;course&gt;Math&lt;/course&gt;&lt;/student&gt;";
stanfordXML = "&lt;movie&gt;&lt;title&gt;Avatar&lt;/title&gt;&lt;director&gt;James Cameron&lt;/director&gt;&lt;/movie&gt;";
stanfordXML = "&lt;animal&gt;&lt;name&gt;Tiger&lt;/name&gt;&lt;color&gt;orange and black&lt;/color&gt;&lt;/animal&gt;";
stanfordXML = "&lt;product&gt;&lt;name&gt;Laptop&lt;/name&gt;&lt;brand&gt;Dell&lt;/brand&gt;&lt;</product>";
stanfordXML = "&lt;city&gt;&lt;name&gt;New York&lt;/namegt;&ltpopulationgt21 million&lt/populationgt&ltaareagt789 km2&ltaareagt&lta/citygt";
stanfordXML = "&ltaalbumgt&ltaartistgtTaylor Swift&ltaartistgt&ltaatitlegtRed (Taylor's Version)&ltaatitlegt&ltaaalbumgt";
stanfordXML = "&ltaemployeegt&ltaanamegtDavid&ltaanamegt&ltaasalarygt$5000&ltaasalarygt&ltaaemployeegt";
stanfordXML = "&ltacontactgt&ltaanamegtEmma&ltaanamegt&ltaaphonegt123-456-7890&ltaaphonegt&ltaacontactgt";
stanfordXML = "&ltafruitgt&ltaanamegtBanana&ltaanamegt&ltaacolorgtyellow&ltaacolorgt&ltaafruitgt";
stanfordXML = "&ltagamegt&ltaanamegtChess&ltaanamegt&ltaaplayersgt2&ltaaplayersgt&ltagamegt";
stanfordXML = "&ltagreetinggt&ltaamessagegthello world!&ltaamessagegt&lagreetingg";
PATTERN : what pattern to use for matching
[a-z]+ : what pattern to use for matching
\d{4}-\d{2}-\d{2} : what pattern to use for matching
.*@.*\..* : what pattern to use for matching
^[A-Z][a-z]*$ : what pattern to use for matching
\b(cat|dog|fish)\b : what pattern to use for matching
[^aeiou] : what pattern to use for matching
\w+ : what pattern to use for matching
\s+ : what pattern to use for matching
(.)\1+ : what pattern to use for matching
[A-Z]{3} : what pattern to use for matching
\d+(\.\d+)? : what pattern to use for matching
[a-zA-Z0-9]+@[a-zA-Z0-9]+\.(com|net|org) : what pattern to use for matching
<[^>]+> : what pattern to use for matching
\$(\d+|\d{1,3}(,\d{3})*)\.(\d{2}) : what pattern to use for matching
Making result printers from classes
Making result printers from subclasses
Making result printers from interfaces
Making result printers from abstract classes
Making result printers from generic classes
Making result printers from inner classes
Making result printers from anonymous classes
Making result printers from lambda expressions
Making result printers from enums
Making result printers from arrays
Making result printers from collections
Making result printers from maps
Making result printers from sets
Making result printers from lists
Making result printers from streams
- UNARY_GRAMMAR
+ UNARY_GRAMMAR
* UNARY_GRAMMAR
/ UNARY_GRAMMAR
! UNARY_GRAMMAR
~ UNARY_GRAMMAR
++ UNARY_GRAMMAR
-- UNARY_GRAMMAR
& UNARY_GRAMMAR
| UNARY_GRAMMAR
^ UNARY_GRAMMAR
% UNARY_GRAMMAR
? UNARY_GRAMMAR
# UNARY_GRAMMAR
$ UNARY_GRAMMAR
ABA WORD_INDEX
ZZZ WORD_INDEX
CAT WORD_INDEX
DOG WORD_INDEX
RED WORD_INDEX
FOX WORD_INDEX
QWE WORD_INDEX
RTY WORD_INDEX
UIO WORD_INDEX
ASD WORD_INDEX
FGH WORD_INDEX
JKL WORD_INDEX
XYZ WORD_INDEX
MNO WORD_INDEX
PQR WORD_INDEX
DBG For label PERSON True Positives: 12 False Positives: 3 False Negatives: 4
DBG For label LOCATION True Positives: 8 False Positives: 2 False Negatives: 5
DBG For label ORGANIZATION True Positives: 10 False Positives: 4 False Negatives: 6
DBG For label DATE True Positives: 9 False Positives: 1 False Negatives: 2
DBG For label TIME True Positives: 7 False Positives: 3 False Negatives: 4
DBG For label MONEY True Positives: 6 False Positives: 2 False Negatives: 3
DBG For label PERCENT True Positives: 5 False Positives: 1 False Negatives: 2
DBG For label QUANTITY True Positives: 4 False Positives: 2 False Negatives: 3
DBG For label ORDINAL True Positives: 3 False Positives: 1 False Negatives: 2
DBG For label CARDINAL True Positives: 11 False Positives: 5 False Negatives: 7
DBG For label EVENT True Positives: 2 False Positives: 1 False Negatives: 3
DBG For label WORK_OF_ART True Positives: 1 False Positives: 0 False Negatives: 2
DBG For label LAW True Positives: 1 False Positives: 0 False Negatives: 1
DBG For label LANGUAGE True Positives: 1 False Positives: 0 False Negatives: 0
DBG For label OTHER True Positives: 0 False Positives: 0 False Negatives: 0
minimaldebug Writing justification files
minimaldebug Writing justification files
minimaldebug Writing justification files
minimaldebug Writing justification files
minimaldebug Writing justification files
minimaldebug Writing justification files
minimaldebug Writing justification files
minimaldebug Writing justification files
minimaldebug Writing justification files
minimaldebug Writing justification files
minimaldebug Writing justification files
minimaldebug Writing justification files
minimaldebug Writing justification files
minimaldebug Writing justification files
minimaldebug Writing justification files
train summary evalb: LP: 89.34 LR: 88.76 F1: 89.05 Exact: 37.21 N: 2417
dev summary evalb: LP: 86.45 LR: 85.92 F1: 86.18 Exact: 33.67 N: 1700
test summary evalb: LP: 85.67 LR: 85.12 F1: 85.39 Exact: 32.54 N: 2000
train summary evalb: LP: 90.12 LR: 89.56 F1: 89.84 Exact: 38.45 N: 2500
dev summary evalb: LP: 87.23 LR: 86.78 F1: 87.01 Exact: 34.82 N: 1800
test summary evalb: LP: 86.34 LR: 85.89 F1: 86.11 Exact: 33.76 N: 2100
train summary evalb: LP: 88.76 LR: 88.21 F1: 88.48 Exact: 36.78 N: 2300
dev summary evalb: LP: 85.89 LR: 85.34 F1: 85.61 Exact: 32.89 N: 1600
test summary evalb: LP: 84.98 LR: 84.45 F1: 84.71 Exact: 31.67 N: 1900
train summary evalb: LP: 91.23 LR: 90.67 F1: 90.95 Exact: 39.56 N: 2600
dev summary evalb: LP: 88.34 LR: 87.89 F1: 88.11 Exact: 35.78 N: 1900
test summary evalb: LP: 87.45 LR: 86.98 F1: 87.21 Exact: 34.65 N: 2200
train summary evalb: LP: 87.45 LR: 86.89 F1: 87.17 Exact: 35.67 N: 2200
dev summary evalb: LP: 84.56 LR: 84.01 F1: 84.28 Exact: 31.45 N: 1500
DBG Sampling datums from 0.23
DBG Sampling datums from 0.56
DBG Sampling datums from 0.78
DBG Sampling datums from 0.91
DBG Sampling datums from 0.34
DBG Sampling datums from 0.67
DBG Sampling datums from 0.89
DBG Sampling datums from 0.12
DBG Sampling datums from 0.45
DBG Sampling datums from 0.69
DBG Sampling datums from 0.81
DBG Sampling datums from 0.24
DBG Sampling datums from 0.57
DBG Sampling datums from 0.79
DBG Sampling datums from 0.92
Entity extraction results AFTER consistency checks for partition #0 using printer com.example.Printer@1a2b3c : Success
Entity extraction results AFTER consistency checks for partition #1 using printer com.example.Printer@7g8h9i : Warning: Low ink level
Entity extraction results AFTER consistency checks for partition #3 using printer com.example.Printer@6p7q8r : Success
Entity extraction results AFTER consistency checks for partition #4 using printer com.example.Printer@2v3w4x : Success
Entity extraction results AFTER consistency checks for partition #5 using printer com.example.Printer@8b9c0d : Success
Entity extraction results AFTER consistency checks for partition #6 using printer com.example.Printer@4h5i6j : Success
Entity extraction results AFTER consistency checks for partition #7 using printer com.example.Printer@0n1o2p : Success
probabilities do not sum to one 0.3 0.6
probabilities do not sum to one 0.5 0.4
probabilities do not sum to one 0.2 0.7
probabilities do not sum to one 0.4 0.5
probabilities do not sum to one 0.6 0.3
probabilities do not sum to one 0.1 0.8
probabilities do not sum to one 0.7 0.2
probabilities do not sum to one 0.8 0.1
probabilities do not sum to one 0.9 0.05
probabilities do not sum to one 0.05 0.9
probabilities do not sum to one 0.3 0.65
probabilities do not sum to one 0.4 0.55
probabilities do not sum to one 0.5 0.45
probabilities do not sum to one 0.6 0.35
probabilities do not sum to one 0.7 0.25
Caught bad number: For input string: "abc"
Caught bad number: Division by zero
Caught bad number: Out of range: -2147483649
Caught bad number: NaN
Caught bad number: Invalid format: "12.34.56"
Caught bad number: Overflow: 9223372036854775808
Caught bad number: Underflow: -9223372036854775809
Caught bad number: Negative square root: -4
Caught bad number: Null pointer
Caught bad number: Infinite loop
Caught bad number: Stack overflow
Caught bad number: Arithmetic exception
Caught bad number: Number format exception
Caught bad number: Illegal argument exception
Caught bad number: Unsupported operation exception
[/127.0.0.1:8080] API call w/annotators tokenize,ssplit,pos
[/192.168.1.100:54321] API call w/annotators ner,coref
[/10.0.0.5:80] API call w/annotators lemma,parse
[/172.16.0.1:443] API call w/annotators sentiment,quote
[/203.0.113.25:1234] API call w/annotators dcoref,kbp
[/198.51.100.42:5678] API call w/annotators relation,entitylink
[/203.0.113.25:1234] API call w/annotators <unknown>
[/192.168.1.100:54321] API call w/annotators tokenize,ssplit,pos,ner
[/127.0.0.1:8080] API call w/annotators parse,sentiment
[/10.0.0.5:80] API call w/annotators coref,quote,dcoref
[/172.16.0.1:443] API call w/annotators kbp,relation,entitylink
[/198.51.100.42:5678] API call w/annotators lemma,ner,coref
[/10.0.0.5:80] API call w/annotators <unknown>
[/172.16.0.1:443] API call w/annotators tokenize,ssplit,pos,lemma,parse,sentiment,quote,dcoref,kbp,relation,entitylink
[/198.51.100.42:5678] API call w/annotators ner,sentiment
Successfully loaded classifier # 3 from /home/user/models/model1.pkl.
Successfully loaded classifier # 5 from /opt/data/classifiers/classifier2.h5.
Successfully loaded classifier # 1 from C:\Users\user\Documents\model3.pt.
Successfully loaded classifier # 4 from /mnt/disk/models/classifier4.sav.
Successfully loaded classifier # 2 from /var/lib/model5.joblib.
Successfully loaded classifier # 6 from /tmp/classifiers/model6.onnx.
Successfully loaded classifier # 7 from /usr/local/share/models/classifier7.tflite.
Successfully loaded classifier # 8 from /media/user/USB/model8.mlmodel.
Successfully loaded classifier # 9 from /dev/shm/classifiers/model9.dill.
Successfully loaded classifier # 10 from /etc/models/classifier10.pickle.
Successfully loaded classifier # 11 from /root/models/model11.sklearn.
Successfully loaded classifier # 12 from /run/user/1000/classifiers/classifier12.keras.
Successfully loaded classifier # 13 from /srv/models/model13.torch.
Successfully loaded classifier # 14 from /snap/models/classifier14.tf.
DBG Adding 12 eval sents to the training set
DBG Adding 7 eval sents to the training set
DBG Adding 15 eval sents to the training set
DBG Adding 9 eval sents to the training set
DBG Adding 10 eval sents to the training set
DBG Adding 11 eval sents to the training set
DBG Adding 8 eval sents to the training set
DBG Adding 13 eval sents to the training set
DBG Adding 14 eval sents to the training set
DBG Adding 6 eval sents to the training set
DBG Adding 5 eval sents to the training set
DBG Adding 4 eval sents to the training set
DBG Adding 3 eval sents to the training set
DBG Adding 2 eval sents to the training set
DBG Adding 1 eval sents to the training set
Mention #0: John Smith
Mention #1: Mary Jones
Mention #2: New York
Mention #3: yesterday
Mention #4: the book
Mention #5: he
Mention #6: she
Mention #7: London
Mention #8: tomorrow
Mention #9: the movie
Mention #10: they
Mention #11: Alice Cooper
Mention #12: Bob Dylan
Mention #13: the concert
Mention #14: we
Redwood.DBG Pattern: 0x12345678
Redwood.DBG Pattern: 0xABCDEF12
Redwood.DBG Pattern: 0x87654321
Redwood.DBG Pattern: 0x12AB34CD
Redwood.DBG Pattern: 0x56789ABC
Redwood.DBG Pattern: 0xDEADBEEF
Redwood.DBG Pattern: 0xCAFEBABE
Redwood.DBG Pattern: 0xFACEBOOK
Redwood.DBG Pattern: 0x1234ABCD
Redwood.DBG Pattern: 0x9876CDEF
Redwood.DBG Pattern: 0xACE13579
Redwood.DBG Pattern: 0xBED24680
Redwood.DBG Pattern: 0xCAB43210
Redwood.DBG Pattern: 0xFEDCBA98
Redwood.DBG Pattern: 0x76543210
Current Sieves: EmailSieve, SpamSieve, AttachmentSieve
Current Sieves: NameSieve, PhoneSieve, AddressSieve
Current Sieves: DateSieve, TimeSieve, EventSieve
Current Sieves: NumberSieve, CurrencySieve, UnitSieve
Current Sieves: KeywordSieve, TopicSieve, SentimentSieve
Current Sieves: ColorSieve, ShapeSieve, SizeSieve
Current Sieves: LocationSieve, DirectionSieve, DistanceSieve
Current Sieves: LanguageSieve, TranslationSieve, GrammarSieve
Current Sieves: MathSieve, LogicSieve, EquationSieve
Current Sieves: ImageSieve, VideoSieve, AudioSieve
Current Sieves: TextSieve, HTMLSieve, MarkdownSieve
Current Sieves: UserSieve, RoleSieve, PermissionSieve
Current Sieves: ProductSieve, CategorySieve, PriceSieve
Current Sieves: OrderSieve, StatusSieve, DeliverySieve
Current Sieves: ReviewSieve, RatingSieve, FeedbackSievel
Writing out binary trees to /home/user/train/tree1.txt ...
Writing out binary trees to /tmp/train/tree2.bin ...
Writing out binary trees to /data/train/tree3.json ...
Writing out binary trees to /var/log/train/tree4.log ...
Writing out binary trees to /opt/train/tree5.xml ...
Writing out binary trees to /mnt/train/tree6.dat ...
Writing out binary trees to /dev/train/tree7.raw ...
Writing out binary trees to /usr/local/train/tree8.csv ...
Writing out binary trees to /etc/train/tree9.conf ...
Writing out binary trees to /root/train/tree10.pkl ...
Writing out binary trees to /media/train/tree11.hdf5 ...
Writing out binary trees to /srv/train/tree12.yaml ...
Writing out binary trees to /run/train/tree13.sock ...
Writing out binary trees to /proc/train/tree14.pid ...
Writing out binary trees to /sys/train/tree15.stat ...
Redwood.DBG No similarity recorded between the positives and the unknown!
Redwood.DBG No similarity recorded between the positives and the unknown!
Redwood.DBG No similarity recorded between the positives and the unknown!
Redwood.DBG No similarity recorded between the positives and the unknown!
Redwood.DBG No similarity recorded between the positives and the unknown!
Redwood.DBG No similarity recorded between the positives and the unknown!
Redwood.DBG No similarity recorded between the positives and the unknown!
Redwood.DBG No similarity recorded between the positives and the unknown!
Redwood.DBG No similarity recorded between the positives and the unknown!
Redwood.DBG No similarity recorded between the positives and the unknown!
Redwood.DBG No similarity recorded between the positives and the unknown!
Redwood.DBG No similarity recorded between the positives and the unknown!
Redwood.DBG No similarity recorded between the positives and the unknown!
Redwood.DBG No similarity recorded between the positives and the unknown!
Redwood.DBG No similarity recorded between the positives and the unknown!
Serialized annotation saved in data.ser
Serialized annotation saved in config.ser
Serialized annotation saved in model.ser
Serialized annotation saved in test.ser
Serialized annotation saved in result.ser
Serialized annotation saved in input.ser
Serialized annotation saved in output.ser
Serialized annotation saved in log.ser
Serialized annotation saved in temp.ser
Serialized annotation saved in backup.ser
Serialized annotation saved in report.ser
Serialized annotation saved in image.ser
Serialized annotation saved in video.ser
Serialized annotation saved in audio.ser
Serialized annotation saved in document.ser
Final Sieve Ordering: SieveA,SieveB,SieveC
Final Sieve Ordering: SieveX,SieveY,SieveZ
Final Sieve Ordering: Sieve1,Sieve2,Sieve3,Sieve4
Final Sieve Ordering: SieveM,SieveN
Final Sieve Ordering: SieveQ,SieveR,SieveS
Final Sieve Ordering: Sieve5,Sieve6
Final Sieve Ordering: SieveD,SieveE,SieveF
Final Sieve Ordering: Sieve7,Sieve8,Sieve9
Final Sieve Ordering: SieveG,SieveH
Final Sieve Ordering: SieveT,SieveU,SieveV
Final Sieve Ordering: SieveI,SieveJ,SieveK
Final Sieve Ordering: SieveW,SieveX
Final Sieve Ordering: SieveL,SieveM
Final Sieve Ordering: SieveY,SieveZ
Adding sieve 0=Filter to existing sieves:
Adding sieve 1=Sorter to existing sieves:
Adding sieve 2=Mapper to existing sieves:
Adding sieve 3=Reducer to existing sieves:
Adding sieve 4=Aggregator to existing sieves:
Adding sieve 5=Joiner to existing sieves:
Adding sieve 6=Splitter to existing sieves:
Adding sieve 7=Counter to existing sieves:
Adding sieve 8=Ranker to existing sieves:
Adding sieve 9=Grouper to existing sieves:
Adding sieve 10=Sampler to existing sieves:
Adding sieve 11=Shuffler to existing sieves:
Adding sieve 12=Merger to existing sieves:
Adding sieve 13=Deduplicator to existing sieves:
Adding sieve 14=Compressor to existing sieves:
Trying sieve 0=SimpleSieve:
Trying sieve 1=CompositeSieve:
Trying sieve 2=PrimeSieve:
Trying sieve 3=OddSieve:
Trying sieve 4=EvenSieve:
Trying sieve 5=DivisibleSieve:
Trying sieve 6=MultipleSieve:
Trying sieve 7=SquareSieve:
Trying sieve 8=CubeSieve:
Trying sieve 9=PalindromeSieve:
Trying sieve 10=ReverseSieve:
Trying sieve 11=BinarySieve:
Trying sieve 12=HexadecimalSieve:
Trying sieve 13=FactorialSieve:
Trying sieve 14=FibonacciSieve:
Failed to parse gold.yieldHasWord()
Failed to parse gold.yieldHasWord(0.5)
Failed to parse gold.yieldHasWord("hello")
Failed to parse gold.yieldHasWord(true)
Failed to parse gold.yieldHasWord([1,2,3])
Failed to parse gold.yieldHasWord({a:1, b:2})
Failed to parse gold.yieldHasWord(null)
Failed to parse gold.yieldHasWord(undefined)
Failed to parse gold.yieldHasWord(NaN)
Failed to parse gold.yieldHasWord(Infinity)
Failed to parse gold.yieldHasWord(-Infinity)
Failed to parse gold.yieldHasWord(new Date())
Failed to parse gold.yieldHasWord(Math.random())
Failed to parse gold.yieldHasWord(function(){return 42;})
Failed to parse gold.yieldHasWord(gold)
Looking for head of NP; value is |John|, baseCat is |N|
Looking for head of VP; value is |likes|, baseCat is |V|
Looking for head of PP; value is |in|, baseCat is |P|
Looking for head of ADJP; value is |happy|, baseCat is |ADJ|
Looking for head of ADVP; value is |quickly|, baseCat is |ADV|
Looking for head of S; value is |He|, baseCat is |N|
Looking for head of QP; value is |two|, baseCat is |NUM|
Looking for head of NP; value is |dogs|, baseCat is |N|
Looking for head of VP; value is |chased|, baseCat is |V|
Looking for head of PP; value is |by|, baseCat is |P|
Looking for head of NP; value is |the|, baseCat is |DET|
Looking for head of ADJP; value is |big|, baseCat is |ADJ|
Looking for head of NP; value is |cat|, baseCat is |N|
Looking for head of VP; value is |ran|, baseCat is |V|
Looking for head of ADVP; value is |slowly|, baseCat is |ADV|
Averaging model1.h5
Averaging model2.h5, model3.h5
Averaging model4.h5, model5.h5, model6.h5
Averaging model7.h5, model8.h5, model9.h5, model10.h5
Averaging model11.h5, model12.h5
Averaging model13.h5
Averaging model14.h5, model15.h5, model16.h5
Averaging model17.h5, model18.h5, model19.h5
Averaging model20.h5
Averaging model21.h5, model22.h5, model23.h5, model24.h5
Averaging model25.h5, model26.h5
Averaging model27.h5, model28.h5, model29.h5
Averaging model30.h5
Averaging model31.h5, model32.h5, model33.h5
Averaging model34.h5, model35.h5
normalize(thisLine) UTF-8
normalize(Hello world!) UTF-8
normalize(Bonjour, le monde !) UTF-8
normalize(Hola, mundo!) UTF-8
normalize(Salut, lume!) UTF-8
normalize(Ciao, mondo!) UTF-8
normalize(Hallo, Welt!) UTF-8
normalize(Sawubona, umhlaba!) UTF-8
WARNING: gold mentions with the same offsets: 192.168.0.1 mentions=123, [456, 789], John Smith
WARNING: gold mentions with the same offsets: 10.0.0.2 mentions=234, [567, 890], Jane Doe
WARNING: gold mentions with the same offsets: 172.16.0.3 mentions=345, [678, 901], Bob Lee
WARNING: gold mentions with the same offsets: 127.0.0.1 mentions=456, [789, 123], Alice Chen
WARNING: gold mentions with the same offsets: 192.168.1.4 mentions=567, [890, 234], Tom Green
WARNING: gold mentions with the same offsets: 10.0.1.5 mentions=678, [901, 345], Mary Jones
WARNING: gold mentions with the same offsets: 172.16.1.6 mentions=789, [123, 456], Jack Brown
WARNING: gold mentions with the same offsets: 127.0.1.7 mentions=890, [234, 567], Anna Smith
WARNING: gold mentions with the same offsets: 192.168.2.8 mentions=901, [345, 678], David Lee
WARNING: gold mentions with the same offsets: 10.0.2.9 mentions=123, [456, 789], Emma Chen
WARNING: gold mentions with the same offsets: 172.16.2.10 mentions=234, [567, 890], Mike Green
WARNING: gold mentions with the same offsets: 127.0.2.11 mentions=345, [678, 901], Lily Jones
WARNING: gold mentions with the same offsets: 192.168.3.12 mentions=456, [789, 123], James Brown
WARNING: gold mentions with the same offsets: 10.0.3.13 mentions=567, [890, 234], Kate Smith
WARNING: gold mentions with the same offsets: 172.16.3.14 mentions=678, [901, 345], Eric Lee
com.example.service.UserService: Splicing in DUMMYTAG for User{id=1, name='Alice', email='alice@example.com'}
com.example.dao.ProductDao: Splicing in DUMMYTAG for Product{id=2, name='Book', price=9.99}
com.example.controller.OrderController: Splicing in DUMMYTAG for Order{id=3, userId=1, productId=2, quantity=1, status='PAID'}
com.example.util.Logger: Splicing in DUMMYTAG for Log{level='INFO', message='Order created successfully', timestamp='2023-11-29T10:45:32Z'}
com.example.exception.CustomException: Splicing in DUMMYTAG for Exception{message='Invalid input', cause='NumberFormatException', stackTrace='...'}
com.example.config.AppConfig: Splicing in DUMMYTAG for Config{key='database.url', value='jdbc:mysql://localhost:3306/example'}
com.example.model.Customer: Splicing in DUMMYTAG for Customer{id=4, name='Bob', address='123 Main Street', phone='555-1234'}
com.example.repository.OrderRepository: Splicing in DUMMYTAG for List<Order>{[Order{id=3, userId=1, productId=2, quantity=1, status='PAID'}, Order{id=5, userId=4, productId=6, quantity=2, status='SHIPPED'}]}
com.example.service.ProductService: Splicing in DUMMYTAG for Map<String, Double>{['Book'=9.99, 'Pen'=0.99, 'Notebook'=4.99]}
com.example.dao.CustomerDao: Splicing in DUMMYTAG for Optional<Customer>{Customer{id=4, name='Bob', address='123 Main Street', phone='555-1234'}}
com.example.controller.CustomerController: Splicing in DUMMYTAG for ResponseEntity<Customer>{status=200, body=Customer{id=4, name='Bob', address='123 Main Street', phone='555-1234'}, headers={'Content-Type'='application/json'}}
com.example.util.JsonUtil: Splicing in DUMMYTAG for String{'{"id":4,"name":"Bob","address":"123 Main Street","phone":"555-1234"}'}
com.example.exception.NotFoundException: Splicing in DUMMYTAG for Exception{message='Customer not found', cause=null, stackTrace='...'}
com.example.config.SecurityConfig: Splicing in DUMMYTAG for Config{key='jwt.secret', value='********'}
com.example.model.OrderItem: Splicing in DUMMYTAG for OrderItem{orderId=5, productId=6, productName='Laptop', productPrice=999.99, quantity=2}
String: hello basic: Noun basicAndFunc: Noun, Greeting
String: apple basic: Noun basicAndFunc: Noun, Food
String: run basic: Verb basicAndFunc: Verb, Action
String: happy basic: Adjective basicAndFunc: Adjective, Emotion
String: quickly basic: Adverb basicAndFunc: Adverb, Modifier
String: red basic: Adjective basicAndFunc: Adjective, Color
String: book basic: Noun basicAndFunc: Noun, Object
String: sing basic: Verb basicAndFunc: Verb, Sound
String: big basic: Adjective basicAndFunc: Adjective, Size
String: slowly basic: Adverb basicAndFunc: Adverb, Modifier
String: dog basic: Noun basicAndFunc: Noun, Animal
String: jump basic: Verb basicAndFunc: Verb, Action
String: sad basic: Adjective basicAndFunc: Adjective, Emotion
String: loudly basic: Adverb basicAndFunc: Adverb, Modifier
String: blue basic: Adjective basicAndFunc: Adjective, Color
0.5; outputFrequency
0.75; outputFrequency
0.25; outputFrequency
0.9; outputFrequency
0.1; outputFrequency
0.6; outputFrequency
0.8; outputFrequency
0.4; outputFrequency
0.7; outputFrequency
0.3; outputFrequency
1; outputFrequency
0; outputFrequency
0.2; outputFrequency
0.95; outputFrequency
0.05; outputFrequency
decision toReturn: true
decision toReturn: false
decision toReturn: null
decision toReturn: 1
decision toReturn: 0
decision toReturn: -1
decision toReturn: "yes"
decision toReturn: "no"
decision toReturn: "maybe"
decision toReturn: undefined
decision toReturn: NaN
decision toReturn: 0.5
decision toReturn: "true"
decision toReturn: "false"
decision toReturn: "error"
Loading relation model from relationModel_1.json
Loading relation model from relationModel_2.csv
Loading relation model from relationModel_3.xml
Loading relation model from relationModel_4.pkl
Loading relation model from relationModel_5.h5
Loading relation model from relationModel_6.mat
Loading relation model from relationModel_7.npz
Loading relation model from relationModel_8.db
Loading relation model from relationModel_9.sqlite
Loading relation model from relationModel_10.parquet
Loading relation model from relationModel_11.feather
Loading relation model from relationModel_12.orc
Loading relation model from relationModel_13.avro
Loading relation model from relationModel_14.hdf
Loading relation model from relationModel_15.nc
Using unknown dashed word vector heuristics: true
Using unknown dashed word vector heuristics: false
Using unknown dashed word vector heuristics: 0.5
Using unknown dashed word vector heuristics: 1
Using unknown dashed word vector heuristics: -1
Using unknown dashed word vector heuristics: null
Using unknown dashed word vector heuristics: "average"
Using unknown dashed word vector heuristics: "random"
Using unknown dashed word vector heuristics: "zero"
Using unknown dashed word vector heuristics: "none"
Using unknown dashed word vector heuristics: "sum"
Using unknown dashed word vector heuristics: "concat"
Using unknown dashed word vector heuristics: "max"
Using unknown dashed word vector heuristics: "min"
Using unknown dashed word vector heuristics: "custom"
output: The sky is blue today.
output: She loves reading books.
output: He ran fast to catch the bus.
output: They went to the park together.
output: I ate a delicious pizza for lunch.
output: You are very kind and helpful.
output: It was a cold and rainy night.
output: He scored a goal in the last minute.
output: She plays the piano very well.
output: They have a cute dog named Spot.
output: I have to finish my homework by tomorrow.
output: You should drink more water.
output: It is a beautiful and sunny day.
output: He likes to watch movies on Netflix.
output: She is studying hard for the exam.
Added outgoingDependency due to id 5678
Added outgoingDependency due to id 4321
Added outgoingDependency due to id 9876
Added outgoingDependency due to id 2468
Added outgoingDependency due to id 1357
Added outgoingDependency due to id 8642
Added outgoingDependency due to id 9513
Added outgoingDependency due to id 7284
Added outgoingDependency due to id 6149
Added outgoingDependency due to id 3927
Added outgoingDependency due to id 4756
Added outgoingDependency due to id 1834
Added outgoingDependency due to id 6295
Added outgoingDependency due to id 7412
Added outgoingDependency due to id 8563
MATCHED hello ... world with a score of 0.95
MATCHED foo ... bar with a score of 0.76
MATCHED apple ... pie with a score of 0.87
MATCHED cat ... dog with a score of 0.65
MATCHED red ... blue with a score of 0.71
MATCHED star ... wars with a score of 0.92
MATCHED love ... hate with a score of 0.68
MATCHED fire ... ice with a score of 0.79
MATCHED sun ... moon with a score of 0.83
MATCHED king ... queen with a score of 0.89
MATCHED car ... bike with a score of 0.74
MATCHED book ... movie with a score of 0.81
MATCHED coffee ... tea with a score of 0.77
MATCHED rain ... snow with a score of 0.72
MATCHED fish ... chips with a score of 0.86
debug-clusterNE: PERSON first Mention's ID: 12 Heads: John words: John Smith
debug-clusterNE: ORGANIZATION first Mention's ID: 34 Heads: Microsoft words: Microsoft Corporation
debug-clusterNE: LOCATION first Mention's ID: 56 Heads: London words: London, England
debug-clusterNE: DATE first Mention's ID: 78 Heads: December words: December 3, 2023
debug-clusterNE: MONEY first Mention's ID: 90 Heads: $100 words: one hundred dollars
debug-clusterNE: PERCENT first Mention's ID: 11 Heads: 50% words: fifty percent
debug-clusterNE: TIME first Mention's ID: 33 Heads: 10am words: ten o'clock in the morning
debug-clusterNE: QUANTITY first Mention's ID: 55 Heads: 5kg words: five kilograms
debug-clusterNE: ORDINAL first Mention's ID: 77 Heads: third words: the third one
debug-clusterNE: CARDINAL first Mention's ID: 99 Heads: four words: four people
Constraint not satisfied 0 0.5 0.4 lambda 0.1
Constraint not satisfied 1 0.7 0.6 lambda 0.2
Constraint not satisfied 2 0.9 0.8 lambda 0.3
Constraint not satisfied 3 1.1 1.0 lambda 0.4
Constraint not satisfied 4 1.3 1.2 lambda 0.5
Constraint not satisfied 5 1.5 1.4 lambda 0.6
Constraint not satisfied 6 1.7 1.6 lambda 0.7
Constraint not satisfied 7 1.9 1.8 lambda 0.8
Constraint not satisfied 8 2.1 2.0 lambda 0.9
Constraint not satisfied 9 2.3 2.2 lambda 1.0
Constraint not satisfied 10 -0.5 -0.4 lambda -0.1
Constraint not satisfied 11 -0.7 -0.6 lambda -0.2
Constraint not satisfied 12 -0.9 -0.8 lambda -0.3
Constraint not satisfied 13 -1.1 -1.0 lambda -0.4
Constraint not satisfied 14 -1.3 -1.2 lambda -0.5
Redwood.DBG labeling sentences from sents.second() with the already learned words
Redwood.DBG labeling sentences from "The dog barked loudly." with the already learned words
Redwood.DBG labeling sentences from "She likes to read books." with the already learned words
Redwood.DBG labeling sentences from "He ran fast to catch the bus." with the already learned words
Redwood.DBG labeling sentences from "They went to the park together." with the already learned words
Redwood.DBG labeling sentences from "It was a sunny day." with the already learned words
Redwood.DBG labeling sentences from "She gave him a hug." with the already learned words
Redwood.DBG labeling sentences from "He played the guitar." with the already learned words
Redwood.DBG labeling sentences from "They ate pizza for dinner." with the already learned words
Redwood.DBG labeling sentences from "It rained heavily." with the already learned words
Redwood.DBG labeling sentences from "She sang a song." with the already learned words
Redwood.DBG labeling sentences from "He watched a movie." with the already learned words
Redwood.DBG labeling sentences from "They played chess." with the already learned words
Redwood.DBG labeling sentences from "It snowed a lot." with the already learned words
Redwood.DBG labeling sentences from "She painted a picture." with the already learned words
Redwood.DBG labeling sentences from "He wrote a poem." with the already learned words
Running 4 threads of different taggers
Running 7 threads of different taggers
Running 2 threads of different taggers
Running 9 threads of different taggers
Running 3 threads of different taggers
Running 5 threads of different taggers
Running 8 threads of different taggers
Running 6 threads of different taggers
Running 10 threads of different taggers
Running 11 threads of different taggers
Running 12 threads of different taggers
Running 13 threads of different taggers
Running 14 threads of different taggers
Running 15 threads of different taggers
Results:  gain: 0.56  batch 32   mu0.12  lam0.34
Results:  gain: 0.67  batch 64   mu0.15  lam0.29
Results:  gain: 0.49  batch 16   mu0.09  lam0.37
Results:  gain: 0.58  batch 48   mu0.13  lam0.31
Results:  gain: 0.62  batch 24   mu0.11  lam0.33
Results:  gain: 0.54  batch 40   mu0.14  lam0.32
Results:  gain: 0.59  batch 56   mu0.16  lam0.28
Results:  gain: 0.52  batch 8    mu0.08  lam0.38
Results:  gain: 0.64  batch 80   mu0.18  lam0.26
Results:  gain: 0.51  batch 72   mu0.17  lam0.27
Results:  gain: 0.55  batch 96   mu0.19  lam0.25
Results:  gain: 0.61  batch 88   mu0.20  lam0.24
Results:  gain: 0.50  batch -4   mu-1    lam-1
Results:  gain: -1    batch -1   mu-1    lam-1
Results:  gain: -1    batch -1   mu-1    lam-1
differing NER tags detected in entity: John
differing NER tags detected in entity: London
differing NER tags detected in entity: 2023
differing NER tags detected in entity: Microsoft
differing NER tags detected in entity: iPhone 14
differing NER tags detected in entity: $199.99
differing NER tags detected in entity: Harry Potter
differing NER tags detected in entity: China
differing NER tags detected in entity: COVID-19
differing NER tags detected in entity: Tesla
differing NER tags detected in entity: Bing
differing NER tags detected in entity: 3.14159
differing NER tags detected in entity: Mona Lisa
differing NER tags detected in entity: New York Times
differing NER tags detected in entity: Netflix
Running gradient check on 100 trees
Running gradient check on 50 trees
Running gradient check on 200 trees
Running gradient check on 75 trees
Running gradient check on 150 trees
Running gradient check on 25 trees
Running gradient check on 125 trees
Running gradient check on 175 trees
Running gradient check on 10 trees
Running gradient check on 80 trees
Running gradient check on 40 trees
Running gradient check on 160 trees
Running gradient check on 60 trees
Running gradient check on 140 trees
Running gradient check on 30 trees
#richtag+lemmaTagPairs: 0
#richtag+lemmaTagPairs: 1
#richtag+lemmaTagPairs: 2
#richtag+lemmaTagPairs: 3
#richtag+lemmaTagPairs: 4
#richtag+lemmaTagPairs: 5
#richtag+lemmaTagPairs: 6
#richtag+lemmaTagPairs: 7
#richtag+lemmaTagPairs: 8
#richtag+lemmaTagPairs: 9
#richtag+lemmaTagPairs: 10
#richtag+lemmaTagPairs: 11
#richtag+lemmaTagPairs: 12
#richtag+lemmaTagPairs: 13
#richtag+lemmaTagPairs: 14
labelIndices[0].size()=3
labelIndices[1].size()=4
labelIndices[2].size()=2
labelIndices[3].size()=5
labelIndices[4].size()=1
labelIndices[5].size()=6
labelIndices[6].size()=3
labelIndices[7].size()=4
labelIndices[8].size()=2
labelIndices[9].size()=5
labelIndices[10].size()=1
labelIndices[11].size()=6
labelIndices[12].size()=3
labelIndices[13].size()=4
labelIndices[14].size()=2
Reading unsupervised dropout data from file: /home/user/data/unsup_dropout_1.csv
Reading unsupervised dropout data from file: C:\Users\user\data\unsup_dropout_2.txt
Reading unsupervised dropout data from file: /data/unsup_dropout_3.json
Reading unsupervised dropout data from file: /home/user/data/unsup_dropout_4.tsv
Reading unsupervised dropout data from file: C:\Users\user\data\unsup_dropout_5.xlsx
Reading unsupervised dropout data from file: /data/unsup_dropout_6.pkl
Reading unsupervised dropout data from file: /home/user/data/unsup_dropout_7.hdf5
Reading unsupervised dropout data from file: C:\Users\user\data\unsup_dropout_8.mat
Reading unsupervised dropout data from file: /data/unsup_dropout_9.npz
Reading unsupervised dropout data from file: /home/user/data/unsup_dropout_10.parquet
Reading unsupervised dropout data from file: C:\Users\user\data\unsup_dropout_11.feather
Reading unsupervised dropout data from file: /data/unsup_dropout_12.dbf
Reading unsupervised dropout data from file: /home/user/data/unsup_dropout_13.arff
Reading unsupervised dropout data from file: C:\Users\user\data\unsup_dropout_14.dta
Reading unsupervised dropout data from file: /data/unsup_dropout_15.sas7bdat
added vertex node1
added vertex node2
added vertex node3
added vertex node4
added vertex node5
added vertex node6
added vertex node7
added vertex node8
added vertex node9
added vertex node10
added vertex node11
added vertex node12
added vertex node13
added vertex node14
added vertex node15
PTBTokenizer untokenized 456 tokens at 12.3 tokens per second.
PTBTokenizer untokenized 789 tokens at 16.5 tokens per second.
PTBTokenizer untokenized 123 tokens at 9.8 tokens per second.
PTBTokenizer untokenized 234 tokens at 11.2 tokens per second.
PTBTokenizer untokenized 567 tokens at 14.7 tokens per second.
PTBTokenizer untokenized 891 tokens at 18.1 tokens per second.
PTBTokenizer untokenized 345 tokens at 10.4 tokens per second.
PTBTokenizer untokenized 678 tokens at 15.9 tokens per second.
PTBTokenizer untokenized 901 tokens at 19.6 tokens per second.
PTBTokenizer untokenized 432 tokens at 12.6 tokens per second.
PTBTokenizer untokenized 765 tokens at 17.3 tokens per second.
PTBTokenizer untokenized 198 tokens at 8.9 tokens per second.
PTBTokenizer untokenized 321 tokens at 10.8 tokens per second.
PTBTokenizer untokenized 654 tokens at 16.2 tokens per second.
PTBTokenizer untokenized 987 tokens at 20.1 tokens per second.
Maximum memory used: 3.2 GB
Maximum memory used: 4.5 GB
Maximum memory used: 2.7 GB
Maximum memory used: 5.1 GB
Maximum memory used: 3.8 GB
Maximum memory used: 4.2 GB
Maximum memory used: 2.9 GB
Maximum memory used: 5.4 GB
Maximum memory used: 3.5 GB
Maximum memory used: 4.7 GB
Maximum memory used: 2.6 GB
Maximum memory used: 5.2 GB
Maximum memory used: 3.9 GB
Maximum memory used: 4.3 GB
Maximum memory used: 2.8 GB
Word is hello; annotated to be the end of a sentence; [0.99, 0.01]
Word is goodbye; annotated to be the end of a sentence; [0.98, 0.02]
Word is yes; annotated to be the end of a sentence; [0.95, 0.05]
Word is no; annotated to be the end of a sentence; [0.94, 0.06]
Word is thanks; annotated to be the end of a sentence; [0.97, 0.03]
Word is please; annotated to be the end of a sentence; [0.96, 0.04]
Word is today; annotated to be the end of a sentence; [0.92, 0.08]
Word is tomorrow; annotated to be the end of a sentence; [0.91, 0.09]
Word is happy; annotated to be the end of a sentence; [0.93, 0.07]
Word is sad; annotated to be the end of a sentence; [0.90, 0.10]
Word is great; annotated to be the end of a sentence; [0.89, 0.11]
Word is terrible; annotated to be the end of a sentence; [0.88, 0.12]
Word is awesome; annotated to be the end of a sentence; [0.87, 0.13]
Word is awful; annotated to be the end of a sentence; [0.86, 0.14]
Word is okay; annotated to be the end of a sentence; [0.85, 0.15]
Word is the; is sentence boundary (matched multi-token pattern); [the, DT]
Word is hello; is sentence boundary (matched multi-token pattern); [hello, UH]
Word is .; is sentence boundary (matched multi-token pattern); [., .]
Word is and; is sentence boundary (matched multi-token pattern); [and, CC]
Word is happy; is sentence boundary (matched multi-token pattern); [happy, JJ]
Word is ?; is sentence boundary (matched multi-token pattern); [?, .]
Word is he; is sentence boundary (matched multi-token pattern); [he, PRP]
Word is she; is sentence boundary (matched multi-token pattern); [she, PRP]
Word is they; is sentence boundary (matched multi-token pattern); [they, PRP]
Word is it; is sentence boundary (matched multi-token pattern); [it, PRP]
Word is was; is sentence boundary (matched multi-token pattern); [was, VBD]
Word is were; is sentence boundary (matched multi-token pattern); [were, VBD]
Word is are; is sentence boundary (matched multi-token pattern); [are, VBP]
Word is be; is sentence boundary (matched multi-token pattern); [be, VB]
Word is to; is sentence boundary (matched multi-token pattern); [to, TO]
Predicted Speaker: John Predictor: Deterministic
Predicted Speaker: Alice Predictor: Coreference
Predicted Speaker: Bob Predictor: ConvexHull
Predicted Speaker: Mary Predictor: SpeakerName
Predicted Speaker: David Predictor: Paragraph
Predicted Speaker: Emma Predictor: Quotation
Predicted Speaker: James Predictor: Multipartite
Predicted Speaker: Anna Predictor: Supervised
Predicted Speaker: Tom Predictor: Triplet
Predicted Speaker: Lucy Predictor: NeuralNetwork
Predicted Speaker: Jack Predictor: Bayesian
Predicted Speaker: Kate Predictor: LinearRegression
Predicted Speaker: Mike Predictor: DecisionTree
Predicted Speaker: Lily Predictor: SVM
Predicted Speaker: Sam Predictor: KNN
TagAffixDetector: building TagAffixDetector from /home/user/cc.txt and /home/user/ad.txt
TagAffixDetector: building TagAffixDetector from /var/log/cc.log and /var/log/ad.log
TagAffixDetector: building TagAffixDetector from /opt/cc/cc.conf and /opt/ad/ad.conf
TagAffixDetector: building TagAffixDetector from /tmp/cc.tmp and /tmp/ad.tmp
TagAffixDetector: building TagAffixDetector from /usr/local/cc/cc.jar and /usr/local/ad/ad.jar
TagAffixDetector: building TagAffixDetector from /etc/cc/cc.ini and /etc/ad/ad.ini
TagAffixDetector: building TagAffixDetector from /dev/cc/cc.bin and /dev/ad/ad.bin
TagAffixDetector: building TagAffixDetector from /mnt/cc/cc.iso and /mnt/ad/ad.iso
TagAffixDetector: building TagAffixDetector from /root/cc/cc.dat and /root/ad/ad.dat
TagAffixDetector: building TagAffixDetector from /media/cc/cc.img and /media/ad/ad.img
TagAffixDetector: building TagAffixDetector from /srv/cc/cc.xml and /srv/ad/ad.xml
TagAffixDetector: building TagAffixDetector from /proc/cc/cc.pid and /proc/ad/ad.pid
TagAffixDetector: building TagAffixDetector from /sys/cc/cc.sys and /sys/ad/ad.sys
TagAffixDetector: building TagAffixDetector from /lib/cc/cc.so and /lib/ad/ad.so
TagAffixDetector: building TagAffixDetector from /run/cc/cc.sock and /run/ad/ad.sock
Relation extraction results AFTER consistency checks for partition #0 using printer ConsoleResultPrinter: SUCCESS
Relation extraction results AFTER consistency checks for partition #1 using printer FileResultPrinter: FAILED
Relation extraction results AFTER consistency checks for partition #2 using printer FileResultPrinter: SUCCESS
Relation extraction results AFTER consistency checks for partition #3 using printer ConsoleResultPrinter: FAILED
Relation extraction results AFTER consistency checks for partition #4 using printer ConsoleResultPrinter: SUCCESS
Relation extraction results AFTER consistency checks for partition #5 using printer FileResultPrinter: FAILED
Relation extraction results AFTER consistency checks for partition #6 using printer FileResultPrinter: ERROR
Relation extraction results AFTER consistency checks for partition #7 using printer ConsoleResultPrinter: FAILED
Relation extraction results AFTER consistency checks for partition #8 using printer ConsoleResultPrinter: ERROR
Relation extraction results AFTER consistency checks for partition #9 using printer FileResultPrinter: SUCCESS
directed path nodes btw 1 and 5 is [2, 3, 4]
directed path nodes btw 1 and 5 is [2, 4]
directed path nodes btw 1 and 5 is [3, 4]
directed path nodes btw 1 and 5 is [2, 3, 5]
directed path nodes btw 1 and 5 is [3, 5]
directed path nodes btw 1 and 5 is [4, 5]
directed path nodes btw 1 and 5 is [2, 4, 5]
directed path nodes btw 1 and 5 is [3, 4, 5]
directed path nodes btw 1 and 5 is [2, 3, 4, 5]
directed path nodes btw 1 and 5 is [1, 2, 3, 4, 5]
directed path nodes btw 1 and 5 is [1, 2, 4, 5]
directed path nodes btw 1 and 5 is [1, 3, 4, 5]
directed path nodes btw 1 and 5 is [1, 2, 3, 5]
directed path nodes btw 1 and 5 is [1, 3, 5]
directed path nodes btw 1 and 5 is [1, 4, 5]
Exception thrown attempting to get tag text for tag=div, from element=header
Exception thrown attempting to get tag text for tag=span, from element=footer
Exception thrown attempting to get tag text for tag=a, from element=nav
Exception thrown attempting to get tag text for tag=h1, from element=main
Exception thrown attempting to get tag text for tag=img, from element=aside
Exception thrown attempting to get tag text for tag=p, from element=article
Exception thrown attempting to get tag text for tag=li, from element=ul
Exception thrown attempting to get tag text for tag=button, from element=form
Exception thrown attempting to get tag text for tag=strong, from element=p
Exception thrown attempting to get tag text for tag=em, from element=h2
Exception thrown attempting to get tag text for tag=input, from element=label
Exception thrown attempting to get tag text for tag=select, from element=form
Exception thrown attempting to get tag text for tag=option, from element=select
Exception thrown attempting to get tag text for tag=script, from element=head
Exception thrown attempting to get tag text for tag=style, from element=head
Exception e=NullPointerException thrown calling getEltText on element=button
Exception e=IndexOutOfBoundsException thrown calling getEltText on element=list
Exception e=NoSuchElementException thrown calling getEltText on element=dropdown
Exception e=TimeoutException thrown calling getEltText on element=link
Exception e=StaleElementReferenceException thrown calling getEltText on element=input
Exception e=ElementNotVisibleException thrown calling getEltText on element=image
Exception e=InvalidSelectorException thrown calling getEltText on element=table
Exception e=WebDriverException thrown calling getEltText on element=frame
Exception e=ElementNotInteractableException thrown calling getEltText on element=checkbox
Exception e=ElementClickInterceptedException thrown calling getEltText on element=radiobutton
Exception e=NoSuchFrameException thrown calling getEltText on element=iframe
Exception e=NoSuchWindowException thrown calling getEltText on element=popup
Exception e=JavascriptException thrown calling getEltText on element=script
Exception e=SessionNotCreatedException thrown calling getEltText on element=session
Exception e=InvalidArgumentException thrown calling getEltText on element=null
Warning: tree is leaf: java.lang.String
Warning: tree is leaf: false
Warning: tree is leaf: null
Warning: tree is leaf: 42
Warning: tree is leaf: true
Warning: tree is leaf: java.util.ArrayList
Warning: tree is leaf: "Hello"
Warning: tree is leaf: 3.14
Warning: tree is leaf: java.lang.Object
Warning: tree is leaf: -1
Warning: tree is leaf: "World"
Warning: tree is leaf: java.util.HashMap
Warning: tree is leaf: 0
Warning: tree is leaf: java.lang.Integer
Warning: tree is leaf: 1.23
unknown annealingType: log.  Please use linear|exp|exponential
unknown annealingType: sin.  Please use linear|exp|exponential
unknown annealingType: cos.  Please use linear|exp|exponential
unknown annealingType: tan.  Please use linear|exp|exponential
unknown annealingType: sigmoid.  Please use linear|exp|exponential
unknown annealingType: relu.  Please use linear|exp|exponential
unknown annealingType: step.  Please use linear|exp|exponential
unknown annealingType: quadratic.  Please use linear|exp|exponential
unknown annealingType: cubic.  Please use linear|exp|exponential
unknown annealingType: quartic.  Please use linear|exp|exponential
unknown annealingType: quintic.  Please use linear|exp|exponential
unknown annealingType: hexic.  Please use linear|exp|exponential
unknown annealingType: septic.  Please use linear|exp|exponential
unknown annealingType: octic.  Please use linear|exp|exponential
unknown annealingType: nonic.  Please use linear|exp|exponential
TagAffixDetector: useChPos= true | useCTBChar2= false | usePKChar2= true
TagAffixDetector: useChPos= false | useCTBChar2= true | usePKChar2= false
TagAffixDetector: useChPos= true | useCTBChar2= true | usePKChar2= false
TagAffixDetector: useChPos= false | useCTBChar2= false | usePKChar2= true
TagAffixDetector: useChPos= true | useCTBChar2= true | usePKChar2= true
TagAffixDetector: useChPos= false | useCTBChar2= false | usePKChar2= false
TagAffixDetector: useChPos= true | useCTBChar2= false | usePKChar2= false
TagAffixDetector: useChPos= false | useCTBChar2= true | usePKChar2= true
TagAffixDetector: useChPos= 1 | useCTBChar2= 0 | usePKChar2= 1
TagAffixDetector: useChPos= 0 | useCTBChar2= 1 | usePKChar2= 0
TagAffixDetector: useChPos= 1 | useCTBChar2= 1 | usePKChar2= 0
TagAffixDetector: useChPos= 0 | useCTBChar2= 0 | usePKChar2= 1
TagAffixDetector: useChPos= 1 | useCTBChar2= 1 | usePKChar2= 1
TagAffixDetector: useChPos= 0 | useCTBChar2= 0 | usePKChar2= 0
TagAffixDetector: useChPos= yes | useCTBChar2= no | usePKChar2= yes
evaluating recall...
evaluating recall...
evaluating recall...
evaluating recall...
evaluating recall...
evaluating recall...
evaluating recall...
evaluating recall...
evaluating recall...
evaluating recall...
evaluating recall...
evaluating recall...
evaluating recall...
evaluating recall...
evaluating recall...
f_Parsed:  false
f_Parsed:  true
f_Parsed:  true
f_Parsed:  false
f_Parsed:  false
f_Parsed:  true
f_Parsed:  true
f_Parsed:  false
f_Parsed:  false
f_Parsed:  true
f_Parsed:  false
f_Parsed:  true
f_Parsed:  false
ERROR score: 42.8
WARNING score: 75.2
INFO score: 88.9
FATAL score: 55.6
DEBUG score: 63.4
CRITICAL score: 81.7
NOTICE score: 92.3
ALERT score: 66.1
EMERGENCY score: 79.4
INFO score: 87.2
SUCCESS score: 93.8
ERROR score: 48.6
WARNING score: 71.9
evaluating precision...
evaluating precision...
evaluating precision...
evaluating precision...
evaluating precision...
evaluating precision...
evaluating precision...
evaluating precision...
evaluating precision...
evaluating precision...
evaluating precision...
evaluating precision...
evaluating precision...
evaluating precision...
evaluating precision...
Most frequently overproposed categories:
Most frequently overproposed categories:
Most frequently overproposed categories:
Most frequently overproposed categories:
Most frequently overproposed categories:
Most frequently overproposed categories:
Most frequently overproposed categories:
Most frequently overproposed categories:
Most frequently overproposed categories:
Most frequently overproposed categories:
Most frequently overproposed categories:
Most frequently overproposed categories:
Most frequently overproposed categories:
Most frequently overproposed categories:
Most frequently overproposed categories:
Parsed:    5
Parsed:    7
Parsed:    2
Parsed:    12
Parsed:    8
Parsed:    3
Parsed:    6
Parsed:    9
Parsed:    4
Parsed:    11
Parsed:    14
Parsed:    1
Parsed:    13
Most frequently underproposed categories:
Most frequently underproposed categories:
Most frequently underproposed categories:
Most frequently underproposed categories:
Most frequently underproposed categories:
Most frequently underproposed categories:
Most frequently underproposed categories:
Most frequently underproposed categories:
Most frequently underproposed categories:
Most frequently underproposed categories:
Most frequently underproposed categories:
Most frequently underproposed categories:
Most frequently underproposed categories:
Most frequently underproposed categories:
Most frequently underproposed categories:
Most frequently overproposed rules:
Most frequently overproposed rules:
Most frequently overproposed rules:
Most frequently overproposed rules:
Most frequently overproposed rules:
Most frequently overproposed rules:
Most frequently overproposed rules:
Most frequently overproposed rules:
Most frequently overproposed rules:
Most frequently overproposed rules:
Most frequently overproposed rules:
Most frequently overproposed rules:
Most frequently overproposed rules:
Most frequently overproposed rules:
Most frequently overproposed rules:
Distributions.klDivergence (remaining mass) returning +inf: p1=0.3, p2=0.1
Distributions.klDivergence (remaining mass) returning +inf: p1=0.2, p2=0.4
Distributions.klDivergence (remaining mass) returning +inf: p1=0.8, p2=0.9
Distributions.klDivergence (remaining mass) returning +inf: p1=0.6, p2=0.3
Distributions.klDivergence (remaining mass) returning +inf: p1=0.9, p2=0.2
Distributions.klDivergence (remaining mass) returning +inf: p1=0.4, p2=0.8
Distributions.klDivergence (remaining mass) returning +inf: p1=0.7, p2=0.6
Distributions.klDivergence (remaining mass) returning +inf: p1=0.1, p2=0.5
Distributions.klDivergence (remaining mass) returning +inf: p1=0.2, p2=0.6
Distributions.klDivergence (remaining mass) returning +inf: p1=0.3, p2=0.8
Distributions.klDivergence (remaining mass) returning +inf: p1=0.7, p2=0.4
Distributions.klDivergence (remaining mass) returning +inf: p1=0.4, p2=0.1
Distributions.klDivergence (remaining mass) returning +inf: p1=0.9, p2=0.5
Most frequently underproposed rules:
Most frequently underproposed rules:
Most frequently underproposed rules:
Most frequently underproposed rules:
Most frequently underproposed rules:
Most frequently underproposed rules:
Most frequently underproposed rules:
Most frequently underproposed rules:
Most frequently underproposed rules:
Most frequently underproposed rules:
Most frequently underproposed rules:
Most frequently underproposed rules:
Most frequently underproposed rules:
Most frequently underproposed rules:
Most frequently underproposed rules:
Didtributions.kldivergence returning +inf: p1=0.65, p2=0.90
Didtributions.kldivergence returning +inf: p1=0.95, p2=0.60
Didtributions.kldivergence returning +inf: p1=0.80, p2=0.70
Didtributions.kldivergence returning +inf: p1=0.55, p2=0.95
Didtributions.kldivergence returning +inf: p1=0.90, p2=0.65
Didtributions.kldivergence returning +inf: p1=0.70, p2=0.80
Didtributions.kldivergence returning +inf: p1=0.85, p2=0.75
Didtributions.kldivergence returning +inf: p1=0.60, p2=0.95
Didtributions.kldivergence returning +inf: p1=0.75, p2=0.85
Didtributions.kldivergence returning +inf: p1=0.80, p2=0.70
Didtributions.kldivergence returning +inf: p1=0.90, p2=0.65
Didtributions.kldivergence returning +inf: p1=0.55, p2=0.95
Didtributions.kldivergence returning +inf: p1=0.95, p2=0.60
Post mortem:
Post mortem:
Post mortem:
Post mortem:
Post mortem:
Post mortem:
Post mortem:
Post mortem:
Post mortem:
Post mortem:
Post mortem:
Post mortem:
Post mortem:
Post mortem:
Post mortem:
Distribution sums to less than 1
Distribution sums to less than 1
Distribution sums to less than 1
Distribution sums to less than 1
Distribution sums to less than 1
Distribution sums to less than 1
Distribution sums to less than 1
Distribution sums to less than 1
Distribution sums to less than 1
Distribution sums to less than 1
Distribution sums to less than 1
Distribution sums to less than 1
Distribution sums to less than 1
Distribution sums to less than 1
Distribution sums to less than 1
3 count prob: 1.8
3 count prob: 2.9
3 count prob: 2.2
3 count prob: 1.6
3 count prob: 2.7
3 count prob: 2.0
3 count prob: 1.4
3 count prob: 2.5
3 count prob: 1.9
3 count prob: 1.1
3 count prob: 2.8
3 count prob: 1.3
3 count prob: 2.1
Done extracting grammars and lexicon.
Done extracting grammars and lexicon.
Done extracting grammars and lexicon.
Done extracting grammars and lexicon.
Done extracting grammars and lexicon.
Done extracting grammars and lexicon.
Done extracting grammars and lexicon.
Done extracting grammars and lexicon.
Done extracting grammars and lexicon.
Done extracting grammars and lexicon.
Done extracting grammars and lexicon.
Done extracting grammars and lexicon.
Done extracting grammars and lexicon.
Done extracting grammars and lexicon.
Done extracting grammars and lexicon.
2 count prob: 0.9
2 count prob: 0.6
2 count prob: 0.8
2 count prob: 0.5
2 count prob: 1.0
2 count prob: 0.3
2 count prob: 0.4
2 count prob: 0.2
2 count prob: 0.1
2 count prob: 0.6
2 count prob: 0.8
2 count prob: 0.9
2 count prob: 0.7
Extracting Dependencies...
Extracting Dependencies...
Extracting Dependencies...
Extracting Dependencies...
Extracting Dependencies...
Extracting Dependencies...
Extracting Dependencies...
Extracting Dependencies...
Extracting Dependencies...
Extracting Dependencies...
Extracting Dependencies...
Extracting Dependencies...
Extracting Dependencies...
Extracting Dependencies...
Extracting Dependencies...
1 count prob: 0.5
1 count prob: 0.75
1 count prob: 0.33
1 count prob: 0.67
1 count prob: 0.1
1 count prob: 0.2
1 count prob: 0.3
1 count prob: 0.4
1 count prob: 0.6
1 count prob: 0.7
1 count prob: 0.8
1 count prob: 0.9
1 count prob: 0.95
Extracting Lexicon...
Extracting Lexicon...
Extracting Lexicon...
Extracting Lexicon...
Extracting Lexicon...
Extracting Lexicon...
Extracting Lexicon...
Extracting Lexicon...
Extracting Lexicon...
Extracting Lexicon...
Extracting Lexicon...
Extracting Lexicon...
Extracting Lexicon...
Extracting Lexicon...
Extracting Lexicon...
Extracting PCFG...
Extracting PCFG...
Extracting PCFG...
Extracting PCFG...
Extracting PCFG...
Extracting PCFG...
Extracting PCFG...
Extracting PCFG...
Extracting PCFG...
Extracting PCFG...
Extracting PCFG...
Extracting PCFG...
Extracting PCFG...
Extracting PCFG...
Extracting PCFG...
0 count prob: 0.50
0 count prob: 0.75
0 count prob: 1.00
0 count prob: 0.10
0 count prob: 0.30
0 count prob: 0.80
0 count prob: 0.20
0 count prob: 0.60
0 count prob: 0.90
0 count prob: 0.05
0 count prob: 0.15
0 count prob: 0.95
0 count prob: 0.35
Binarizing training trees...
Binarizing training trees...
Binarizing training trees...
Binarizing training trees...
Binarizing training trees...
Binarizing training trees...
Binarizing training trees...
Binarizing training trees...
Binarizing training trees...
Binarizing training trees...
Binarizing training trees...
Binarizing training trees...
Binarizing training trees...
Binarizing training trees...
Binarizing training trees...
unseenKeys=2000 seenKeys=1000 reservedMass=1.0
unseenKeys=3000 seenKeys=1500 reservedMass=1.5
unseenKeys=4000 seenKeys=2000 reservedMass=2.0
unseenKeys=5000 seenKeys=2500 reservedMass=2.5
unseenKeys=6000 seenKeys=3000 reservedMass=3.0
unseenKeys=7000 seenKeys=3500 reservedMass=3.5
unseenKeys=8000 seenKeys=4000 reservedMass=4.0
unseenKeys=9000 seenKeys=4500 reservedMass=4.5
unseenKeys=10000 seenKeys=5000 reservedMass=5.0
unseenKeys=11000 seenKeys=5500 reservedMass=5.5
unseenKeys=12000 seenKeys=6000 reservedMass=6.0
unseenKeys=13000 seenKeys=6500 reservedMass=6.5
unseenKeys=14000 seenKeys=7000 reservedMass=7.0
Binarizing trees...
Binarizing trees...
Binarizing trees...
Binarizing trees...
Binarizing trees...
Binarizing trees...
Binarizing trees...
Binarizing trees...
Binarizing trees...
Binarizing trees...
Binarizing trees...
Binarizing trees...
Binarizing trees...
Binarizing trees...
Binarizing trees...
N: 2
N: 3
N: 4
N: 5
N: 6
N: 7
N: 8
N: 9
N: 10
N: 11
N: 12
N: 13
N: 14
Completed processing at 12:31:17
Completed processing at 15:59:45
Completed processing at 09:15:22
Completed processing at 14:27:59
Completed processing at 17:10:33
Completed processing at 11:05:44
Completed processing at 13:48:15
Completed processing at 16:22:58
Completed processing at 08:10:37
Completed processing at 19:33:29
Completed processing at 21:15:50
Completed processing at 22:58:12
Completed processing at 23:59:59
Counters are equal.
Counters are equal.
Counters are equal.
Counters are equal.
Counters are equal.
Counters are equal.
Counters are equal.
Counters are equal.
Counters are equal.
Counters are equal.
Counters are equal.
Counters are equal.
Counters are equal.
Counters are equal.
Counters are equal.
> wrong = false ; total = 20
> wrong = true ; total = 15
> wrong = false ; total = 5
> wrong = true ; total = 8
> wrong = false ; total = 12
> wrong = true ; total = 3
> wrong = false ; total = 7
> wrong = true ; total = 13
> wrong = false ; total = 6
> wrong = true ; total = 9
> wrong = false ; total = 18
> wrong = true ; total = 4
> wrong = false ; total = 14
Using random sampling
Using random sampling
Using random sampling
Using random sampling
Using random sampling
Using random sampling
Using random sampling
Using random sampling
Using random sampling
Using random sampling
Using random sampling
Using random sampling
Using random sampling
Using random sampling
Using random sampling
Successful shutdown!
Successful shutdown!
Successful shutdown!
Successful shutdown!
Successful shutdown!
Successful shutdown!
Successful shutdown!
Successful shutdown!
Successful shutdown!
Successful shutdown!
Successful shutdown!
Successful shutdown!
Successful shutdown!
Successful shutdown!
Successful shutdown!
Using sequential sampling
Using sequential sampling
Using sequential sampling
Using sequential sampling
Using sequential sampling
Using sequential sampling
Using sequential sampling
Using sequential sampling
Using sequential sampling
Using sequential sampling
Using sequential sampling
Using sequential sampling
Using sequential sampling
Using sequential sampling
Using sequential sampling
Parsing model failure.
Parsing model failure.
Parsing model failure.
Parsing model failure.
Parsing model failure.
Parsing model failure.
Parsing model failure.
Parsing model failure.
Parsing model failure.
Parsing model failure.
Parsing model failure.
Parsing model failure.
Parsing model failure.
Parsing model failure.
Parsing model failure.
and guess tree
and guess tree
and guess tree
and guess tree
and guess tree
and guess tree
and guess tree
and guess tree
and guess tree
and guess tree
and guess tree
and guess tree
and guess tree
and guess tree
and guess tree
Collecting samples
Collecting samples
Collecting samples
Collecting samples
Collecting samples
Collecting samples
Collecting samples
Collecting samples
Collecting samples
Collecting samples
Collecting samples
Collecting samples
Collecting samples
Collecting samples
Collecting samples
### Joint Segmentation / Parser ###
### Joint Segmentation / Parser ###
### Joint Segmentation / Parser ###
### Joint Segmentation / Parser ###
### Joint Segmentation / Parser ###
### Joint Segmentation / Parser ###
### Joint Segmentation / Parser ###
### Joint Segmentation / Parser ###
### Joint Segmentation / Parser ###
### Joint Segmentation / Parser ###
### Joint Segmentation / Parser ###
### Joint Segmentation / Parser ###
### Joint Segmentation / Parser ###
### Joint Segmentation / Parser ###
### Joint Segmentation / Parser ###
Evaluating gold tree:
Evaluating gold tree:
Evaluating gold tree:
Evaluating gold tree:
Evaluating gold tree:
Evaluating gold tree:
Evaluating gold tree:
Evaluating gold tree:
Evaluating gold tree:
Evaluating gold tree:
Evaluating gold tree:
Evaluating gold tree:
Evaluating gold tree:
Evaluating gold tree:
Evaluating gold tree:
Doing annealing
Doing annealing
Doing annealing
Doing annealing
Doing annealing
Doing annealing
Doing annealing
Doing annealing
Doing annealing
Doing annealing
Doing annealing
Doing annealing
Doing annealing
Doing annealing
Doing annealing
found new best (99)
found new best (76)
found new best (92)
found new best (80)
found new best (93)
found new best (88)
found new best (95)
found new best (79)
found new best (84)
found new best (97)
found new best (81)
found new best (87)
found new best (91)
Characters that could not be converted [passed through!]:utf-8
Characters that could not be converted [passed through!]:utf-8
Characters that could not be converted [passed through!]:utf-8
Characters that could not be converted [passed through!]:utf-8
Characters that could not be converted [passed through!]:utf-8
Characters that could not be converted [passed through!]:utf-8
Characters that could not be converted [passed through!]:utf-8
Characters that could not be converted [passed through!]:utf-8
Characters that could not be converted [passed through!]:utf-8
Characters that could not be converted [passed through!]:utf-8
Characters that could not be converted [passed through!]:utf-8
Characters that could not be converted [passed through!]:utf-8
Characters that could not be converted [passed through!]:utf-8
Characters that could not be converted [passed through!]:utf-8
Characters that could not be converted [passed through!]:utf-8
endFold > numFolds -> setting to numFolds
endFold > numFolds -> setting to numFolds
endFold > numFolds -> setting to numFolds
endFold > numFolds -> setting to numFolds
endFold > numFolds -> setting to numFolds
endFold > numFolds -> setting to numFolds
endFold > numFolds -> setting to numFolds
endFold > numFolds -> setting to numFolds
endFold > numFolds -> setting to numFolds
endFold > numFolds -> setting to numFolds
endFold > numFolds -> setting to numFolds
endFold > numFolds -> setting to numFolds
endFold > numFolds -> setting to numFolds
endFold > numFolds -> setting to numFolds
endFold > numFolds -> setting to numFolds
All characters successfully converted!utf-8
All characters successfully converted!utf-8
All characters successfully converted!utf-8
All characters successfully converted!utf-8
All characters successfully converted!utf-8
All characters successfully converted!utf-8
All characters successfully converted!utf-8
All characters successfully converted!utf-8
All characters successfully converted!utf-8
All characters successfully converted!utf-8
All characters successfully converted!utf-8
All characters successfully converted!utf-8
All characters successfully converted!utf-8
All characters successfully converted!utf-8
All characters successfully converted!utf-8
startFold > numFolds -> setting startFold to 1
startFold > numFolds -> setting startFold to 1
startFold > numFolds -> setting startFold to 1
startFold > numFolds -> setting startFold to 1
startFold > numFolds -> setting startFold to 1
startFold > numFolds -> setting startFold to 1
startFold > numFolds -> setting startFold to 1
startFold > numFolds -> setting startFold to 1
startFold > numFolds -> setting startFold to 1
startFold > numFolds -> setting startFold to 1
startFold > numFolds -> setting startFold to 1
startFold > numFolds -> setting startFold to 1
startFold > numFolds -> setting startFold to 1
startFold > numFolds -> setting startFold to 1
startFold > numFolds -> setting startFold to 1
Eval Failed to find: username
Eval Failed to find: password
Eval Failed to find: display_name
Eval Failed to find: email
Eval Failed to find: phone_number
Eval Failed to find: address
Eval Failed to find: age
Eval Failed to find: gender
Eval Failed to find: account_number
Eval Failed to find: balance
Eval Failed to find: transaction_id
Eval Failed to find: product_code
Eval Failed to find: order_date
Total word types: 50
Total word types: 200
Total word types: 75
Total word types: 120
Total word types: 90
Total word types: 150
Total word types: 80
Total word types: 110
Total word types: 160
Total word types: 70
Total word types: 130
Total word types: 180
Total word types: 60
Please use -readerAndWriter instead.
Please use -readerAndWriter instead.
Please use -readerAndWriter instead.
Please use -readerAndWriter instead.
Please use -readerAndWriter instead.
Please use -readerAndWriter instead.
Please use -readerAndWriter instead.
Please use -readerAndWriter instead.
Please use -readerAndWriter instead.
Please use -readerAndWriter instead.
Please use -readerAndWriter instead.
Please use -readerAndWriter instead.
Please use -readerAndWriter instead.
Please use -readerAndWriter instead.
Please use -readerAndWriter instead.
Total tag types: 5
Total tag types: 8
Total tag types: 12
Total tag types: 6
Total tag types: 3
Total tag types: 9
Total tag types: 7
Total tag types: 11
Total tag types: 4
Total tag types: 13
Total tag types: 2
Total tag types: 15
Total tag types: 14
trainMap and testMap are no longer valid options - please use map instead.
trainMap and testMap are no longer valid options - please use map instead.
trainMap and testMap are no longer valid options - please use map instead.
trainMap and testMap are no longer valid options - please use map instead.
trainMap and testMap are no longer valid options - please use map instead.
trainMap and testMap are no longer valid options - please use map instead.
trainMap and testMap are no longer valid options - please use map instead.
trainMap and testMap are no longer valid options - please use map instead.
trainMap and testMap are no longer valid options - please use map instead.
trainMap and testMap are no longer valid options - please use map instead.
trainMap and testMap are no longer valid options - please use map instead.
trainMap and testMap are no longer valid options - please use map instead.
trainMap and testMap are no longer valid options - please use map instead.
trainMap and testMap are no longer valid options - please use map instead.
trainMap and testMap are no longer valid options - please use map instead.
Total WordTag types: 231
Total WordTag types: 88
Total WordTag types: 189
Total WordTag types: 73
Total WordTag types: 210
Total WordTag types: 104
Total WordTag types: 178
Total WordTag types: 92
Total WordTag types: 219
Total WordTag types: 137
Total WordTag types: 205
Total WordTag types: 125
Total WordTag types: 198
Please use -maxDocSize -1 instead.
Please use -maxDocSize -1 instead.
Please use -maxDocSize -1 instead.
Please use -maxDocSize -1 instead.
Please use -maxDocSize -1 instead.
Please use -maxDocSize -1 instead.
Please use -maxDocSize -1 instead.
Please use -maxDocSize -1 instead.
Please use -maxDocSize -1 instead.
Please use -maxDocSize -1 instead.
Please use -maxDocSize -1 instead.
Please use -maxDocSize -1 instead.
Please use -maxDocSize -1 instead.
Please use -maxDocSize -1 instead.
Please use -maxDocSize -1 instead.
Reading input...
Reading input...
Reading input...
Reading input...
Reading input...
Reading input...
Reading input...
Reading input...
Reading input...
Reading input...
Reading input...
Reading input...
Reading input...
Reading input...
Reading input...
You are using an outdated flag: -splitDocuments
You are using an outdated flag: -splitDocuments
You are using an outdated flag: -splitDocuments
You are using an outdated flag: -splitDocuments
You are using an outdated flag: -splitDocuments
You are using an outdated flag: -splitDocuments
You are using an outdated flag: -splitDocuments
You are using an outdated flag: -splitDocuments
You are using an outdated flag: -splitDocuments
You are using an outdated flag: -splitDocuments
You are using an outdated flag: -splitDocuments
You are using an outdated flag: -splitDocuments
You are using an outdated flag: -splitDocuments
You are using an outdated flag: -splitDocuments
You are using an outdated flag: -splitDocuments
Total tokens: 200
Total tokens: 300
Total tokens: 400
Total tokens: 500
Total tokens: 600
Total tokens: 700
Total tokens: 800
Total tokens: 900
Total tokens: 1000
Total tokens: 1100
Total tokens: 1200
Total tokens: 1300
Total tokens: 1400
printXML is disused; perhaps try using the -outputFormat xml option.
printXML is disused; perhaps try using the -outputFormat xml option.
printXML is disused; perhaps try using the -outputFormat xml option.
printXML is disused; perhaps try using the -outputFormat xml option.
printXML is disused; perhaps try using the -outputFormat xml option.
printXML is disused; perhaps try using the -outputFormat xml option.
printXML is disused; perhaps try using the -outputFormat xml option.
printXML is disused; perhaps try using the -outputFormat xml option.
printXML is disused; perhaps try using the -outputFormat xml option.
printXML is disused; perhaps try using the -outputFormat xml option.
printXML is disused; perhaps try using the -outputFormat xml option.
printXML is disused; perhaps try using the -outputFormat xml option.
printXML is disused; perhaps try using the -outputFormat xml option.
printXML is disused; perhaps try using the -outputFormat xml option.
printXML is disused; perhaps try using the -outputFormat xml option.
ERROR: IO exception while reading file (line 25).
ERROR: IO exception while reading file (line 50).
ERROR: IO exception while reading file (line 100).
ERROR: IO exception while reading file (line 150).
ERROR: IO exception while reading file (line 200).
ERROR: IO exception while reading file (line 250).
ERROR: IO exception while reading file (line 300).
ERROR: IO exception while reading file (line 350).
ERROR: IO exception while reading file (line 400).
ERROR: IO exception while reading file (line 450).
ERROR: IO exception while reading file (line 500).
ERROR: IO exception while reading file (line 550).
ERROR: IO exception while reading file (line 600).
Unary closedRulesWithParentIterator
Unary closedRulesWithParentIterator
Unary closedRulesWithParentIterator
Unary closedRulesWithParentIterator
Unary closedRulesWithParentIterator
Unary closedRulesWithParentIterator
Unary closedRulesWithParentIterator
Unary closedRulesWithParentIterator
Unary closedRulesWithParentIterator
Unary closedRulesWithParentIterator
Unary closedRulesWithParentIterator
Unary closedRulesWithParentIterator
Unary closedRulesWithParentIterator
Unary closedRulesWithParentIterator
Unary closedRulesWithParentIterator
Unary rulesWithParentIterator
Unary rulesWithParentIterator
Unary rulesWithParentIterator
Unary rulesWithParentIterator
Unary rulesWithParentIterator
Unary rulesWithParentIterator
Unary rulesWithParentIterator
Unary rulesWithParentIterator
Unary rulesWithParentIterator
Unary rulesWithParentIterator
Unary rulesWithParentIterator
Unary rulesWithParentIterator
Unary rulesWithParentIterator
Unary rulesWithParentIterator
Unary rulesWithParentIterator
File does not exist: /path/to/file2.txt
File does not exist: /path/to/file3.txt
File does not exist: /path/to/file4.txt
File does not exist: /path/to/file5.txt
File does not exist: /path/to/file6.txt
File does not exist: /path/to/file7.txt
File does not exist: /path/to/file8.txt
File does not exist: /path/to/file9.txt
File does not exist: /path/to/file10.txt
File does not exist: /path/to/file11.txt
File does not exist: /path/to/file12.txt
File does not exist: /path/to/file13.txt
File does not exist: /path/to/file14.txt
Unary closedRuleIterator
Unary closedRuleIterator
Unary closedRuleIterator
Unary closedRuleIterator
Unary closedRuleIterator
Unary closedRuleIterator
Unary closedRuleIterator
Unary closedRuleIterator
Unary closedRuleIterator
Unary closedRuleIterator
Unary closedRuleIterator
Unary closedRuleIterator
Unary closedRuleIterator
Unary closedRuleIterator
Unary closedRuleIterator
useKnownLCWords is deprecated; see maxAdditionalKnownLCWords (true = -1, false = 0)
useKnownLCWords is deprecated; see maxAdditionalKnownLCWords (true = -1, false = 0)
useKnownLCWords is deprecated; see maxAdditionalKnownLCWords (true = -1, false = 0)
useKnownLCWords is deprecated; see maxAdditionalKnownLCWords (true = -1, false = 0)
useKnownLCWords is deprecated; see maxAdditionalKnownLCWords (true = -1, false = 0)
useKnownLCWords is deprecated; see maxAdditionalKnownLCWords (true = -1, false = 0)
useKnownLCWords is deprecated; see maxAdditionalKnownLCWords (true = -1, false = 0)
useKnownLCWords is deprecated; see maxAdditionalKnownLCWords (true = -1, false = 0)
useKnownLCWords is deprecated; see maxAdditionalKnownLCWords (true = -1, false = 0)
useKnownLCWords is deprecated; see maxAdditionalKnownLCWords (true = -1, false = 0)
useKnownLCWords is deprecated; see maxAdditionalKnownLCWords (true = -1, false = 0)
useKnownLCWords is deprecated; see maxAdditionalKnownLCWords (true = -1, false = 0)
useKnownLCWords is deprecated; see maxAdditionalKnownLCWords (true = -1, false = 0)
useKnownLCWords is deprecated; see maxAdditionalKnownLCWords (true = -1, false = 0)
useKnownLCWords is deprecated; see maxAdditionalKnownLCWords (true = -1, false = 0)
File system does not support UTF-8 encoding.
File system does not support UTF-8 encoding.
File system does not support UTF-8 encoding.
File system does not support UTF-8 encoding.
File system does not support UTF-8 encoding.
File system does not support UTF-8 encoding.
File system does not support UTF-8 encoding.
File system does not support UTF-8 encoding.
File system does not support UTF-8 encoding.
File system does not support UTF-8 encoding.
File system does not support UTF-8 encoding.
File system does not support UTF-8 encoding.
File system does not support UTF-8 encoding.
File system does not support UTF-8 encoding.
File system does not support UTF-8 encoding.
Unary ruleIterator
Unary ruleIterator
Unary ruleIterator
Unary ruleIterator
Unary ruleIterator
Unary ruleIterator
Unary ruleIterator
Unary ruleIterator
Unary ruleIterator
Unary ruleIterator
Unary ruleIterator
Unary ruleIterator
Unary ruleIterator
Unary ruleIterator
Unary ruleIterator
Read 20 lines
Read 30 lines
Read 40 lines
Read 50 lines
Read 60 lines
Read 70 lines
Read 80 lines
Read 90 lines
Read 100 lines
Read 110 lines
Read 120 lines
Read 130 lines
Read 140 lines
TueBaDZParserParams nodeCleanup=False mKonjParent=True mContainsV=False mZu=True mColons=False
TueBaDZParserParams nodeCleanup=True mKonjParent=True mContainsV=True mZu=True mColons=True
TueBaDZParserParams nodeCleanup=False mKonjParent=False mContainsV=False mZu=False mColons=False
TueBaDZParserParams nodeCleanup=True mKonjParent=True mContainsV=False mZu=True mColons=False
TueBaDZParserParams nodeCleanup=False mKonjParent=False mContainsV=True mZu=False mColons=True
TueBaDZParserParams nodeCleanup=True mKonjParent=False mContainsV=False mZu=True mColons=False
TueBaDZParserParams nodeCleanup=False mKonjParent=True mContainsV=True mZu=False mColons=True
TueBaDZParserParams nodeCleanup=True mKonjParent=True mContainsV=False mZu=False mColons=True
TueBaDZParserParams nodeCleanup=False mKonjParent=False mContainsV=False mZu=True mColons=False
TueBaDZParserParams nodeCleanup=True mKonjParent=False mContainsV=True mZu=True mColons=False
TueBaDZParserParams nodeCleanup=False mKonjParent=True mContainsV=False mZu=False mColons=False
TueBaDZParserParams nodeCleanup=True mKonjParent=True mContainsV=True mZu=False mColons=False
TueBaDZParserParams nodeCleanup=False mKonjParent=True mContainsV=True mZu=True mColons=True
Parse exception on annotation pattern #2 initialization: IllegalArgumentException
Parse exception on annotation pattern #3 initialization: IllegalStateException
Parse exception on annotation pattern #4 initialization: ArrayIndexOutOfBoundsException
Parse exception on annotation pattern #5 initialization: ClassCastException
Parse exception on annotation pattern #6 initialization: NumberFormatException
Parse exception on annotation pattern #7 initialization: IndexOutOfBoundsException
Parse exception on annotation pattern #8 initialization: UnsupportedOperationException
Parse exception on annotation pattern #9 initialization: NoSuchElementException
Parse exception on annotation pattern #10 initialization: ConcurrentModificationException
Parse exception on annotation pattern #11 initialization: FileNotFoundException
Parse exception on annotation pattern #12 initialization: SecurityException
Parse exception on annotation pattern #13 initialization: IOException
Parse exception on annotation pattern #14 initialization: SQLException
usage: java TreeBinarizer [-tlpp class|-markovOrder int|...] treebankPath
usage: java TreeBinarizer [-tlpp class|-markovOrder int|...] treebankPath
usage: java TreeBinarizer [-tlpp class|-markovOrder int|...] treebankPath
usage: java TreeBinarizer [-tlpp class|-markovOrder int|...] treebankPath
usage: java TreeBinarizer [-tlpp class|-markovOrder int|...] treebankPath
usage: java TreeBinarizer [-tlpp class|-markovOrder int|...] treebankPath
usage: java TreeBinarizer [-tlpp class|-markovOrder int|...] treebankPath
usage: java TreeBinarizer [-tlpp class|-markovOrder int|...] treebankPath
usage: java TreeBinarizer [-tlpp class|-markovOrder int|...] treebankPath
usage: java TreeBinarizer [-tlpp class|-markovOrder int|...] treebankPath
usage: java TreeBinarizer [-tlpp class|-markovOrder int|...] treebankPath
usage: java TreeBinarizer [-tlpp class|-markovOrder int|...] treebankPath
usage: java TreeBinarizer [-tlpp class|-markovOrder int|...] treebankPath
usage: java TreeBinarizer [-tlpp class|-markovOrder int|...] treebankPath
usage: java TreeBinarizer [-tlpp class|-markovOrder int|...] treebankPath
Output filename?
Output filename?
Output filename?
Output filename?
Output filename?
Output filename?
Output filename?
Output filename?
Output filename?
Output filename?
Output filename?
Output filename?
Output filename?
Output filename?
Output filename?
Input filename?
Input filename?
Input filename?
Input filename?
Input filename?
Input filename?
Input filename?
Input filename?
Input filename?
Input filename?
Input filename?
Input filename?
Input filename?
Input filename?
Input filename?
unknown type: document.getComponentAnnotationClasses()
unknown type: sentence.size()
unknown type: token.isCorefCluster()
unknown type: span.get(CoreAnnotations.NamedEntityTagAnnotation.class)
unknown type: mention.canonicalEntityMention()
unknown type: coref.destinationsList()
unknown type: coreMap.values()
unknown type: tree.getLeaves()
unknown type: dependency.getGovernor()
unknown type: relation.getSpecific()
unknown type: mention.entityFeats()
unknown type: nerTag.getClass().getSimpleName()
unknown type: sentence.sentimentTree()
unknown type: member.getNamedEntityTag()}
leftScan found weight: def
leftScan found weight: xyz
leftScan found weight: 123
leftScan found weight: 456
leftScan found weight: 789
leftScan found weight: foo
leftScan found weight: bar
leftScan found weight: baz
leftScan found weight: lorem
leftScan found weight: ipsum
leftScan found weight: dolor
leftScan found weight: sit
leftScan found weight: amet
Missing guess entity
Missing guess entity
Missing guess entity
Missing guess entity
Missing guess entity
Missing guess entity
Missing guess entity
Missing guess entity
Missing guess entity
Missing guess entity
Missing guess entity
Missing guess entity
Missing guess entity
Missing guess entity
Missing guess entity
Missing gold entity
Missing gold entity
Missing gold entity
Missing gold entity
Missing gold entity
Missing gold entity
Missing gold entity
Missing gold entity
Missing gold entity
Missing gold entity
Missing gold entity
Missing gold entity
Missing gold entity
Missing gold entity
Missing gold entity
### No head found for:
### No head found for:
### No head found for:
### No head found for:
### No head found for:
### No head found for:
### No head found for:
### No head found for:
### No head found for:
### No head found for:
### No head found for:
### No head found for:
### No head found for:
### No head found for:
### No head found for:
Uh-oh, bad parameters passed to markovInsideBinarizeLocalTree
Uh-oh, bad parameters passed to markovInsideBinarizeLocalTree
Uh-oh, bad parameters passed to markovInsideBinarizeLocalTree
Uh-oh, bad parameters passed to markovInsideBinarizeLocalTree
Uh-oh, bad parameters passed to markovInsideBinarizeLocalTree
Uh-oh, bad parameters passed to markovInsideBinarizeLocalTree
Uh-oh, bad parameters passed to markovInsideBinarizeLocalTree
Uh-oh, bad parameters passed to markovInsideBinarizeLocalTree
Uh-oh, bad parameters passed to markovInsideBinarizeLocalTree
Uh-oh, bad parameters passed to markovInsideBinarizeLocalTree
Uh-oh, bad parameters passed to markovInsideBinarizeLocalTree
Uh-oh, bad parameters passed to markovInsideBinarizeLocalTree
Uh-oh, bad parameters passed to markovInsideBinarizeLocalTree
Uh-oh, bad parameters passed to markovInsideBinarizeLocalTree
Uh-oh, bad parameters passed to markovInsideBinarizeLocalTree
Found currency sign:EFGH
Found currency sign:IJKL
Found currency sign:MNOP
Found currency sign:QRST
Found currency sign:UVWX
Found currency sign:YZAB
Found currency sign:CDEF
Found currency sign:GHIJ
Found currency sign:KLMN
Found currency sign:OPQR
Found currency sign:STUV
Found currency sign:WXYZ
Found currency sign:1234
Found number:x * 2
Found number:sum(a, b)
Found number:Math.sqrt(x)
Found number:list.get(0)
Found number:Math.random()
Found number:str.substring(2, 5)
Found number:Math.abs(value)
Found number:array.length
Found number:map.containsKey(key)
Found number:set.size()
Found number:Math.max(a, b)
Found number:Integer.parseInt(str)
Found number:Double.isNaN(value)
Binarized tree:
Binarized tree:
Binarized tree:
Binarized tree:
Binarized tree:
Binarized tree:
Binarized tree:
Binarized tree:
Binarized tree:
Binarized tree:
Binarized tree:
Binarized tree:
Binarized tree:
Binarized tree:
Binarized tree:
Original tree:
Original tree:
Original tree:
Original tree:
Original tree:
Original tree:
Original tree:
Original tree:
Original tree:
Original tree:
Original tree:
Original tree:
Original tree:
Original tree:
Original tree:
Found number and:456
Found number and:789
Found number and:234
Found number and:567
Found number and:890
Found number and:345
Found number and:678
Found number and:901
Found number and:456
Found number and:789
Found number and:012
Found number and:567
Found number and:890
Annotating from treebank dir: /home/user/treebank2
Annotating from treebank dir: D:/data/treebank3
Annotating from treebank dir: /usr/local/treebank4
Annotating from treebank dir: /var/www/treebank5
Annotating from treebank dir: /data/treebank6
Annotating from treebank dir: E:/treebank7
Annotating from treebank dir: /opt/treebank8
Annotating from treebank dir: F:/projects/treebank9
Annotating from treebank dir: /mnt/treebank10
Annotating from treebank dir: G:/corpus/treebank11
Annotating from treebank dir: /backup/treebank12
Annotating from treebank dir: H:/documents/treebank13
Annotating from treebank dir: /media/treebank14
Options are like for lexicalized parser including -train treebankPath fileRange]
Options are like for lexicalized parser including -train treebankPath fileRange]
Options are like for lexicalized parser including -train treebankPath fileRange]
Options are like for lexicalized parser including -train treebankPath fileRange]
Options are like for lexicalized parser including -train treebankPath fileRange]
Options are like for lexicalized parser including -train treebankPath fileRange]
Options are like for lexicalized parser including -train treebankPath fileRange]
Options are like for lexicalized parser including -train treebankPath fileRange]
Options are like for lexicalized parser including -train treebankPath fileRange]
Options are like for lexicalized parser including -train treebankPath fileRange]
Options are like for lexicalized parser including -train treebankPath fileRange]
Options are like for lexicalized parser including -train treebankPath fileRange]
Options are like for lexicalized parser including -train treebankPath fileRange]
Options are like for lexicalized parser including -train treebankPath fileRange]
Options are like for lexicalized parser including -train treebankPath fileRange]
usage: java TreeAnnotatorAndBinarizer options*
usage: java TreeAnnotatorAndBinarizer options*
usage: java TreeAnnotatorAndBinarizer options*
usage: java TreeAnnotatorAndBinarizer options*
usage: java TreeAnnotatorAndBinarizer options*
usage: java TreeAnnotatorAndBinarizer options*
usage: java TreeAnnotatorAndBinarizer options*
usage: java TreeAnnotatorAndBinarizer options*
usage: java TreeAnnotatorAndBinarizer options*
usage: java TreeAnnotatorAndBinarizer options*
usage: java TreeAnnotatorAndBinarizer options*
usage: java TreeAnnotatorAndBinarizer options*
usage: java TreeAnnotatorAndBinarizer options*
usage: java TreeAnnotatorAndBinarizer options*
usage: java TreeAnnotatorAndBinarizer options*
FOUND TEMPORALS: 2022-02-15
FOUND TEMPORALS: 2022-03-25
FOUND TEMPORALS: 2022-04-10
FOUND TEMPORALS: 2022-05-20
FOUND TEMPORALS: 2022-06-05
FOUND TEMPORALS: 2022-07-15
FOUND TEMPORALS: 2022-08-20
FOUND TEMPORALS: 2022-09-10
FOUND TEMPORALS: 2022-10-05
FOUND TEMPORALS: 2022-11-15
FOUND TEMPORALS: 2022-12-25
FOUND TEMPORALS: 2023-01-10
FOUND TEMPORALS: 2023-02-20
scores: [5, 8, 1, 3, 7]
scores: [9, 2, 6, 4, 5]
scores: [7, 9, 2, 6, 8]
scores: [1, 3, 7, 9, 2]
scores: [6, 4, 5, 8, 1]
scores: [3, 7, 9, 2, 6]
scores: [4, 5, 8, 1, 3]
scores: [7, 9, 2, 6, 4]
scores: [5, 8, 1, 3, 7]
scores: [9, 2, 6, 4, 5]
scores: [1, 3, 7, 9, 2]
scores: [6, 4, 5, 8, 1]
scores: [3, 7, 9, 2, 6]
Annotated state counts
Annotated state counts
Annotated state counts
Annotated state counts
Annotated state counts
Annotated state counts
Annotated state counts
Annotated state counts
Annotated state counts
Annotated state counts
Annotated state counts
Annotated state counts
Annotated state counts
Annotated state counts
Annotated state counts
The above CoreMap has the following fields:
The above CoreMap has the following fields:
The above CoreMap has the following fields:
The above CoreMap has the following fields:
The above CoreMap has the following fields:
The above CoreMap has the following fields:
The above CoreMap has the following fields:
The above CoreMap has the following fields:
The above CoreMap has the following fields:
The above CoreMap has the following fields:
The above CoreMap has the following fields:
The above CoreMap has the following fields:
The above CoreMap has the following fields:
The above CoreMap has the following fields:
The above CoreMap has the following fields:
NumberSequence: Currency pattern broken
NumberSequence: Currency pattern broken
NumberSequence: Currency pattern broken
NumberSequence: Currency pattern broken
NumberSequence: Currency pattern broken
NumberSequence: Currency pattern broken
NumberSequence: Currency pattern broken
NumberSequence: Currency pattern broken
NumberSequence: Currency pattern broken
NumberSequence: Currency pattern broken
NumberSequence: Currency pattern broken
NumberSequence: Currency pattern broken
NumberSequence: Currency pattern broken
NumberSequence: Currency pattern broken
NumberSequence: Currency pattern broken
Warning: ChineseNumberSequenceClassifier does not have SUTime implementation.
Warning: ChineseNumberSequenceClassifier does not have SUTime implementation.
Warning: ChineseNumberSequenceClassifier does not have SUTime implementation.
Warning: ChineseNumberSequenceClassifier does not have SUTime implementation.
Warning: ChineseNumberSequenceClassifier does not have SUTime implementation.
Warning: ChineseNumberSequenceClassifier does not have SUTime implementation.
Warning: ChineseNumberSequenceClassifier does not have SUTime implementation.
Warning: ChineseNumberSequenceClassifier does not have SUTime implementation.
Warning: ChineseNumberSequenceClassifier does not have SUTime implementation.
Warning: ChineseNumberSequenceClassifier does not have SUTime implementation.
Warning: ChineseNumberSequenceClassifier does not have SUTime implementation.
Warning: ChineseNumberSequenceClassifier does not have SUTime implementation.
Warning: ChineseNumberSequenceClassifier does not have SUTime implementation.
Warning: ChineseNumberSequenceClassifier does not have SUTime implementation.
Warning: ChineseNumberSequenceClassifier does not have SUTime implementation.
Converged for cycle 2 in 8 iterations
Converged for cycle 3 in 12 iterations
Converged for cycle 4 in 5 iterations
Converged for cycle 5 in 9 iterations
Converged for cycle 6 in 7 iterations
Converged for cycle 7 in 11 iterations
Converged for cycle 8 in 6 iterations
Converged for cycle 9 in 13 iterations
Converged for cycle 10 in 4 iterations
Converged for cycle 11 in 15 iterations
Converged for cycle 12 in 3 iterations
Converged for cycle 13 in 14 iterations
Converged for cycle 14 in 2 iterations
FOUND DATE/TIME "2022-02-14 15:30:00" with offsets 27 45 and value 2022-02-14
FOUND DATE/TIME "2022-03-20 12:45:00" with offsets 63 81 and value 2022-03-20
FOUND DATE/TIME "2022-04-09 09:15:00" with offsets 99 117 and value 2022-04-09
FOUND DATE/TIME "2022-05-31 16:20:00" with offsets 135 153 and value 2022-05-31
FOUND DATE/TIME "2022-06-18 11:50:00" with offsets 171 189 and value 2022-06-18
FOUND DATE/TIME "2022-07-22 14:25:00" with offsets 207 225 and value 2022-07-22
FOUND DATE/TIME "2022-08-11 10:05:00" with offsets 243 261 and value 2022-08-11
FOUND DATE/TIME "2022-09-03 17:40:00" with offsets 279 297 and value 2022-09-03
FOUND DATE/TIME "2022-10-19 13:55:00" with offsets 315 333 and value 2022-10-19
FOUND DATE/TIME "2022-11-28 09:30:00" with offsets 351 369 and value 2022-11-28
FOUND DATE/TIME "2022-12-12 18:15:00" with offsets 387 405 and value 2022-12-12
FOUND DATE/TIME "2023-01-05 14:25:00" with offsets 423 441 and value 2023-01-05
FOUND DATE/TIME "2023-02-19 10:40:00" with offsets 459 477 and value 2023-02-19
windowScore[1] has size (productSizes[pos]) 5
windowScore[2] has size (productSizes[pos]) 8
windowScore[3] has size (productSizes[pos]) 3
windowScore[4] has size (productSizes[pos]) 7
windowScore[5] has size (productSizes[pos]) 9
windowScore[6] has size (productSizes[pos]) 4
windowScore[7] has size (productSizes[pos]) 6
windowScore[8] has size (productSizes[pos]) 2
windowScore[9] has size (productSizes[pos]) 1
windowScore[10] has size (productSizes[pos]) 12
windowScore[11] has size (productSizes[pos]) 17
windowScore[12] has size (productSizes[pos]) 13
windowScore[13] has size (productSizes[pos]) 11
Found 15 trees with total weight 300
Found 5 trees with total weight 150
Found 20 trees with total weight 400
Found 8 trees with total weight 160
Found 12 trees with total weight 240
Found 3 trees with total weight 90
Found 18 trees with total weight 360
Found 7 trees with total weight 140
Found 14 trees with total weight 280
Found 6 trees with total weight 180
Found 16 trees with total weight 320
Found 9 trees with total weight 270
Found 11 trees with total weight 220
scoring word 5 / 6, productSizes =  7, tagNum = 8...
scoring word 9 / 10, productSizes =  11, tagNum = 12...
scoring word 13 / 14, productSizes =  15, tagNum = 16...
scoring word 17 / 18, productSizes =  19, tagNum = 20...
scoring word 21 / 22, productSizes =  23, tagNum = 24...
scoring word 25 / 26, productSizes =  27, tagNum = 28...
scoring word 29 / 30, productSizes =  31, tagNum = 32...
scoring word 33 / 34, productSizes =  35, tagNum = 36...
scoring word 37 / 38, productSizes =  39, tagNum = 40...
scoring word 41 / 42, productSizes =  43, tagNum = 44...
scoring word 45 / 46, productSizes =  47, tagNum = 48...
scoring word 49 / 50, productSizes =  51, tagNum = 52...
scoring word 53 / 54, productSizes =  55, tagNum = 56...
rightScan from: orange
rightScan from: banana
rightScan from: kiwi
rightScan from: mango
rightScan from: pineapple
rightScan from: strawberry
rightScan from: watermelon
rightScan from: grape
rightScan from: peach
rightScan from: cherry
rightScan from: lemon
rightScan from: pear
rightScan from: blueberry
SUTime currently does not support Chinese. Ignore property ner.useSUTime.
SUTime currently does not support Chinese. Ignore property ner.useSUTime.
SUTime currently does not support Chinese. Ignore property ner.useSUTime.
SUTime currently does not support Chinese. Ignore property ner.useSUTime.
SUTime currently does not support Chinese. Ignore property ner.useSUTime.
SUTime currently does not support Chinese. Ignore property ner.useSUTime.
SUTime currently does not support Chinese. Ignore property ner.useSUTime.
SUTime currently does not support Chinese. Ignore property ner.useSUTime.
SUTime currently does not support Chinese. Ignore property ner.useSUTime.
SUTime currently does not support Chinese. Ignore property ner.useSUTime.
SUTime currently does not support Chinese. Ignore property ner.useSUTime.
SUTime currently does not support Chinese. Ignore property ner.useSUTime.
SUTime currently does not support Chinese. Ignore property ner.useSUTime.
SUTime currently does not support Chinese. Ignore property ner.useSUTime.
SUTime currently does not support Chinese. Ignore property ner.useSUTime.
There are 3 values at position 1: [a, b, c]
There are 2 values at position 2: [true, false]
There are 4 values at position 3: [apple, banana, orange, mango]
There are 6 values at position 4: [red, green, blue, yellow, purple, pink]
There are 1 values at position 5: [hello]
There are 0 values at position 6: []
There are 3 values at position 7: [x, y, z]
There are 2 values at position 8: [Monday, Tuesday]
There are 4 values at position 9: [John, Mary, Robert, Michael]
There are 3 values at position 10: [cat, dog, bird]
There are 1 values at position 11: [1]
There are 2 values at position 12: [A, B]
There are 5 values at position 13: [10, 20, 30, 40, 50]
Quantifiable: error! tag is invalid
Quantifiable: error! tag is null
Quantifiable: error! tag is empty
Quantifiable: error! tag is unrecognized
Quantifiable: error! tag is expired
Quantifiable: error! tag is duplicated
Quantifiable: error! tag is corrupted
Quantifiable: error! tag is not found
Quantifiable: error! tag is blocked
Quantifiable: error! tag is unauthorized
Quantifiable: error! tag is deprecated
Quantifiable: error! tag is out of range
Quantifiable: error! tag is misconfigured
Doing bestSequence length 15; leftWin 8; rightWin 7; padLength 4
Doing bestSequence length 20; leftWin 10; rightWin 10; padLength 3
Doing bestSequence length 12; leftWin 6; rightWin 6; padLength 1
Doing bestSequence length 18; leftWin 9; rightWin 9; padLength 5
Doing bestSequence length 8; leftWin 4; rightWin 4; padLength 2
Doing bestSequence length 17; leftWin 9; rightWin 8; padLength 3
Doing bestSequence length 13; leftWin 7; rightWin 6; padLength 1
Doing bestSequence length 22; leftWin 11; rightWin 11; padLength 4
Doing bestSequence length 9; leftWin 5; rightWin 4; padLength 2
Doing bestSequence length 16; leftWin 8; rightWin 8; padLength 3
Doing bestSequence length 14; leftWin 7; rightWin 7; padLength 1
Doing bestSequence length 25; leftWin 12; rightWin 13; padLength 5
Doing bestSequence length 11; leftWin 6; rightWin 5; padLength 2
Here are the 20 MISC items with counts:
Here are the 5 MISC items with counts:
Here are the 15 MISC items with counts:
Here are the 8 MISC items with counts:
Here are the 3 MISC items with counts:
Here are the 12 MISC items with counts:
Here are the 6 MISC items with counts:
Here are the 17 MISC items with counts:
Here are the 9 MISC items with counts:
Here are the 4 MISC items with counts:
Here are the 13 MISC items with counts:
Here are the 7 MISC items with counts:
Here are the 19 MISC items with counts:
fixupNerBeforeNormalization: wi is deleted
fixupNerBeforeNormalization: wi is merged
fixupNerBeforeNormalization: wi is processed
fixupNerBeforeNormalization: wi is not found
fixupNerBeforeNormalization: wi is invalid
fixupNerBeforeNormalization: wi is skipped
fixupNerBeforeNormalization: wi is not supported
fixupNerBeforeNormalization: wi is duplicated
fixupNerBeforeNormalization: wi is ignored
fixupNerBeforeNormalization: wi is rejected
fixupNerBeforeNormalization: wi is added
fixupNerBeforeNormalization: wi is modified
fixupNerBeforeNormalization: wi is flagged
File report.docx has 5 documents, 500 (non-blank line) tokens and 20 entities.
File presentation.pptx has 8 documents, 800 (non-blank line) tokens and 30 entities.
File image.jpg has 1 documents, 50 (non-blank line) tokens and 5 entities.
File code.py has 3 documents, 300 (non-blank line) tokens and 15 entities.
File readme.txt has 2 documents, 200 (non-blank line) tokens and 10 entities.
File notes.docx has 4 documents, 400 (non-blank line) tokens and 25 entities.
File spreadsheet.xlsx has 6 documents, 600 (non-blank line) tokens and 40 entities.
File presentation.pptx has 8 documents, 800 (non-blank line) tokens and 30 entities.
File image.jpg has 1 documents, 50 (non-blank line) tokens and 5 entities.
File code.py has 3 documents, 300 (non-blank line) tokens and 15 entities.
File readme.txt has 2 documents, 200 (non-blank line) tokens and 10 entities.
File notes.docx has 4 documents, 400 (non-blank line) tokens and 25 entities.
File spreadsheet.xlsx has 6 documents, 600 (non-blank line) tokens and 40 entities.
ROOT PROBABILITY: 0.5
ROOT PROBABILITY: 0.3
ROOT PROBABILITY: 0.9
ROOT PROBABILITY: 0.2
ROOT PROBABILITY: 0.4
ROOT PROBABILITY: 0.7
ROOT PROBABILITY: 0.6
ROOT PROBABILITY: 0.1
ROOT PROBABILITY: 0.95
ROOT PROBABILITY: 0.15
ROOT PROBABILITY: 0.25
ROOT PROBABILITY: 0.85
ROOT PROBABILITY: 0.35
Error on line 25: Missing semicolon
Error on line 8: Variable not declared
Error on line 17: Invalid syntax
Error on line 32: Undefined function
Error on line 12: Type mismatch
Error on line 5: Null pointer exception
Error on line 21: Division by zero
Error on line 14: Invalid argument
Error on line 29: Unreachable code
Error on line 3: Syntax error
Error on line 19: File not found
Error on line 7: Out of memory
Error on line 23: Infinite loop
Error on line 11: Invalid identifier
Incrementing trees read: 15
Incrementing trees read: 20
Incrementing trees read: 5
Incrementing trees read: 12
Incrementing trees read: 18
Incrementing trees read: 8
Incrementing trees read: 25
Incrementing trees read: 11
Incrementing trees read: 17
Incrementing trees read: 6
Incrementing trees read: 14
Incrementing trees read: 21
Incrementing trees read: 9
Beam empty -- no best sequence.
Beam empty -- no best sequence.
Beam empty -- no best sequence.
Beam empty -- no best sequence.
Beam empty -- no best sequence.
Beam empty -- no best sequence.
Beam empty -- no best sequence.
Beam empty -- no best sequence.
Beam empty -- no best sequence.
Beam empty -- no best sequence.
Beam empty -- no best sequence.
Beam empty -- no best sequence.
Beam empty -- no best sequence.
Beam empty -- no best sequence.
Beam empty -- no best sequence.
Post-split betas
Post-split betas
Post-split betas
Post-split betas
Post-split betas
Post-split betas
Post-split betas
Post-split betas
Post-split betas
Post-split betas
Post-split betas
Post-split betas
Post-split betas
Post-split betas
Post-split betas
Image (F1) = 0.86
File (F1) = 0.92
Network (F1) = 0.68
Security (F1) = 0.79
Database (F1) = 0.93
Memory (F1) = 0.81
Performance (F1) = 0.67
CPU (F1) = 0.75
Algorithm (F1) = 0.88
Storage (F1) = 0.79
Authentication (F1) = 0.91
Validation (F1) = 0.84
Configuration (F1) = 0.73
Pre-split betas
Pre-split betas
Pre-split betas
Pre-split betas
Pre-split betas
Pre-split betas
Pre-split betas
Pre-split betas
Pre-split betas
Pre-split betas
Pre-split betas
Pre-split betas
Pre-split betas
Pre-split betas
Pre-split betas
Error (P)  = 0.789
Success (P)  = 0.123
Info (P)  = 0.456
Result (P)  = 0.987
Message (P)  = 0.654
Fatal (P)  = 0.345
Debug (P)  = 0.123
Failure (P)  = 0.789
Notice (P)  = 0.456
Exception (P)  = 0.987
Log (P)  = 0.654
Debugging (P)  = 0.345
Critical (P)  = 0.123
2-fold accuracy: 0.76
3-fold accuracy: 0.89
4-fold accuracy: 0.71
5-fold accuracy: 0.95
6-fold accuracy: 0.83
7-fold accuracy: 0.78
8-fold accuracy: 0.92
9-fold accuracy: 0.87
10-fold accuracy: 0.79
11-fold accuracy: 0.65
12-fold accuracy: 0.91
13-fold accuracy: 0.75
14-fold accuracy: 0.82
normalizedPercentString: 0.76
normalizedPercentString: 0.28
normalizedPercentString: 0.92
normalizedPercentString: 0.63
normalizedPercentString: 0.81
normalizedPercentString: 0.34
normalizedPercentString: 0.57
normalizedPercentString: 0.19
normalizedPercentString: 0.71
normalizedPercentString: 0.39
normalizedPercentString: 0.85
normalizedPercentString: 0.52
normalizedPercentString: 0.09
Added 500 datums
Added 2000 datums
Added 1500 datums
Added 800 datums
Added 3000 datums
Added 1200 datums
Added 600 datums
Added 2500 datums
Added 1800 datums
Added 900 datums
Added 4000 datums
Added 2200 datums
Added 1100 datums
Filtered dev trees: 5
Filtered dev trees: 20
Filtered dev trees: 8
Filtered dev trees: 15
Filtered dev trees: 3
Filtered dev trees: 12
Filtered dev trees: 7
Filtered dev trees: 18
Filtered dev trees: 9
Filtered dev trees: 6
Filtered dev trees: 14
Filtered dev trees: 2
Filtered dev trees: 13
Read in 15 dev trees
Read in 8 dev trees
Read in 20 dev trees
Read in 12 dev trees
Read in 5 dev trees
Read in 19 dev trees
Read in 7 dev trees
Read in 13 dev trees
Read in 9 dev trees
Read in 16 dev trees
Read in 11 dev trees
Read in 6 dev trees
Read in 18 dev trees
Filtered training trees: 150
Filtered training trees: 200
Filtered training trees: 250
Filtered training trees: 300
Filtered training trees: 350
Filtered training trees: 400
Filtered training trees: 450
Filtered training trees: 500
Filtered training trees: 550
Filtered training trees: 600
Filtered training trees: 650
Filtered training trees: 700
Filtered training trees: 750
Read in 20 training trees
Read in 30 training trees
Read in 40 training trees
Read in 50 training trees
Read in 60 training trees
Read in 70 training trees
Read in 80 training trees
Read in 90 training trees
Read in 100 training trees
Read in 110 training trees
Read in 120 training trees
Read in 130 training trees
Read in 140 training trees
Finished epoch 2 batch 64; total training time 2000 ms
Finished epoch 3 batch 128; total training time 3000 ms
Finished epoch 4 batch 256; total training time 4000 ms
Finished epoch 5 batch 512; total training time 5000 ms
Finished epoch 6 batch 1024; total training time 6000 ms
Finished epoch 7 batch 2048; total training time 7000 ms
Finished epoch 8 batch 4096; total training time 8000 ms
Finished epoch 9 batch 8192; total training time 9000 ms
Finished epoch 10 batch 16384; total training time 10000 ms
Finished epoch 11 batch 32768; total training time 11000 ms
Finished epoch 12 batch 65536; total training time 12000 ms
Finished epoch 13 batch 131072; total training time 13000 ms
Finished epoch 14 batch 262144; total training time 14000 ms
Found units: monsters
Found units: buildings
Found units: items
Found units: abilities
Found units: quests
Found units: resources
Found units: currencies
Found units: characters
Found units: weapons
Found units: armor
Found units: potions
Found units: spells
Found units: skills
Epoch 2 batch 64
Epoch 3 batch 128
Epoch 4 batch 256
Epoch 5 batch 512
Epoch 6 batch 1024
Epoch 7 batch 2048
Epoch 8 batch 4096
Epoch 9 batch 8192
Epoch 10 batch 16384
Epoch 11 batch 32768
Epoch 12 batch 65536
Epoch 13 batch 131072
Epoch 14 batch 262144
Usage: ParentAnnotationStats treebankPath
Usage: ParentAnnotationStats treebankPath
Usage: ParentAnnotationStats treebankPath
Usage: ParentAnnotationStats treebankPath
Usage: ParentAnnotationStats treebankPath
Usage: ParentAnnotationStats treebankPath
Usage: ParentAnnotationStats treebankPath
Usage: ParentAnnotationStats treebankPath
Usage: ParentAnnotationStats treebankPath
Usage: ParentAnnotationStats treebankPath
Usage: ParentAnnotationStats treebankPath
Usage: ParentAnnotationStats treebankPath
Usage: ParentAnnotationStats treebankPath
Usage: ParentAnnotationStats treebankPath
Usage: ParentAnnotationStats treebankPath
KL Divergence: 0.456
KL Divergence: 0.789
KL Divergence: 0.321
KL Divergence: 0.654
KL Divergence: 0.987
KL Divergence: 0.246
KL Divergence: 0.579
KL Divergence: 0.912
KL Divergence: 0.135
KL Divergence: 0.468
KL Divergence: 0.791
KL Divergence: 0.523
KL Divergence: 0.856
Resetting adagrad weights to 0.005
Resetting adagrad weights to 0.02
Resetting adagrad weights to 0.015
Resetting adagrad weights to 0.03
Resetting adagrad weights to 0.025
Resetting adagrad weights to 0.035
Resetting adagrad weights to 0.04
Resetting adagrad weights to 0.045
Resetting adagrad weights to 0.055
Resetting adagrad weights to 0.05
Resetting adagrad weights to 0.06
Resetting adagrad weights to 0.065
Resetting adagrad weights to 0.07
Starting epoch 2
Starting epoch 3
Starting epoch 4
Starting epoch 5
Starting epoch 6
Starting epoch 7
Starting epoch 8
Starting epoch 9
Starting epoch 10
Starting epoch 11
Starting epoch 12
Starting epoch 13
Starting epoch 14
Processing will end when EOF is reached.
Processing will end when EOF is reached.
Processing will end when EOF is reached.
Processing will end when EOF is reached.
Processing will end when EOF is reached.
Processing will end when EOF is reached.
Processing will end when EOF is reached.
Processing will end when EOF is reached.
Processing will end when EOF is reached.
Processing will end when EOF is reached.
Processing will end when EOF is reached.
Processing will end when EOF is reached.
Processing will end when EOF is reached.
Processing will end when EOF is reached.
Processing will end when EOF is reached.
Please enter one sentence per line.
Please enter one sentence per line.
Please enter one sentence per line.
Please enter one sentence per line.
Please enter one sentence per line.
Please enter one sentence per line.
Please enter one sentence per line.
Please enter one sentence per line.
Please enter one sentence per line.
Please enter one sentence per line.
Please enter one sentence per line.
Please enter one sentence per line.
Please enter one sentence per line.
Please enter one sentence per line.
Please enter one sentence per line.
public static String[] sisterSplit =
public static String[] sisterSplit =
public static String[] sisterSplit =
public static String[] sisterSplit =
public static String[] sisterSplit =
public static String[] sisterSplit =
public static String[] sisterSplit =
public static String[] sisterSplit =
public static String[] sisterSplit =
public static String[] sisterSplit =
public static String[] sisterSplit =
public static String[] sisterSplit =
public static String[] sisterSplit =
public static String[] sisterSplit =
public static String[] sisterSplit =
Reading in text from stdin.
Reading in text from stdin.
Reading in text from stdin.
Reading in text from stdin.
Reading in text from stdin.
Reading in text from stdin.
Reading in text from stdin.
Reading in text from stdin.
Reading in text from stdin.
Reading in text from stdin.
Reading in text from stdin.
Reading in text from stdin.
Reading in text from stdin.
Reading in text from stdin.
Reading in text from stdin.
// Automatically generated by SisterAnnotationStats -- preferably don't edit
// Automatically generated by SisterAnnotationStats -- preferably don't edit
// Automatically generated by SisterAnnotationStats -- preferably don't edit
// Automatically generated by SisterAnnotationStats -- preferably don't edit
// Automatically generated by SisterAnnotationStats -- preferably don't edit
// Automatically generated by SisterAnnotationStats -- preferably don't edit
// Automatically generated by SisterAnnotationStats -- preferably don't edit
// Automatically generated by SisterAnnotationStats -- preferably don't edit
// Automatically generated by SisterAnnotationStats -- preferably don't edit
// Automatically generated by SisterAnnotationStats -- preferably don't edit
// Automatically generated by SisterAnnotationStats -- preferably don't edit
// Automatically generated by SisterAnnotationStats -- preferably don't edit
// Automatically generated by SisterAnnotationStats -- preferably don't edit
// Automatically generated by SisterAnnotationStats -- preferably don't edit
// Automatically generated by SisterAnnotationStats -- preferably don't edit
-filterUnknown: remove unknown trees from the input.  Only applies to TREES input, in which case the trees must be binarized with sentiment labels
-filterUnknown: remove unknown trees from the input.  Only applies to TREES input, in which case the trees must be binarized with sentiment labels
-filterUnknown: remove unknown trees from the input.  Only applies to TREES input, in which case the trees must be binarized with sentiment labels
-filterUnknown: remove unknown trees from the input.  Only applies to TREES input, in which case the trees must be binarized with sentiment labels
-filterUnknown: remove unknown trees from the input.  Only applies to TREES input, in which case the trees must be binarized with sentiment labels
-filterUnknown: remove unknown trees from the input.  Only applies to TREES input, in which case the trees must be binarized with sentiment labels
-filterUnknown: remove unknown trees from the input.  Only applies to TREES input, in which case the trees must be binarized with sentiment labels
-filterUnknown: remove unknown trees from the input.  Only applies to TREES input, in which case the trees must be binarized with sentiment labels
-filterUnknown: remove unknown trees from the input.  Only applies to TREES input, in which case the trees must be binarized with sentiment labels
-filterUnknown: remove unknown trees from the input.  Only applies to TREES input, in which case the trees must be binarized with sentiment labels
-filterUnknown: remove unknown trees from the input.  Only applies to TREES input, in which case the trees must be binarized with sentiment labels
-filterUnknown: remove unknown trees from the input.  Only applies to TREES input, in which case the trees must be binarized with sentiment labels
-filterUnknown: remove unknown trees from the input.  Only applies to TREES input, in which case the trees must be binarized with sentiment labels
-filterUnknown: remove unknown trees from the input.  Only applies to TREES input, in which case the trees must be binarized with sentiment labels
-filterUnknown: remove unknown trees from the input.  Only applies to TREES input, in which case the trees must be binarized with sentiment labels
Sentence skipped: out of memory or error calling TreePrint.
Sentence skipped: out of memory or error calling TreePrint.
Sentence skipped: out of memory or error calling TreePrint.
Sentence skipped: out of memory or error calling TreePrint.
Sentence skipped: out of memory or error calling TreePrint.
Sentence skipped: out of memory or error calling TreePrint.
Sentence skipped: out of memory or error calling TreePrint.
Sentence skipped: out of memory or error calling TreePrint.
Sentence skipped: out of memory or error calling TreePrint.
Sentence skipped: out of memory or error calling TreePrint.
Sentence skipped: out of memory or error calling TreePrint.
Sentence skipped: out of memory or error calling TreePrint.
Sentence skipped: out of memory or error calling TreePrint.
Sentence skipped: out of memory or error calling TreePrint.
Sentence skipped: out of memory or error calling TreePrint.
-output <format>: Which format to output, PENNTREES, VECTORS, PROBABILITIES, or ROOT.  Multiple formats can be specified as a comma separated list.
-output <format>: Which format to output, PENNTREES, VECTORS, PROBABILITIES, or ROOT.  Multiple formats can be specified as a comma separated list.
-output <format>: Which format to output, PENNTREES, VECTORS, PROBABILITIES, or ROOT.  Multiple formats can be specified as a comma separated list.
-output <format>: Which format to output, PENNTREES, VECTORS, PROBABILITIES, or ROOT.  Multiple formats can be specified as a comma separated list.
-output <format>: Which format to output, PENNTREES, VECTORS, PROBABILITIES, or ROOT.  Multiple formats can be specified as a comma separated list.
-output <format>: Which format to output, PENNTREES, VECTORS, PROBABILITIES, or ROOT.  Multiple formats can be specified as a comma separated list.
-output <format>: Which format to output, PENNTREES, VECTORS, PROBABILITIES, or ROOT.  Multiple formats can be specified as a comma separated list.
-output <format>: Which format to output, PENNTREES, VECTORS, PROBABILITIES, or ROOT.  Multiple formats can be specified as a comma separated list.
-output <format>: Which format to output, PENNTREES, VECTORS, PROBABILITIES, or ROOT.  Multiple formats can be specified as a comma separated list.
-output <format>: Which format to output, PENNTREES, VECTORS, PROBABILITIES, or ROOT.  Multiple formats can be specified as a comma separated list.
-output <format>: Which format to output, PENNTREES, VECTORS, PROBABILITIES, or ROOT.  Multiple formats can be specified as a comma separated list.
-output <format>: Which format to output, PENNTREES, VECTORS, PROBABILITIES, or ROOT.  Multiple formats can be specified as a comma separated list.
-output <format>: Which format to output, PENNTREES, VECTORS, PROBABILITIES, or ROOT.  Multiple formats can be specified as a comma separated list.
-output <format>: Which format to output, PENNTREES, VECTORS, PROBABILITIES, or ROOT.  Multiple formats can be specified as a comma separated list.
-output <format>: Which format to output, PENNTREES, VECTORS, PROBABILITIES, or ROOT.  Multiple formats can be specified as a comma separated list.
TreePrint.printTree skipped: out of memory (or other error)
TreePrint.printTree skipped: out of memory (or other error)
TreePrint.printTree skipped: out of memory (or other error)
TreePrint.printTree skipped: out of memory (or other error)
TreePrint.printTree skipped: out of memory (or other error)
TreePrint.printTree skipped: out of memory (or other error)
TreePrint.printTree skipped: out of memory (or other error)
TreePrint.printTree skipped: out of memory (or other error)
TreePrint.printTree skipped: out of memory (or other error)
TreePrint.printTree skipped: out of memory (or other error)
TreePrint.printTree skipped: out of memory (or other error)
TreePrint.printTree skipped: out of memory (or other error)
TreePrint.printTree skipped: out of memory (or other error)
TreePrint.printTree skipped: out of memory (or other error)
TreePrint.printTree skipped: out of memory (or other error)
-input <format>: Which format to input, TEXT or TREES.  Will not process stdin as trees.  If trees are not already binarized, they will be binarized with -tlppClass's headfinder, which means they must have labels in that treebank's tagset.
-input <format>: Which format to input, TEXT or TREES.  Will not process stdin as trees.  If trees are not already binarized, they will be binarized with -tlppClass's headfinder, which means they must have labels in that treebank's tagset.
-input <format>: Which format to input, TEXT or TREES.  Will not process stdin as trees.  If trees are not already binarized, they will be binarized with -tlppClass's headfinder, which means they must have labels in that treebank's tagset.
-input <format>: Which format to input, TEXT or TREES.  Will not process stdin as trees.  If trees are not already binarized, they will be binarized with -tlppClass's headfinder, which means they must have labels in that treebank's tagset.
-input <format>: Which format to input, TEXT or TREES.  Will not process stdin as trees.  If trees are not already binarized, they will be binarized with -tlppClass's headfinder, which means they must have labels in that treebank's tagset.
-input <format>: Which format to input, TEXT or TREES.  Will not process stdin as trees.  If trees are not already binarized, they will be binarized with -tlppClass's headfinder, which means they must have labels in that treebank's tagset.
-input <format>: Which format to input, TEXT or TREES.  Will not process stdin as trees.  If trees are not already binarized, they will be binarized with -tlppClass's headfinder, which means they must have labels in that treebank's tagset.
-input <format>: Which format to input, TEXT or TREES.  Will not process stdin as trees.  If trees are not already binarized, they will be binarized with -tlppClass's headfinder, which means they must have labels in that treebank's tagset.
-input <format>: Which format to input, TEXT or TREES.  Will not process stdin as trees.  If trees are not already binarized, they will be binarized with -tlppClass's headfinder, which means they must have labels in that treebank's tagset.
-input <format>: Which format to input, TEXT or TREES.  Will not process stdin as trees.  If trees are not already binarized, they will be binarized with -tlppClass's headfinder, which means they must have labels in that treebank's tagset.
-input <format>: Which format to input, TEXT or TREES.  Will not process stdin as trees.  If trees are not already binarized, they will be binarized with -tlppClass's headfinder, which means they must have labels in that treebank's tagset.
-input <format>: Which format to input, TEXT or TREES.  Will not process stdin as trees.  If trees are not already binarized, they will be binarized with -tlppClass's headfinder, which means they must have labels in that treebank's tagset.
-input <format>: Which format to input, TEXT or TREES.  Will not process stdin as trees.  If trees are not already binarized, they will be binarized with -tlppClass's headfinder, which means they must have labels in that treebank's tagset.
-input <format>: Which format to input, TEXT or TREES.  Will not process stdin as trees.  If trees are not already binarized, they will be binarized with -tlppClass's headfinder, which means they must have labels in that treebank's tagset.
-input <format>: Which format to input, TEXT or TREES.  Will not process stdin as trees.  If trees are not already binarized, they will be binarized with -tlppClass's headfinder, which means they must have labels in that treebank's tagset.
normalizingTime: 3.2
normalizingTime: 1.8
normalizingTime: 4.6
normalizingTime: 3.9
normalizingTime: 2.1
normalizingTime: 5.0
normalizingTime: 1.6
normalizingTime: 3.7
normalizingTime: 2.9
normalizingTime: 4.3
normalizingTime: 3.4
normalizingTime: 1.4
normalizingTime: 2.7
-stdin: Process stdin instead of a file
-stdin: Process stdin instead of a file
-stdin: Process stdin instead of a file
-stdin: Process stdin instead of a file
-stdin: Process stdin instead of a file
-stdin: Process stdin instead of a file
-stdin: Process stdin instead of a file
-stdin: Process stdin instead of a file
-stdin: Process stdin instead of a file
-stdin: Process stdin instead of a file
-stdin: Process stdin instead of a file
-stdin: Process stdin instead of a file
-stdin: Process stdin instead of a file
-stdin: Process stdin instead of a file
-stdin: Process stdin instead of a file
-fileList <file>,<file>,...: Comma separated list of files to process.  Output goes to file.out
-fileList <file>,<file>,...: Comma separated list of files to process.  Output goes to file.out
-fileList <file>,<file>,...: Comma separated list of files to process.  Output goes to file.out
-fileList <file>,<file>,...: Comma separated list of files to process.  Output goes to file.out
-fileList <file>,<file>,...: Comma separated list of files to process.  Output goes to file.out
-fileList <file>,<file>,...: Comma separated list of files to process.  Output goes to file.out
-fileList <file>,<file>,...: Comma separated list of files to process.  Output goes to file.out
-fileList <file>,<file>,...: Comma separated list of files to process.  Output goes to file.out
-fileList <file>,<file>,...: Comma separated list of files to process.  Output goes to file.out
-fileList <file>,<file>,...: Comma separated list of files to process.  Output goes to file.out
-fileList <file>,<file>,...: Comma separated list of files to process.  Output goes to file.out
-fileList <file>,<file>,...: Comma separated list of files to process.  Output goes to file.out
-fileList <file>,<file>,...: Comma separated list of files to process.  Output goes to file.out
-fileList <file>,<file>,...: Comma separated list of files to process.  Output goes to file.out
-fileList <file>,<file>,...: Comma separated list of files to process.  Output goes to file.out
-file <filename>: Which file to process
-file <filename>: Which file to process
-file <filename>: Which file to process
-file <filename>: Which file to process
-file <filename>: Which file to process
-file <filename>: Which file to process
-file <filename>: Which file to process
-file <filename>: Which file to process
-file <filename>: Which file to process
-file <filename>: Which file to process
-file <filename>: Which file to process
-file <filename>: Which file to process
-file <filename>: Which file to process
-file <filename>: Which file to process
-file <filename>: Which file to process
-parserModel <model>: Which parser to use
-parserModel <model>: Which parser to use
-parserModel <model>: Which parser to use
-parserModel <model>: Which parser to use
-parserModel <model>: Which parser to use
-parserModel <model>: Which parser to use
-parserModel <model>: Which parser to use
-parserModel <model>: Which parser to use
-parserModel <model>: Which parser to use
-parserModel <model>: Which parser to use
-parserModel <model>: Which parser to use
-parserModel <model>: Which parser to use
-parserModel <model>: Which parser to use
-parserModel <model>: Which parser to use
-parserModel <model>: Which parser to use
5 were skipped as length 0 or greater than 15
3 were skipped as length 0 or greater than 12
7 were skipped as length 0 or greater than 18
2 were skipped as length 0 or greater than 10
4 were skipped as length 0 or greater than 14
9 were skipped as length 0 or greater than 19
1 were skipped as length 0 or greater than 8
6 were skipped as length 0 or greater than 16
8 were skipped as length 0 or greater than 17
12 were skipped as length 0 or greater than 25
15 were skipped as length 0 or greater than 30
20 were skipped as length 0 or greater than 40
13 were skipped as length 0 or greater than 27
-sentimentModel <model>: Which model to use
-sentimentModel <model>: Which model to use
-sentimentModel <model>: Which model to use
-sentimentModel <model>: Which model to use
-sentimentModel <model>: Which model to use
-sentimentModel <model>: Which model to use
-sentimentModel <model>: Which model to use
-sentimentModel <model>: Which model to use
-sentimentModel <model>: Which model to use
-sentimentModel <model>: Which model to use
-sentimentModel <model>: Which model to use
-sentimentModel <model>: Which model to use
-sentimentModel <model>: Which model to use
-sentimentModel <model>: Which model to use
-sentimentModel <model>: Which model to use
Known command line arguments:
Known command line arguments:
Known command line arguments:
Known command line arguments:
Known command line arguments:
Known command line arguments:
Known command line arguments:
Known command line arguments:
Known command line arguments:
Known command line arguments:
Known command line arguments:
Known command line arguments:
Known command line arguments:
Known command line arguments:
Known command line arguments:
"    20 were skipped because of insufficient memory.
"    30 were skipped because of insufficient memory.
"    40 were skipped because of insufficient memory.
"    50 were skipped because of insufficient memory.
"    60 were skipped because of insufficient memory.
"    70 were skipped because of insufficient memory.
"    80 were skipped because of insufficient memory.
"    90 were skipped because of insufficient memory.
"    100 were skipped because of insufficient memory.
"    110 were skipped because of insufficient memory.
"    120 were skipped because of insufficient memory.
"    130 were skipped because of insufficient memory.
"    140 were skipped because of insufficient memory.
#2: Changing normalized NER from LOCATION to DATE at index 1
#2: Changing normalized NER from MONEY to PERCENT at index 2
#2: Changing normalized NER from PERSON to ORGANIZATION at index 3
#2: Changing normalized NER from LOCATION to DATE at index 4
#2: Changing normalized NER from MONEY to PERCENT at index 5
#2: Changing normalized NER from PERSON to ORGANIZATION at index 6
#2: Changing normalized NER from LOCATION to DATE at index 7
#2: Changing normalized NER from MONEY to PERCENT at index 8
#2: Changing normalized NER from PERSON to ORGANIZATION at index 9
#2: Changing normalized NER from LOCATION to DATE at index 10
#2: Changing normalized NER from MONEY to PERCENT at index 11
#2: Changing normalized NER from PERSON to ORGANIZATION at index 12
#2: Changing normalized NER from LOCATION to DATE at index 13
7 were not parsable with non-zero probability.
1 were not parsable with non-zero probability.
4 were not parsable with non-zero probability.
2 were not parsable with non-zero probability.
6 were not parsable with non-zero probability.
9 were not parsable with non-zero probability.
5 were not parsable with non-zero probability.
8 were not parsable with non-zero probability.
12 were not parsable with non-zero probability.
10 were not parsable with non-zero probability.
14 were not parsable with non-zero probability.
13 were not parsable with non-zero probability.
11 were not parsable with non-zero probability.
Index 1 is beyond the length of the parameters; total parameter space was 15
Index 2 is beyond the length of the parameters; total parameter space was 15
Index 3 is beyond the length of the parameters; total parameter space was 15
Index 4 is beyond the length of the parameters; total parameter space was 15
Index 5 is beyond the length of the parameters; total parameter space was 15
Index 6 is beyond the length of the parameters; total parameter space was 15
Index 7 is beyond the length of the parameters; total parameter space was 15
Index 8 is beyond the length of the parameters; total parameter space was 15
Index 9 is beyond the length of the parameters; total parameter space was 15
Index 10 is beyond the length of the parameters; total parameter space was 15
Index 11 is beyond the length of the parameters; total parameter space was 15
Index 12 is beyond the length of the parameters; total parameter space was 15
Index 13 is beyond the length of the parameters; total parameter space was 15
15 8 6 sentences were not parsed:
7 2 4 sentences were not parsed:
12 6 9 sentences were not parsed:
8 4 2 sentences were not parsed:
9 3 7 sentences were not parsed:
11 7 5 sentences were not parsed:
6 2 3 sentences were not parsed:
13 9 6 sentences were not parsed:
5 3 1 sentences were not parsed:
14 8 4 sentences were not parsed:
7 5 2 sentences were not parsed:
10 6 3 sentences were not parsed:
9 4 8 sentences were not parsed:
12 7 6 sentences were not parsed:
Index 1 is element -1 of wordVector "banana"
Index 2 is element 2 of wordVector "orange"
Index 3 is element 1 of wordVector "grape"
Index 4 is element -3 of wordVector "pineapple"
Index 5 is element 4 of wordVector "kiwi"
Index 6 is element -2 of wordVector "mango"
Index 7 is element 3 of wordVector "pear"
Index 8 is element 0 of wordVector "cherry"
Index 9 is element -1 of wordVector "strawberry"
Index 10 is element 2 of wordVector "melon"
Index 11 is element 1 of wordVector "peach"
Index 12 is element -3 of wordVector "plum"
Index 13 is element 4 of wordVector "lemon"
5 sentences were parsed by fallback to PCFG.
8 sentences were parsed by fallback to PCFG.
3 sentences were parsed by fallback to PCFG.
12 sentences were parsed by fallback to PCFG.
7 sentences were parsed by fallback to PCFG.
9 sentences were parsed by fallback to PCFG.
4 sentences were parsed by fallback to PCFG.
6 sentences were parsed by fallback to PCFG.
11 sentences were parsed by fallback to PCFG.
13 sentences were parsed by fallback to PCFG.
2 sentences were parsed by fallback to PCFG.
1 sentences were parsed by fallback to PCFG.
15 sentences were parsed by fallback to PCFG.
Average: 5.7% slower.
Average: 15.9% faster.
Average: 8.3% slower.
Average: 12.6% faster.
Average: 3.1% slower.
Average: 21.8% faster.
Average: 6.4% slower.
Average: 9.7% faster.
Average: 13.5% slower.
Average: 17.9% faster.
Average: 2.5% slower.
Average: 14.1% faster.
Average: 7.8% slower.
Average: 19.6% faster.
Found potential two digit year: 07
Found potential two digit year: 42
Found potential two digit year: 99
Found potential two digit year: 18
Found potential two digit year: 33
Found potential two digit year: 55
Found potential two digit year: 79
Found potential two digit year: 06
Found potential two digit year: 10
Found potential two digit year: 26
Found potential two digit year: 13
Found potential two digit year: 98
Found potential two digit year: 04
Parsed 240 words in 20 sentences (24 wds/sec; 2 sents/sec).
Parsed 360 words in 30 sentences (36 wds/sec; 3 sents/sec).
Parsed 480 words in 40 sentences (48 wds/sec; 4 sents/sec).
Parsed 600 words in 50 sentences (60 wds/sec; 5 sents/sec).
Parsed 720 words in 60 sentences (72 wds/sec; 6 sents/sec).
Parsed 840 words in 70 sentences (84 wds/sec; 7 sents/sec).
Parsed 960 words in 80 sentences (96 wds/sec; 8 sents/sec).
Parsed 1080 words in 90 sentences (108 wds/sec; 9 sents/sec).
Parsed 1200 words in 100 sentences (120 wds/sec; 10 sents/sec).
Parsed 1320 words in 110 sentences (132 wds/sec; 11 sents/sec).
Parsed 1440 words in 120 sentences (144 wds/sec; 12 sents/sec).
Parsed 1560 words in 130 sentences (156 wds/sec; 13 sents/sec).
Parsed 1680 words in 140 sentences (168 wds/sec; 14 sents/sec).
Parsed 1800 words in 150 sentences (180 wds/sec; 15 sents/sec).
CoreMap @ 50 keys: 22 secs/2million gets (45.6% slower)
CoreMap @ 75 keys: 35 secs/2million gets (38.2% faster)
CoreMap @ 80 keys: 37 secs/2million gets (27.9% slower)
CoreMap @ 30 keys: 15 secs/2million gets (56.8% faster)
CoreMap @ 90 keys: 39 secs/2million gets (14.2% faster)
CoreMap @ 120 keys: 52 secs/2million gets (33.7% slower)
CoreMap @ 70 keys: 30 secs/2million gets (46.1% slower)
CoreMap @ 110 keys: 47 secs/2million gets (22.5% faster)
CoreMap @ 60 keys: 26 secs/2million gets (17.3% faster)
CoreMap @ 40 keys: 18 secs/2million gets (30.9% slower)
CoreMap @ 95 keys: 41 secs/2million gets (12.6% slower)
CoreMap @ 85 keys: 37 secs/2million gets (25.8% faster)
CoreMap @ 65 keys: 28 secs/2million gets (40.4% slower)
Parsed file: data.txt [512 sentences].
Parsed file: report.txt [89 sentences].
Parsed file: log.txt [324 sentences].
Parsed file: script.txt [78 sentences].
Parsed file: notes.txt [421 sentences].
Parsed file: error.txt [145 sentences].
Parsed file: output.txt [267 sentences].
Parsed file: input.txt [517 sentences].
Parsed file: test.txt [101 sentences].
Parsed file: log2.txt [297 sentences].
Parsed file: result.txt [207 sentences].
Parsed file: document.txt [432 sentences].
Parsed file: record.txt [186 sentences].
HashMap @ 500 keys: 0.2761 secs/2million gets
HashMap @ 200 keys: 0.7123 secs/2million gets
HashMap @ 100 keys: 1.5698 secs/2million gets
HashMap @ 50 keys: 2.9486 secs/2million gets
HashMap @ 20 keys: 7.2164 secs/2million gets
HashMap @ 10 keys: 14.4253 secs/2million gets
HashMap @ 5 keys: 29.1044 secs/2million gets
HashMap @ 2 keys: 74.6107 secs/2million gets
HashMap @ 1 keys: 151.5153 secs/2million gets
HashMap @ 5000 keys: 0.0346 secs/2million gets
HashMap @ 10000 keys: 0.0171 secs/2million gets
HashMap @ 20000 keys: 0.0086 secs/2million gets
HashMap @ 50000 keys: 0.0034 secs/2million gets
Quantifiable: Collapsing data
Quantifiable: Collapsing records
Quantifiable: Collapsing files
Quantifiable: Collapsing categories
Quantifiable: Collapsing items
Quantifiable: Collapsing resources
Quantifiable: Collapsing elements
Quantifiable: Collapsing parameters
Quantifiable: Collapsing values
Quantifiable: Collapsing results
Quantifiable: Collapsing documents
Quantifiable: Collapsing entries
Quantifiable: Collapsing objects
Parsing file: data.csv
Parsing file: log.txt
Parsing file: image.jpg
Parsing file: config.ini
Parsing file: document.docx
Parsing file: music.mp3
Parsing file: video.mp4
Parsing file: script.js
Parsing file: style.css
Parsing file: index.html
Parsing file: database.sql
Parsing file: presentation.pptx
Parsing file: code.java
parseFiles: Tokenizer factory is: WhitespaceTokenizerFactory
parseFiles: Tokenizer factory is: NGramTokenizerFactory
parseFiles: Tokenizer factory is: PunctuationTokenizerFactory
parseFiles: Tokenizer factory is: StopwordTokenizerFactory
parseFiles: Tokenizer factory is: RegexTokenizerFactory
parseFiles: Tokenizer factory is: SentenceTokenizerFactory
parseFiles: Tokenizer factory is: CustomTokenizerFactory
parseFiles: Tokenizer factory is: MaxScoreTokenizerFactory
parseFiles: Tokenizer factory is: SoundexTokenizerFactory
parseFiles: Tokenizer factory is: NLPTokenizerFactory
parseFiles: Tokenizer factory is: LuceneTokenizerFactory
parseFiles: Tokenizer factory is: PatternTokenizerFactory
parseFiles: Tokenizer factory is: DateTokenizerFactory
parseFiles: Tokenizer factory is: TokenizerFactory
Result2: failure
Result2: error
Result2: timeout
Result2: incomplete
Result2: invalid
Result2: skipped
Result2: passed
Result2: blocked
Result2: warning
Result2: aborted
Result2: crashed
Result2: pending
Result2: unavailable
SentimentCostAndGradient: warning: No CoreLabels in nodes: tree2
SentimentCostAndGradient: warning: No CoreLabels in nodes: tree3
SentimentCostAndGradient: warning: No CoreLabels in nodes: tree4
SentimentCostAndGradient: warning: No CoreLabels in nodes: tree5
SentimentCostAndGradient: warning: No CoreLabels in nodes: tree6
SentimentCostAndGradient: warning: No CoreLabels in nodes: tree7
SentimentCostAndGradient: warning: No CoreLabels in nodes: tree8
SentimentCostAndGradient: warning: No CoreLabels in nodes: tree9
SentimentCostAndGradient: warning: No CoreLabels in nodes: tree10
SentimentCostAndGradient: warning: No CoreLabels in nodes: tree11
SentimentCostAndGradient: warning: No CoreLabels in nodes: tree12
SentimentCostAndGradient: warning: No CoreLabels in nodes: tree13
SentimentCostAndGradient: warning: No CoreLabels in nodes: tree14
Sentence final words are: [., ?, !]
Sentence final words are: [, ., !, ?]
Sentence final words are: [., ,, ;, :, ?, !]
Sentence final words are: [., ,, ;, :, !, ?]
Sentence final words are: [., ,, ...]
Sentence final words are: [., ,, ...]
Sentence final words are: [, ., ,, ...]
Sentence final words are: [., ,, :, !]
Usage: java edu.stanford.nlp.parser.lexparser.ParentAnnotationStats [-tags] treebankPath
Usage: java edu.stanford.nlp.parser.lexparser.ParentAnnotationStats [-tags] treebankPath
Usage: java edu.stanford.nlp.parser.lexparser.ParentAnnotationStats [-tags] treebankPath
Usage: java edu.stanford.nlp.parser.lexparser.ParentAnnotationStats [-tags] treebankPath
Usage: java edu.stanford.nlp.parser.lexparser.ParentAnnotationStats [-tags] treebankPath
Usage: java edu.stanford.nlp.parser.lexparser.ParentAnnotationStats [-tags] treebankPath
Usage: java edu.stanford.nlp.parser.lexparser.ParentAnnotationStats [-tags] treebankPath
Usage: java edu.stanford.nlp.parser.lexparser.ParentAnnotationStats [-tags] treebankPath
Usage: java edu.stanford.nlp.parser.lexparser.ParentAnnotationStats [-tags] treebankPath
Usage: java edu.stanford.nlp.parser.lexparser.ParentAnnotationStats [-tags] treebankPath
Usage: java edu.stanford.nlp.parser.lexparser.ParentAnnotationStats [-tags] treebankPath
Usage: java edu.stanford.nlp.parser.lexparser.ParentAnnotationStats [-tags] treebankPath
Usage: java edu.stanford.nlp.parser.lexparser.ParentAnnotationStats [-tags] treebankPath
Usage: java edu.stanford.nlp.parser.lexparser.ParentAnnotationStats [-tags] treebankPath
Usage: java edu.stanford.nlp.parser.lexparser.ParentAnnotationStats [-tags] treebankPath
Unknown default unit 20 for pressure
Unknown default unit 30 for speed
Unknown default unit 40 for volume
Unknown default unit 50 for time
Unknown default unit 60 for length
Unknown default unit 70 for mass
Unknown default unit 80 for energy
Unknown default unit 90 for power
Unknown default unit 100 for frequency
Unknown default unit 110 for angle
Unknown default unit 120 for voltage
Unknown default unit 130 for current
Unknown default unit 140 for resistance
Illegal tree: trainingTree2
Illegal tree: trainingTree3
Illegal tree: trainingTree4
Illegal tree: trainingTree5
Illegal tree: trainingTree6
Illegal tree: trainingTree7
Illegal tree: trainingTree8
Illegal tree: trainingTree9
Illegal tree: trainingTree10
Illegal tree: trainingTree11
Illegal tree: trainingTree12
Illegal tree: trainingTree13
Illegal tree: trainingTree14
Result: failure
Result: error
Result: incomplete
Result: timeout
Result: invalid
Result: skipped
Result: passed
Result: rejected
Result: pending
Result: canceled
Result: blocked
Result: terminated
Result: crashed
ENV.defaults["stage"] = 0
ENV.defaults["stage"] = 0
ENV.defaults["stage"] = 0
ENV.defaults["stage"] = 0
ENV.defaults["stage"] = 0
ENV.defaults["stage"] = 0
ENV.defaults["stage"] = 0
ENV.defaults["stage"] = 0
ENV.defaults["stage"] = 0
ENV.defaults["stage"] = 0
ENV.defaults["stage"] = 0
ENV.defaults["stage"] = 0
ENV.defaults["stage"] = 0
ENV.defaults["stage"] = 0
ENV.defaults["stage"] = 0
predictedPath not set. Exit.
predictedPath not set. Exit.
predictedPath not set. Exit.
predictedPath not set. Exit.
predictedPath not set. Exit.
predictedPath not set. Exit.
predictedPath not set. Exit.
predictedPath not set. Exit.
predictedPath not set. Exit.
predictedPath not set. Exit.
predictedPath not set. Exit.
predictedPath not set. Exit.
predictedPath not set. Exit.
predictedPath not set. Exit.
predictedPath not set. Exit.
$SymUnits = CreateRegex(Keys(SYM_UNIT_MAP))
$SymUnits = CreateRegex(Keys(SYM_UNIT_MAP))
$SymUnits = CreateRegex(Keys(SYM_UNIT_MAP))
$SymUnits = CreateRegex(Keys(SYM_UNIT_MAP))
$SymUnits = CreateRegex(Keys(SYM_UNIT_MAP))
$SymUnits = CreateRegex(Keys(SYM_UNIT_MAP))
$SymUnits = CreateRegex(Keys(SYM_UNIT_MAP))
$SymUnits = CreateRegex(Keys(SYM_UNIT_MAP))
$SymUnits = CreateRegex(Keys(SYM_UNIT_MAP))
$SymUnits = CreateRegex(Keys(SYM_UNIT_MAP))
$SymUnits = CreateRegex(Keys(SYM_UNIT_MAP))
$SymUnits = CreateRegex(Keys(SYM_UNIT_MAP))
$SymUnits = CreateRegex(Keys(SYM_UNIT_MAP))
$SymUnits = CreateRegex(Keys(SYM_UNIT_MAP))
$SymUnits = CreateRegex(Keys(SYM_UNIT_MAP))
goldPath not set. Exit.
goldPath not set. Exit.
goldPath not set. Exit.
goldPath not set. Exit.
goldPath not set. Exit.
goldPath not set. Exit.
goldPath not set. Exit.
goldPath not set. Exit.
goldPath not set. Exit.
goldPath not set. Exit.
goldPath not set. Exit.
goldPath not set. Exit.
goldPath not set. Exit.
goldPath not set. Exit.
goldPath not set. Exit.
SYM_UNIT_MAP = {
SYM_UNIT_MAP = {
SYM_UNIT_MAP = {
SYM_UNIT_MAP = {
SYM_UNIT_MAP = {
SYM_UNIT_MAP = {
SYM_UNIT_MAP = {
SYM_UNIT_MAP = {
SYM_UNIT_MAP = {
SYM_UNIT_MAP = {
SYM_UNIT_MAP = {
SYM_UNIT_MAP = {
SYM_UNIT_MAP = {
SYM_UNIT_MAP = {
SYM_UNIT_MAP = {
$SiSymUnits = CreateRegex(Keys(SI_SYM_UNIT_MAP))
$SiSymUnits = CreateRegex(Keys(SI_SYM_UNIT_MAP))
$SiSymUnits = CreateRegex(Keys(SI_SYM_UNIT_MAP))
$SiSymUnits = CreateRegex(Keys(SI_SYM_UNIT_MAP))
$SiSymUnits = CreateRegex(Keys(SI_SYM_UNIT_MAP))
$SiSymUnits = CreateRegex(Keys(SI_SYM_UNIT_MAP))
$SiSymUnits = CreateRegex(Keys(SI_SYM_UNIT_MAP))
$SiSymUnits = CreateRegex(Keys(SI_SYM_UNIT_MAP))
$SiSymUnits = CreateRegex(Keys(SI_SYM_UNIT_MAP))
$SiSymUnits = CreateRegex(Keys(SI_SYM_UNIT_MAP))
$SiSymUnits = CreateRegex(Keys(SI_SYM_UNIT_MAP))
$SiSymUnits = CreateRegex(Keys(SI_SYM_UNIT_MAP))
$SiSymUnits = CreateRegex(Keys(SI_SYM_UNIT_MAP))
$SiSymUnits = CreateRegex(Keys(SI_SYM_UNIT_MAP))
$SiSymUnits = CreateRegex(Keys(SI_SYM_UNIT_MAP))
public static HashSet splitters = new HashSet(Arrays.asList(
public static HashSet splitters = new HashSet(Arrays.asList(
public static HashSet splitters = new HashSet(Arrays.asList(
public static HashSet splitters = new HashSet(Arrays.asList(
public static HashSet splitters = new HashSet(Arrays.asList(
public static HashSet splitters = new HashSet(Arrays.asList(
public static HashSet splitters = new HashSet(Arrays.asList(
public static HashSet splitters = new HashSet(Arrays.asList(
public static HashSet splitters = new HashSet(Arrays.asList(
public static HashSet splitters = new HashSet(Arrays.asList(
public static HashSet splitters = new HashSet(Arrays.asList(
public static HashSet splitters = new HashSet(Arrays.asList(
public static HashSet splitters = new HashSet(Arrays.asList(
public static HashSet splitters = new HashSet(Arrays.asList(
public static HashSet splitters = new HashSet(Arrays.asList(
$SiUnits = CreateRegex(Keys(SI_UNIT_MAP))
$SiUnits = CreateRegex(Keys(SI_UNIT_MAP))
$SiUnits = CreateRegex(Keys(SI_UNIT_MAP))
$SiUnits = CreateRegex(Keys(SI_UNIT_MAP))
$SiUnits = CreateRegex(Keys(SI_UNIT_MAP))
$SiUnits = CreateRegex(Keys(SI_UNIT_MAP))
$SiUnits = CreateRegex(Keys(SI_UNIT_MAP))
$SiUnits = CreateRegex(Keys(SI_UNIT_MAP))
$SiUnits = CreateRegex(Keys(SI_UNIT_MAP))
$SiUnits = CreateRegex(Keys(SI_UNIT_MAP))
$SiUnits = CreateRegex(Keys(SI_UNIT_MAP))
$SiUnits = CreateRegex(Keys(SI_UNIT_MAP))
$SiUnits = CreateRegex(Keys(SI_UNIT_MAP))
$SiUnits = CreateRegex(Keys(SI_UNIT_MAP))
$SiUnits = CreateRegex(Keys(SI_UNIT_MAP))
Saw 23 trees with at least one unknown token.
Saw 7 trees with at least one unknown token.
Saw 16 trees with at least one unknown token.
Saw 3 trees with at least one unknown token.
Saw 9 trees with at least one unknown token.
Saw 12 trees with at least one unknown token.
Saw 5 trees with at least one unknown token.
Saw 18 trees with at least one unknown token.
Saw 2 trees with at least one unknown token.
Saw 14 trees with at least one unknown token.
Saw 8 trees with at least one unknown token.
Saw 21 trees with at least one unknown token.
Saw 6 trees with at least one unknown token.
// Automatically generated by ParentAnnotationStats -- preferably don't edit
// Automatically generated by ParentAnnotationStats -- preferably don't edit
// Automatically generated by ParentAnnotationStats -- preferably don't edit
// Automatically generated by ParentAnnotationStats -- preferably don't edit
// Automatically generated by ParentAnnotationStats -- preferably don't edit
// Automatically generated by ParentAnnotationStats -- preferably don't edit
// Automatically generated by ParentAnnotationStats -- preferably don't edit
// Automatically generated by ParentAnnotationStats -- preferably don't edit
// Automatically generated by ParentAnnotationStats -- preferably don't edit
// Automatically generated by ParentAnnotationStats -- preferably don't edit
// Automatically generated by ParentAnnotationStats -- preferably don't edit
// Automatically generated by ParentAnnotationStats -- preferably don't edit
// Automatically generated by ParentAnnotationStats -- preferably don't edit
// Automatically generated by ParentAnnotationStats -- preferably don't edit
// Automatically generated by ParentAnnotationStats -- preferably don't edit
SI_UNIT_MAP = {
SI_UNIT_MAP = {
SI_UNIT_MAP = {
SI_UNIT_MAP = {
SI_UNIT_MAP = {
SI_UNIT_MAP = {
SI_UNIT_MAP = {
SI_UNIT_MAP = {
SI_UNIT_MAP = {
SI_UNIT_MAP = {
SI_UNIT_MAP = {
SI_UNIT_MAP = {
SI_UNIT_MAP = {
SI_UNIT_MAP = {
SI_UNIT_MAP = {
$SiSymPrefixes = CreateRegex(Keys(SI_SYM_PREFIX_MAP))
$SiSymPrefixes = CreateRegex(Keys(SI_SYM_PREFIX_MAP))
$SiSymPrefixes = CreateRegex(Keys(SI_SYM_PREFIX_MAP))
$SiSymPrefixes = CreateRegex(Keys(SI_SYM_PREFIX_MAP))
$SiSymPrefixes = CreateRegex(Keys(SI_SYM_PREFIX_MAP))
$SiSymPrefixes = CreateRegex(Keys(SI_SYM_PREFIX_MAP))
$SiSymPrefixes = CreateRegex(Keys(SI_SYM_PREFIX_MAP))
$SiSymPrefixes = CreateRegex(Keys(SI_SYM_PREFIX_MAP))
$SiSymPrefixes = CreateRegex(Keys(SI_SYM_PREFIX_MAP))
$SiSymPrefixes = CreateRegex(Keys(SI_SYM_PREFIX_MAP))
$SiSymPrefixes = CreateRegex(Keys(SI_SYM_PREFIX_MAP))
$SiSymPrefixes = CreateRegex(Keys(SI_SYM_PREFIX_MAP))
$SiSymPrefixes = CreateRegex(Keys(SI_SYM_PREFIX_MAP))
$SiSymPrefixes = CreateRegex(Keys(SI_SYM_PREFIX_MAP))
$SiSymPrefixes = CreateRegex(Keys(SI_SYM_PREFIX_MAP))
Lines in vocab file: 1023
Lines in vocab file: 376
Lines in vocab file: 797
Lines in vocab file: 635
Lines in vocab file: 421
Lines in vocab file: 293
Lines in vocab file: 978
Lines in vocab file: 754
Lines in vocab file: 610
Lines in vocab file: 846
Lines in vocab file: 721
Lines in vocab file: 432
Lines in vocab file: 685
Sorted descending support * KL
Sorted descending support * KL
Sorted descending support * KL
Sorted descending support * KL
Sorted descending support * KL
Sorted descending support * KL
Sorted descending support * KL
Sorted descending support * KL
Sorted descending support * KL
Sorted descending support * KL
Sorted descending support * KL
Sorted descending support * KL
Sorted descending support * KL
Sorted descending support * KL
Sorted descending support * KL
SI_SYM_PREFIX_MAP = {
SI_SYM_PREFIX_MAP = {
SI_SYM_PREFIX_MAP = {
SI_SYM_PREFIX_MAP = {
SI_SYM_PREFIX_MAP = {
SI_SYM_PREFIX_MAP = {
SI_SYM_PREFIX_MAP = {
SI_SYM_PREFIX_MAP = {
SI_SYM_PREFIX_MAP = {
SI_SYM_PREFIX_MAP = {
SI_SYM_PREFIX_MAP = {
SI_SYM_PREFIX_MAP = {
SI_SYM_PREFIX_MAP = {
SI_SYM_PREFIX_MAP = {
SI_SYM_PREFIX_MAP = {
Word matrix size: 300x150
Word matrix size: 250x400
Word matrix size: 500x100
Word matrix size: 150x250
Word matrix size: 200x300
Word matrix size: 450x350
Word matrix size: 400x500
Word matrix size: 350x450
Word matrix size: 600x200
Word matrix size: 250x150
Word matrix size: 100x300
Word matrix size: 400x300
Word matrix size: 350x250
$SiPrefixes = CreateRegex(Keys(SI_PREFIX_MAP))
$SiPrefixes = CreateRegex(Keys(SI_PREFIX_MAP))
$SiPrefixes = CreateRegex(Keys(SI_PREFIX_MAP))
$SiPrefixes = CreateRegex(Keys(SI_PREFIX_MAP))
$SiPrefixes = CreateRegex(Keys(SI_PREFIX_MAP))
$SiPrefixes = CreateRegex(Keys(SI_PREFIX_MAP))
$SiPrefixes = CreateRegex(Keys(SI_PREFIX_MAP))
$SiPrefixes = CreateRegex(Keys(SI_PREFIX_MAP))
$SiPrefixes = CreateRegex(Keys(SI_PREFIX_MAP))
$SiPrefixes = CreateRegex(Keys(SI_PREFIX_MAP))
$SiPrefixes = CreateRegex(Keys(SI_PREFIX_MAP))
$SiPrefixes = CreateRegex(Keys(SI_PREFIX_MAP))
$SiPrefixes = CreateRegex(Keys(SI_PREFIX_MAP))
$SiPrefixes = CreateRegex(Keys(SI_PREFIX_MAP))
$SiPrefixes = CreateRegex(Keys(SI_PREFIX_MAP))
Tsurgeon: node_insert(node: ROOT, position: left, label: NP)
Tsurgeon: node_swap(source: NP, target: VP)
Tsurgeon: node_delete(label: VP)
Tsurgeon: node_insert(node: ROOT, position: left, label: ADVP)
Tsurgeon: node_swap(source: ADVP, target: ADJP)
Tsurgeon: node_delete(label: ADJP)
Tsurgeon: node_insert(node: ROOT, position: right, label: PP)
Tsurgeon: node_swap(source: PP, target: ADVP)
Tsurgeon: node_delete(label: NP)
Tsurgeon: node_insert(node: ROOT, position: right, label: VP)
Tsurgeon: node_swap(source: VP, target: NP)
Tsurgeon: node_delete(label: ROOT)
Tsurgeon: node_insert(node: S, position: right, label: SBAR)
Nothing!  Absolutely nothing!
Nothing!  Absolutely nothing!
Nothing!  Absolutely nothing!
Nothing!  Absolutely nothing!
Nothing!  Absolutely nothing!
Nothing!  Absolutely nothing!
Nothing!  Absolutely nothing!
Nothing!  Absolutely nothing!
Nothing!  Absolutely nothing!
Nothing!  Absolutely nothing!
Nothing!  Absolutely nothing!
Nothing!  Absolutely nothing!
Nothing!  Absolutely nothing!
Nothing!  Absolutely nothing!
Nothing!  Absolutely nothing!
W cat size: 20x8
W cat size: 15x6
W cat size: 8x3
W cat size: 12x4
W cat size: 25x10
W cat size: 18x7
W cat size: 16x9
W cat size: 14x5
W cat size: 11x6
W cat size: 9x3
W cat size: 21x8
W cat size: 13x4
W cat size: 23x11
W matrix size: 7x3
W matrix size: 15x8
W matrix size: 12x6
W matrix size: 9x4
W matrix size: 6x2
W matrix size: 14x7
W matrix size: 11x5
W matrix size: 8x3
W matrix size: 13x6
W matrix size: 5x2
W matrix size: 20x10
W matrix size: 17x8
W matrix size: 16x8
Skipping: line2
Skipping: line3
Skipping: line4
Skipping: line5
Skipping: line6
Skipping: line7
Skipping: line8
Skipping: line9
Skipping: line10
Skipping: line11
Skipping: line12
Skipping: line13
Skipping: line14
Options parameters:
Options parameters:
Options parameters:
Options parameters:
Options parameters:
Options parameters:
Options parameters:
Options parameters:
Options parameters:
Options parameters:
Options parameters:
Options parameters:
Options parameters:
Options parameters:
Options parameters:
PascalTemplate:
PascalTemplate:
PascalTemplate:
PascalTemplate:
PascalTemplate:
PascalTemplate:
PascalTemplate:
PascalTemplate:
PascalTemplate:
PascalTemplate:
PascalTemplate:
PascalTemplate:
PascalTemplate:
PascalTemplate:
PascalTemplate:
Using wordVector document.docx for presentation.pptx
Using wordVector photo.jpg for video.mp4
Using wordVector data.csv for model.h5
Using wordVector audio.wav for text.txt
Using wordVector code.py for output.log
Using wordVector input.txt for output.txt
Using wordVector config.json for result.json
Using wordVector template.docx for report.pdf
Using wordVector instruction.txt for script.sh
Using wordVector input.xml for output.xml
Using wordVector image.png for thumbnail.jpg
Using wordVector style.css for index.html
Using wordVector input.docx for output.docx
Using wordVector script.py for output.py
Error processing deleteSplitters
Error processing deleteSplitters
Error processing deleteSplitters
Error processing deleteSplitters
Error processing deleteSplitters
Error processing deleteSplitters
Error processing deleteSplitters
Error processing deleteSplitters
Error processing deleteSplitters
Error processing deleteSplitters
Error processing deleteSplitters
Error processing deleteSplitters
Error processing deleteSplitters
Error processing deleteSplitters
Error processing deleteSplitters
Combined approximate root label accuracy: 0.932
Combined approximate root label accuracy: 0.754
Combined approximate root label accuracy: 0.689
Combined approximate root label accuracy: 0.921
Combined approximate root label accuracy: 0.805
Combined approximate root label accuracy: 0.926
Combined approximate root label accuracy: 0.783
Combined approximate root label accuracy: 0.897
Combined approximate root label accuracy: 0.938
Combined approximate root label accuracy: 0.792
Combined approximate root label accuracy: 0.845
Combined approximate root label accuracy: 0.914
Combined approximate root label accuracy: 0.809
Stemmed: oranges to: stemmed_oranges
Stemmed: bananas to: stemmed_bananas
Stemmed: grapes to: stemmed_grapes
Stemmed: pineapples to: stemmed_pineapples
Stemmed: strawberries to: stemmed_strawberries
Stemmed: watermelons to: stemmed_watermelons
Stemmed: peaches to: stemmed_peaches
Stemmed: pears to: stemmed_pears
Stemmed: plums to: stemmed_plums
Stemmed: cherries to: stemmed_cherries
Stemmed: mangoes to: stemmed_mangoes
Stemmed: kiwis to: stemmed_kiwis
Stemmed: lemons to: stemmed_lemons
vs. correct: [4, 5, 6]
vs. correct: [7, 8, 9]
vs. correct: [10, 11, 12]
vs. correct: [13, 14, 15]
vs. correct: [16, 17, 18]
vs. correct: [19, 20, 21]
vs. correct: [22, 23, 24]
vs. correct: [25, 26, 27]
vs. correct: [28, 29, 30]
vs. correct: [31, 32, 33]
vs. correct: [34, 35, 36]
vs. correct: [37, 38, 39]
vs. correct: [40, 41, 42]
Approximate class2 root label accuracy: 0.567
Approximate class3 root label accuracy: 0.901
Approximate class4 root label accuracy: 0.432
Approximate class5 root label accuracy: 0.654
Approximate class6 root label accuracy: 0.876
Approximate class7 root label accuracy: 0.543
Approximate class8 root label accuracy: 0.789
Approximate class9 root label accuracy: 0.456
Approximate class10 root label accuracy: 0.789
Approximate class11 root label accuracy: 0.654
Approximate class12 root label accuracy: 0.901
Approximate class13 root label accuracy: 0.456
Approximate class14 root label accuracy: 0.234
Not a valid acronym: XYZ
Not a valid acronym: DEF
Not a valid acronym: GHI
Not a valid acronym: JKL
Not a valid acronym: MNO
Not a valid acronym: PQR
Not a valid acronym: STU
Not a valid acronym: VWX
Not a valid acronym: YZA
Not a valid acronym: BCD
Not a valid acronym: EFG
Not a valid acronym: HIJ
Not a valid acronym: KLM
Best sequence: [false, true, true, false, true]
Best sequence: [true, true, false, false, true]
Best sequence: [false, false, true, false, true]
Best sequence: [true, true, true, false, false]
Best sequence: [false, true, false, true, false]
Best sequence: [true, false, false, true, false]
Best sequence: [false, true, false, true, true]
Best sequence: [true, false, true, true, false]
Best sequence: [true, true, false, true, false]
Best sequence: [true, false, true, false, false]
Best sequence: [false, false, true, true, true]
Best sequence: [true, true, true, true, true]
Best sequence: [false, false, false, true, false]
Pruning: m from 7 to 2
Pruning: x from 15 to 9
Pruning: y from 8 to 3
Pruning: z from 12 to 6
Pruning: p from 6 to 1
Pruning: q from 9 to 4
Pruning: r from 11 to 5
Pruning: s from 18 to 13
Pruning: t from 13 to 8
Pruning: u from 7 to 2
Pruning: v from 14 to 9
Pruning: w from 9 to 4
Pruning: a from 16 to 11
annotating this guy as RC:
annotating this guy as RC:
annotating this guy as RC:
annotating this guy as RC:
annotating this guy as RC:
annotating this guy as RC:
annotating this guy as RC:
annotating this guy as RC:
annotating this guy as RC:
annotating this guy as RC:
annotating this guy as RC:
annotating this guy as RC:
annotating this guy as RC:
annotating this guy as RC:
annotating this guy as RC:
Start = sg.toCompactString()
Start = sg.toCompactString()
Start = sg.toCompactString()
Start = sg.toCompactString()
Start = sg.toCompactString()
Start = sg.toCompactString()
Start = sg.toCompactString()
Start = sg.toCompactString()
Start = sg.toCompactString()
Start = sg.toCompactString()
Start = sg.toCompactString()
Start = sg.toCompactString()
Start = sg.toCompactString()
WARNING! lexparser.Options: Unknown option ignored: -h
WARNING! lexparser.Options: Unknown option ignored: -v
WARNING! lexparser.Options: Unknown option ignored: -t
WARNING! lexparser.Options: Unknown option ignored: --output
WARNING! lexparser.Options: Unknown option ignored: -q
WARNING! lexparser.Options: Unknown option ignored: -p
WARNING! lexparser.Options: Unknown option ignored: -f
WARNING! lexparser.Options: Unknown option ignored: -d
WARNING! lexparser.Options: Unknown option ignored: --file
WARNING! lexparser.Options: Unknown option ignored: -r
WARNING! lexparser.Options: Unknown option ignored: --mode
WARNING! lexparser.Options: Unknown option ignored: -m
WARNING! lexparser.Options: Unknown option ignored: -c
Approximate <class2> label accuracy: 0.92
Approximate <class3> label accuracy: 0.78
Approximate <class4> label accuracy: 0.91
Approximate <class5> label accuracy: 0.87
Approximate <class6> label accuracy: 0.79
Approximate <class7> label accuracy: 0.93
Approximate <class8> label accuracy: 0.82
Approximate <class9> label accuracy: 0.88
Approximate <class10> label accuracy: 0.96
Approximate <class11> label accuracy: 0.85
Approximate <class12> label accuracy: 0.89
Approximate <class13> label accuracy: 0.93
Approximate <class14> label accuracy: 0.84
Tested static dynamic roots
Tested persistent dynamic roots
Tested active disabled roots
Tested static disabled roots
Tested persistent disabled roots
Tested active expired roots
Tested static expired roots
Tested persistent expired roots
Tested active corrupted roots
Tested static corrupted roots
Tested persistent corrupted roots
Tested active hidden roots
Tested static hidden roots
Was: syntaxTree vs parseTree
Was: originalTree vs modifiedTree
Was: binaryTree vs AVLTree
Was: decisionTree vs randomForest
Was: classificationTree vs regressionTree
Was: redTree vs blueTree
Was: balancedTree vs unbalancedTree
Was: BTree vs skipList
Was: expressionTree vs AST
Was: syntaxAnalysisTree vs semanticAnalysisTree
Was: syntaxTree vs derivationTree
Was: vocabularyTree vs grammarTree
Was: linkedTree vs arrayTree
Was: searchTree vs sortTree
tokens size :15
tokens size :20
tokens size :25
tokens size :30
tokens size :35
tokens size :40
tokens size :45
tokens size :50
tokens size :55
tokens size :60
tokens size :65
tokens size :70
tokens size :75
<0.85> accuracy
<0.92> accuracy
<0.78> accuracy
<0.94> accuracy
<0.81> accuracy
<0.87> accuracy
<0.96> accuracy
<0.79> accuracy
<0.93> accuracy
<0.85> accuracy
<0.91> accuracy
<0.77> accuracy
<0.95> accuracy
Exception in extract Day.
Exception in extract Day.
Exception in extract Day.
Exception in extract Day.
Exception in extract Day.
Exception in extract Day.
Exception in extract Day.
Exception in extract Day.
Exception in extract Day.
Exception in extract Day.
Exception in extract Day.
Exception in extract Day.
Exception in extract Day.
Exception in extract Day.
Exception in extract Day.
"stylesIncorrect incorrect
"optionsIncorrect incorrect
"formatIncorrect incorrect
"syntaxIncorrect incorrect
"valuesIncorrect incorrect
"inputIncorrect incorrect
"outputIncorrect incorrect
"permissionsIncorrect incorrect
"argumentsIncorrect incorrect
"charactersIncorrect incorrect
"syntaxesIncorrect incorrect
"dataIncorrect incorrect
"structureIncorrect incorrect
Result: parent is file2.txt and node is folder2
Result: parent is file3.txt and node is folder3
Result: parent is file4.txt and node is folder4
Result: parent is file5.txt and node is folder5
Result: parent is file6.txt and node is folder6
Result: parent is file7.txt and node is folder7
Result: parent is file8.txt and node is folder8
Result: parent is file9.txt and node is folder9
Result: parent is file10.txt and node is folder10
Result: parent is file11.txt and node is folder11
Result: parent is file12.txt and node is folder12
Result: parent is file13.txt and node is folder13
Result: parent is file14.txt and node is folder14
leaveGF=43
leaveGF=29
leaveGF=8
leaveGF=55
leaveGF=17
leaveGF=36
leaveGF=22
leaveGF=11
leaveGF=31
leaveGF=26
leaveGF=19
leaveGF=40
leaveGF=7
syntaxCorrect correct
formatCorrect correct
inputCorrect correct
outputCorrect correct
logicCorrect correct
performanceCorrect correct
securityCorrect correct
stabilityCorrect correct
scalabilityCorrect correct
efficiencyCorrect correct
validationCorrect correct
integrationCorrect correct
compatibilityCorrect correct
usabilityCorrect correct
inputDate: |2022-02-15|
inputDate: |2022-03-25|
inputDate: |2022-04-10|
inputDate: |2022-05-06|
inputDate: |2022-06-20|
inputDate: |2022-07-12|
inputDate: |2022-08-30|
inputDate: |2022-09-18|
inputDate: |2022-10-05|
inputDate: |2022-11-11|
inputDate: |2022-12-24|
inputDate: |2023-01-17|
inputDate: |2023-02-22|
woo: dog
woo: bird
woo: fish
woo: rabbit
woo: turtle
woo: snake
woo: lion
woo: elephant
woo: monkey
woo: tiger
woo: horse
woo: bear
woo: wolf
year extracted:2018
year extracted:2016
year extracted:2014
year extracted:2012
year extracted:2010
year extracted:2008
year extracted:2006
year extracted:2004
year extracted:2002
year extracted:2000
year extracted:1998
year extracted:1996
year extracted:1994
Label accuracy at various lengths:
Label accuracy at various lengths:
Label accuracy at various lengths:
Label accuracy at various lengths:
Label accuracy at various lengths:
Label accuracy at various lengths:
Label accuracy at various lengths:
Label accuracy at various lengths:
Label accuracy at various lengths:
Label accuracy at various lengths:
Label accuracy at various lengths:
Label accuracy at various lengths:
Label accuracy at various lengths:
Label accuracy at various lengths:
Label accuracy at various lengths:
NegraPennTreebankParserParams
NegraPennTreebankParserParams
NegraPennTreebankParserParams
NegraPennTreebankParserParams
NegraPennTreebankParserParams
NegraPennTreebankParserParams
NegraPennTreebankParserParams
NegraPennTreebankParserParams
NegraPennTreebankParserParams
NegraPennTreebankParserParams
NegraPennTreebankParserParams
NegraPennTreebankParserParams
NegraPennTreebankParserParams
NegraPennTreebankParserParams
NegraPennTreebankParserParams
Warning! gfCharacter argument ignored; must specify a character, not a String
Warning! gfCharacter argument ignored; must specify a character, not a String
Warning! gfCharacter argument ignored; must specify a character, not a String
Warning! gfCharacter argument ignored; must specify a character, not a String
Warning! gfCharacter argument ignored; must specify a character, not a String
Warning! gfCharacter argument ignored; must specify a character, not a String
Warning! gfCharacter argument ignored; must specify a character, not a String
Warning! gfCharacter argument ignored; must specify a character, not a String
Warning! gfCharacter argument ignored; must specify a character, not a String
Warning! gfCharacter argument ignored; must specify a character, not a String
Warning! gfCharacter argument ignored; must specify a character, not a String
Warning! gfCharacter argument ignored; must specify a character, not a String
Warning! gfCharacter argument ignored; must specify a character, not a String
Warning! gfCharacter argument ignored; must specify a character, not a String
Warning! gfCharacter argument ignored; must specify a character, not a String
# generated  = 15
# generated  = 5
# generated  = 20
# generated  = 8
# generated  = 12
# generated  = 3
# generated  = 6
# generated  = 17
# generated  = 11
# generated  = 9
# generated  = 14
# generated  = 4
# generated  = 7
Sentence The weather is sunny quoted=false
Sentence Please bring your umbrella quoted=true
Sentence Can you help me with my homework quoted=false
Sentence I love eating pizza quoted=true
Sentence The concert was amazing quoted=false
Sentence This book is so interesting quoted=true
Sentence I need to buy groceries quoted=false
Sentence Let's go for a walk quoted=true
Sentence It's raining outside quoted=false
Sentence The movie starts at 7 PM quoted=true
Sentence I'm so tired quoted=false
Sentence Cooking dinner for my family tonight quoted=true
Sentence I forgot my wallet at home quoted=false
Third annotator: a3_2
Third annotator: a3_3
Third annotator: a3_4
Third annotator: a3_5
Third annotator: a3_6
Third annotator: a3_7
Third annotator: a3_8
Third annotator: a3_9
Third annotator: a3_10
Third annotator: a3_11
Third annotator: a3_12
Third annotator: a3_13
Third annotator: a3_14
Age confusion matrix
Gender confusion matrix
Education confusion matrix
Occupation confusion matrix
Location confusion matrix
Hobby confusion matrix
Income confusion matrix
Marital Status confusion matrix
Language confusion matrix
Nationality confusion matrix
Religion confusion matrix
Health condition confusion matrix
Interest confusion matrix
Usage: SsurgeonPattern FILEPATH ["COMPACT_SEMANTIC_GRAPH"], FILEPATH=path to ssurgeon pattern to parse and print., SENTENCE=test sentence (in quotes)
Usage: SsurgeonPattern FILEPATH ["COMPACT_SEMANTIC_GRAPH"], FILEPATH=path to ssurgeon pattern to parse and print., SENTENCE=test sentence (in quotes)
Usage: SsurgeonPattern FILEPATH ["COMPACT_SEMANTIC_GRAPH"], FILEPATH=path to ssurgeon pattern to parse and print., SENTENCE=test sentence (in quotes)
Usage: SsurgeonPattern FILEPATH ["COMPACT_SEMANTIC_GRAPH"], FILEPATH=path to ssurgeon pattern to parse and print., SENTENCE=test sentence (in quotes)
Usage: SsurgeonPattern FILEPATH ["COMPACT_SEMANTIC_GRAPH"], FILEPATH=path to ssurgeon pattern to parse and print., SENTENCE=test sentence (in quotes)
Usage: SsurgeonPattern FILEPATH ["COMPACT_SEMANTIC_GRAPH"], FILEPATH=path to ssurgeon pattern to parse and print., SENTENCE=test sentence (in quotes)
Usage: SsurgeonPattern FILEPATH ["COMPACT_SEMANTIC_GRAPH"], FILEPATH=path to ssurgeon pattern to parse and print., SENTENCE=test sentence (in quotes)
Usage: SsurgeonPattern FILEPATH ["COMPACT_SEMANTIC_GRAPH"], FILEPATH=path to ssurgeon pattern to parse and print., SENTENCE=test sentence (in quotes)
Usage: SsurgeonPattern FILEPATH ["COMPACT_SEMANTIC_GRAPH"], FILEPATH=path to ssurgeon pattern to parse and print., SENTENCE=test sentence (in quotes)
Usage: SsurgeonPattern FILEPATH ["COMPACT_SEMANTIC_GRAPH"], FILEPATH=path to ssurgeon pattern to parse and print., SENTENCE=test sentence (in quotes)
Usage: SsurgeonPattern FILEPATH ["COMPACT_SEMANTIC_GRAPH"], FILEPATH=path to ssurgeon pattern to parse and print., SENTENCE=test sentence (in quotes)
Usage: SsurgeonPattern FILEPATH ["COMPACT_SEMANTIC_GRAPH"], FILEPATH=path to ssurgeon pattern to parse and print., SENTENCE=test sentence (in quotes)
Usage: SsurgeonPattern FILEPATH ["COMPACT_SEMANTIC_GRAPH"], FILEPATH=path to ssurgeon pattern to parse and print., SENTENCE=test sentence (in quotes)
Usage: SsurgeonPattern FILEPATH ["COMPACT_SEMANTIC_GRAPH"], FILEPATH=path to ssurgeon pattern to parse and print., SENTENCE=test sentence (in quotes)
Usage: SsurgeonPattern FILEPATH ["COMPACT_SEMANTIC_GRAPH"], FILEPATH=path to ssurgeon pattern to parse and print., SENTENCE=test sentence (in quotes)
tokens:def
tokens:123
tokens:xyz
tokens:456
tokens:uvw
tokens:789
tokens:ghi
tokens:jkl
tokens:mno
tokens:rst
tokens:pqrs
tokens:1234
tokens:5678
Exception thrown getting all children for element=span, e=IndexOutOfBoundsException
Exception thrown getting all children for element=ul, e=IllegalArgumentException
Exception thrown getting all children for element=a, e=ArrayIndexOutOfBoundsException
Exception thrown getting all children for element=img, e=NoSuchElementException
Exception thrown getting all children for element=table, e=NullPointerException
Exception thrown getting all children for element=button, e=ClassCastException
Exception thrown getting all children for element=li, e=NumberFormatException
Exception thrown getting all children for element=input, e=IllegalStateException
Exception thrown getting all children for element=div, e=UnsupportedOperationException
Exception thrown getting all children for element=span, e=StringIndexOutOfBoundsException
Exception thrown getting all children for element=ul, e=NullPointerException
Exception thrown getting all children for element=a, e=IndexOutOfBoundsException
Exception thrown getting all children for element=img, e=IllegalArgumentException
Extracting date: 2022-02-15
Extracting date: 2022-03-05
Extracting date: 2022-04-10
Extracting date: 2022-05-21
Extracting date: 2022-06-30
Extracting date: 2022-07-12
Extracting date: 2022-08-08
Extracting date: 2022-09-22
Extracting date: 2022-10-18
Extracting date: 2022-11-27
Extracting date: 2022-12-05
Extracting date: 2023-01-10
Extracting date: 2023-02-08
pats size is 5
pats size is 20
pats size is 15
pats size is 8
pats size is 12
pats size is 3
pats size is 7
pats size is 17
pats size is 9
pats size is 14
pats size is 6
pats size is 11
pats size is 23
Answer is: [4, 5, 6]
Answer is: [7, 8, 9]
Answer is: [10, 11, 12]
Answer is: [13, 14, 15]
Answer is: [16, 17, 18]
Answer is: [19, 20, 21]
Answer is: [22, 23, 24]
Answer is: [25, 26, 27]
Answer is: [28, 29, 30]
Answer is: [31, 32, 33]
Answer is: [34, 35, 36]
Answer is: [37, 38, 39]
Answer is: [40, 41, 42]
interp=0.3, prescore=0.65, P(go|hTWds)=0.6, score=0.85
interp=0.7, prescore=0.85, P(go|hTWds)=0.9, score=0.75
interp=0.2, prescore=0.55, P(go|hTWds)=0.4, score=0.65
interp=0.4, prescore=0.45, P(go|hTWds)=0.3, score=0.55
interp=0.9, prescore=0.95, P(go|hTWds)=0.5, score=0.45
interp=0.8, prescore=0.25, P(go|hTWds)=0.7, score=0.35
interp=0.6, prescore=0.35, P(go|hTWds)=0.8, score=0.25
interp=0.1, prescore=0.15, P(go|hTWds)=0.2, score=0.15
interp=0.55, prescore=0.25, P(go|hTWds)=0.4, score=0.75
interp=0.95, prescore=0.75, P(go|hTWds)=0.9, score=0.35
interp=0.25, prescore=0.85, P(go|hTWds)=0.5, score=0.65
interp=0.75, prescore=0.95, P(go|hTWds)=0.6, score=0.55
interp=0.15, prescore=0.45, P(go|hTWds)=0.8, score=0.45
Error getting first tag script under element=<body>
Error getting first tag title under element=<head>
Error getting first tag meta under element=<head>
Error getting first tag link under element=<head>
Error getting first tag ul under element=<nav>
Error getting first tag li under element=<ul>
Error getting first tag a under element=<li>
Error getting first tag span under element=<a>
Error getting first tag img under element=<div>
Error getting first tag p under element=<div>
Error getting first tag h1 under element=<div>
Error getting first tag div under element=<body>
Error getting first tag section under element=<div>
Error getting first tag form under element=<section>
Error getting first child Element for element=<div>, exception=<ArgumentException>
Error getting first child Element for element=<ul>, exception=<InvalidOperationException>
Error getting first child Element for element=<form>, exception=<IndexOutOfRangeException>
Error getting first child Element for element=<table>, exception=<FileNotFoundException>
Error getting first child Element for element=<a>, exception=<KeyNotFoundException>
Error getting first child Element for element=<span>, exception=<InvalidCastException>
Error getting first child Element for element=<img>, exception=<FormatException>
Error getting first child Element for element=<input>, exception=<NotSupportedException>
Error getting first child Element for element=<p>, exception=<OutOfMemoryException>
Error getting first child Element for element=<div>, exception=<ArgumentNullException>
Error getting first child Element for element=<h1>, exception=<TimeoutException>
Error getting first child Element for element=<li>, exception=<OverflowException>
Error getting first child Element for element=<button>, exception=<FormatException>
Second annotator: a2_content2
Second annotator: a2_content3
Second annotator: a2_content4
Second annotator: a2_content5
Second annotator: a2_content6
Second annotator: a2_content7
Second annotator: a2_content8
Second annotator: a2_content9
Second annotator: a2_content10
Second annotator: a2_content11
Second annotator: a2_content12
Second annotator: a2_content13
Second annotator: a2_content14
c_aT_hTWd=7, c_hTWd=2, smooth_aT_hTWd=3.5, ## p(aT|hTWd)=0.5
c_aT_hTWd=13, c_hTWd=8, smooth_aT_hTWd=4.2, ## p(aT|hTWd)=0.6
c_aT_hTWd=9, c_hTWd=3, smooth_aT_hTWd=1.9, ## p(aT|hTWd)=0.7
c_aT_hTWd=6, c_hTWd=4, smooth_aT_hTWd=3.8, ## p(aT|hTWd)=0.9
c_aT_hTWd=11, c_hTWd=6, smooth_aT_hTWd=2.1, ## p(aT|hTWd)=0.4
c_aT_hTWd=8, c_hTWd=2, smooth_aT_hTWd=4.5, ## p(aT|hTWd)=0.3
c_aT_hTWd=14, c_hTWd=9, smooth_aT_hTWd=2.8, ## p(aT|hTWd)=0.2
c_aT_hTWd=12, c_hTWd=7, smooth_aT_hTWd=4.1, ## p(aT|hTWd)=0.1
c_aT_hTWd=9, c_hTWd=4, smooth_aT_hTWd=1.5, ## p(aT|hTWd)=0.6
c_aT_hTWd=7, c_hTWd=3, smooth_aT_hTWd=2.9, ## p(aT|hTWd)=0.7
c_aT_hTWd=10, c_hTWd=6, smooth_aT_hTWd=3.4, ## p(aT|hTWd)=0.8
c_aT_hTWd=8, c_hTWd=5, smooth_aT_hTWd=2.3, ## p(aT|hTWd)=0.9
c_aT_hTWd=11, c_hTWd=7, smooth_aT_hTWd=3.9, ## p(aT|hTWd)=0.5
inf-norm: 9.8
inf-norm: 3.6
inf-norm: 7.1
inf-norm: 2.5
inf-norm: 4.3
inf-norm: 6.9
inf-norm: 1.7
inf-norm: 8.2
inf-norm: 3.9
inf-norm: 6.5
inf-norm: 2.1
inf-norm: 4.7
inf-norm: 7.3
# generated = 1
# generated = 2
# generated = 3
# generated = 4
# generated = 5
# generated = 6
# generated = 7
# generated = 8
# generated = 9
# generated = 10
# generated = 11
# generated = 12
# generated = 13
# generated = 14
I/O error in the connection to:
I/O error in the connection to:
I/O error in the connection to:
I/O error in the connection to:
I/O error in the connection to:
I/O error in the connection to:
I/O error in the connection to:
I/O error in the connection to:
I/O error in the connection to:
I/O error in the connection to:
I/O error in the connection to:
I/O error in the connection to:
I/O error in the connection to:
I/O error in the connection to:
I/O error in the connection to:
Cannot find host:
Cannot find host:
Cannot find host:
Cannot find host:
Cannot find host:
Cannot find host:
Cannot find host:
Cannot find host:
Cannot find host:
Cannot find host:
Cannot find host:
Cannot find host:
Cannot find host:
Cannot find host:
Cannot find host:
Input some text and press RETURN to NER tag it,  or just RETURN to finish.
Input some text and press RETURN to NER tag it,  or just RETURN to finish.
Input some text and press RETURN to NER tag it,  or just RETURN to finish.
Input some text and press RETURN to NER tag it,  or just RETURN to finish.
Input some text and press RETURN to NER tag it,  or just RETURN to finish.
Input some text and press RETURN to NER tag it,  or just RETURN to finish.
Input some text and press RETURN to NER tag it,  or just RETURN to finish.
Input some text and press RETURN to NER tag it,  or just RETURN to finish.
Input some text and press RETURN to NER tag it,  or just RETURN to finish.
Input some text and press RETURN to NER tag it,  or just RETURN to finish.
Input some text and press RETURN to NER tag it,  or just RETURN to finish.
Input some text and press RETURN to NER tag it,  or just RETURN to finish.
Input some text and press RETURN to NER tag it,  or just RETURN to finish.
Input some text and press RETURN to NER tag it,  or just RETURN to finish.
Input some text and press RETURN to NER tag it,  or just RETURN to finish.
Closing connection to client
Closing connection to client
Closing connection to client
Closing connection to client
Closing connection to client
Closing connection to client
Closing connection to client
Closing connection to client
Closing connection to client
Closing connection to client
Closing connection to client
Closing connection to client
Closing connection to client
Closing connection to client
Closing connection to client
c_aPT_hPTd=789, c_hPTd=234, p(aPT|hPTd)=0.456
c_aPT_hPTd=567, c_hPTd=890, p(aPT|hPTd)=0.123
c_aPT_hPTd=901, c_hPTd=345, p(aPT|hPTd)=0.678
c_aPT_hPTd=678, c_hPTd=012, p(aPT|hPTd)=0.901
c_aPT_hPTd=234, c_hPTd=567, p(aPT|hPTd)=0.345
c_aPT_hPTd=890, c_hPTd=901, p(aPT|hPTd)=0.567
c_aPT_hPTd=012, c_hPTd=678, p(aPT|hPTd)=0.890
c_aPT_hPTd=345, c_hPTd=890, p(aPT|hPTd)=0.012
c_aPT_hPTd=567, c_hPTd=123, p(aPT|hPTd)=0.234
c_aPT_hPTd=901, c_hPTd=789, p(aPT|hPTd)=0.678
c_aPT_hPTd=456, c_hPTd=012, p(aPT|hPTd)=0.901
c_aPT_hPTd=123, c_hPTd=234, p(aPT|hPTd)=0.456
c_aPT_hPTd=789, c_hPTd=567, p(aPT|hPTd)=0.789
NERServer:Session: can't close session
NERServer:Session: can't close session
NERServer:Session: can't close session
NERServer:Session: can't close session
NERServer:Session: can't close session
NERServer:Session: can't close session
NERServer:Session: can't close session
NERServer:Session: can't close session
NERServer:Session: can't close session
NERServer:Session: can't close session
NERServer:Session: can't close session
NERServer:Session: can't close session
NERServer:Session: can't close session
NERServer:Session: can't close session
NERServer:Session: can't close session
testing for eps 0.01
testing for eps 0.001
testing for eps 0.0001
testing for eps 1e-05
testing for eps 1e-06
testing for eps 1e-07
testing for eps 1e-08
testing for eps 1e-09
testing for eps 1e-10
testing for eps 1e-11
testing for eps 1e-12
testing for eps 1e-13
testing for eps 1e-14
Discovered better point: 0.958
Discovered better point: 0.694
Discovered better point: 0.516
Discovered better point: 0.972
Discovered better point: 0.731
Discovered better point: 0.893
Discovered better point: 0.647
Discovered better point: 0.924
Discovered better point: 0.633
Discovered better point: 0.788
Discovered better point: 0.899
Discovered better point: 0.741
Discovered better point: 0.972
NERServer.Session: error classifying string.
NERServer.Session: error classifying string.
NERServer.Session: error classifying string.
NERServer.Session: error classifying string.
NERServer.Session: error classifying string.
NERServer.Session: error classifying string.
NERServer.Session: error classifying string.
NERServer.Session: error classifying string.
NERServer.Session: error classifying string.
NERServer.Session: error classifying string.
NERServer.Session: error classifying string.
NERServer.Session: error classifying string.
NERServer.Session: error classifying string.
NERServer.Session: error classifying string.
NERServer.Session: error classifying string.
c_aTW_hTd=2.345, c_hTd=1.234, smooth_aTW_hTd=1.123, p(aTW|hTd)=0.789
c_aTW_hTd=3.456, c_hTd=2.345, smooth_aTW_hTd=1.234, p(aTW|hTd)=0.890
c_aTW_hTd=4.567, c_hTd=3.456, smooth_aTW_hTd=2.345, p(aTW|hTd)=0.901
c_aTW_hTd=5.678, c_hTd=4.567, smooth_aTW_hTd=3.456, p(aTW|hTd)=0.912
c_aTW_hTd=6.789, c_hTd=5.678, smooth_aTW_hTd=4.567, p(aTW|hTd)=0.923
c_aTW_hTd=7.890, c_hTd=6.789, smooth_aTW_hTd=5.678, p(aTW|hTd)=0.934
c_aTW_hTd=8.901, c_hTd=7.890, smooth_aTW_hTd=6.789, p(aTW|hTd)=0.945
c_aTW_hTd=9.912, c_hTd=8.901, smooth_aTW_hTd=7.890, p(aTW|hTd)=0.956
c_aTW_hTd=0.101, c_hTd=9.912, smooth_aTW_hTd=8.901, p(aTW|hTd)=0.967
c_aTW_hTd=1.212, c_hTd=0.101, smooth_aTW_hTd=9.912, p(aTW|hTd)=0.978
c_aTW_hTd=2.323, c_hTd=1.212, smooth_aTW_hTd=0.101, p(aTW|hTd)=0.989
c_aTW_hTd=3.434, c_hTd=2.323, smooth_aTW_hTd=1.212, p(aTW|hTd)=0.990
c_aTW_hTd=4.545, c_hTd=3.434, smooth_aTW_hTd=2.323, p(aTW|hTd)=0.991
Thought optimal point was: -3.8
Thought optimal point was: -4.5
Thought optimal point was: -6.7
Thought optimal point was: -5.1
Thought optimal point was: -4.9
Thought optimal point was: -6.2
Thought optimal point was: -3.6
Thought optimal point was: -4.8
Thought optimal point was: -7.3
Thought optimal point was: -6.5
Thought optimal point was: -3.9
Thought optimal point was: -5.7
Thought optimal point was: -4.1
NERServer:Session: connection closed by peer
NERServer:Session: connection closed by peer
NERServer:Session: connection closed by peer
NERServer:Session: connection closed by peer
NERServer:Session: connection closed by peer
NERServer:Session: connection closed by peer
NERServer:Session: connection closed by peer
NERServer:Session: connection closed by peer
NERServer:Session: connection closed by peer
NERServer:Session: connection closed by peer
NERServer:Session: connection closed by peer
NERServer:Session: connection closed by peer
NERServer:Session: connection closed by peer
NERServer:Session: connection closed by peer
NERServer:Session: connection closed by peer
NERServer:Session: couldn't read input
NERServer:Session: couldn't read input
NERServer:Session: couldn't read input
NERServer:Session: couldn't read input
NERServer:Session: couldn't read input
NERServer:Session: couldn't read input
NERServer:Session: couldn't read input
NERServer:Session: couldn't read input
NERServer:Session: couldn't read input
NERServer:Session: couldn't read input
NERServer:Session: couldn't read input
NERServer:Session: couldn't read input
NERServer:Session: couldn't read input
NERServer:Session: couldn't read input
NERServer:Session: couldn't read input
c_aTW_hTWd=0.881, c_hTWd=0.526, smooth_aTW_hTWd=0.943, ## p(aTW|hTWd)=0.765
c_aTW_hTWd=0.142, c_hTWd=0.935, smooth_aTW_hTWd=0.297, ## p(aTW|hTWd)=0.612
c_aTW_hTWd=0.743, c_hTWd=0.315, smooth_aTW_hTWd=0.518, ## p(aTW|hTWd)=0.926
c_aTW_hTWd=0.652, c_hTWd=0.421, smooth_aTW_hTWd=0.876, ## p(aTW|hTWd)=0.134
c_aTW_hTWd=0.982, c_hTWd=0.854, smooth_aTW_hTWd=0.426, ## p(aTW|hTWd)=0.973
c_aTW_hTWd=0.264, c_hTWd=0.732, smooth_aTW_hTWd=0.171, ## p(aTW|hTWd)=0.539
c_aTW_hTWd=0.612, c_hTWd=0.845, smooth_aTW_hTWd=0.956, ## p(aTW|hTWd)=0.822
c_aTW_hTWd=0.375, c_hTWd=0.263, smooth_aTW_hTWd=0.739, ## p(aTW|hTWd)=0.385
c_aTW_hTWd=0.928, c_hTWd=0.608, smooth_aTW_hTWd=0.315, ## p(aTW|hTWd)=0.871
c_aTW_hTWd=0.517, c_hTWd=0.497, smooth_aTW_hTWd=0.615, ## p(aTW|hTWd)=0.279
c_aTW_hTWd=0.183, c_hTWd=0.773, smooth_aTW_hTWd=0.928, ## p(aTW|hTWd)=0.739
c_aTW_hTWd=0.683, c_hTWd=0.972, smooth_aTW_hTWd=0.563, ## p(aTW|hTWd)=0.017
c_aTW_hTWd=0.749, c_hTWd=0.382, smooth_aTW_hTWd=0.280, ## p(aTW|hTWd)=0.697
Calculated: -0.8
Calculated: 0.2
Calculated: -2.3
Calculated: 4.7
Calculated: -1.1
Calculated: 3.9
Calculated: -0.5
Calculated: 2.0
Calculated: -3.2
Calculated: 1.9
Calculated: -4.6
Calculated: 0.9
Calculated: -2.0
Patterns, num = 15
Patterns, num = 5
Patterns, num = 8
Patterns, num = 12
Patterns, num = 3
Patterns, num = 6
Patterns, num = 9
Patterns, num = 2
Patterns, num = 7
Patterns, num = 11
Patterns, num = 4
Patterns, num = 13
Patterns, num = 1
Created new session
Created new session
Created new session
Created new session
Created new session
Created new session
Created new session
Created new session
Created new session
Created new session
Created new session
Created new session
Created new session
Created new session
Created new session
c_aPTW_hPTd=0.178, c_hPTd=0.212, p(aPTW|hPTd)=0.065
c_aPTW_hPTd=0.301, c_hPTd=0.278, p(aPTW|hPTd)=0.189
c_aPTW_hPTd=0.186, c_hPTd=0.215, p(aPTW|hPTd)=0.071
c_aPTW_hPTd=0.316, c_hPTd=0.301, p(aPTW|hPTd)=0.212
c_aPTW_hPTd=0.250, c_hPTd=0.319, p(aPTW|hPTd)=0.146
c_aPTW_hPTd=0.260, c_hPTd=0.292, p(aPTW|hPTd)=0.155
c_aPTW_hPTd=0.224, c_hPTd=0.197, p(aPTW|hPTd)=0.098
c_aPTW_hPTd=0.193, c_hPTd=0.181, p(aPTW|hPTd)=0.081
c_aPTW_hPTd=0.302, c_hPTd=0.333, p(aPTW|hPTd)=0.202
c_aPTW_hPTd=0.245, c_hPTd=0.220, p(aPTW|hPTd)=0.105
c_aPTW_hPTd=0.211, c_hPTd=0.239, p(aPTW|hPTd)=0.123
c_aPTW_hPTd=0.270, c_hPTd=0.262, p(aPTW|hPTd)=0.169
c_aPTW_hPTd=0.294, c_hPTd=0.275, p(aPTW|hPTd)=0.186
Accepted request from
Accepted request from
Accepted request from
Accepted request from
Accepted request from
Accepted request from
Accepted request from
Accepted request from
Accepted request from
Accepted request from
Accepted request from
Accepted request from
Accepted request from
Accepted request from
Accepted request from
Definition approx: 0.6
Definition approx: 0.75
Definition approx: 0.3
Definition approx: 0.85
Definition approx: 0.4
Definition approx: 0.7
Definition approx: 0.5
Definition approx: 0.8
Definition approx: 0.2
Definition approx: 0.65
Definition approx: 0.1
Definition approx: 0.95
Definition approx: 0.35
NERServer: couldn't close client
NERServer: couldn't close client
NERServer: couldn't close client
NERServer: couldn't close client
NERServer: couldn't close client
NERServer: couldn't close client
NERServer: couldn't close client
NERServer: couldn't close client
NERServer: couldn't close client
NERServer: couldn't close client
NERServer: couldn't close client
NERServer: couldn't close client
NERServer: couldn't close client
NERServer: couldn't close client
NERServer: couldn't close client
Definitional and calculated gradient differ!
Definitional and calculated gradient differ!
Definitional and calculated gradient differ!
Definitional and calculated gradient differ!
Definitional and calculated gradient differ!
Definitional and calculated gradient differ!
Definitional and calculated gradient differ!
Definitional and calculated gradient differ!
Definitional and calculated gradient differ!
Definitional and calculated gradient differ!
Definitional and calculated gradient differ!
Definitional and calculated gradient differ!
Definitional and calculated gradient differ!
Definitional and calculated gradient differ!
Definitional and calculated gradient differ!
NERServer: couldn't accept
NERServer: couldn't accept
NERServer: couldn't accept
NERServer: couldn't accept
NERServer: couldn't accept
NERServer: couldn't accept
NERServer: couldn't accept
NERServer: couldn't accept
NERServer: couldn't accept
NERServer: couldn't accept
NERServer: couldn't accept
NERServer: couldn't accept
NERServer: couldn't accept
NERServer: couldn't accept
NERServer: couldn't accept
Reading ssurgeon file=/user/Documents/ssurgeon2.txt
Reading ssurgeon file=/user/Documents/ssurgeon3.txt
Reading ssurgeon file=/user/Documents/ssurgeon4.txt
Reading ssurgeon file=/user/Documents/ssurgeon5.txt
Reading ssurgeon file=/user/Documents/ssurgeon6.txt
Reading ssurgeon file=/user/Documents/ssurgeon7.txt
Reading ssurgeon file=/user/Documents/ssurgeon8.txt
Reading ssurgeon file=/user/Documents/ssurgeon9.txt
Reading ssurgeon file=/user/Documents/ssurgeon10.txt
Reading ssurgeon file=/user/Documents/ssurgeon11.txt
Reading ssurgeon file=/user/Documents/ssurgeon12.txt
Reading ssurgeon file=/user/Documents/ssurgeon13.txt
Reading ssurgeon file=/user/Documents/ssurgeon14.txt
Forced assignments: [4, 5, 6]
Forced assignments: [7, 8, 9]
Forced assignments: [10, 11, 12]
Forced assignments: [13, 14, 15]
Forced assignments: [16, 17, 18]
Forced assignments: [19, 20, 21]
Forced assignments: [22, 23, 24]
Forced assignments: [25, 26, 27]
Forced assignments: [28, 29, 30]
Forced assignments: [31, 32, 33]
Forced assignments: [34, 35, 36]
Forced assignments: [37, 38, 39]
Forced assignments: [40, 41, 42]
Was not able to create XML document for pattern list.
Was not able to create XML document for pattern list.
Was not able to create XML document for pattern list.
Was not able to create XML document for pattern list.
Was not able to create XML document for pattern list.
Was not able to create XML document for pattern list.
Was not able to create XML document for pattern list.
Was not able to create XML document for pattern list.
Was not able to create XML document for pattern list.
Was not able to create XML document for pattern list.
Was not able to create XML document for pattern list.
Was not able to create XML document for pattern list.
Was not able to create XML document for pattern list.
Was not able to create XML document for pattern list.
Was not able to create XML document for pattern list.
Brute force map: [6, 7, 8, 9, 10]
Brute force map: [11, 12, 13, 14, 15]
Brute force map: [16, 17, 18, 19, 20]
Brute force map: [21, 22, 23, 24, 25]
Brute force map: [26, 27, 28, 29, 30]
Brute force map: [31, 32, 33, 34, 35]
Brute force map: [36, 37, 38, 39, 40]
Brute force map: [41, 42, 43, 44, 45]
Brute force map: [46, 47, 48, 49, 50]
Brute force map: [51, 52, 53, 54, 55]
Brute force map: [56, 57, 58, 59, 60]
Brute force map: [61, 62, 63, 64, 65]
Brute force map: [66, 67, 68, 69, 70]
c_aT_hTd=0.6, c_hTd=0.3, smooth_aT_hTd=0.8, p(aT|hTd)=0.2
c_aT_hTd=0.1, c_hTd=0.4, smooth_aT_hTd=0.9, p(aT|hTd)=0.5
c_aT_hTd=0.8, c_hTd=0.2, smooth_aT_hTd=0.6, p(aT|hTd)=0.4
c_aT_hTd=0.9, c_hTd=0.1, smooth_aT_hTd=0.5, p(aT|hTd)=0.7
c_aT_hTd=0.7, c_hTd=0.6, smooth_aT_hTd=0.4, p(aT|hTd)=0.1
c_aT_hTd=0.3, c_hTd=0.9, smooth_aT_hTd=0.2, p(aT|hTd)=0.6
c_aT_hTd=0.5, c_hTd=0.7, smooth_aT_hTd=0.1, p(aT|hTd)=0.9
c_aT_hTd=0.4, c_hTd=0.8, smooth_aT_hTd=0.3, p(aT|hTd)=0.8
c_aT_hTd=0.1, c_hTd=0.2, smooth_aT_hTd=0.5, p(aT|hTd)=0.4
c_aT_hTd=0.5, c_hTd=0.1, smooth_aT_hTd=0.2, p(aT|hTd)=0.7
c_aT_hTd=0.7, c_hTd=0.8, smooth_aT_hTd=0.4, p(aT|hTd)=0.9
c_aT_hTd=0.2, c_hTd=0.3, smooth_aT_hTd=0.6, p(aT|hTd)=0.5
c_aT_hTd=0.4, c_hTd=0.5, smooth_aT_hTd=0.8, p(aT|hTd)=0.3
useSUTime: false
useSUTime: true
useSUTime: false
useSUTime: true
useSUTime: false
useSUTime: true
useSUTime: false
useSUTime: true
useSUTime: false
useSUTime: true
useSUTime: false
useSUTime: true
useSUTime: false
Was not able to create XML document for pattern list, file not written.
Was not able to create XML document for pattern list, file not written.
Was not able to create XML document for pattern list, file not written.
Was not able to create XML document for pattern list, file not written.
Was not able to create XML document for pattern list, file not written.
Was not able to create XML document for pattern list, file not written.
Was not able to create XML document for pattern list, file not written.
Was not able to create XML document for pattern list, file not written.
Was not able to create XML document for pattern list, file not written.
Was not able to create XML document for pattern list, file not written.
Was not able to create XML document for pattern list, file not written.
Was not able to create XML document for pattern list, file not written.
Was not able to create XML document for pattern list, file not written.
Was not able to create XML document for pattern list, file not written.
Was not able to create XML document for pattern list, file not written.
info for this NERClassifierCombiner:
info for this NERClassifierCombiner:
info for this NERClassifierCombiner:
info for this NERClassifierCombiner:
info for this NERClassifierCombiner:
info for this NERClassifierCombiner:
info for this NERClassifierCombiner:
info for this NERClassifierCombiner:
info for this NERClassifierCombiner:
info for this NERClassifierCombiner:
info for this NERClassifierCombiner:
info for this NERClassifierCombiner:
info for this NERClassifierCombiner:
info for this NERClassifierCombiner:
info for this NERClassifierCombiner:
MAP: [5, 6, 7, 8]
MAP: [9, 10, 11, 12]
MAP: [13, 14, 15, 16]
MAP: [17, 18, 19, 20]
MAP: [21, 22, 23, 24]
MAP: [25, 26, 27, 28]
MAP: [29, 30, 31, 32]
MAP: [33, 34, 35, 36]
MAP: [37, 38, 39, 40]
MAP: [41, 42, 43, 44]
MAP: [45, 46, 47, 48]
MAP: [49, 50, 51, 52]
MAP: [53, 54, 55, 56]
Ignored an assertion in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an assertion in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an assertion in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an assertion in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an assertion in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an assertion in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an assertion in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an assertion in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an assertion in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an assertion in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an assertion in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an assertion in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an assertion in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an assertion in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an assertion in QuantifiableEntityNormalizer: (result is that entities were not normalized)
WARNING: Ssurgeon.class.getName()writeToFile
INFO: Ssurgeon.class.getName()writeToFile
DEBUG: Ssurgeon.class.getName()writeToFile
ERROR: Ssurgeon.class.getName()writeToFile
WARNING: Ssurgeon.class.getName()writeToFile
INFO: Ssurgeon.class.getName()writeToFile
DEBUG: Ssurgeon.class.getName()writeToFile
ERROR: Ssurgeon.class.getName()writeToFile
WARNING: Ssurgeon.class.getName()writeToFile
INFO: Ssurgeon.class.getName()writeToFile
DEBUG: Ssurgeon.class.getName()writeToFile
ERROR: Ssurgeon.class.getName()writeToFile
WARNING: Ssurgeon.class.getName()writeToFile
Ignored an exception in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an exception in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an exception in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an exception in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an exception in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an exception in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an exception in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an exception in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an exception in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an exception in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an exception in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an exception in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an exception in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an exception in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Ignored an exception in QuantifiableEntityNormalizer: (result is that entities were not normalized)
Error in writeToString, could not process pattern=MM/dd/yyyy
Error in writeToString, could not process pattern=HH:mm
Error in writeToString, could not process pattern=dd MMM yyyy
Error in writeToString, could not process pattern=E, MMM dd yyyy
Error in writeToString, could not process pattern=yyyy-MM-dd'T'HH:mm:ss
Error in writeToString, could not process pattern=M d, yyyy
Error in writeToString, could not process pattern=dd/MM/yyyy
Error in writeToString, could not process pattern=HH:mm:ss
Error in writeToString, could not process pattern=MMM dd, yyyy
Error in writeToString, could not process pattern=yyyy/MM/dd
Error in writeToString, could not process pattern=h:mm a
Error in writeToString, could not process pattern=MMMM dd, yyyy
Error in writeToString, could not process pattern=yyyy-MM-dd
Exhaust from patterns, depth=8
Exhaust from patterns, depth=2
Exhaust from patterns, depth=10
Exhaust from patterns, depth=3
Exhaust from patterns, depth=7
Exhaust from patterns, depth=4
Exhaust from patterns, depth=9
Exhaust from patterns, depth=6
Exhaust from patterns, depth=1
Exhaust from patterns, depth=12
Exhaust from patterns, depth=11
Exhaust from patterns, depth=14
Exhaust from patterns, depth=15
Ignored an exception in NumberSequenceClassifier: (result is that some numbers were not classified)
Ignored an exception in NumberSequenceClassifier: (result is that some numbers were not classified)
Ignored an exception in NumberSequenceClassifier: (result is that some numbers were not classified)
Ignored an exception in NumberSequenceClassifier: (result is that some numbers were not classified)
Ignored an exception in NumberSequenceClassifier: (result is that some numbers were not classified)
Ignored an exception in NumberSequenceClassifier: (result is that some numbers were not classified)
Ignored an exception in NumberSequenceClassifier: (result is that some numbers were not classified)
Ignored an exception in NumberSequenceClassifier: (result is that some numbers were not classified)
Ignored an exception in NumberSequenceClassifier: (result is that some numbers were not classified)
Ignored an exception in NumberSequenceClassifier: (result is that some numbers were not classified)
Ignored an exception in NumberSequenceClassifier: (result is that some numbers were not classified)
Ignored an exception in NumberSequenceClassifier: (result is that some numbers were not classified)
Ignored an exception in NumberSequenceClassifier: (result is that some numbers were not classified)
Ignored an exception in NumberSequenceClassifier: (result is that some numbers were not classified)
Ignored an exception in NumberSequenceClassifier: (result is that some numbers were not classified)
WARNING: no NER models specified
WARNING: no NER models specified
WARNING: no NER models specified
WARNING: no NER models specified
WARNING: no NER models specified
WARNING: no NER models specified
WARNING: no NER models specified
WARNING: no NER models specified
WARNING: no NER models specified
WARNING: no NER models specified
WARNING: no NER models specified
WARNING: no NER models specified
WARNING: no NER models specified
WARNING: no NER models specified
WARNING: no NER models specified
This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/CRF-NER.html">publicly released distribution</a>.
This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/CRF-NER.html">publicly released distribution</a>.
This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/CRF-NER.html">publicly released distribution</a>.
This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/CRF-NER.html">publicly released distribution</a>.
This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/CRF-NER.html">publicly released distribution</a>.
This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/CRF-NER.html">publicly released distribution</a>.
This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/CRF-NER.html">publicly released distribution</a>.
This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/CRF-NER.html">publicly released distribution</a>.
This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/CRF-NER.html">publicly released distribution</a>.
This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/CRF-NER.html">publicly released distribution</a>.
This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/CRF-NER.html">publicly released distribution</a>.
This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/CRF-NER.html">publicly released distribution</a>.
This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/CRF-NER.html">publicly released distribution</a>.
This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/CRF-NER.html">publicly released distribution</a>.
This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/CRF-NER.html">publicly released distribution</a>.
Before remove dupe, size=20
Before remove dupe, size=30
Before remove dupe, size=40
Before remove dupe, size=50
Before remove dupe, size=60
Before remove dupe, size=70
Before remove dupe, size=80
Before remove dupe, size=90
Before remove dupe, size=100
Before remove dupe, size=110
Before remove dupe, size=120
Before remove dupe, size=130
Before remove dupe, size=140
Built Scorer for 5000 words, clique pre=1 post=4
Built Scorer for 2000 words, clique pre=3 post=2
Built Scorer for 3000 words, clique pre=6 post=1
Built Scorer for 4000 words, clique pre=4 post=3
Built Scorer for 6000 words, clique pre=2 post=5
Built Scorer for 8000 words, clique pre=5 post=2
Built Scorer for 7000 words, clique pre=3 post=4
Built Scorer for 9000 words, clique pre=4 post=6
Built Scorer for 10000 words, clique pre=1 post=5
Built Scorer for 12000 words, clique pre=5 post=3
Built Scorer for 11000 words, clique pre=2 post=4
Built Scorer for 14000 words, clique pre=4 post=5
Built Scorer for 15000 words, clique pre=3 post=6
Built Scorer for 13000 words, clique pre=6 post=2
98%   412 hits per sec, position=007, legal=75
36%   89 hits per sec, position=013, legal=92
59%   204 hits per sec, position=002, legal=79
72%   312 hits per sec, position=009, legal=83
82%   375 hits per sec, position=015, legal=88
45%   148 hits per sec, position=006, legal=95
91%   408 hits per sec, position=011, legal=90
26%   68 hits per sec, position=003, legal=87
64%   253 hits per sec, position=010, legal=79
79%   349 hits per sec, position=014, legal=91
33%   101 hits per sec, position=005, legal=86
54%   185 hits per sec, position=012, legal=80
87%   389 hits per sec, position=004, legal=93
c_aTW=8, c_aT=3, smooth_aTW_aT=2.67, ## p(aTW|aT)=0.3
c_aTW=15, c_aT=9, smooth_aTW_aT=1.67, ## p(aTW|aT)=0.18
c_aTW=12, c_aT=7, smooth_aTW_aT=1.71, ## p(aTW|aT)=0.21
c_aTW=6, c_aT=2, smooth_aTW_aT=3, ## p(aTW|aT)=0.33
c_aTW=9, c_aT=4, smooth_aTW_aT=2.25, ## p(aTW|aT)=0.2
c_aTW=11, c_aT=6, smooth_aTW_aT=1.83, ## p(aTW|aT)=0.23
c_aTW=13, c_aT=8, smooth_aTW_aT=1.63, ## p(aTW|aT)=0.17
c_aTW=7, c_aT=3, smooth_aTW_aT=2.33, ## p(aTW|aT)=0.27
c_aTW=16, c_aT=10, smooth_aTW_aT=1.6, ## p(aTW|aT)=0.15
c_aTW=14, c_aT=7, smooth_aTW_aT=2, ## p(aTW|aT)=0.22
c_aTW=10, c_aT=4, smooth_aTW_aT=2.5, ## p(aTW|aT)=0.25
c_aTW=8, c_aT=3, smooth_aTW_aT=2.67, ## p(aTW|aT)=0.3
c_aTW=15, c_aT=9, smooth_aTW_aT=1.67, ## p(aTW|aT)=0.18
REWRITE 1
REWRITE 2
REWRITE 3
REWRITE 4
REWRITE 5
REWRITE 6
REWRITE 7
REWRITE 8
REWRITE 9
REWRITE 10
REWRITE 11
REWRITE 12
REWRITE 13
c_aPTW_aPT=7, c_aPT=3, smooth_aPTW_aPT=5, p(aPTW|aPT)=0.6
c_aPTW_aPT=12, c_aPT=6, smooth_aPTW_aPT=9, p(aPTW|aPT)=0.8
c_aPTW_aPT=9, c_aPT=4, smooth_aPTW_aPT=7, p(aPTW|aPT)=0.72
c_aPTW_aPT=11, c_aPT=5, smooth_aPTW_aPT=8, p(aPTW|aPT)=0.76
c_aPTW_aPT=6, c_aPT=3, smooth_aPTW_aPT=5, p(aPTW|aPT)=0.57
c_aPTW_aPT=13, c_aPT=7, smooth_aPTW_aPT=10, p(aPTW|aPT)=0.82
c_aPTW_aPT=8, c_aPT=4, smooth_aPTW_aPT=7, p(aPTW|aPT)=0.68
c_aPTW_aPT=12, c_aPT=6, smooth_aPTW_aPT=9, p(aPTW|aPT)=0.8
c_aPTW_aPT=7, c_aPT=3, smooth_aPTW_aPT=5, p(aPTW|aPT)=0.6
c_aPTW_aPT=11, c_aPT=5, smooth_aPTW_aPT=8, p(aPTW|aPT)=0.76
c_aPTW_aPT=5, c_aPT=2, smooth_aPTW_aPT=4, p(aPTW|aPT)=0.5
c_aPTW_aPT=13, c_aPT=7, smooth_aPTW_aPT=10, p(aPTW|aPT)=0.82
c_aPTW_aPT=9, c_aPT=4, smooth_aPTW_aPT=7, p(aPTW|aPT)=0.72
Error serializing to data.bin
Error serializing to backup.dat
Error serializing to config.json
Error serializing to image.jpg
Error serializing to log.txt
Error serializing to report.pdf
Error serializing to template.docx
Error serializing to settings.ini
Error serializing to output.csv
Error serializing to log.txt
Error serializing to database.db
Error serializing to backup.zip
Error serializing to index.html
Pattern = 'regex' generated 15 matches
Pattern = 'template' generated 30 matches
Pattern = 'string' generated 25 matches
Pattern = 'number' generated 18 matches
Pattern = 'email' generated 12 matches
Pattern = 'date' generated 17 matches
Pattern = 'password' generated 22 matches
Pattern = 'username' generated 27 matches
Pattern = 'phone' generated 19 matches
Pattern = 'address' generated 23 matches
Pattern = 'code' generated 16 matches
Pattern = 'country' generated 21 matches
Pattern = 'city' generated 26 matches
Pattern = 'language' generated 31 matches
Building reduced dataset...
Building reduced dataset...
Building reduced dataset...
Building reduced dataset...
Building reduced dataset...
Building reduced dataset...
Building reduced dataset...
Building reduced dataset...
Building reduced dataset...
Building reduced dataset...
Building reduced dataset...
Building reduced dataset...
Building reduced dataset...
Building reduced dataset...
Building reduced dataset...
Expand from patterns
Expand from patterns
Expand from patterns
Expand from patterns
Expand from patterns
Expand from patterns
Expand from patterns
Expand from patterns
Expand from patterns
Expand from patterns
Expand from patterns
Expand from patterns
Expand from patterns
Expand from patterns
Expand from patterns
applying thresholds...
applying thresholds...
applying thresholds...
applying thresholds...
applying thresholds...
applying thresholds...
applying thresholds...
applying thresholds...
applying thresholds...
applying thresholds...
applying thresholds...
applying thresholds...
applying thresholds...
applying thresholds...
applying thresholds...
c_aW=5, numWordTokens=172, p(aW)=0.029
c_aW=12, numWordTokens=241, p(aW)=0.050
c_aW=3, numWordTokens=113, p(aW)=0.027
c_aW=9, numWordTokens=185, p(aW)=0.049
c_aW=6, numWordTokens=197, p(aW)=0.032
c_aW=10, numWordTokens=217, p(aW)=0.046
c_aW=8, numWordTokens=176, p(aW)=0.045
c_aW=4, numWordTokens=129, p(aW)=0.031
c_aW=2, numWordTokens=81, p(aW)=0.025
c_aW=11, numWordTokens=233, p(aW)=0.048
c_aW=15, numWordTokens=284, p(aW)=0.053
c_aW=13, numWordTokens=255, p(aW)=0.051
c_aW=1, numWordTokens=41, p(aW)=0.022
(This is used when getting Dataset from adaptation set. We want to make the index consistent.)
(This is used when getting Dataset from adaptation set. We want to make the index consistent.)
(This is used when getting Dataset from adaptation set. We want to make the index consistent.)
(This is used when getting Dataset from adaptation set. We want to make the index consistent.)
(This is used when getting Dataset from adaptation set. We want to make the index consistent.)
(This is used when getting Dataset from adaptation set. We want to make the index consistent.)
(This is used when getting Dataset from adaptation set. We want to make the index consistent.)
(This is used when getting Dataset from adaptation set. We want to make the index consistent.)
(This is used when getting Dataset from adaptation set. We want to make the index consistent.)
(This is used when getting Dataset from adaptation set. We want to make the index consistent.)
(This is used when getting Dataset from adaptation set. We want to make the index consistent.)
(This is used when getting Dataset from adaptation set. We want to make the index consistent.)
(This is used when getting Dataset from adaptation set. We want to make the index consistent.)
(This is used when getting Dataset from adaptation set. We want to make the index consistent.)
(This is used when getting Dataset from adaptation set. We want to make the index consistent.)
Pre remove duplicates, num=5
Pre remove duplicates, num=20
Pre remove duplicates, num=8
Pre remove duplicates, num=15
Pre remove duplicates, num=3
Pre remove duplicates, num=12
Pre remove duplicates, num=6
Pre remove duplicates, num=18
Pre remove duplicates, num=9
Pre remove duplicates, num=14
Pre remove duplicates, num=7
Pre remove duplicates, num=17
Pre remove duplicates, num=11
Using feature/class Index from existing Dataset...
Using feature/class Index from existing Dataset...
Using feature/class Index from existing Dataset...
Using feature/class Index from existing Dataset...
Using feature/class Index from existing Dataset...
Using feature/class Index from existing Dataset...
Using feature/class Index from existing Dataset...
Using feature/class Index from existing Dataset...
Using feature/class Index from existing Dataset...
Using feature/class Index from existing Dataset...
Using feature/class Index from existing Dataset...
Using feature/class Index from existing Dataset...
Using feature/class Index from existing Dataset...
Using feature/class Index from existing Dataset...
Using feature/class Index from existing Dataset...
regular=graph2
regular=graph3
regular=graph4
regular=graph5
regular=graph6
regular=graph7
regular=graph8
regular=graph9
regular=graph10
regular=graph11
regular=graph12
regular=graph13
regular=graph14
Current best interp: GRU with score 0.92
Current best interp: Transformer with score 0.76
Current best interp: BiLSTM with score 0.81
Current best interp: CNN with score 0.68
Current best interp: MLP with score 0.95
Current best interp: RNN with score 0.78
Current best interp: Attention with score 0.89
Current best interp: GPT with score 0.93
Current best interp: LSTM with score 0.87
Current best interp: GRU with score 0.96
Current best interp: Transformer with score 0.79
Current best interp: BiLSTM with score 0.82
Current best interp: CNN with score 0.72
Starting retrain: # of original features789, # of original labels012
Starting retrain: # of original features345, # of original labels678
Starting retrain: # of original features901, # of original labels234
Starting retrain: # of original features567, # of original labels890
Starting retrain: # of original features123, # of original labels456
Starting retrain: # of original features789, # of original labels012
Starting retrain: # of original features345, # of original labels678
Starting retrain: # of original features901, # of original labels234
Starting retrain: # of original features567, # of original labels890
Starting retrain: # of original features123, # of original labels456
Starting retrain: # of original features789, # of original labels012
Starting retrain: # of original features345, # of original labels678
Starting retrain: # of original features901, # of original labels234
Tuning other parameters...
Tuning other parameters...
Tuning other parameters...
Tuning other parameters...
Tuning other parameters...
Tuning other parameters...
Tuning other parameters...
Tuning other parameters...
Tuning other parameters...
Tuning other parameters...
Tuning other parameters...
Tuning other parameters...
Tuning other parameters...
Tuning other parameters...
Tuning other parameters...
Tuning selected smooth_stop: 0.5
Tuning selected smooth_stop: 0.9
Tuning selected smooth_stop: 0.3
Tuning selected smooth_stop: 0.7
Tuning selected smooth_stop: 0.2
Tuning selected smooth_stop: 0.6
Tuning selected smooth_stop: 0.8
Tuning selected smooth_stop: 0.4
Tuning selected smooth_stop: 0.15
Tuning selected smooth_stop: 0.25
Tuning selected smooth_stop: 0.35
Tuning selected smooth_stop: 0.45
Tuning selected smooth_stop: 0.55
Tuning selected smooth_stop: 0.65
Tuning smooth_stop...
Tuning smooth_stop...
Tuning smooth_stop...
Tuning smooth_stop...
Tuning smooth_stop...
Tuning smooth_stop...
Tuning smooth_stop...
Tuning smooth_stop...
Tuning smooth_stop...
Tuning smooth_stop...
Tuning smooth_stop...
Tuning smooth_stop...
Tuning smooth_stop...
Tuning smooth_stop...
Tuning smooth_stop...
Var Sizes: [2, 4, 6, 8, 10]
Var Sizes: [3, 6, 9, 12, 15]
Var Sizes: [4, 8, 12, 16, 20]
Var Sizes: [5, 10, 15, 20, 25]
Var Sizes: [6, 12, 18, 24, 30]
Var Sizes: [7, 14, 21, 28, 35]
Var Sizes: [8, 16, 24, 32, 40]
Var Sizes: [9, 18, 27, 36, 45]
Var Sizes: [10, 20, 30, 40, 50]
Var Sizes: [11, 22, 33, 44, 55]
Var Sizes: [12, 24, 36, 48, 60]
Var Sizes: [13, 26, 39, 52, 65]
Var Sizes: [14, 28, 42, 56, 70]
stop counter 10
stop counter 3
stop counter 7
stop counter 12
stop counter 8
stop counter 2
stop counter 6
stop counter 9
stop counter 4
stop counter 11
stop counter 1
stop counter 15
stop counter 13
arg counter 1
arg counter 2
arg counter 3
arg counter 4
arg counter 5
arg counter 6
arg counter 7
arg counter 8
arg counter 9
arg counter 10
arg counter 11
arg counter 12
arg counter 13
com.example.parser.Parser: 32 basic symbols in the grammar
com.example.compiler.Compiler: 45 basic symbols in the grammar
com.example.interpreter.Interpreter: 55 basic symbols in the grammar
com.example.validator.Validator: 18 basic symbols in the grammar
com.example.generator.Generator: 37 basic symbols in the grammar
com.example.optimizer.Optimizer: 29 basic symbols in the grammar
com.example.transpiler.Transpiler: 64 basic symbols in the grammar
com.example.loader.Loader: 21 basic symbols in the grammar
com.example.analyzer.Analyzer: 41 basic symbols in the grammar
com.example.executor.Executor: 51 basic symbols in the grammar
com.example.serializer.Serializer: 33 basic symbols in the grammar
com.example.controller.Controller: 14 basic symbols in the grammar
com.example.logger.Logger: 38 basic symbols in the grammar
com.example.database.Database: 27 basic symbols in the grammar
Adding missing final punctuation to sentence.
Adding missing final punctuation to sentence.
Adding missing final punctuation to sentence.
Adding missing final punctuation to sentence.
Adding missing final punctuation to sentence.
Adding missing final punctuation to sentence.
Adding missing final punctuation to sentence.
Adding missing final punctuation to sentence.
Adding missing final punctuation to sentence.
Adding missing final punctuation to sentence.
Adding missing final punctuation to sentence.
Adding missing final punctuation to sentence.
Adding missing final punctuation to sentence.
Adding missing final punctuation to sentence.
Adding missing final punctuation to sentence.
Sentence too long for dependency parser.  Falling back to PCFG parse...
Sentence too long for lexparser parser.  Falling back to PCFG parse...
Sentence too long for constituency parser.  Falling back to PCFG parse...
Sentence too long for neural parser.  Falling back to PCFG parse...
Sentence too long for chart parser.  Falling back to PCFG parse...
Sentence too long for lexicalized parser.  Falling back to PCFG parse...
Sentence too long for shift-reduce parser.  Falling back to PCFG parse...
Sentence too long for probabilistic parser.  Falling back to PCFG parse...
Sentence too long for rule-based parser.  Falling back to PCFG parse...
Sentence too long for statistical parser.  Falling back to PCFG parse...
Sentence too long for machine learning parser.  Falling back to PCFG parse...
Sentence too long for deep learning parser.  Falling back to PCFG parse...
Sentence too long for hybrid parser.  Falling back to PCFG parse...
Sentence too long for ensemble parser.  Falling back to PCFG parse...
Sentence couldn't be parsed by grammar.... falling back to PCFG parse.
Sentence couldn't be parsed by grammar.... falling back to PCFG parse.
Sentence couldn't be parsed by grammar.... falling back to PCFG parse.
Sentence couldn't be parsed by grammar.... falling back to PCFG parse.
Sentence couldn't be parsed by grammar.... falling back to PCFG parse.
Sentence couldn't be parsed by grammar.... falling back to PCFG parse.
Sentence couldn't be parsed by grammar.... falling back to PCFG parse.
Sentence couldn't be parsed by grammar.... falling back to PCFG parse.
Sentence couldn't be parsed by grammar.... falling back to PCFG parse.
Sentence couldn't be parsed by grammar.... falling back to PCFG parse.
Sentence couldn't be parsed by grammar.... falling back to PCFG parse.
Sentence couldn't be parsed by grammar.... falling back to PCFG parse.
Sentence couldn't be parsed by grammar.... falling back to PCFG parse.
Sentence couldn't be parsed by grammar.... falling back to PCFG parse.
Sentence couldn't be parsed by grammar.... falling back to PCFG parse.
Sentence couldn't be parsed by grammar.
Sentence couldn't be parsed by grammar.
Sentence couldn't be parsed by grammar.
Sentence couldn't be parsed by grammar.
Sentence couldn't be parsed by grammar.
Sentence couldn't be parsed by grammar.
Sentence couldn't be parsed by grammar.
Sentence couldn't be parsed by grammar.
Sentence couldn't be parsed by grammar.
Sentence couldn't be parsed by grammar.
Sentence couldn't be parsed by grammar.
Sentence couldn't be parsed by grammar.
Sentence couldn't be parsed by grammar.
Sentence couldn't be parsed by grammar.
Sentence couldn't be parsed by grammar.
No memory to gather PCFG parse. Skipping...
No memory to gather PCFG parse. Skipping...
No memory to gather PCFG parse. Skipping...
No memory to gather PCFG parse. Skipping...
No memory to gather PCFG parse. Skipping...
No memory to gather PCFG parse. Skipping...
No memory to gather PCFG parse. Skipping...
No memory to gather PCFG parse. Skipping...
No memory to gather PCFG parse. Skipping...
No memory to gather PCFG parse. Skipping...
No memory to gather PCFG parse. Skipping...
No memory to gather PCFG parse. Skipping...
No memory to gather PCFG parse. Skipping...
No memory to gather PCFG parse. Skipping...
No memory to gather PCFG parse. Skipping...
Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...
Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...
Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...
Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...
Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...
Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...
Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...
Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...
Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...
Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...
Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...
Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...
Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...
Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...
Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...
Sentence too long (or zero words).
Sentence too long (or zero words).
Sentence too long (or zero words).
Sentence too long (or zero words).
Sentence too long (or zero words).
Sentence too long (or zero words).
Sentence too long (or zero words).
Sentence too long (or zero words).
Sentence too long (or zero words).
Sentence too long (or zero words).
Sentence too long (or zero words).
Sentence too long (or zero words).
Sentence too long (or zero words).
Sentence too long (or zero words).
Sentence too long (or zero words).
NOT ENOUGH MEMORY TO PARSE SENTENCES OF LENGTH
NOT ENOUGH MEMORY TO PARSE SENTENCES OF LENGTH
NOT ENOUGH MEMORY TO PARSE SENTENCES OF LENGTH
NOT ENOUGH MEMORY TO PARSE SENTENCES OF LENGTH
NOT ENOUGH MEMORY TO PARSE SENTENCES OF LENGTH
NOT ENOUGH MEMORY TO PARSE SENTENCES OF LENGTH
NOT ENOUGH MEMORY TO PARSE SENTENCES OF LENGTH
NOT ENOUGH MEMORY TO PARSE SENTENCES OF LENGTH
NOT ENOUGH MEMORY TO PARSE SENTENCES OF LENGTH
NOT ENOUGH MEMORY TO PARSE SENTENCES OF LENGTH
NOT ENOUGH MEMORY TO PARSE SENTENCES OF LENGTH
NOT ENOUGH MEMORY TO PARSE SENTENCES OF LENGTH
NOT ENOUGH MEMORY TO PARSE SENTENCES OF LENGTH
NOT ENOUGH MEMORY TO PARSE SENTENCES OF LENGTH
NOT ENOUGH MEMORY TO PARSE SENTENCES OF LENGTH
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser -train trainFilesPath [fileRange] -saveToSerializedFile serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser -train trainFilesPath [fileRange] -saveToSerializedFile serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser -train trainFilesPath [fileRange] -saveToSerializedFile serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser -train trainFilesPath [fileRange] -saveToSerializedFile serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser -train trainFilesPath [fileRange] -saveToSerializedFile serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser -train trainFilesPath [fileRange] -saveToSerializedFile serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser -train trainFilesPath [fileRange] -saveToSerializedFile serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser -train trainFilesPath [fileRange] -saveToSerializedFile serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser -train trainFilesPath [fileRange] -saveToSerializedFile serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser -train trainFilesPath [fileRange] -saveToSerializedFile serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser -train trainFilesPath [fileRange] -saveToSerializedFile serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser -train trainFilesPath [fileRange] -saveToSerializedFile serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser -train trainFilesPath [fileRange] -saveToSerializedFile serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser -train trainFilesPath [fileRange] -saveToSerializedFile serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser -train trainFilesPath [fileRange] -saveToSerializedFile serializedParserFilename
Usage: must specify a text grammar output path
Usage: must specify a text grammar output path
Usage: must specify a text grammar output path
Usage: must specify a text grammar output path
Usage: must specify a text grammar output path
Usage: must specify a text grammar output path
Usage: must specify a text grammar output path
Usage: must specify a text grammar output path
Usage: must specify a text grammar output path
Usage: must specify a text grammar output path
Usage: must specify a text grammar output path
Usage: must specify a text grammar output path
Usage: must specify a text grammar output path
Usage: must specify a text grammar output path
Usage: must specify a text grammar output path
Error loading parser, exiting...
Error loading parser, exiting...
Error loading parser, exiting...
Error loading parser, exiting...
Error loading parser, exiting...
Error loading parser, exiting...
Error loading parser, exiting...
Error loading parser, exiting...
Error loading parser, exiting...
Error loading parser, exiting...
Error loading parser, exiting...
Error loading parser, exiting...
Error loading parser, exiting...
Error loading parser, exiting...
Error loading parser, exiting...
No grammar specified, exiting...
No grammar specified, exiting...
No grammar specified, exiting...
No grammar specified, exiting...
No grammar specified, exiting...
No grammar specified, exiting...
No grammar specified, exiting...
No grammar specified, exiting...
No grammar specified, exiting...
No grammar specified, exiting...
No grammar specified, exiting...
No grammar specified, exiting...
No grammar specified, exiting...
No grammar specified, exiting...
No grammar specified, exiting...
Illegal accessNullPointerException
Illegal accessArrayIndexOutOfBoundsException
Illegal accessClassNotFoundException
Illegal accessIllegalArgumentException
Illegal accessIllegalStateException
Illegal accessNoSuchMethodException
Illegal accessNumberFormatException
Illegal accessOutOfMemoryError
Illegal accessStackOverflowError
Illegal accessStringIndexOutOfBoundsException
Illegal accessUnsupportedOperationException
Illegal accessFileNotFoundException
Illegal accessInterruptedException
Method not found: network.connect
Method not found: database.query
Method not found: utils.format
Method not found: math.calculate
Method not found: user.login
Method not found: validation.validate
Method not found: api.request
Method not found: encryption.encrypt
Method not found: logging.log
Method not found: email.send
Method not found: cache.retrieve
Method not found: security.validateToken
Method not found: scheduler.scheduleTask
Method not found: notification.sendNotification
Missing path: -saveToSerialized filename
Missing path: -saveToSerialized filename
Missing path: -saveToSerialized filename
Missing path: -saveToSerialized filename
Missing path: -saveToSerialized filename
Missing path: -saveToSerialized filename
Missing path: -saveToSerialized filename
Missing path: -saveToSerialized filename
Missing path: -saveToSerialized filename
Missing path: -saveToSerialized filename
Missing path: -saveToSerialized filename
Missing path: -saveToSerialized filename
Missing path: -saveToSerialized filename
Missing path: -saveToSerialized filename
Missing path: -saveToSerialized filename
Couldn't instantiate TokenizerFactory NGramTokenizerFactory with options {minGramSize=2, maxGramSize=4}
Couldn't instantiate TokenizerFactory WhitespaceTokenizerFactory with options {}
Couldn't instantiate TokenizerFactory RegexTokenizerFactory with options {pattern=\\w+}
Couldn't instantiate TokenizerFactory KeywordTokenizerFactory with options {lowercase=true}
Couldn't instantiate TokenizerFactory StandardTokenizerFactory with options {}
Couldn't instantiate TokenizerFactory EdgeNGramTokenizerFactory with options {minGramSize=1, maxGramSize=6}
Couldn't instantiate TokenizerFactory PathHierarchyTokenizerFactory with options {delimiter=/, replace=true}
Couldn't instantiate TokenizerFactory ClassicTokenizerFactory with options {}
Couldn't instantiate TokenizerFactory UAX29URLEmailTokenizerFactory with options {}
Couldn't instantiate TokenizerFactory ICUFoldingTokenizerFactory with options {}
Couldn't instantiate TokenizerFactory LetterTokenizerFactory with options {}
Couldn't instantiate TokenizerFactory ThaiTokenizerFactory with options {}
Couldn't instantiate TokenizerFactory CommongTermsTokenizerFactory with options {enablePositionIncrements=true}
Basic usage (see Javadoc for more): java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
Basic usage (see Javadoc for more): java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
Basic usage (see Javadoc for more): java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
Basic usage (see Javadoc for more): java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
Basic usage (see Javadoc for more): java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
Basic usage (see Javadoc for more): java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
Basic usage (see Javadoc for more): java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
Basic usage (see Javadoc for more): java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
Basic usage (see Javadoc for more): java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
Basic usage (see Javadoc for more): java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
Basic usage (see Javadoc for more): java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
Basic usage (see Javadoc for more): java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
Basic usage (see Javadoc for more): java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
Basic usage (see Javadoc for more): java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
Basic usage (see Javadoc for more): java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
Problem writing out binary trees.
Problem writing out binary trees.
Problem writing out binary trees.
Problem writing out binary trees.
Problem writing out binary trees.
Problem writing out binary trees.
Problem writing out binary trees.
Problem writing out binary trees.
Problem writing out binary trees.
Problem writing out binary trees.
Problem writing out binary trees.
Problem writing out binary trees.
Problem writing out binary trees.
Problem writing out binary trees.
Problem writing out binary trees.
Done training parser.
Done training parser.
Done training parser.
Done training parser.
Done training parser.
Done training parser.
Done training parser.
Done training parser.
Done training parser.
Done training parser.
Done training parser.
Done training parser.
Done training parser.
Done training parser.
Done training parser.
Loading parser from serialized file file2.dat ... done [1.2 sec].
Loading parser from serialized file file3.dat ... done [0.8 sec].
Loading parser from serialized file file4.dat ... done [2.3 sec].
Loading parser from serialized file file5.dat ... done [1.0 sec].
Loading parser from serialized file file6.dat ... done [0.7 sec].
Loading parser from serialized file file7.dat ... done [0.9 sec].
Loading parser from serialized file file8.dat ... done [0.3 sec].
Loading parser from serialized file file9.dat ... done [1.8 sec].
Loading parser from serialized file file10.dat ... done [0.6 sec].
Loading parser from serialized file file11.dat ... done [1.5 sec].
Loading parser from serialized file file12.dat ... done [1.3 sec].
Loading parser from serialized file file13.dat ... done [0.4 sec].
Loading parser from serialized file file14.dat ... done [1.6 sec].
Attempting to load grammar.txt as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load grammar.bin as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load grammar.dat as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load grammar.xml as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load grammar.html as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load grammar.json as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load grammar.csv as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load grammar.ini as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load grammar.yaml as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load grammar.docx as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load grammar.xls as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load grammar.pdf as a serialized grammar caused error below, but this may just be because it's a text grammar!
Attempting to load grammar.pptx as a serialized grammar caused error below, but this may just be because it's a text grammar!
Loading parser from text file data.xml ... done [0.56 sec].
Loading parser from text file config.ini ... done [0.12 sec].
Loading parser from text file script.js ... done [0.82 sec].
Loading parser from text file log.txt ... done [0.37 sec].
Loading parser from text file style.css ... done [1.15 sec].
Loading parser from text file index.html ... done [0.43 sec].
Loading parser from text file code.py ... done [0.97 sec].
Loading parser from text file data.json ... done [0.61 sec].
Loading parser from text file settings.ini ... done [0.19 sec].
Loading parser from text file script.py ... done [0.92 sec].
Loading parser from text file log.txt ... done [0.37 sec].
Loading parser from text file style.css ... done [1.15 sec].
Loading parser from text file index.html ... done [0.43 sec].
Observed Value: [4, 5, 6]
Observed Value: [7, 8, 9]
Observed Value: [10, 11, 12]
Observed Value: [13, 14, 15]
Observed Value: [16, 17, 18]
Observed Value: [19, 20, 21]
Observed Value: [22, 23, 24]
Observed Value: [25, 26, 27]
Observed Value: [28, 29, 30]
Observed Value: [31, 32, 33]
Observed Value: [34, 35, 36]
Observed Value: [37, 38, 39]
Observed Value: [40, 41, 42]
Assignment: [6, 7, 8, 9, 10]
Assignment: [11, 12, 13, 14, 15]
Assignment: [16, 17, 18, 19, 20]
Assignment: [21, 22, 23, 24, 25]
Assignment: [26, 27, 28, 29, 30]
Assignment: [31, 32, 33, 34, 35]
Assignment: [36, 37, 38, 39, 40]
Assignment: [41, 42, 43, 44, 45]
Assignment: [46, 47, 48, 49, 50]
Assignment: [51, 52, 53, 54, 55]
Assignment: [56, 57, 58, 59, 60]
Assignment: [61, 62, 63, 64, 65]
Assignment: [66, 67, 68, 69, 70]
Using NEWGENE threshold: 0.8
Using NEWGENE threshold: 0.3
Using NEWGENE threshold: 0.2
Using NEWGENE threshold: 0.7
Using NEWGENE threshold: 0.9
Using NEWGENE threshold: 0.6
Using NEWGENE threshold: 0.4
Using NEWGENE threshold: 0.1
Using NEWGENE threshold: 0.85
Using NEWGENE threshold: 0.25
Using NEWGENE threshold: 0.55
Using NEWGENE threshold: 0.75
Using NEWGENE threshold: 0.35
Joint marginals: [2, 4, 6, 8, 10]
Joint marginals: [3, 5, 7, 9, 11]
Joint marginals: [4, 6, 8, 10, 12]
Joint marginals: [5, 7, 9, 11, 13]
Joint marginals: [6, 8, 10, 12, 14]
Joint marginals: [7, 9, 11, 13, 15]
Joint marginals: [8, 10, 12, 14, 16]
Joint marginals: [9, 11, 13, 15, 17]
Joint marginals: [10, 12, 14, 16, 18]
Joint marginals: [11, 13, 15, 17, 19]
Joint marginals: [12, 14, 16, 18, 20]
Joint marginals: [13, 15, 17, 19, 21]
Joint marginals: [14, 16, 18, 20, 22]
<dog> chose <bone>
<bird> chose <seed>
<elephant> chose <banana>
<lion> chose <zebra>
<tiger> chose <prey>
<monkey> chose <banana>
<snake> chose <mouse>
<rabbit> chose <carrot>
<horse> chose <grass>
<cow> chose <hay>
<sheep> chose <grass>
<pig> chose <mud>
<bear> chose <fish>
Loading file tree2.txt
Loading file tree3.txt
Loading file tree4.txt
Loading file tree5.txt
Loading file tree6.txt
Loading file tree7.txt
Loading file tree8.txt
Loading file tree9.txt
Loading file tree10.txt
Loading file tree11.txt
Loading file tree12.txt
Loading file tree13.txt
Loading file tree14.txt
Result: [0.1, 0.4, 0.5]
Result: [0.3, 0.3, 0.4]
Result: [0.4, 0.2, 0.4]
Result: [0.2, 0.5, 0.3]
Result: [0.3, 0.1, 0.6]
Result: [0.5, 0.3, 0.2]
Result: [0.1, 0.2, 0.7]
Result: [0.3, 0.4, 0.3]
Result: [0.2, 0.2, 0.6]
Result: [0.4, 0.3, 0.3]
Result: [0.1, 0.6, 0.3]
Result: [0.5, 0.4, 0.1]
Result: [0.4, 0.5, 0.1]
ERROR:LEXICON image
INFO:LEXICON socket
WARNING:LEXICON message
WARNING:LEXICON test
ERROR:LEXICON data
INFO:LEXICON code
INFO:LEXICON log
ERROR:LEXICON document
WARNING:LEXICON client
ERROR:LEXICON server
INFO:LEXICON request
WARNING:LEXICON response
INFO:LEXICON command
Failed to match relation argument, so keeping gold: entity2
Failed to match relation argument, so keeping gold: entity3
Failed to match relation argument, so keeping gold: entity4
Failed to match relation argument, so keeping gold: entity5
Failed to match relation argument, so keeping gold: entity6
Failed to match relation argument, so keeping gold: entity7
Failed to match relation argument, so keeping gold: entity8
Failed to match relation argument, so keeping gold: entity9
Failed to match relation argument, so keeping gold: entity10
Failed to match relation argument, so keeping gold: entity11
Failed to match relation argument, so keeping gold: entity12
Failed to match relation argument, so keeping gold: entity13
Failed to match relation argument, so keeping gold: entity14
Arrays not equal! Variable 1
Arrays not equal! Variable 2
Arrays not equal! Variable 3
Arrays not equal! Variable 4
Arrays not equal! Variable 5
Arrays not equal! Variable 6
Arrays not equal! Variable 7
Arrays not equal! Variable 8
Arrays not equal! Variable 9
Arrays not equal! Variable 10
Arrays not equal! Variable 11
Arrays not equal! Variable 12
Arrays not equal! Variable 13
Connected component: 2
Connected component: 3
Connected component: 4
Connected component: 5
Connected component: 6
Connected component: 7
Connected component: 8
Connected component: 9
Connected component: 10
Connected component: 11
Connected component: 12
Connected component: 13
Connected component: 14
<INFO>OPTIONS
<ERROR>OPTIONS
<VERBOSE>OPTIONS
<DEBUG>OPTIONS
<CONFIG>OPTIONS
<CRITICAL>OPTIONS
<TRACE>OPTIONS
<NOTICE>OPTIONS
<ALERT>OPTIONS
<EMERGENCY>OPTIONS
<SUCCESS>OPTIONS
<FAILURE>OPTIONS
<START>OPTIONS
EVENT #2: ArrayIndexOutOfBoundsException
EVENT #3: FileNotFoundException
EVENT #4: InterruptedException
EVENT #5: ClassCastException
EVENT #6: IllegalStateException
EVENT #7: IllegalArgumentException
EVENT #8: ArithmeticException
EVENT #9: IndexOutOfBoundsException
EVENT #10: UnsupportedOperationException
EVENT #11: NoSuchElementException
EVENT #12: NumberFormatException
EVENT #13: StackOverflowError
EVENT #14: OutOfMemoryError
Trouble saving parser data to ASCII format.
Trouble saving parser data to ASCII format.
Trouble saving parser data to ASCII format.
Trouble saving parser data to ASCII format.
Trouble saving parser data to ASCII format.
Trouble saving parser data to ASCII format.
Trouble saving parser data to ASCII format.
Trouble saving parser data to ASCII format.
Trouble saving parser data to ASCII format.
Trouble saving parser data to ASCII format.
Trouble saving parser data to ASCII format.
Trouble saving parser data to ASCII format.
Trouble saving parser data to ASCII format.
Trouble saving parser data to ASCII format.
Trouble saving parser data to ASCII format.
EVENTS IN SENTENCE:
EVENTS IN SENTENCE:
EVENTS IN SENTENCE:
EVENTS IN SENTENCE:
EVENTS IN SENTENCE:
EVENTS IN SENTENCE:
EVENTS IN SENTENCE:
EVENTS IN SENTENCE:
EVENTS IN SENTENCE:
EVENTS IN SENTENCE:
EVENTS IN SENTENCE:
EVENTS IN SENTENCE:
EVENTS IN SENTENCE:
EVENTS IN SENTENCE:
EVENTS IN SENTENCE:
Writing parser in serialized format to file report.csv
Writing parser in serialized format to file graph.xml
Writing parser in serialized format to file code.py
Writing parser in serialized format to file data.json
Writing parser in serialized format to file log.txt
Writing parser in serialized format to file image.jpg
Writing parser in serialized format to file config.ini
Writing parser in serialized format to file backup.bak
Writing parser in serialized format to file template.html
Writing parser in serialized format to file archive.zip
Writing parser in serialized format to file log.txt
Writing parser in serialized format to file data.txt
Writing parser in serialized format to file config.yaml
undirected edges btw 2 and 4 is 8
undirected edges btw 2 and 4 is 10
undirected edges btw 2 and 4 is 3
undirected edges btw 2 and 4 is 12
undirected edges btw 2 and 4 is 7
undirected edges btw 2 and 4 is 9
undirected edges btw 2 and 4 is 6
undirected edges btw 2 and 4 is 2
undirected edges btw 2 and 4 is 11
undirected edges btw 2 and 4 is 4
undirected edges btw 2 and 4 is 15
undirected edges btw 2 and 4 is 13
undirected edges btw 2 and 4 is 1
undirected edges btw 1 and 3 is 7
undirected edges btw 1 and 3 is 2
undirected edges btw 1 and 3 is 9
undirected edges btw 1 and 3 is 4
undirected edges btw 1 and 3 is 6
undirected edges btw 1 and 3 is 8
undirected edges btw 1 and 3 is 3
undirected edges btw 1 and 3 is 1
undirected edges btw 1 and 3 is 10
undirected edges btw 1 and 3 is 12
undirected edges btw 1 and 3 is 14
undirected edges btw 1 and 3 is 11
undirected edges btw 1 and 3 is 13
PARENT #2: This is the second simulation log
PARENT #3: This is the third simulation log
PARENT #4: This is the fourth simulation log
PARENT #5: This is the fifth simulation log
PARENT #6: This is the sixth simulation log
PARENT #7: This is the seventh simulation log
PARENT #8: This is the eighth simulation log
PARENT #9: This is the ninth simulation log
PARENT #10: This is the tenth simulation log
PARENT #11: This is the eleventh simulation log
PARENT #12: This is the twelfth simulation log
PARENT #13: This is the thirteenth simulation log
PARENT #14: This is the fourteenth simulation log
undirected nodes btw 2 and 4 is CD
undirected nodes btw 2 and 4 is EF
undirected nodes btw 2 and 4 is GH
undirected nodes btw 2 and 4 is IJ
undirected nodes btw 2 and 4 is KL
undirected nodes btw 2 and 4 is MN
undirected nodes btw 2 and 4 is OP
undirected nodes btw 2 and 4 is QR
undirected nodes btw 2 and 4 is ST
undirected nodes btw 2 and 4 is UV
undirected nodes btw 2 and 4 is WX
undirected nodes btw 2 and 4 is YZ
undirected nodes btw 2 and 4 is 123
WARNING: whether or not to use extras
ERROR: whether or not to use extras
DEBUG: whether or not to use extras
INFO: whether or not to use extras
WARNING: whether or not to use extras
ERROR: whether or not to use extras
DEBUG: whether or not to use extras
INFO: whether or not to use extras
WARNING: whether or not to use extras
ERROR: whether or not to use extras
DEBUG: whether or not to use extras
INFO: whether or not to use extras
WARNING: whether or not to use extras
This event has multiple parents: parent2
This event has multiple parents: parent3
This event has multiple parents: parent4
This event has multiple parents: parent5
This event has multiple parents: parent6
This event has multiple parents: parent7
This event has multiple parents: parent8
This event has multiple parents: parent9
This event has multiple parents: parent10
This event has multiple parents: parent11
This event has multiple parents: parent12
This event has multiple parents: parent13
This event has multiple parents: parent14
Training a parser from treebank dir: /data/treebank/spanish
Training a parser from treebank dir: /data/treebank/french
Training a parser from treebank dir: /data/treebank/german
Training a parser from treebank dir: /data/treebank/italian
Training a parser from treebank dir: /data/treebank/japanese
Training a parser from treebank dir: /data/treebank/chinese
Training a parser from treebank dir: /data/treebank/russian
Training a parser from treebank dir: /data/treebank/arabic
Training a parser from treebank dir: /data/treebank/portuguese
Training a parser from treebank dir: /data/treebank/korean
Training a parser from treebank dir: /data/treebank/dutch
Training a parser from treebank dir: /data/treebank/swedish
Training a parser from treebank dir: /data/treebank/norwegian
Recovering using fall through strategy: will construct an (X ...) tree.
Recovering using fall through strategy: will construct an (X ...) tree.
Recovering using fall through strategy: will construct an (X ...) tree.
Recovering using fall through strategy: will construct an (X ...) tree.
Recovering using fall through strategy: will construct an (X ...) tree.
Recovering using fall through strategy: will construct an (X ...) tree.
Recovering using fall through strategy: will construct an (X ...) tree.
Recovering using fall through strategy: will construct an (X ...) tree.
Recovering using fall through strategy: will construct an (X ...) tree.
Recovering using fall through strategy: will construct an (X ...) tree.
Recovering using fall through strategy: will construct an (X ...) tree.
Recovering using fall through strategy: will construct an (X ...) tree.
Recovering using fall through strategy: will construct an (X ...) tree.
Recovering using fall through strategy: will construct an (X ...) tree.
Recovering using fall through strategy: will construct an (X ...) tree.
Following exception caught during parsing:
Following exception caught during parsing:
Following exception caught during parsing:
Following exception caught during parsing:
Following exception caught during parsing:
Following exception caught during parsing:
Following exception caught during parsing:
Following exception caught during parsing:
Following exception caught during parsing:
Following exception caught during parsing:
Following exception caught during parsing:
Following exception caught during parsing:
Following exception caught during parsing:
Following exception caught during parsing:
Following exception caught during parsing:
Done. Loaded 25 sentences.
Done. Loaded 50 sentences.
Done. Loaded 100 sentences.
Done. Loaded 200 sentences.
Done. Loaded 500 sentences.
Done. Loaded 1000 sentences.
Done. Loaded 2000 sentences.
Done. Loaded 5000 sentences.
Done. Loaded 10000 sentences.
Done. Loaded 20000 sentences.
Done. Loaded 50000 sentences.
Done. Loaded 100000 sentences.
Done. Loaded 200000 sentences.
Loaded 10 lattices
Loaded 15 lattices
Loaded 20 lattices
Loaded 25 lattices
Loaded 30 lattices
Loaded 35 lattices
Loaded 40 lattices
Loaded 45 lattices
Loaded 50 lattices
Loaded 55 lattices
Loaded 60 lattices
Loaded 65 lattices
Loaded 70 lattices
ERROR: a file of trees to process
INFO: a file of trees to process
DEBUG: a file of trees to process
SUCCESS: a file of trees to process
ALERT: a file of trees to process
NOTICE: a file of trees to process
FATAL: a file of trees to process
CRITICAL: a file of trees to process
IMPORTANT: a file of trees to process
LOG: a file of trees to process
MESSAGE: a file of trees to process
EVENT: a file of trees to process
EXCEPTION: a file of trees to process
<ERROR>: what mode for dependencies.  basic, collapsed, or ccprocessed.  To get 'noncollapsed', use basic with extras
<INFO>: what mode for dependencies.  basic, collapsed, or ccprocessed.  To get 'noncollapsed', use basic with extras
<DEBUG>: what mode for dependencies.  basic, collapsed, or ccprocessed.  To get 'noncollapsed', use basic with extras
<TRACE>: what mode for dependencies.  basic, collapsed, or ccprocessed.  To get 'noncollapsed', use basic with extras
<WARNING>: what mode for dependencies.  basic, collapsed, or ccprocessed.  To get 'noncollapsed', use basic with extras
<ERROR>: what mode for dependencies.  basic, collapsed, or ccprocessed.  To get 'noncollapsed', use basic with extras
<INFO>: what mode for dependencies.  basic, collapsed, or ccprocessed.  To get 'noncollapsed', use basic with extras
<DEBUG>: what mode for dependencies.  basic, collapsed, or ccprocessed.  To get 'noncollapsed', use basic with extras
<TRACE>: what mode for dependencies.  basic, collapsed, or ccprocessed.  To get 'noncollapsed', use basic with extras
<WARNING>: what mode for dependencies.  basic, collapsed, or ccprocessed.  To get 'noncollapsed', use basic with extras
<ERROR>: what mode for dependencies.  basic, collapsed, or ccprocessed.  To get 'noncollapsed', use basic with extras
<INFO>: what mode for dependencies.  basic, collapsed, or ccprocessed.  To get 'noncollapsed', use basic with extras
<DEBUG>: what mode for dependencies.  basic, collapsed, or ccprocessed.  To get 'noncollapsed', use basic with extras
WARNING: what pattern to use for matching
INFO: what pattern to use for matching
DEBUG: what pattern to use for matching
CRITICAL: what pattern to use for matching
ALERT: what pattern to use for matching
NOTICE: what pattern to use for matching
TRACE: what pattern to use for matching
FATAL: what pattern to use for matching
SUCCESS: what pattern to use for matching
FAILURE: what pattern to use for matching
INFO: what pattern to use for matching
WARNING: what pattern to use for matching
ERROR: what pattern to use for matching
Possible arguments for SemgrexPattern:
Possible arguments for SemgrexPattern:
Possible arguments for SemgrexPattern:
Possible arguments for SemgrexPattern:
Possible arguments for SemgrexPattern:
Possible arguments for SemgrexPattern:
Possible arguments for SemgrexPattern:
Possible arguments for SemgrexPattern:
Possible arguments for SemgrexPattern:
Possible arguments for SemgrexPattern:
Possible arguments for SemgrexPattern:
Possible arguments for SemgrexPattern:
Possible arguments for SemgrexPattern:
Possible arguments for SemgrexPattern:
Possible arguments for SemgrexPattern:
Loaded serialized sentences from /path/to/file2...
Loaded serialized sentences from /path/to/file3...
Loaded serialized sentences from /path/to/file4...
Loaded serialized sentences from /path/to/file5...
Loaded serialized sentences from /path/to/file6...
Loaded serialized sentences from /path/to/file7...
Loaded serialized sentences from /path/to/file8...
Loaded serialized sentences from /path/to/file9...
Loaded serialized sentences from /path/to/file10...
Loaded serialized sentences from /path/to/file11...
Loaded serialized sentences from /path/to/file12...
Loaded serialized sentences from /path/to/file13...
Loaded serialized sentences from /path/to/file14...
directed path edges btw 1 and 5 is 7
directed path edges btw 1 and 5 is 5
directed path edges btw 1 and 5 is 2
directed path edges btw 1 and 5 is 9
directed path edges btw 1 and 5 is 4
directed path edges btw 1 and 5 is 6
directed path edges btw 1 and 5 is 8
directed path edges btw 1 and 5 is 1
directed path edges btw 1 and 5 is 10
directed path edges btw 1 and 5 is 12
directed path edges btw 1 and 5 is 15
directed path edges btw 1 and 5 is 11
directed path edges btw 1 and 5 is 14
Done. Serialized 200 sentences.
Done. Serialized 300 sentences.
Done. Serialized 400 sentences.
Done. Serialized 500 sentences.
Done. Serialized 600 sentences.
Done. Serialized 700 sentences.
Done. Serialized 800 sentences.
Done. Serialized 900 sentences.
Done. Serialized 1000 sentences.
Done. Serialized 1100 sentences.
Done. Serialized 1200 sentences.
Done. Serialized 1300 sentences.
Done. Serialized 1400 sentences.
directed path edges btw 2 and 4 is 8
directed path edges btw 2 and 4 is 3
directed path edges btw 2 and 4 is 10
directed path edges btw 2 and 4 is 6
directed path edges btw 2 and 4 is 9
directed path edges btw 2 and 4 is 7
directed path edges btw 2 and 4 is 2
directed path edges btw 2 and 4 is 1
directed path edges btw 2 and 4 is 12
directed path edges btw 2 and 4 is 11
directed path edges btw 2 and 4 is 14
directed path edges btw 2 and 4 is 15
directed path edges btw 2 and 4 is 13
SemanticGraphUtils.replaceNode: previous node does not exist
SemanticGraphUtils.replaceNode: previous node does not exist
SemanticGraphUtils.replaceNode: previous node does not exist
SemanticGraphUtils.replaceNode: previous node does not exist
SemanticGraphUtils.replaceNode: previous node does not exist
SemanticGraphUtils.replaceNode: previous node does not exist
SemanticGraphUtils.replaceNode: previous node does not exist
SemanticGraphUtils.replaceNode: previous node does not exist
SemanticGraphUtils.replaceNode: previous node does not exist
SemanticGraphUtils.replaceNode: previous node does not exist
SemanticGraphUtils.replaceNode: previous node does not exist
SemanticGraphUtils.replaceNode: previous node does not exist
SemanticGraphUtils.replaceNode: previous node does not exist
SemanticGraphUtils.replaceNode: previous node does not exist
SemanticGraphUtils.replaceNode: previous node does not exist
Done. Parsed 20 sentences.
Done. Parsed 30 sentences.
Done. Parsed 40 sentences.
Done. Parsed 50 sentences.
Done. Parsed 60 sentences.
Done. Parsed 70 sentences.
Done. Parsed 80 sentences.
Done. Parsed 90 sentences.
Done. Parsed 100 sentences.
Done. Parsed 110 sentences.
Done. Parsed 120 sentences.
Done. Parsed 130 sentences.
Done. Parsed 140 sentences.
Save not implemented!
Save not implemented!
Save not implemented!
Save not implemented!
Save not implemented!
Save not implemented!
Save not implemented!
Save not implemented!
Save not implemented!
Save not implemented!
Save not implemented!
Save not implemented!
Save not implemented!
Save not implemented!
Save not implemented!
Usage: java SemanticGraph [-sentFile file|-treeFile file] [-testGraph]
Usage: java SemanticGraph [-sentFile file|-treeFile file] [-testGraph]
Usage: java SemanticGraph [-sentFile file|-treeFile file] [-testGraph]
Usage: java SemanticGraph [-sentFile file|-treeFile file] [-testGraph]
Usage: java SemanticGraph [-sentFile file|-treeFile file] [-testGraph]
Usage: java SemanticGraph [-sentFile file|-treeFile file] [-testGraph]
Usage: java SemanticGraph [-sentFile file|-treeFile file] [-testGraph]
Usage: java SemanticGraph [-sentFile file|-treeFile file] [-testGraph]
Usage: java SemanticGraph [-sentFile file|-treeFile file] [-testGraph]
Usage: java SemanticGraph [-sentFile file|-treeFile file] [-testGraph]
Usage: java SemanticGraph [-sentFile file|-treeFile file] [-testGraph]
Usage: java SemanticGraph [-sentFile file|-treeFile file] [-testGraph]
Usage: java SemanticGraph [-sentFile file|-treeFile file] [-testGraph]
Usage: java SemanticGraph [-sentFile file|-treeFile file] [-testGraph]
Usage: java SemanticGraph [-sentFile file|-treeFile file] [-testGraph]
<1> (<nodeTimes[1]>)--><2> (<nodeTimes[2]>) **
<2> (<nodeTimes[2]>)--><3> (<nodeTimes[3]>) **
<3> (<nodeTimes[3]>)--><4> (<nodeTimes[4]>) **
<4> (<nodeTimes[4]>)--><5> (<nodeTimes[5]>) **
<5> (<nodeTimes[5]>)--><6> (<nodeTimes[6]>) **
<6> (<nodeTimes[6]>)--><7> (<nodeTimes[7]>) **
<7> (<nodeTimes[7]>)--><8> (<nodeTimes[8]>) **
<8> (<nodeTimes[8]>)--><9> (<nodeTimes[9]>) **
<9> (<nodeTimes[9]>)--><10> (<nodeTimes[10]>) **
<10> (<nodeTimes[10]>)--><11> (<nodeTimes[11]>) **
<11> (<nodeTimes[11]>)--><12> (<nodeTimes[12]>) **
<12> (<nodeTimes[12]>)--><13> (<nodeTimes[13]>) **
<13> (<nodeTimes[13]>)--><14> (<nodeTimes[14]>) **
These sentences will be serialized to file:/path/to/anotherSerializedSentences
These sentences will be serialized to file:/path/to/thirdSerializedSentences
These sentences will be serialized to file:/path/to/fourthSerializedSentences
These sentences will be serialized to file:/path/to/fifthSerializedSentences
These sentences will be serialized to file:/path/to/sixthSerializedSentences
These sentences will be serialized to file:/path/to/seventhSerializedSentences
These sentences will be serialized to file:/path/to/eighthSerializedSentences
These sentences will be serialized to file:/path/to/ninthSerializedSentences
These sentences will be serialized to file:/path/to/tenthSerializedSentences
These sentences will be serialized to file:/path/to/eleventhSerializedSentences
These sentences will be serialized to file:/path/to/twelfthSerializedSentences
These sentences will be serialized to file:/path/to/thirteenthSerializedSentences
These sentences will be serialized to file:/path/to/fourteenthSerializedSentences
Parsing corpus sentences...
Parsing corpus sentences...
Parsing corpus sentences...
Parsing corpus sentences...
Parsing corpus sentences...
Parsing corpus sentences...
Parsing corpus sentences...
Parsing corpus sentences...
Parsing corpus sentences...
Parsing corpus sentences...
Parsing corpus sentences...
Parsing corpus sentences...
Parsing corpus sentences...
Parsing corpus sentences...
Parsing corpus sentences...
directed path edges btw 1 and 3 is edge2
directed path edges btw 1 and 3 is edge3
directed path edges btw 1 and 3 is edge4
directed path edges btw 1 and 3 is edge5
directed path edges btw 1 and 3 is edge6
directed path edges btw 1 and 3 is edge7
directed path edges btw 1 and 3 is edge8
directed path edges btw 1 and 3 is edge9
directed path edges btw 1 and 3 is edge10
directed path edges btw 1 and 3 is edge11
directed path edges btw 1 and 3 is edge12
directed path edges btw 1 and 3 is edge13
directed path edges btw 1 and 3 is edge14
Making result printers from ClassB
Making result printers from ClassC
Making result printers from ClassD
Making result printers from ClassE
Making result printers from ClassF
Making result printers from ClassG
Making result printers from ClassH
Making result printers from ClassI
Making result printers from ClassJ
Making result printers from ClassK
Making result printers from ClassL
Making result printers from ClassM
Making result printers from ClassN
Processing sentence: Lorem ipsum dolor sit amet, consectetur adipiscing elit
Processing sentence: Sed ut perspiciatis unde omnis iste natus error
Processing sentence: Et harum quidem rerum facilis est et expedita distinctio
Processing sentence: At vero eos et accusamus et iusto odio dignissimos ducimus
Processing sentence: Nam libero tempore, cum soluta nobis est eligendi optio
Processing sentence: Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus
Processing sentence: Itaque earum rerum hic tenetur a sapiente delectus
Processing sentence: Ut aut reiciendis voluptatibus maiores alias consequatur
Processing sentence: Nam libero tempore, cum soluta nobis est eligendi optio
Processing sentence: Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus
Processing sentence: Itaque earum rerum hic tenetur a sapiente delectus
Processing sentence: Ut aut reiciendis voluptatibus maiores alias consequatur
Processing sentence: Lorem ipsum dolor sit amet, consectetur adipiscing elit
DEBUG t=5
ERROR t=8
WARNING t=3
INFO  t=2
DEBUG t=6
ERROR t=7
WARNING t=4
INFO  t=9
DEBUG t=10
ERROR t=12
WARNING t=11
INFO  t=15
DEBUG t=14
TRAIN corpus size reduced from 50000 to 30000
TRAIN corpus size reduced from 200000 to 150000
TRAIN corpus size reduced from 120000 to 90000
TRAIN corpus size reduced from 300000 to 250000
TRAIN corpus size reduced from 25000 to 15000
TRAIN corpus size reduced from 180000 to 130000
TRAIN corpus size reduced from 90000 to 60000
TRAIN corpus size reduced from 150000 to 100000
TRAIN corpus size reduced from 70000 to 45000
TRAIN corpus size reduced from 220000 to 180000
TRAIN corpus size reduced from 40000 to 28000
TRAIN corpus size reduced from 280000 to 230000
TRAIN corpus size reduced from 60000 to 40000
Input File Error
Input File Error
Input File Error
Input File Error
Input File Error
Input File Error
Input File Error
Input File Error
Input File Error
Input File Error
Input File Error
Input File Error
Input File Error
Input File Error
Input File Error
Load not implemented!
Load not implemented!
Load not implemented!
Load not implemented!
Load not implemented!
Load not implemented!
Load not implemented!
Load not implemented!
Load not implemented!
Load not implemented!
Load not implemented!
Load not implemented!
Load not implemented!
Load not implemented!
Load not implemented!
Serializing parsed sentences to /path/to/file2...
Serializing parsed sentences to /path/to/file3...
Serializing parsed sentences to /path/to/file4...
Serializing parsed sentences to /path/to/file5...
Serializing parsed sentences to /path/to/file6...
Serializing parsed sentences to /path/to/file7...
Serializing parsed sentences to /path/to/file8...
Serializing parsed sentences to /path/to/file9...
Serializing parsed sentences to /path/to/file10...
Serializing parsed sentences to /path/to/file11...
Serializing parsed sentences to /path/to/file12...
Serializing parsed sentences to /path/to/file13...
Serializing parsed sentences to /path/to/file14...
score: 87
score: 92
score: 78
score: 85
score: 91
score: 81
score: 89
score: 93
score: 72
score: 90
score: 84
score: 88
score: 96
Number of minimized states: 50
Number of minimized states: 200
Number of minimized states: 75
Number of minimized states: 150
Number of minimized states: 125
Number of minimized states: 80
Number of minimized states: 180
Number of minimized states: 60
Number of minimized states: 90
Number of minimized states: 110
Number of minimized states: 70
Number of minimized states: 130
Number of minimized states: 160
Number of minimized states: 140
Number of minimized rules: 20
Number of minimized rules: 30
Number of minimized rules: 40
Number of minimized rules: 50
Number of minimized rules: 60
Number of minimized rules: 70
Number of minimized rules: 80
Number of minimized rules: 90
Number of minimized rules: 100
Number of minimized rules: 110
Number of minimized rules: 120
Number of minimized rules: 130
Number of minimized rules: 140
directed path nodes btw 1 and 3 is 9
directed path nodes btw 1 and 3 is 2
directed path nodes btw 1 and 3 is 7
directed path nodes btw 1 and 3 is 4
directed path nodes btw 1 and 3 is 6
directed path nodes btw 1 and 3 is 8
directed path nodes btw 1 and 3 is 3
directed path nodes btw 1 and 3 is 1
directed path nodes btw 1 and 3 is 0
directed path nodes btw 1 and 3 is 11
directed path nodes btw 1 and 3 is 10
directed path nodes btw 1 and 3 is 13
directed path nodes btw 1 and 3 is 12
Number of raw states: 150
Number of raw states: 200
Number of raw states: 250
Number of raw states: 300
Number of raw states: 350
Number of raw states: 400
Number of raw states: 450
Number of raw states: 500
Number of raw states: 550
Number of raw states: 600
Number of raw states: 650
Number of raw states: 700
Number of raw states: 750
WARNING: List of attributes is empty! Won't split any conjunctions.
WARNING: List of attributes is empty! Won't split any conjunctions.
WARNING: List of attributes is empty! Won't split any conjunctions.
WARNING: List of attributes is empty! Won't split any conjunctions.
WARNING: List of attributes is empty! Won't split any conjunctions.
WARNING: List of attributes is empty! Won't split any conjunctions.
WARNING: List of attributes is empty! Won't split any conjunctions.
WARNING: List of attributes is empty! Won't split any conjunctions.
WARNING: List of attributes is empty! Won't split any conjunctions.
WARNING: List of attributes is empty! Won't split any conjunctions.
WARNING: List of attributes is empty! Won't split any conjunctions.
WARNING: List of attributes is empty! Won't split any conjunctions.
WARNING: List of attributes is empty! Won't split any conjunctions.
WARNING: List of attributes is empty! Won't split any conjunctions.
WARNING: List of attributes is empty! Won't split any conjunctions.
SemanticGraphParser warning: IndexOutOfBoundsException
SemanticGraphParser warning: FileNotFoundException
SemanticGraphParser warning: RuntimeException
SemanticGraphParser warning: IllegalStateException
SemanticGraphParser warning: ArrayIndexOutOfBoundsException
SemanticGraphParser warning: ClassCastException
SemanticGraphParser warning: IllegalArgumentException
SemanticGraphParser warning: UnsupportedOperationException
SemanticGraphParser warning: StackOverflowError
SemanticGraphParser warning: OutOfMemoryError
SemanticGraphParser warning: ArithmeticException
SemanticGraphParser warning: SecurityException
SemanticGraphParser warning: NoSuchFieldException
Number of raw rules: 200
Number of raw rules: 300
Number of raw rules: 400
Number of raw rules: 500
Number of raw rules: 600
Number of raw rules: 700
Number of raw rules: 800
Number of raw rules: 900
Number of raw rules: 1000
Number of raw rules: 1100
Number of raw rules: 1200
Number of raw rules: 1300
Number of raw rules: 1400
Macro-averaged F1: 0.92
Macro-averaged F1: 0.78
Macro-averaged F1: 0.95
Macro-averaged F1: 0.86
Macro-averaged F1: 0.91
Macro-averaged F1: 0.79
Macro-averaged F1: 0.83
Macro-averaged F1: 0.88
Macro-averaged F1: 0.94
Macro-averaged F1: 0.81
Macro-averaged F1: 0.89
Macro-averaged F1: 0.93
Macro-averaged F1: 0.77
Prec: 0.89, Recall: 0.94, F1: 0.92
Prec: 0.62, Recall: 0.76, F1: 0.68
Prec: 0.81, Recall: 0.87, F1: 0.84
Prec: 0.93, Recall: 0.96, F1: 0.94
Prec: 0.59, Recall: 0.73, F1: 0.65
Prec: 0.78, Recall: 0.85, F1: 0.81
Prec: 0.88, Recall: 0.92, F1: 0.90
Prec: 0.67, Recall: 0.78, F1: 0.72
Prec: 0.79, Recall: 0.86, F1: 0.82
Prec: 0.98, Recall: 0.99, F1: 0.98
Prec: 0.73, Recall: 0.80, F1: 0.76
Prec: 0.85, Recall: 0.90, F1: 0.88
Prec: 0.71, Recall: 0.79, F1: 0.75
Using consistency checker: false
Using consistency checker: true
Using consistency checker: false
Using consistency checker: true
Using consistency checker: false
Using consistency checker: true
Using consistency checker: false
Using consistency checker: true
Using consistency checker: false
Using consistency checker: true
Using consistency checker: false
Using consistency checker: true
Using consistency checker: false
Using consistency checker: true
quote w/ no mention. ID: 456
quote w/ no mention. ID: 789
quote w/ no mention. ID: 012
quote w/ no mention. ID: 345
quote w/ no mention. ID: 678
quote w/ no mention. ID: 901
quote w/ no mention. ID: 234
quote w/ no mention. ID: 567
quote w/ no mention. ID: 890
quote w/ no mention. ID: 123
quote w/ no mention. ID: 456
quote w/ no mention. ID: 789
quote w/ no mention. ID: 012
There are 10 categories to compact.
There are 3 categories to compact.
There are 7 categories to compact.
There are 2 categories to compact.
There are 8 categories to compact.
There are 6 categories to compact.
There are 9 categories to compact.
There are 4 categories to compact.
There are 12 categories to compact.
There are 1 categories to compact.
There are 11 categories to compact.
There are 15 categories to compact.
There are 13 categories to compact.
Using relation extraction post processor: MachineLearningProperties.featureExtractionPostProcessorClass
Using relation extraction post processor: DataProcessingProperties.dataCleaningPostProcessorClass
Using relation extraction post processor: TextAnalysisProperties.sentimentAnalysisPostProcessorClass
Using relation extraction post processor: ImageProcessingProperties.objectRecognitionPostProcessorClass
Using relation extraction post processor: SpeechRecognitionProperties.speakerIdentificationPostProcessorClass
Using relation extraction post processor: NaturalLanguageProcessingProperties.namedEntityRecognitionPostProcessorClass
Using relation extraction post processor: VideoProcessingProperties.videoSummarizationPostProcessorClass
Using relation extraction post processor: MachineTranslationProperties.languageTranslationPostProcessorClass
Using relation extraction post processor: DataVisualizationProperties.dataVisualizerPostProcessorClass
Using relation extraction post processor: AudioProcessingProperties.audioTranscriptionPostProcessorClass
Using relation extraction post processor: TextGenerationProperties.textSummarizationPostProcessorClass
Using relation extraction post processor: DataMiningProperties.patternDiscoveryPostProcessorClass
Using relation extraction post processor: SpeechSynthesisProperties.speechEmotionRecognitionPostProcessorClass
Scanning with cache UNfriendly lookups took 200 ms
Scanning with cache UNfriendly lookups took 300 ms
Scanning with cache UNfriendly lookups took 400 ms
Scanning with cache UNfriendly lookups took 500 ms
Scanning with cache UNfriendly lookups took 600 ms
Scanning with cache UNfriendly lookups took 700 ms
Scanning with cache UNfriendly lookups took 800 ms
Scanning with cache UNfriendly lookups took 900 ms
Scanning with cache UNfriendly lookups took 1000 ms
Scanning with cache UNfriendly lookups took 1100 ms
Scanning with cache UNfriendly lookups took 1200 ms
Scanning with cache UNfriendly lookups took 1300 ms
Scanning with cache UNfriendly lookups took 1400 ms
Scanning with cache friendly lookups took 200 ms
Scanning with cache friendly lookups took 300 ms
Scanning with cache friendly lookups took 400 ms
Scanning with cache friendly lookups took 500 ms
Scanning with cache friendly lookups took 600 ms
Scanning with cache friendly lookups took 700 ms
Scanning with cache friendly lookups took 800 ms
Scanning with cache friendly lookups took 900 ms
Scanning with cache friendly lookups took 1000 ms
Scanning with cache friendly lookups took 1100 ms
Scanning with cache friendly lookups took 1200 ms
Scanning with cache friendly lookups took 1300 ms
Scanning with cache friendly lookups took 1400 ms
retained 8
retained 3
retained 10
retained 6
retained 2
retained 7
retained 9
retained 4
retained 1
retained 12
retained 15
retained 11
retained 13
Base directory: documents
Base directory: downloads
Base directory: pictures
Base directory: music
Base directory: videos
Base directory: desktop
Base directory: application
Base directory: projects
Base directory: temp
Base directory: config
Base directory: scripts
Base directory: backups
Base directory: logs
QuoteAttribution doCoreference: Null pronounCorefMap
QuoteAttribution doCoreference: Null pronounCorefMap
QuoteAttribution doCoreference: Null pronounCorefMap
QuoteAttribution doCoreference: Null pronounCorefMap
QuoteAttribution doCoreference: Null pronounCorefMap
QuoteAttribution doCoreference: Null pronounCorefMap
QuoteAttribution doCoreference: Null pronounCorefMap
QuoteAttribution doCoreference: Null pronounCorefMap
QuoteAttribution doCoreference: Null pronounCorefMap
QuoteAttribution doCoreference: Null pronounCorefMap
QuoteAttribution doCoreference: Null pronounCorefMap
QuoteAttribution doCoreference: Null pronounCorefMap
QuoteAttribution doCoreference: Null pronounCorefMap
QuoteAttribution doCoreference: Null pronounCorefMap
QuoteAttribution doCoreference: Null pronounCorefMap
after exact minimization graph has 100 arcs and 40 nodes.
after exact minimization graph has 150 arcs and 60 nodes.
after exact minimization graph has 200 arcs and 80 nodes.
after exact minimization graph has 250 arcs and 100 nodes.
after exact minimization graph has 300 arcs and 120 nodes.
after exact minimization graph has 350 arcs and 140 nodes.
after exact minimization graph has 400 arcs and 160 nodes.
after exact minimization graph has 450 arcs and 180 nodes.
after exact minimization graph has 500 arcs and 200 nodes.
after exact minimization graph has 550 arcs and 220 nodes.
after exact minimization graph has 600 arcs and 240 nodes.
after exact minimization graph has 650 arcs and 260 nodes.
after exact minimization graph has 700 arcs and 280 nodes.
MultiVector took 250 ms
MultiVector took 400 ms
MultiVector took 600 ms
MultiVector took 800 ms
MultiVector took 1000 ms
MultiVector took 1200 ms
MultiVector took 1400 ms
MultiVector took 1600 ms
MultiVector took 1800 ms
MultiVector took 2000 ms
MultiVector took 2200 ms
MultiVector took 2400 ms
MultiVector took 2600 ms
initial graph has 75 arcs and 150 nodes.
initial graph has 30 arcs and 60 nodes.
initial graph has 90 arcs and 180 nodes.
initial graph has 40 arcs and 80 nodes.
initial graph has 65 arcs and 130 nodes.
initial graph has 25 arcs and 50 nodes.
initial graph has 35 arcs and 70 nodes.
initial graph has 80 arcs and 160 nodes.
initial graph has 55 arcs and 110 nodes.
initial graph has 70 arcs and 140 nodes.
initial graph has 45 arcs and 90 nodes.
initial graph has 20 arcs and 40 nodes.
initial graph has 85 arcs and 170 nodes.
Extracting other paths...
Extracting other paths...
Extracting other paths...
Extracting other paths...
Extracting other paths...
Extracting other paths...
Extracting other paths...
Extracting other paths...
Extracting other paths...
Extracting other paths...
Extracting other paths...
Extracting other paths...
Extracting other paths...
Extracting other paths...
Extracting other paths...
2. Compacted grammar for dog from 200 arcs to 100 arcs.
3. Compacted grammar for bird from 150 arcs to 75 arcs.
4. Compacted grammar for fish from 80 arcs to 40 arcs.
5. Compacted grammar for rabbit from 120 arcs to 60 arcs.
6. Compacted grammar for lion from 90 arcs to 45 arcs.
7. Compacted grammar for tiger from 70 arcs to 35 arcs.
8. Compacted grammar for elephant from 180 arcs to 90 arcs.
9. Compacted grammar for monkey from 60 arcs to 30 arcs.
10. Compacted grammar for horse from 110 arcs to 55 arcs.
11. Compacted grammar for cow from 130 arcs to 65 arcs.
12. Compacted grammar for sheep from 140 arcs to 70 arcs.
13. Compacted grammar for pig from 75 arcs to 37 arcs.
14. Compacted grammar for bear from 160 arcs to 80 arcs.
Dataset construction took 50 ms
Dataset construction took 22 ms
Dataset construction took 15 ms
Dataset construction took 40 ms
Dataset construction took 30 ms
Dataset construction took 60 ms
Dataset construction took 28 ms
Dataset construction took 42 ms
Dataset construction took 55 ms
Dataset construction took 20 ms
Dataset construction took 38 ms
Dataset construction took 18 ms
Dataset construction took 25 ms
Dropped relation mention due to failed argument mapping: rm2
Dropped relation mention due to failed argument mapping: rm3
Dropped relation mention due to failed argument mapping: rm4
Dropped relation mention due to failed argument mapping: rm5
Dropped relation mention due to failed argument mapping: rm6
Dropped relation mention due to failed argument mapping: rm7
Dropped relation mention due to failed argument mapping: rm8
Dropped relation mention due to failed argument mapping: rm9
Dropped relation mention due to failed argument mapping: rm10
Dropped relation mention due to failed argument mapping: rm11
Dropped relation mention due to failed argument mapping: rm12
Dropped relation mention due to failed argument mapping: rm13
Dropped relation mention due to failed argument mapping: rm14
no name found :( 456
no name found :( 789
no name found :( abc
no name found :( def
no name found :( ghi
no name found :( 123
no name found :( 456
no name found :( 789
no name found :( abc
no name found :( def
no name found :( ghi
no name found :( 123
no name found :( 456
contingency name found character002
contingency name found character003
contingency name found character004
contingency name found character005
contingency name found character006
contingency name found character007
contingency name found character008
contingency name found character009
contingency name found character010
contingency name found character011
contingency name found character012
contingency name found character013
contingency name found character014
Usage: java QuoteAttributionTest familywordsfile animatefile gendernamesfile charactersfile modelfile
Usage: java QuoteAttributionTest familywordsfile animatefile gendernamesfile charactersfile modelfile
Usage: java QuoteAttributionTest familywordsfile animatefile gendernamesfile charactersfile modelfile
Usage: java QuoteAttributionTest familywordsfile animatefile gendernamesfile charactersfile modelfile
Usage: java QuoteAttributionTest familywordsfile animatefile gendernamesfile charactersfile modelfile
Usage: java QuoteAttributionTest familywordsfile animatefile gendernamesfile charactersfile modelfile
Usage: java QuoteAttributionTest familywordsfile animatefile gendernamesfile charactersfile modelfile
Usage: java QuoteAttributionTest familywordsfile animatefile gendernamesfile charactersfile modelfile
Usage: java QuoteAttributionTest familywordsfile animatefile gendernamesfile charactersfile modelfile
Usage: java QuoteAttributionTest familywordsfile animatefile gendernamesfile charactersfile modelfile
Usage: java QuoteAttributionTest familywordsfile animatefile gendernamesfile charactersfile modelfile
Usage: java QuoteAttributionTest familywordsfile animatefile gendernamesfile charactersfile modelfile
Usage: java QuoteAttributionTest familywordsfile animatefile gendernamesfile charactersfile modelfile
Usage: java QuoteAttributionTest familywordsfile animatefile gendernamesfile charactersfile modelfile
Usage: java QuoteAttributionTest familywordsfile animatefile gendernamesfile charactersfile modelfile
set2 left with: oranges
set2 left with: bananas
set2 left with: pears
set2 left with: grapes
set2 left with: watermelons
set2 left with: cherries
set2 left with: strawberries
set2 left with: peaches
set2 left with: mangoes
set2 left with: pineapples
set2 left with: coconuts
set2 left with: lemons
set2 left with: limes
Training took 200 ms
Training took 300 ms
Training took 400 ms
Training took 500 ms
Training took 600 ms
Training took 700 ms
Training took 800 ms
Training took 900 ms
Training took 1000 ms
Training took 1100 ms
Training took 1200 ms
Training took 1300 ms
Training took 1400 ms
set1 left with: oranges
set1 left with: bananas
set1 left with: pears
set1 left with: grapes
set1 left with: strawberries
set1 left with: watermelons
set1 left with: pineapples
set1 left with: mangos
set1 left with: peaches
set1 left with: cherries
set1 left with: lemons
set1 left with: kiwis
set1 left with: blueberries
set1 left with: raspberries
Loading event extraction model from model02 ...
Loading event extraction model from model03 ...
Loading event extraction model from model04 ...
Loading event extraction model from model05 ...
Loading event extraction model from model06 ...
Loading event extraction model from model07 ...
Loading event extraction model from model08 ...
Loading event extraction model from model09 ...
Loading event extraction model from model10 ...
Loading event extraction model from model11 ...
Loading event extraction model from model12 ...
Loading event extraction model from model13 ...
Loading event extraction model from model14 ...
Predicted Speaker: none
Predicted Speaker: none
Predicted Speaker: none
Predicted Speaker: none
Predicted Speaker: none
Predicted Speaker: none
Predicted Speaker: none
Predicted Speaker: none
Predicted Speaker: none
Predicted Speaker: none
Predicted Speaker: none
Predicted Speaker: none
Predicted Speaker: none
Predicted Speaker: none
Predicted Speaker: none
Not hypothesizing a boundary at pos 456, since between two numeral characters (4 and 5).
Not hypothesizing a boundary at pos 789, since between two numeral characters (7 and 8).
Not hypothesizing a boundary at pos 234, since between two numeral characters (2 and 3).
Not hypothesizing a boundary at pos 567, since between two numeral characters (5 and 6).
Not hypothesizing a boundary at pos 890, since between two numeral characters (8 and 9).
Not hypothesizing a boundary at pos 345, since between two numeral characters (3 and 4).
Not hypothesizing a boundary at pos 678, since between two numeral characters (6 and 7).
Not hypothesizing a boundary at pos 901, since between two numeral characters (9 and 0).
Not hypothesizing a boundary at pos 123, since between two numeral characters (1 and 2).
Not hypothesizing a boundary at pos 456, since between two numeral characters (4 and 5).
Not hypothesizing a boundary at pos 789, since between two numeral characters (7 and 8).
Not hypothesizing a boundary at pos 234, since between two numeral characters (2 and 3).
Not hypothesizing a boundary at pos 567, since between two numeral characters (5 and 6).
sizes different: 5 vs. 3
sizes different: 7 vs. 12
sizes different: 2 vs. 9
sizes different: 6 vs. 4
sizes different: 11 vs. 1
sizes different: 8 vs. 6
sizes different: 3 vs. 2
sizes different: 9 vs. 10
sizes different: 4 vs. 5
sizes different: 12 vs. 7
sizes different: 1 vs. 11
sizes different: 10 vs. 3
sizes different: 5 vs. 9
Predicted Mention: QuoteAttributionAnnotator.MentionAnnotation Predictor: quote.get(QuoteAttributionAnnotator.MentionSieveAnnotation.class)
Predicted Mention: QuoteAttributionAnnotator.MentionAnnotation Predictor: quote.get(QuoteAttributionAnnotator.MentionSieveAnnotation.class)
Predicted Mention: QuoteAttributionAnnotator.MentionAnnotation Predictor: quote.get(QuoteAttributionAnnotator.MentionSieveAnnotation.class)
Predicted Mention: QuoteAttributionAnnotator.MentionAnnotation Predictor: quote.get(QuoteAttributionAnnotator.MentionSieveAnnotation.class)
Predicted Mention: QuoteAttributionAnnotator.MentionAnnotation Predictor: quote.get(QuoteAttributionAnnotator.MentionSieveAnnotation.class)
Predicted Mention: QuoteAttributionAnnotator.MentionAnnotation Predictor: quote.get(QuoteAttributionAnnotator.MentionSieveAnnotation.class)
Predicted Mention: QuoteAttributionAnnotator.MentionAnnotation Predictor: quote.get(QuoteAttributionAnnotator.MentionSieveAnnotation.class)
Predicted Mention: QuoteAttributionAnnotator.MentionAnnotation Predictor: quote.get(QuoteAttributionAnnotator.MentionSieveAnnotation.class)
Predicted Mention: QuoteAttributionAnnotator.MentionAnnotation Predictor: quote.get(QuoteAttributionAnnotator.MentionSieveAnnotation.class)
Predicted Mention: QuoteAttributionAnnotator.MentionAnnotation Predictor: quote.get(QuoteAttributionAnnotator.MentionSieveAnnotation.class)
Predicted Mention: QuoteAttributionAnnotator.MentionAnnotation Predictor: quote.get(QuoteAttributionAnnotator.MentionSieveAnnotation.class)
Predicted Mention: QuoteAttributionAnnotator.MentionAnnotation Predictor: quote.get(QuoteAttributionAnnotator.MentionSieveAnnotation.class)
Predicted Mention: QuoteAttributionAnnotator.MentionAnnotation Predictor: quote.get(QuoteAttributionAnnotator.MentionSieveAnnotation.class)
Predicted Mention: none
Predicted Mention: none
Predicted Mention: none
Predicted Mention: none
Predicted Mention: none
Predicted Mention: none
Predicted Mention: none
Predicted Mention: none
Predicted Mention: none
Predicted Mention: none
Predicted Mention: none
Predicted Mention: none
Predicted Mention: none
Predicted Mention: none
Predicted Mention: none
left over: []
left over: [name, age, address]
left over: [item1, item2, item3, item4]
left over: [id1, id2, id3, id4, id5]
left over: [1, 2, 3, 4, 5, 6]
left over: [apple, banana, orange, strawberry]
left over: [true, false]
left over: [element1, element2, element3, element4, element5, element6]
left over: [A, B, C, D, E, F, G, H]
left over: [Monday, Tuesday, Wednesday, Thursday, Friday]
left over: [red, green, blue, yellow, orange]
left over: [user1, user2, user3, user4, user5, user6, user7]
left over: [100, 200, 300, 400, 500, 600, 700, 800, 900]
Quote: In the end, it's not the years in your life that count. It's the life in your years.
Quote: Be yourself; everyone else is already taken
Quote: Two things are infinite: the universe and human stupidity; and I'm not sure about the universe.
Quote: The greatest glory in living lies not in never falling, but in rising every time we fall
Quote: Life is what happens when you're busy making other plans
Quote: Get busy living or get busy dying
Quote: The only way to do great work is to love what you do
Quote: The best and most beautiful things in the world cannot be seen or even touched - they must be felt with the heart
Quote: Success is not final, failure is not fatal: It is the courage to continue that counts
Quote: The secret of success is to know something nobody else knows
Quote: Immortality is to live your life with others in mind
Quote: The only source of knowledge is experience
Quote: You miss 100% of the shots you don't take
no rule for rule2
no rule for rule3
no rule for rule4
no rule for rule5
no rule for rule6
no rule for rule7
no rule for rule8
no rule for rule9
no rule for rule10
no rule for rule11
no rule for rule12
no rule for rule13
no rule for rule14
Serializing event extraction model to model_2 ...
Serializing event extraction model to model_3 ...
Serializing event extraction model to model_4 ...
Serializing event extraction model to model_5 ...
Serializing event extraction model to model_6 ...
Serializing event extraction model to model_7 ...
Serializing event extraction model to model_8 ...
Serializing event extraction model to model_9 ...
Serializing event extraction model to model_10 ...
Serializing event extraction model to model_11 ...
Serializing event extraction model to model_12 ...
Serializing event extraction model to model_13 ...
Serializing event extraction model to model_14 ...
Training event extraction model...
Training event extraction model...
Training event extraction model...
Training event extraction model...
Training event extraction model...
Training event extraction model...
Training event extraction model...
Training event extraction model...
Training event extraction model...
Training event extraction model...
Training event extraction model...
Training event extraction model...
Training event extraction model...
Training event extraction model...
Training event extraction model...
Using properties file: application.properties
Using properties file: database.properties
Using properties file: settings.properties
Using properties file: connection.properties
Using properties file: environment.properties
Using properties file: logging.properties
Using properties file: resources.properties
Using properties file: security.properties
Using properties file: cache.properties
Using properties file: mail.properties
Using properties file: validation.properties
Using properties file: localization.properties
Using properties file: auth.properties
Usage: java QuoteAttributionEvaluation path_to_properties_file
Usage: java QuoteAttributionEvaluation path_to_properties_file
Usage: java QuoteAttributionEvaluation path_to_properties_file
Usage: java QuoteAttributionEvaluation path_to_properties_file
Usage: java QuoteAttributionEvaluation path_to_properties_file
Usage: java QuoteAttributionEvaluation path_to_properties_file
Usage: java QuoteAttributionEvaluation path_to_properties_file
Usage: java QuoteAttributionEvaluation path_to_properties_file
Usage: java QuoteAttributionEvaluation path_to_properties_file
Usage: java QuoteAttributionEvaluation path_to_properties_file
Usage: java QuoteAttributionEvaluation path_to_properties_file
Usage: java QuoteAttributionEvaluation path_to_properties_file
Usage: java QuoteAttributionEvaluation path_to_properties_file
Usage: java QuoteAttributionEvaluation path_to_properties_file
Usage: java QuoteAttributionEvaluation path_to_properties_file
Not hypothesizing a boundary at pos 9, since between two ASCII letters (x and y).
Not hypothesizing a boundary at pos 12, since between two ASCII letters (p and q).
Not hypothesizing a boundary at pos 3, since between two ASCII letters (n and m).
Not hypothesizing a boundary at pos 7, since between two ASCII letters (l and k).
Not hypothesizing a boundary at pos 10, since between two ASCII letters (e and f).
Not hypothesizing a boundary at pos 6, since between two ASCII letters (s and t).
Not hypothesizing a boundary at pos 15, since between two ASCII letters (d and c).
Not hypothesizing a boundary at pos 8, since between two ASCII letters (o and p).
Not hypothesizing a boundary at pos 11, since between two ASCII letters (r and s).
Not hypothesizing a boundary at pos 1, since between two ASCII letters (h and i).
Not hypothesizing a boundary at pos 4, since between two ASCII letters (f and g).
Not hypothesizing a boundary at pos 2, since between two ASCII letters (b and c).
Not hypothesizing a boundary at pos 14, since between two ASCII letters (w and x).
oldIndex.size()=7 newIndex.size()=2
oldIndex.size()=15 newIndex.size()=8
oldIndex.size()=3 newIndex.size()=0
oldIndex.size()=12 newIndex.size()=9
oldIndex.size()=20 newIndex.size()=13
oldIndex.size()=6 newIndex.size()=1
oldIndex.size()=9 newIndex.size()=4
oldIndex.size()=18 newIndex.size()=11
oldIndex.size()=14 newIndex.size()=7
oldIndex.size()=2 newIndex.size()=0
oldIndex.size()=16 newIndex.size()=10
oldIndex.size()=5 newIndex.size()=3
oldIndex.size()=11 newIndex.size()=6
Couldn't instantiate GrammarCompactor: Exception
Couldn't instantiate GrammarCompactor: NullPointerException
Couldn't instantiate GrammarCompactor: OutOfMemoryError
Couldn't instantiate GrammarCompactor: StackOverflowError
Couldn't instantiate GrammarCompactor: FileNotFoundException
Couldn't instantiate GrammarCompactor: IllegalArgumentException
Couldn't instantiate GrammarCompactor: IndexOutOfBoundsException
Couldn't instantiate GrammarCompactor: NoSuchMethodError
Couldn't instantiate GrammarCompactor: AssertionError
Couldn't instantiate GrammarCompactor: ClassCastException
Couldn't instantiate GrammarCompactor: ArrayIndexOutOfBoundsException
Couldn't instantiate GrammarCompactor: UnsupportedOperationException
Couldn't instantiate GrammarCompactor: IOException
Instantiating fsm.LossyGrammarCompactor
Instantiating fsm.LossyGrammarCompactor
Instantiating fsm.LossyGrammarCompactor
Instantiating fsm.LossyGrammarCompactor
Instantiating fsm.LossyGrammarCompactor
Instantiating fsm.LossyGrammarCompactor
Instantiating fsm.LossyGrammarCompactor
Instantiating fsm.LossyGrammarCompactor
Instantiating fsm.LossyGrammarCompactor
Instantiating fsm.LossyGrammarCompactor
Instantiating fsm.LossyGrammarCompactor
Instantiating fsm.LossyGrammarCompactor
Instantiating fsm.LossyGrammarCompactor
Instantiating fsm.LossyGrammarCompactor
Instantiating fsm.LossyGrammarCompactor
Instantiating fsm.CategoryMergingGrammarCompactor
Instantiating fsm.CategoryMergingGrammarCompactor
Instantiating fsm.CategoryMergingGrammarCompactor
Instantiating fsm.CategoryMergingGrammarCompactor
Instantiating fsm.CategoryMergingGrammarCompactor
Instantiating fsm.CategoryMergingGrammarCompactor
Instantiating fsm.CategoryMergingGrammarCompactor
Instantiating fsm.CategoryMergingGrammarCompactor
Instantiating fsm.CategoryMergingGrammarCompactor
Instantiating fsm.CategoryMergingGrammarCompactor
Instantiating fsm.CategoryMergingGrammarCompactor
Instantiating fsm.CategoryMergingGrammarCompactor
Instantiating fsm.CategoryMergingGrammarCompactor
Instantiating fsm.CategoryMergingGrammarCompactor
Instantiating fsm.CategoryMergingGrammarCompactor
Instantiating fsm.ExactGrammarCompactor
Instantiating fsm.ExactGrammarCompactor
Instantiating fsm.ExactGrammarCompactor
Instantiating fsm.ExactGrammarCompactor
Instantiating fsm.ExactGrammarCompactor
Instantiating fsm.ExactGrammarCompactor
Instantiating fsm.ExactGrammarCompactor
Instantiating fsm.ExactGrammarCompactor
Instantiating fsm.ExactGrammarCompactor
Instantiating fsm.ExactGrammarCompactor
Instantiating fsm.ExactGrammarCompactor
Instantiating fsm.ExactGrammarCompactor
Instantiating fsm.ExactGrammarCompactor
Instantiating fsm.ExactGrammarCompactor
Instantiating fsm.ExactGrammarCompactor
Invoked with arguments:
Invoked with arguments:
Invoked with arguments:
Invoked with arguments:
Invoked with arguments:
Invoked with arguments:
Invoked with arguments:
Invoked with arguments:
Invoked with arguments:
Invoked with arguments:
Invoked with arguments:
Invoked with arguments:
Invoked with arguments:
Invoked with arguments:
Invoked with arguments:
Training event extraction model(s)
Training event extraction model(s)
Training event extraction model(s)
Training event extraction model(s)
Training event extraction model(s)
Training event extraction model(s)
Training event extraction model(s)
Training event extraction model(s)
Training event extraction model(s)
Training event extraction model(s)
Training event extraction model(s)
Training event extraction model(s)
Training event extraction model(s)
Training event extraction model(s)
Training event extraction model(s)
Skipping non-boundary at pos 25, since space in the input.
Skipping non-boundary at pos 33, since space in the input.
Skipping non-boundary at pos 42, since space in the input.
Skipping non-boundary at pos 57, since space in the input.
Skipping non-boundary at pos 62, since space in the input.
Skipping non-boundary at pos 73, since space in the input.
Skipping non-boundary at pos 81, since space in the input.
Skipping non-boundary at pos 93, since space in the input.
Skipping non-boundary at pos 105, since space in the input.
Skipping non-boundary at pos 114, since space in the input.
Skipping non-boundary at pos 125, since space in the input.
Skipping non-boundary at pos 130, since space in the input.
Skipping non-boundary at pos 144, since space in the input.
Skipping non-boundary at pos 152, since space in the input.
Loading relation extraction model from model2 ...
Loading relation extraction model from model3 ...
Loading relation extraction model from model4 ...
Loading relation extraction model from model5 ...
Loading relation extraction model from model6 ...
Loading relation extraction model from model7 ...
Loading relation extraction model from model8 ...
Loading relation extraction model from model9 ...
Loading relation extraction model from model10 ...
Loading relation extraction model from model11 ...
Loading relation extraction model from model12 ...
Loading relation extraction model from model13 ...
Loading relation extraction model from model14 ...
Skipping transition notebook at pos 0.
Skipping transition algorithm at pos 0.
Skipping transition workspace at pos 0.
Skipping transition settings at pos 0.
Skipping transition session at pos 0.
Skipping transition debug at pos 0.
Skipping transition output at pos 0.
Skipping transition error at pos 0.
Skipping transition terminal at pos 0.
Skipping transition input at pos 0.
Skipping transition plugin at pos 0.
Skipping transition log at pos 0.
Skipping transition database at pos 0.
Skipping transition server at pos 0.
WARNING: Invalid unknown word signature! (com.anotherexample.ClassName)
WARNING: Invalid unknown word signature! (com.sample.ClassName)
WARNING: Invalid unknown word signature! (com.test.ClassName)
WARNING: Invalid unknown word signature! (com.demo.ClassName)
WARNING: Invalid unknown word signature! (com.example.ClassName)
WARNING: Invalid unknown word signature! (com.anotherexample.ClassName)
WARNING: Invalid unknown word signature! (com.sample.ClassName)
WARNING: Invalid unknown word signature! (com.test.ClassName)
WARNING: Invalid unknown word signature! (com.demo.ClassName)
WARNING: Invalid unknown word signature! (com.example.ClassName)
WARNING: Invalid unknown word signature! (com.anotherexample.ClassName)
WARNING: Invalid unknown word signature! (com.sample.ClassName)
WARNING: Invalid unknown word signature! (com.test.ClassName)
Training relation extraction model...
Training relation extraction model...
Training relation extraction model...
Training relation extraction model...
Training relation extraction model...
Training relation extraction model...
Training relation extraction model...
Training relation extraction model...
Training relation extraction model...
Training relation extraction model...
Training relation extraction model...
Training relation extraction model...
Training relation extraction model...
Training relation extraction model...
Training relation extraction model...
Training relation extraction model(s)
Training relation extraction model(s)
Training relation extraction model(s)
Training relation extraction model(s)
Training relation extraction model(s)
Training relation extraction model(s)
Training relation extraction model(s)
Training relation extraction model(s)
Training relation extraction model(s)
Training relation extraction model(s)
Training relation extraction model(s)
Training relation extraction model(s)
Training relation extraction model(s)
Training relation extraction model(s)
Training relation extraction model(s)
Predicted Mention: mention.get(QuoteAttributionAnnotator.NameAnnotation.class) INCORRECT
Predicted Mention: quote.get(QuoteAttributionAnnotator.DateAnnotation.class) INCORRECT
Predicted Mention: mention.get(QuoteAttributionAnnotator.OrganizationAnnotation.class) INCORRECT
Predicted Mention: quote.get(QuoteAttributionAnnotator.LocationAnnotation.class) INCORRECT
Predicted Mention: mention.get(QuoteAttributionAnnotator.TitleAnnotation.class) INCORRECT
Predicted Mention: quote.get(QuoteAttributionAnnotator.IDAnnotation.class) INCORRECT
Predicted Mention: mention.get(QuoteAttributionAnnotator.PersonAnnotation.class) INCORRECT
Predicted Mention: quote.get(QuoteAttributionAnnotator.SourceAnnotation.class) INCORRECT
Predicted Mention: mention.get(QuoteAttributionAnnotator.RoleAnnotation.class) INCORRECT
Predicted Mention: quote.get(QuoteAttributionAnnotator.EventAnnotation.class) INCORRECT
Predicted Mention: mention.get(QuoteAttributionAnnotator.RelationAnnotation.class) INCORRECT
Predicted Mention: quote.get(QuoteAttributionAnnotator.SubjectAnnotation.class) INCORRECT
Predicted Mention: mention.get(QuoteAttributionAnnotator.AttributeAnnotation.class) INCORRECT
INFO: Loaded 50 lines from sample.txt into MWE counter
INFO: Loaded 200 lines from input.txt into MWE counter
INFO: Loaded 75 lines from text.txt into MWE counter
INFO: Loaded 150 lines from content.txt into MWE counter
INFO: Loaded 80 lines from file.txt into MWE counter
INFO: Loaded 120 lines from script.txt into MWE counter
INFO: Loaded 90 lines from dataset.txt into MWE counter
INFO: Loaded 180 lines from records.txt into MWE counter
INFO: Loaded 60 lines from info.txt into MWE counter
INFO: Loaded 140 lines from logs.txt into MWE counter
INFO: Loaded 70 lines from output.txt into MWE counter
INFO: Loaded 110 lines from results.txt into MWE counter
INFO: Loaded 130 lines from document.txt into MWE counter
Grammar size: 200
Grammar size: 300
Grammar size: 400
Grammar size: 500
Grammar size: 600
Grammar size: 700
Grammar size: 800
Grammar size: 900
Grammar size: 1000
Grammar size: 1100
Grammar size: 1200
Grammar size: 1300
Grammar size: 1400
Loading entity extraction model from model2 ...
Loading entity extraction model from model3 ...
Loading entity extraction model from model4 ...
Loading entity extraction model from model5 ...
Loading entity extraction model from model6 ...
Loading entity extraction model from model7 ...
Loading entity extraction model from model8 ...
Loading entity extraction model from model9 ...
Loading entity extraction model from model10 ...
Loading entity extraction model from model11 ...
Loading entity extraction model from model12 ...
Loading entity extraction model from model13 ...
Loading entity extraction model from model14 ...
Output format: XML
Output format: CSV
Output format: Text
Output format: HTML
Output format: PDF
Output format: Excel
Output format: YAML
Output format: Markdown
Output format: Binary
Output format: Serialized
Output format: Compressed
Output format: Encrypted
Output format: Plain
Serializing entity extraction model to model2 ...
Serializing entity extraction model to model3 ...
Serializing entity extraction model to model4 ...
Serializing entity extraction model to model5 ...
Serializing entity extraction model to model6 ...
Serializing entity extraction model to model7 ...
Serializing entity extraction model to model8 ...
Serializing entity extraction model to model9 ...
Serializing entity extraction model to model10 ...
Serializing entity extraction model to model11 ...
Serializing entity extraction model to model12 ...
Serializing entity extraction model to model13 ...
Serializing entity extraction model to model14 ...
Training entity extraction model...
Training entity extraction model...
Training entity extraction model...
Training entity extraction model...
Training entity extraction model...
Training entity extraction model...
Training entity extraction model...
Training entity extraction model...
Training entity extraction model...
Training entity extraction model...
Training entity extraction model...
Training entity extraction model...
Training entity extraction model...
Training entity extraction model...
Training entity extraction model...
In partition #2
In partition #3
In partition #4
In partition #5
In partition #6
In partition #7
In partition #8
In partition #9
In partition #10
In partition #11
In partition #12
In partition #13
In partition #14
Training entity extraction model(s)
Training entity extraction model(s)
Training entity extraction model(s)
Training entity extraction model(s)
Training entity extraction model(s)
Training entity extraction model(s)
Training entity extraction model(s)
Training entity extraction model(s)
Training entity extraction model(s)
Training entity extraction model(s)
Training entity extraction model(s)
Training entity extraction model(s)
Training entity extraction model(s)
Training entity extraction model(s)
Training entity extraction model(s)
The reader log level is set to INFO
The reader log level is set to WARN
The reader log level is set to ERROR
The reader log level is set to TRACE
The reader log level is set to FATAL
The reader log level is set to OFF
The reader log level is set to VERBOSE
The reader log level is set to NOTICE
The reader log level is set to CRITICAL
The reader log level is set to EMERGENCY
The reader log level is set to ALERT
The reader log level is set to SEVERE
The reader log level is set to WARNING
INFO: dict2name specified | building NonDict2 from /path/to/file2
INFO: dict2name specified | building NonDict2 from /path/to/file3
INFO: dict2name specified | building NonDict2 from /path/to/file4
INFO: dict2name specified | building NonDict2 from /path/to/file5
INFO: dict2name specified | building NonDict2 from /path/to/file6
INFO: dict2name specified | building NonDict2 from /path/to/file7
INFO: dict2name specified | building NonDict2 from /path/to/file8
INFO: dict2name specified | building NonDict2 from /path/to/file9
INFO: dict2name specified | building NonDict2 from /path/to/file10
INFO: dict2name specified | building NonDict2 from /path/to/file11
INFO: dict2name specified | building NonDict2 from /path/to/file12
INFO: dict2name specified | building NonDict2 from /path/to/file13
INFO: dict2name specified | building NonDict2 from /path/to/file14
PCFG only: 3.8
PCFG only: 1.2
PCFG only: 4.6
PCFG only: 2.1
PCFG only: 3.3
PCFG only: 1.7
PCFG only: 4.2
PCFG only: 2.9
PCFG only: 3.5
PCFG only: 1.9
PCFG only: 4.0
PCFG only: 2.3
PCFG only: 3.1
PCFG only: 1.5
Blocked, using PCFG parse!
Blocked, using PCFG parse!
Blocked, using PCFG parse!
Blocked, using PCFG parse!
Blocked, using PCFG parse!
Blocked, using PCFG parse!
Blocked, using PCFG parse!
Blocked, using PCFG parse!
Blocked, using PCFG parse!
Blocked, using PCFG parse!
Blocked, using PCFG parse!
Blocked, using PCFG parse!
Blocked, using PCFG parse!
Blocked, using PCFG parse!
Blocked, using PCFG parse!
Speaker Tag: quote.get(QuoteAttributionAnnotator.SpeakerSieveAnnotation.class)
Speaker Tag: quote.get(QuoteAttributionAnnotator.SpeakerSieveAnnotation.class)
Speaker Tag: quote.get(QuoteAttributionAnnotator.SpeakerSieveAnnotation.class)
Speaker Tag: quote.get(QuoteAttributionAnnotator.SpeakerSieveAnnotation.class)
Speaker Tag: quote.get(QuoteAttributionAnnotator.SpeakerSieveAnnotation.class)
Speaker Tag: quote.get(QuoteAttributionAnnotator.SpeakerSieveAnnotation.class)
Speaker Tag: quote.get(QuoteAttributionAnnotator.SpeakerSieveAnnotation.class)
Speaker Tag: quote.get(QuoteAttributionAnnotator.SpeakerSieveAnnotation.class)
Speaker Tag: quote.get(QuoteAttributionAnnotator.SpeakerSieveAnnotation.class)
Speaker Tag: quote.get(QuoteAttributionAnnotator.SpeakerSieveAnnotation.class)
Speaker Tag: quote.get(QuoteAttributionAnnotator.SpeakerSieveAnnotation.class)
Speaker Tag: quote.get(QuoteAttributionAnnotator.SpeakerSieveAnnotation.class)
Speaker Tag: quote.get(QuoteAttributionAnnotator.SpeakerSieveAnnotation.class)
Time: 0.1 sec.
Time: 0.2 sec.
Time: 0.3 sec.
Time: 0.4 sec.
Time: 0.5 sec.
Time: 0.6 sec.
Time: 0.7 sec.
Time: 0.8 sec.
Time: 0.9 sec.
Time: 1.0 sec.
Time: 1.1 sec.
Time: 1.2 sec.
Time: 1.3 sec.
INFO: flags.usePk=true | building NonDict2 from /usr/local/data/file2.txt
INFO: flags.usePk=true | building NonDict2 from /usr/local/data/file3.txt
INFO: flags.usePk=true | building NonDict2 from /usr/local/data/file4.txt
INFO: flags.usePk=true | building NonDict2 from /usr/local/data/file5.txt
INFO: flags.usePk=true | building NonDict2 from /usr/local/data/file6.txt
INFO: flags.usePk=true | building NonDict2 from /usr/local/data/file7.txt
INFO: flags.usePk=true | building NonDict2 from /usr/local/data/file8.txt
INFO: flags.usePk=true | building NonDict2 from /usr/local/data/file9.txt
INFO: flags.usePk=true | building NonDict2 from /usr/local/data/file10.txt
INFO: flags.usePk=true | building NonDict2 from /usr/local/data/file11.txt
INFO: flags.usePk=true | building NonDict2 from /usr/local/data/file12.txt
INFO: flags.usePk=true | building NonDict2 from /usr/local/data/file13.txt
INFO: flags.usePk=true | building NonDict2 from /usr/local/data/file14.txt
INFO: flags.usePk=false | building NonDict2 from C:\Program Files\Data\input.csv
INFO: flags.usePk=false | building NonDict2 from /var/www/files/test.docx
INFO: flags.usePk=false | building NonDict2 from D:\Data\input.json
INFO: flags.usePk=false | building NonDict2 from /home/user/files/sample.xml
INFO: flags.usePk=false | building NonDict2 from E:\Data\input.txt
INFO: flags.usePk=false | building NonDict2 from /data/files/test.xls
INFO: flags.usePk=false | building NonDict2 from C:\Users\Documents\data.xml
INFO: flags.usePk=false | building NonDict2 from /var/www/html/index.html
INFO: flags.usePk=false | building NonDict2 from D:\Data\input.csv
INFO: flags.usePk=false | building NonDict2 from /usr/local/data/test.jpg
INFO: flags.usePk=false | building NonDict2 from C:\Program Files\Data\sample.pdf
INFO: flags.usePk=false | building NonDict2 from /home/user/files/input.doc
INFO: flags.usePk=false | building NonDict2 from E:\Data\sample.png
Speaker: Alice Mention: Bob
Speaker: David Mention: Sarah
Speaker: Michael Mention: Julia
Speaker: Robert Mention: Emily
Speaker: Laura Mention: Daniel
Speaker: Samantha Mention: James
Speaker: Benjamin Mention: Olivia
Speaker: Ethan Mention: Ava
Speaker: Sophia Mention: William
Speaker: Matthew Mention: Isabella
Speaker: Mia Mention: Alexander
Speaker: Jacob Mention: Harper
Speaker: Elizabeth Mention: Samuel
Error: no lexicon file!
Error: no lexicon file!
Error: no lexicon file!
Error: no lexicon file!
Error: no lexicon file!
Error: no lexicon file!
Error: no lexicon file!
Error: no lexicon file!
Error: no lexicon file!
Error: no lexicon file!
Error: no lexicon file!
Error: no lexicon file!
Error: no lexicon file!
Error: no lexicon file!
Error: no lexicon file!
latticeDensity: 0.3 cost: 15
latticeDensity: 0.8 cost: 8
latticeDensity: 0.2 cost: 20
latticeDensity: 0.7 cost: 12
latticeDensity: 0.4 cost: 18
latticeDensity: 0.1 cost: 25
latticeDensity: 0.9 cost: 6
latticeDensity: 0.6 cost: 9
latticeDensity: 0.3 cost: 17
latticeDensity: 0.2 cost: 13
latticeDensity: 0.8 cost: 7
latticeDensity: 0.7 cost: 14
latticeDensity: 0.5 cost: 16
PERCENTAGE OF TRAIN: 65
PERCENTAGE OF TRAIN: 90
PERCENTAGE OF TRAIN: 50
PERCENTAGE OF TRAIN: 70
PERCENTAGE OF TRAIN: 75
PERCENTAGE OF TRAIN: 95
PERCENTAGE OF TRAIN: 60
PERCENTAGE OF TRAIN: 85
PERCENTAGE OF TRAIN: 55
PERCENTAGE OF TRAIN: 45
PERCENTAGE OF TRAIN: 77
PERCENTAGE OF TRAIN: 82
PERCENTAGE OF TRAIN: 87
Annotating dataset with text_processor
Annotating dataset with audio_processor
Annotating dataset with video_processor
Annotating dataset with object_detection_processor
Annotating dataset with sentiment_analysis_processor
Annotating dataset with named_entity_recognition_processor
Annotating dataset with speech_recognition_processor
Annotating dataset with recommendation_processor
Annotating dataset with translation_processor
Annotating dataset with summarization_processor
Annotating dataset with classification_processor
Annotating dataset with regression_processor
Annotating dataset with clustering_processor
Found existing syntactic annotations. Will not use the NLP processor.
Found existing syntactic annotations. Will not use the NLP processor.
Found existing syntactic annotations. Will not use the NLP processor.
Found existing syntactic annotations. Will not use the NLP processor.
Found existing syntactic annotations. Will not use the NLP processor.
Found existing syntactic annotations. Will not use the NLP processor.
Found existing syntactic annotations. Will not use the NLP processor.
Found existing syntactic annotations. Will not use the NLP processor.
Found existing syntactic annotations. Will not use the NLP processor.
Found existing syntactic annotations. Will not use the NLP processor.
Found existing syntactic annotations. Will not use the NLP processor.
Found existing syntactic annotations. Will not use the NLP processor.
Found existing syntactic annotations. Will not use the NLP processor.
Found existing syntactic annotations. Will not use the NLP processor.
Found existing syntactic annotations. Will not use the NLP processor.
WARNING: blank line in lexicon
WARNING: blank line in lexicon
WARNING: blank line in lexicon
WARNING: blank line in lexicon
WARNING: blank line in lexicon
WARNING: blank line in lexicon
WARNING: blank line in lexicon
WARNING: blank line in lexicon
WARNING: blank line in lexicon
WARNING: blank line in lexicon
WARNING: blank line in lexicon
WARNING: blank line in lexicon
WARNING: blank line in lexicon
WARNING: blank line in lexicon
WARNING: blank line in lexicon
WARNING: word with space in lexicon
WARNING: word with space in lexicon
WARNING: word with space in lexicon
WARNING: word with space in lexicon
WARNING: word with space in lexicon
WARNING: word with space in lexicon
WARNING: word with space in lexicon
WARNING: word with space in lexicon
WARNING: word with space in lexicon
WARNING: word with space in lexicon
WARNING: word with space in lexicon
WARNING: word with space in lexicon
WARNING: word with space in lexicon
WARNING: word with space in lexicon
WARNING: word with space in lexicon
For cInfo.get(loc2), features: UTF-8
For cInfo.get(loc3), features: UTF-8
For cInfo.get(loc4), features: UTF-8
For cInfo.get(loc5), features: UTF-8
For cInfo.get(loc6), features: UTF-8
For cInfo.get(loc7), features: UTF-8
For cInfo.get(loc8), features: UTF-8
For cInfo.get(loc9), features: UTF-8
For cInfo.get(loc10), features: UTF-8
For cInfo.get(loc11), features: UTF-8
For cInfo.get(loc12), features: UTF-8
For cInfo.get(loc13), features: UTF-8
For cInfo.get(loc14), features: UTF-8
Loading model 2 for model merging from modelB
Loading model 3 for model merging from modelC
Loading model 4 for model merging from modelD
Loading model 5 for model merging from modelE
Loading model 6 for model merging from modelF
Loading model 7 for model merging from modelG
Loading model 8 for model merging from modelH
Loading model 9 for model merging from modelI
Loading model 10 for model merging from modelJ
Loading model 11 for model merging from modelK
Loading model 12 for model merging from modelL
Loading model 13 for model merging from modelM
Loading model 14 for model merging from modelN
Warning: No pretagging of sentences will be done.
Warning: No pretagging of sentences will be done.
Warning: No pretagging of sentences will be done.
Warning: No pretagging of sentences will be done.
Warning: No pretagging of sentences will be done.
Warning: No pretagging of sentences will be done.
Warning: No pretagging of sentences will be done.
Warning: No pretagging of sentences will be done.
Warning: No pretagging of sentences will be done.
Warning: No pretagging of sentences will be done.
Warning: No pretagging of sentences will be done.
Warning: No pretagging of sentences will be done.
Warning: No pretagging of sentences will be done.
Warning: No pretagging of sentences will be done.
Warning: No pretagging of sentences will be done.
Line 2 of file1.txt is emptyUTF-8
Line 3 of file1.txt is emptyUTF-8
Line 4 of file1.txt is emptyUTF-8
Line 5 of file1.txt is emptyUTF-8
Line 1 of file2.txt is emptyUTF-8
Line 2 of file2.txt is emptyUTF-8
Line 3 of file2.txt is emptyUTF-8
Line 4 of file2.txt is emptyUTF-8
Line 5 of file2.txt is emptyUTF-8
Line 1 of file3.txt is emptyUTF-8
Line 2 of file3.txt is emptyUTF-8
Line 3 of file3.txt is emptyUTF-8
Line 4 of file3.txt is emptyUTF-8
Serializing parser...
Serializing parser...
Serializing parser...
Serializing parser...
Serializing parser...
Serializing parser...
Serializing parser...
Serializing parser...
Serializing parser...
Serializing parser...
Serializing parser...
Serializing parser...
Serializing parser...
Serializing parser...
Serializing parser...
Tuning Dependency Model...
Tuning Dependency Model...
Tuning Dependency Model...
Tuning Dependency Model...
Tuning Dependency Model...
Tuning Dependency Model...
Tuning Dependency Model...
Tuning Dependency Model...
Tuning Dependency Model...
Tuning Dependency Model...
Tuning Dependency Model...
Tuning Dependency Model...
Tuning Dependency Model...
Tuning Dependency Model...
Tuning Dependency Model...
usage: java edu.stanford.nlp.process.WordToTaggedWordProcessor fileOrUrl
usage: java edu.stanford.nlp.process.WordToTaggedWordProcessor fileOrUrl
usage: java edu.stanford.nlp.process.WordToTaggedWordProcessor fileOrUrl
usage: java edu.stanford.nlp.process.WordToTaggedWordProcessor fileOrUrl
usage: java edu.stanford.nlp.process.WordToTaggedWordProcessor fileOrUrl
usage: java edu.stanford.nlp.process.WordToTaggedWordProcessor fileOrUrl
usage: java edu.stanford.nlp.process.WordToTaggedWordProcessor fileOrUrl
usage: java edu.stanford.nlp.process.WordToTaggedWordProcessor fileOrUrl
usage: java edu.stanford.nlp.process.WordToTaggedWordProcessor fileOrUrl
usage: java edu.stanford.nlp.process.WordToTaggedWordProcessor fileOrUrl
usage: java edu.stanford.nlp.process.WordToTaggedWordProcessor fileOrUrl
usage: java edu.stanford.nlp.process.WordToTaggedWordProcessor fileOrUrl
usage: java edu.stanford.nlp.process.WordToTaggedWordProcessor fileOrUrl
usage: java edu.stanford.nlp.process.WordToTaggedWordProcessor fileOrUrl
usage: java edu.stanford.nlp.process.WordToTaggedWordProcessor fileOrUrl
isOneSentence=false
isOneSentence=true
isOneSentence=false
isOneSentence=true
isOneSentence=false
isOneSentence=true
isOneSentence=false
isOneSentence=true
isOneSentence=false
isOneSentence=true
isOneSentence=false
isOneSentence=true
isOneSentence=false
Extractor 1 annotating dataset.
Extractor 2 annotating dataset.
Extractor 3 annotating dataset.
Extractor 4 annotating dataset.
Extractor 5 annotating dataset.
Extractor 6 annotating dataset.
Extractor 7 annotating dataset.
Extractor 8 annotating dataset.
Extractor 9 annotating dataset.
Extractor 10 annotating dataset.
Extractor 11 annotating dataset.
Extractor 12 annotating dataset.
Extractor 13 annotating dataset.
Extractor 0 annotating dataset.
Extractor 0 annotating dataset.
Extractor 0 annotating dataset.
Extractor 0 annotating dataset.
Extractor 0 annotating dataset.
Extractor 0 annotating dataset.
Extractor 0 annotating dataset.
Extractor 0 annotating dataset.
Extractor 0 annotating dataset.
Extractor 0 annotating dataset.
Extractor 0 annotating dataset.
Extractor 0 annotating dataset.
Extractor 0 annotating dataset.
Extractor 0 annotating dataset.
Extractor 0 annotating dataset.
Line 2 of file2.txt has leading/trailing whitespace: | Hello, world! |UTF-8
Line 3 of file3.txt has leading/trailing whitespace: |   Lorem ipsum dolor sit amet   |UTF-8
Line 4 of file4.txt has leading/trailing whitespace: |   example@gmail.com   |UTF-8
Line 5 of file5.txt has leading/trailing whitespace: |1234567890 |UTF-8
Line 6 of file6.txt has leading/trailing whitespace: |{ someKey: 'someValue' } |UTF-8
Line 7 of file7.txt has leading/trailing whitespace: |[] |UTF-8
Line 8 of file8.txt has leading/trailing whitespace: | (123) 456-7890 |UTF-8
Line 9 of file9.txt has leading/trailing whitespace: |\t\t\t\t\t\tText with tabs\t\t\t\t\t\t |UTF-8
Line 10 of file10.txt has leading/trailing whitespace: |   This line contains no whitespace   |UTF-8
Line 11 of file11.txt has leading/trailing whitespace: |            |UTF-8
Line 12 of file12.txt has leading/trailing whitespace: |----====++++====---- |UTF-8
Line 13 of file13.txt has leading/trailing whitespace: |<h1>Hello, world!</h1> |UTF-8
Line 14 of file14.txt has leading/trailing whitespace: |\u3000 Full-width whitespace \u3000|UTF-8
illegal accessIOException
illegal accessNullPointerException
illegal accessSecurityException
illegal accessIllegalArgumentException
illegal accessArrayIndexOutOfBoundsException
illegal accessClassCastException
illegal accessNoSuchMethodException
illegal accessNumberFormatException
illegal accessIllegalAccessException
illegal accessUnsupportedOperationException
illegal accessFileNotFoundException
illegal accessOutOfMemoryError
illegal accessStackOverflowError
Couldn't instantiate: DatabaseConnection: ClassNotFoundException
Couldn't instantiate: Configuration: IllegalArgumentException
Couldn't instantiate: Logger: IllegalAccessException
Couldn't instantiate: WebServer: NoSuchMethodException
Couldn't instantiate: UserAccount: InstantiationException
Couldn't instantiate: EmailSender: ArrayIndexOutOfBoundsException
Couldn't instantiate: CacheManager: NoClassDefFoundError
Couldn't instantiate: PaymentGateway: OutOfMemoryError
Couldn't instantiate: MessageQueue: StackOverflowError
Couldn't instantiate: TaskScheduler: InterruptedException
Couldn't instantiate: CSVParser: NumberFormatException
Couldn't instantiate: AuthenticationProvider: UnsupportedOperationException
Couldn't instantiate: EncryptionAlgorithm: ClassCastException
Loading character dictionary file from words.txt [done].
Loading character dictionary file from vocab.txt [done].
Loading character dictionary file from data.txt [done].
Loading character dictionary file from dictionary.txt [done].
Loading character dictionary file from lexicon.txt [done].
Loading character dictionary file from words_data.txt [done].
Loading character dictionary file from dict.txt [done].
Loading character dictionary file from char_data.txt [done].
Loading character dictionary file from word_list.txt [done].
Loading character dictionary file from char_vocab.txt [done].
Loading character dictionary file from word_data.txt [done].
Loading character dictionary file from frequency.txt [done].
Loading character dictionary file from char_list.txt [done].
ACCURACY: 87.5
ACCURACY: 92.1
ACCURACY: 99.9
ACCURACY: 88.3
ACCURACY: 91.7
ACCURACY: 96.5
ACCURACY: 85.6
ACCURACY: 93.2
ACCURACY: 97.8
ACCURACY: 90.4
ACCURACY: 86.1
ACCURACY: 94.7
ACCURACY: 98.6
Gold Mention: quote.get(QuoteAttributionAnnotator.MentionAnnotation.class) CORRECT
Gold Mention: quote.get(QuoteAttributionAnnotator.MentionAnnotation.class) CORRECT
Gold Mention: quote.get(QuoteAttributionAnnotator.MentionAnnotation.class) CORRECT
Gold Mention: quote.get(QuoteAttributionAnnotator.MentionAnnotation.class) CORRECT
Gold Mention: quote.get(QuoteAttributionAnnotator.MentionAnnotation.class) CORRECT
Gold Mention: quote.get(QuoteAttributionAnnotator.MentionAnnotation.class) CORRECT
Gold Mention: quote.get(QuoteAttributionAnnotator.MentionAnnotation.class) CORRECT
Gold Mention: quote.get(QuoteAttributionAnnotator.MentionAnnotation.class) CORRECT
Gold Mention: quote.get(QuoteAttributionAnnotator.MentionAnnotation.class) CORRECT
Gold Mention: quote.get(QuoteAttributionAnnotator.MentionAnnotation.class) CORRECT
Gold Mention: quote.get(QuoteAttributionAnnotator.MentionAnnotation.class) CORRECT
Gold Mention: quote.get(QuoteAttributionAnnotator.MentionAnnotation.class) CORRECT
Gold Mention: quote.get(QuoteAttributionAnnotator.MentionAnnotation.class) CORRECT
newlineIsSentenceBreak=false
newlineIsSentenceBreak=true
newlineIsSentenceBreak=false
newlineIsSentenceBreak=true
newlineIsSentenceBreak=false
newlineIsSentenceBreak=true
newlineIsSentenceBreak=false
newlineIsSentenceBreak=true
newlineIsSentenceBreak=false
newlineIsSentenceBreak=true
newlineIsSentenceBreak=false
newlineIsSentenceBreak=true
newlineIsSentenceBreak=false
FALSE NEGATIVE: object.toString()
FALSE NEGATIVE: result.toString()
FALSE NEGATIVE: data.toString()
FALSE NEGATIVE: value.toString()
FALSE NEGATIVE: list.toString()
FALSE NEGATIVE: entity.toString()
FALSE NEGATIVE: record.toString()
FALSE NEGATIVE: resource.toString()
FALSE NEGATIVE: config.toString()
FALSE NEGATIVE: obj.toString()
FALSE NEGATIVE: item.toString()
FALSE NEGATIVE: value.toString()
FALSE NEGATIVE: table.toString()
DICT: documentUTF-8
DICT: wordUTF-8
DICT: translationUTF-8
DICT: languageUTF-8
DICT: synonymUTF-8
DICT: meaningUTF-8
DICT: phraseUTF-8
DICT: definitionUTF-8
DICT: vocabularyUTF-8
DICT: pronunciationUTF-8
DICT: exampleUTF-8
DICT: etymologyUTF-8
DICT: usageUTF-8
In sentence: Another example sentence
In sentence: Lorem ipsum dolor sit amet
In sentence: Here is some dummy text
In sentence: A random sentence for simulation
In sentence: The quick brown fox jumps over the lazy dog
In sentence: This sentence is generated for simulation purposes
In sentence: Testing the log generation feature
In sentence: Simulated log content goes here
In sentence: Placeholder for a simulated log entry
In sentence: Generated log for testing purposes
In sentence: Simulating log entries for analysis
In sentence: A log entry to be simulated
In sentence: Randomly generated log content
In sentence: Simulated sentence in the log
Done. Unique words in ChineseDictionary is: 250.
Done. Unique words in ChineseDictionary is: 500.
Done. Unique words in ChineseDictionary is: 1000.
Done. Unique words in ChineseDictionary is: 1500.
Done. Unique words in ChineseDictionary is: 2000.
Done. Unique words in ChineseDictionary is: 2500.
Done. Unique words in ChineseDictionary is: 3000.
Done. Unique words in ChineseDictionary is: 3500.
Done. Unique words in ChineseDictionary is: 4000.
Done. Unique words in ChineseDictionary is: 4500.
Done. Unique words in ChineseDictionary is: 5000.
Done. Unique words in ChineseDictionary is: 5500.
Done. Unique words in ChineseDictionary is: 6000.
lastSentenceEndForced=false
lastSentenceEndForced=true
lastSentenceEndForced=true
lastSentenceEndForced=false
lastSentenceEndForced=false
lastSentenceEndForced=true
lastSentenceEndForced=false
lastSentenceEndForced=true
lastSentenceEndForced=false
lastSentenceEndForced=false
lastSentenceEndForced=true
lastSentenceEndForced=true
lastSentenceEndForced=true
Word is orange; in wait for forced end; debug
Word is banana; in wait for forced end; debug
Word is pineapple; in wait for forced end; debug
Word is mango; in wait for forced end; debug
Word is strawberry; in wait for forced end; debug
Word is watermelon; in wait for forced end; debug
Word is cherry; in wait for forced end; debug
Word is grape; in wait for forced end; debug
Word is lemon; in wait for forced end; debug
Word is lime; in wait for forced end; debug
Word is peach; in wait for forced end; debug
Word is pear; in wait for forced end; debug
Word is kiwi; in wait for forced end; debug
COMPARING PREDICTED MENTION: def
COMPARING PREDICTED MENTION: ghi
COMPARING PREDICTED MENTION: jkl
COMPARING PREDICTED MENTION: mno
COMPARING PREDICTED MENTION: pqr
COMPARING PREDICTED MENTION: stu
COMPARING PREDICTED MENTION: vwx
COMPARING PREDICTED MENTION: yza
COMPARING PREDICTED MENTION: bcd
COMPARING PREDICTED MENTION: efg
COMPARING PREDICTED MENTION: hij
COMPARING PREDICTED MENTION: klm
COMPARING PREDICTED MENTION: nop
Failed to load Chinese dictionary ./dict/dictionary02.ser NullPointerException
Failed to load Chinese dictionary ./dict/dictionary03.ser IOException
Failed to load Chinese dictionary ./dict/dictionary04.ser ClassNotFoundException
Failed to load Chinese dictionary ./dict/dictionary05.ser OutOfMemoryError
Failed to load Chinese dictionary ./dict/dictionary06.ser StackOverflowError
Failed to load Chinese dictionary ./dict/dictionary07.ser IllegalArgumentException
Failed to load Chinese dictionary ./dict/dictionary08.ser InterruptedException
Failed to load Chinese dictionary ./dict/dictionary09.ser IndexOutOfBoundsException
Failed to load Chinese dictionary ./dict/dictionary10.ser AssertionError
Failed to load Chinese dictionary ./dict/dictionary11.ser IllegalAccessError
Failed to load Chinese dictionary ./dict/dictionary12.ser NoSuchMethodError
Failed to load Chinese dictionary ./dict/dictionary13.ser NoClassDefFoundError
Failed to load Chinese dictionary ./dict/dictionary14.ser UnsatisfiedLinkError
SCORING THE FOLLOWING SENTENCE:
SCORING THE FOLLOWING SENTENCE:
SCORING THE FOLLOWING SENTENCE:
SCORING THE FOLLOWING SENTENCE:
SCORING THE FOLLOWING SENTENCE:
SCORING THE FOLLOWING SENTENCE:
SCORING THE FOLLOWING SENTENCE:
SCORING THE FOLLOWING SENTENCE:
SCORING THE FOLLOWING SENTENCE:
SCORING THE FOLLOWING SENTENCE:
SCORING THE FOLLOWING SENTENCE:
SCORING THE FOLLOWING SENTENCE:
SCORING THE FOLLOWING SENTENCE:
SCORING THE FOLLOWING SENTENCE:
SCORING THE FOLLOWING SENTENCE:
FailedNullPointerException
FailedIndexOutOfBoundsException
FailedIllegalArgumentException
FailedArithmeticException
FailedFileNotFoundException
FailedArrayIndexOutOfBoundsException
FailedClassCastException
FailedNoSuchElementException
FailedNumberFormatException
FailedIOException
FailedInterruptedException
FailedUnsupportedOperationException
FailedStringIndexOutOfBoundsException
FailedConcurrentModificationException
Word is banana; a discarded sentence boundary; newSentForced=true
Word is orange; a discarded sentence boundary; newSentForced=false
Word is mango; a discarded sentence boundary; newSentForced=true
Word is strawberry; a discarded sentence boundary; newSentForced=false
Word is pineapple; a discarded sentence boundary; newSentForced=true
Word is watermelon; a discarded sentence boundary; newSentForced=false
Word is grape; a discarded sentence boundary; newSentForced=true
Word is peach; a discarded sentence boundary; newSentForced=false
Word is kiwi; a discarded sentence boundary; newSentForced=true
Word is pear; a discarded sentence boundary; newSentForced=false
Word is cherry; a discarded sentence boundary; newSentForced=true
Word is lemon; a discarded sentence boundary; newSentForced=false
Word is lime; a discarded sentence boundary; newSentForced=true
This file contains NER tags not in the original Roth/Yih dataset, e.g.: location
This file contains NER tags not in the original Roth/Yih dataset, e.g.: organization
This file contains NER tags not in the original Roth/Yih dataset, e.g.: time
This file contains NER tags not in the original Roth/Yih dataset, e.g.: event
This file contains NER tags not in the original Roth/Yih dataset, e.g.: product
This file contains NER tags not in the original Roth/Yih dataset, e.g.: measure
This file contains NER tags not in the original Roth/Yih dataset, e.g.: currency
This file contains NER tags not in the original Roth/Yih dataset, e.g.: language
This file contains NER tags not in the original Roth/Yih dataset, e.g.: document
This file contains NER tags not in the original Roth/Yih dataset, e.g.: country
This file contains NER tags not in the original Roth/Yih dataset, e.g.: animal
This file contains NER tags not in the original Roth/Yih dataset, e.g.: plant
This file contains NER tags not in the original Roth/Yih dataset, e.g.: food
Word is banana; is XML break element; discarded
Word is cat; is XML break element; discarded
Word is dog; is XML break element; discarded
Word is elephant; is XML break element; discarded
Word is fish; is XML break element; discarded
Word is grape; is XML break element; discarded
Word is horse; is XML break element; discarded
Word is ice cream; is XML break element; discarded
Word is jellyfish; is XML break element; discarded
Word is kiwi; is XML break element; discarded
Word is lion; is XML break element; discarded
Word is monkey; is XML break element; discarded
Word is narwhal; is XML break element; discarded
Reading file: /home/user/documents/file2.txt
Reading file: /home/user/documents/file3.txt
Reading file: /home/user/documents/file4.txt
Reading file: /home/user/documents/file5.txt
Reading file: /home/user/documents/file6.txt
Reading file: /home/user/documents/file7.txt
Reading file: /home/user/documents/file8.txt
Reading file: /home/user/documents/file9.txt
Reading file: /home/user/documents/file10.txt
Reading file: /home/user/documents/file11.txt
Reading file: /home/user/documents/file12.txt
Reading file: /home/user/documents/file13.txt
Reading file: /home/user/documents/file14.txt
Usage: java edu.stanford.nlp.ie.machinereading.common.RobustTokenizer <file to tokenize>
Usage: java edu.stanford.nlp.ie.machinereading.common.RobustTokenizer <file to tokenize>
Usage: java edu.stanford.nlp.ie.machinereading.common.RobustTokenizer <file to tokenize>
Usage: java edu.stanford.nlp.ie.machinereading.common.RobustTokenizer <file to tokenize>
Usage: java edu.stanford.nlp.ie.machinereading.common.RobustTokenizer <file to tokenize>
Usage: java edu.stanford.nlp.ie.machinereading.common.RobustTokenizer <file to tokenize>
Usage: java edu.stanford.nlp.ie.machinereading.common.RobustTokenizer <file to tokenize>
Usage: java edu.stanford.nlp.ie.machinereading.common.RobustTokenizer <file to tokenize>
Usage: java edu.stanford.nlp.ie.machinereading.common.RobustTokenizer <file to tokenize>
Usage: java edu.stanford.nlp.ie.machinereading.common.RobustTokenizer <file to tokenize>
Usage: java edu.stanford.nlp.ie.machinereading.common.RobustTokenizer <file to tokenize>
Usage: java edu.stanford.nlp.ie.machinereading.common.RobustTokenizer <file to tokenize>
Usage: java edu.stanford.nlp.ie.machinereading.common.RobustTokenizer <file to tokenize>
Usage: java edu.stanford.nlp.ie.machinereading.common.RobustTokenizer <file to tokenize>
Usage: java edu.stanford.nlp.ie.machinereading.common.RobustTokenizer <file to tokenize>
Failed to handle |s2|
Failed to handle |s3|
Failed to handle |s4|
Failed to handle |s5|
Failed to handle |s6|
Failed to handle |s7|
Failed to handle |s8|
Failed to handle |s9|
Failed to handle |s10|
Failed to handle |s11|
Failed to handle |s12|
Failed to handle |s13|
Failed to handle |s14|
Word is dog; is sentence boundary; Another debug text
Word is mouse; is sentence boundary; More debug text
Word is chair; is sentence boundary; Additional debug text
Word is table; is sentence boundary; Extra debug text
Word is book; is sentence boundary; Debug text
Word is pen; is sentence boundary; Additional debug text
Word is laptop; is sentence boundary; Some debug text
Word is phone; is sentence boundary; More debug text
Word is car; is sentence boundary; Extra debug text
Word is tree; is sentence boundary; Another debug text
Word is flower; is sentence boundary; Debug text
Word is sky; is sentence boundary; Extra debug text
Word is sun; is sentence boundary; Some debug text
Finished loading proximity classes.
Finished loading proximity classes.
Finished loading proximity classes.
Finished loading proximity classes.
Finished loading proximity classes.
Finished loading proximity classes.
Finished loading proximity classes.
Finished loading proximity classes.
Finished loading proximity classes.
Finished loading proximity classes.
Finished loading proximity classes.
Finished loading proximity classes.
Finished loading proximity classes.
Finished loading proximity classes.
Finished loading proximity classes.
Error reading string
Error reading string
Error reading string
Error reading string
Error reading string
Error reading string
Error reading string
Error reading string
Error reading string
Error reading string
Error reading string
Error reading string
Error reading string
Error reading string
Error reading string
Warning: no proximity database found.
Warning: no proximity database found.
Warning: no proximity database found.
Warning: no proximity database found.
Warning: no proximity database found.
Warning: no proximity database found.
Warning: no proximity database found.
Warning: no proximity database found.
Warning: no proximity database found.
Warning: no proximity database found.
Warning: no proximity database found.
Warning: no proximity database found.
Warning: no proximity database found.
Warning: no proximity database found.
Warning: no proximity database found.
Loading proximity classes...
Loading proximity classes...
Loading proximity classes...
Loading proximity classes...
Loading proximity classes...
Loading proximity classes...
Loading proximity classes...
Loading proximity classes...
Loading proximity classes...
Loading proximity classes...
Loading proximity classes...
Loading proximity classes...
Loading proximity classes...
Loading proximity classes...
Loading proximity classes...
NO TAGGINGS: cat dog
NO TAGGINGS: car tree
NO TAGGINGS: book table
NO TAGGINGS: sun moon
NO TAGGINGS: flower grass
NO TAGGINGS: chair sofa
NO TAGGINGS: bird fish
NO TAGGINGS: mountain river
NO TAGGINGS: pen pencil
NO TAGGINGS: bread butter
NO TAGGINGS: phone computer
NO TAGGINGS: music dance
NO TAGGINGS: hat glove
Loading trigger-word gazetteer...
Loading trigger-word gazetteer...
Loading trigger-word gazetteer...
Loading trigger-word gazetteer...
Loading trigger-word gazetteer...
Loading trigger-word gazetteer...
Loading trigger-word gazetteer...
Loading trigger-word gazetteer...
Loading trigger-word gazetteer...
Loading trigger-word gazetteer...
Loading trigger-word gazetteer...
Loading trigger-word gazetteer...
Loading trigger-word gazetteer...
Loading trigger-word gazetteer...
Loading trigger-word gazetteer...
Generating report ... done [1.7 sec].
Updating database ... done [3.2 sec].
Compiling code ... done [0.9 sec].
Downloading file ... done [4.6 sec].
Sending email ... done [2.1 sec].
Running tests ... done [3.9 sec].
Installing software ... done [2.4 sec].
Calculating results ... done [1.3 sec].
Cleaning up ... done [0.6 sec].
Verifying credentials ... done [1.8 sec].
Loading configuration ... done [2.7 sec].
Encrypting data ... done [1.2 sec].
Searching files ... done [3.5 sec].
Loading last-name gazetteer...
Loading last-name gazetteer...
Loading last-name gazetteer...
Loading last-name gazetteer...
Loading last-name gazetteer...
Loading last-name gazetteer...
Loading last-name gazetteer...
Loading last-name gazetteer...
Loading last-name gazetteer...
Loading last-name gazetteer...
Loading last-name gazetteer...
Loading last-name gazetteer...
Loading last-name gazetteer...
Loading last-name gazetteer...
Loading last-name gazetteer...
done [4.2 sec].
done [2.8 sec].
done [1.9 sec].
done [5.6 sec].
done [2.1 sec].
done [4.7 sec].
done [3.2 sec].
done [2.6 sec].
done [4.9 sec].
done [3.8 sec].
done [2.3 sec].
done [4.1 sec].
done [2.4 sec].
Loading first-name gazetteer...
Loading first-name gazetteer...
Loading first-name gazetteer...
Loading first-name gazetteer...
Loading first-name gazetteer...
Loading first-name gazetteer...
Loading first-name gazetteer...
Loading first-name gazetteer...
Loading first-name gazetteer...
Loading first-name gazetteer...
Loading first-name gazetteer...
Loading first-name gazetteer...
Loading first-name gazetteer...
Loading first-name gazetteer...
Loading first-name gazetteer...
...Done! (5 events)
...Done! (7 events)
...Done! (3 events)
...Done! (8 events)
...Done! (12 events)
...Done! (6 events)
...Done! (9 events)
...Done! (4 events)
...Done! (11 events)
...Done! (2 events)
...Done! (14 events)
...Done! (1 events)
...Done! (13 events)
Loading location gazetteer...
Loading location gazetteer...
Loading location gazetteer...
Loading location gazetteer...
Loading location gazetteer...
Loading location gazetteer...
Loading location gazetteer...
Loading location gazetteer...
Loading location gazetteer...
Loading location gazetteer...
Loading location gazetteer...
Loading location gazetteer...
Loading location gazetteer...
Loading location gazetteer...
Loading location gazetteer...
Loading tuning set...
Loading tuning set...
Loading tuning set...
Loading tuning set...
Loading tuning set...
Loading tuning set...
Loading tuning set...
Loading tuning set...
Loading tuning set...
Loading tuning set...
Loading tuning set...
Loading tuning set...
Loading tuning set...
Loading tuning set...
Loading tuning set...
entering region
entering region
entering region
entering region
entering region
entering region
entering region
entering region
entering region
entering region
entering region
entering region
entering region
entering region
entering region
Collecting sufficient statistics for lexicon...
Collecting sufficient statistics for lexicon...
Collecting sufficient statistics for lexicon...
Collecting sufficient statistics for lexicon...
Collecting sufficient statistics for lexicon...
Collecting sufficient statistics for lexicon...
Collecting sufficient statistics for lexicon...
Collecting sufficient statistics for lexicon...
Collecting sufficient statistics for lexicon...
Collecting sufficient statistics for lexicon...
Collecting sufficient statistics for lexicon...
Collecting sufficient statistics for lexicon...
Collecting sufficient statistics for lexicon...
Collecting sufficient statistics for lexicon...
Collecting sufficient statistics for lexicon...
Word is banana; outside region; deleted
Word is cherry; outside region; deleted
Word is date; outside region; deleted
Word is eggplant; outside region; deleted
Word is fig; outside region; deleted
Word is grape; outside region; deleted
Word is honeydew; outside region; deleted
Word is ice cream; outside region; deleted
Word is jackfruit; outside region; deleted
Word is kiwi; outside region; deleted
Word is lemon; outside region; deleted
Word is mango; outside region; deleted
Word is nectarine; outside region; deleted
size: 20
size: 15
size: 5
size: 25
size: 18
size: 12
size: 7
size: 22
size: 9
size: 13
size: 14
size: 30
size: 8
Processed ACE document: doc2
Processed ACE document: doc3
Processed ACE document: doc4
Processed ACE document: doc5
Processed ACE document: doc6
Processed ACE document: doc7
Processed ACE document: doc8
Processed ACE document: doc9
Processed ACE document: doc10
Processed ACE document: doc11
Processed ACE document: doc12
Processed ACE document: doc13
Processed ACE document: doc14
Done! (15 trees)
Done! (20 trees)
Done! (25 trees)
Done! (30 trees)
Done! (35 trees)
Done! (40 trees)
Done! (45 trees)
Done! (50 trees)
Done! (55 trees)
Done! (60 trees)
Done! (65 trees)
Done! (70 trees)
Done! (75 trees)
<ERROR> done [4.32 sec].
<WARNING> done [7.88 sec].
<INFO> done [5.76 sec].
<DEBUG> done [2.95 sec].
<ERROR> done [6.21 sec].
<INFO> done [9.04 sec].
<WARNING> done [3.18 sec].
<DEBUG> done [8.67 sec].
<INFO> done [6.45 sec].
<ERROR> done [1.98 sec].
<WARNING> done [12.33 sec].
<INFO> done [3.79 sec].
<DEBUG> done [9.86 sec].
Usage: java AceDomReader <APF file>
Usage: java AceDomReader <APF file>
Usage: java AceDomReader <APF file>
Usage: java AceDomReader <APF file>
Usage: java AceDomReader <APF file>
Usage: java AceDomReader <APF file>
Usage: java AceDomReader <APF file>
Usage: java AceDomReader <APF file>
Usage: java AceDomReader <APF file>
Usage: java AceDomReader <APF file>
Usage: java AceDomReader <APF file>
Usage: java AceDomReader <APF file>
Usage: java AceDomReader <APF file>
Usage: java AceDomReader <APF file>
Usage: java AceDomReader <APF file>
Loading training trees...
Loading training trees...
Loading training trees...
Loading training trees...
Loading training trees...
Loading training trees...
Loading training trees...
Loading training trees...
Loading training trees...
Loading training trees...
Loading training trees...
Loading training trees...
Loading training trees...
Loading training trees...
Loading training trees...
<ERROR> ... Time elapsed: 4.7 sec
<WARNING> ... Time elapsed: 1.2 sec
<DEBUG> ... Time elapsed: 3.9 sec
<INFO> ... Time elapsed: 0.8 sec
<ERROR> ... Time elapsed: 5.2 sec
<WARNING> ... Time elapsed: 2.3 sec
<DEBUG> ... Time elapsed: 4.4 sec
<INFO> ... Time elapsed: 3.1 sec
<ERROR> ... Time elapsed: 2.6 sec
<WARNING> ... Time elapsed: 0.9 sec
<DEBUG> ... Time elapsed: 4.1 sec
<INFO> ... Time elapsed: 1.7 sec
<ERROR> ... Time elapsed: 3.8 sec
Cutting up: banana
Cutting up: carrot
Cutting up: durian
Cutting up: eggplant
Cutting up: fig
Cutting up: grape
Cutting up: honeydew
Cutting up: ice cream
Cutting up: jackfruit
Cutting up: kiwi
Cutting up: lemon
Cutting up: mango
Cutting up: nectarine
edu.stanford.nlp.process.WordShapeClassifier [-wordShape name] string+
edu.stanford.nlp.process.WordShapeClassifier [-wordShape name] string+
edu.stanford.nlp.process.WordShapeClassifier [-wordShape name] string+
edu.stanford.nlp.process.WordShapeClassifier [-wordShape name] string+
edu.stanford.nlp.process.WordShapeClassifier [-wordShape name] string+
edu.stanford.nlp.process.WordShapeClassifier [-wordShape name] string+
edu.stanford.nlp.process.WordShapeClassifier [-wordShape name] string+
edu.stanford.nlp.process.WordShapeClassifier [-wordShape name] string+
edu.stanford.nlp.process.WordShapeClassifier [-wordShape name] string+
edu.stanford.nlp.process.WordShapeClassifier [-wordShape name] string+
edu.stanford.nlp.process.WordShapeClassifier [-wordShape name] string+
edu.stanford.nlp.process.WordShapeClassifier [-wordShape name] string+
edu.stanford.nlp.process.WordShapeClassifier [-wordShape name] string+
edu.stanford.nlp.process.WordShapeClassifier [-wordShape name] string+
edu.stanford.nlp.process.WordShapeClassifier [-wordShape name] string+
ERROR Time elapsed: 280 ms
DEBUG Time elapsed: 50 ms
INFO Time elapsed: 380 ms
FATAL Time elapsed: 220 ms
TRACE Time elapsed: 120 ms
INFO Time elapsed: 200 ms
ERROR Time elapsed: 320 ms
WARNING Time elapsed: 90 ms
DEBUG Time elapsed: 180 ms
TRACE Time elapsed: 60 ms
FATAL Time elapsed: 250 ms
FATAL Time elapsed: 350 ms
INFO Time elapsed: 130 ms
The memory in use is 512MB
The memory in use is 1024MB
The memory in use is 2048MB
The memory in use is 4096MB
The memory in use is 8192MB
The memory in use is 16384MB
The memory in use is 32768MB
The memory in use is 65536MB
The memory in use is 131072MB
The memory in use is 262144MB
The memory in use is 524288MB
The memory in use is 1048576MB
The memory in use is 2097152MB
Discarding training example: cat NOUN
Discarding training example: dog VERB
Discarding training example: book NOUN
Discarding training example: run VERB
Discarding training example: tree NOUN
Discarding training example: chair NOUN
Discarding training example: jump VERB
Discarding training example: car NOUN
Discarding training example: eat VERB
Discarding training example: ball NOUN
Discarding training example: swim VERB
Discarding training example: flower NOUN
Discarding training example: sleep VERB
The PID is 456
The PID is 789
The PID is 987
The PID is 654
The PID is 321
The PID is 246
The PID is 135
The PID is 768
The PID is 984
The PID is 513
The PID is 642
The PID is 897
The PID is 432
CONVERTED ENTITY MENTION: Apple Inc.
CONVERTED ENTITY MENTION: New York
CONVERTED ENTITY MENTION: Soccer
CONVERTED ENTITY MENTION: 1234567890
CONVERTED ENTITY MENTION: Jane Doe
CONVERTED ENTITY MENTION: Microsoft
CONVERTED ENTITY MENTION: Los Angeles
CONVERTED ENTITY MENTION: Basketball
CONVERTED ENTITY MENTION: ABC Company
CONVERTED ENTITY MENTION: Chicago
CONVERTED ENTITY MENTION: Football
CONVERTED ENTITY MENTION: XYZ Corporation
CONVERTED ENTITY MENTION: San Francisco
The date is Tuesday
The date is Wednesday
The date is Thursday
The date is Friday
The date is Saturday
The date is Sunday
The date is January 1
The date is January 2
The date is January 3
The date is January 4
The date is January 5
The date is January 6
The date is January 7
The 56 open class tags are: [
The 12 open class tags are: [
The 41 open class tags are: [
The 34 open class tags are: [
The 17 open class tags are: [
The 23 open class tags are: [
The 45 open class tags are: [
The 9 open class tags are: [
The 38 open class tags are: [
The 51 open class tags are: [
The 62 open class tags are: [
The 14 open class tags are: [
The 30 open class tags are: [
The 29 open class tags are: [
CONVERTED MENTION HEAD SPAN: Mary
CONVERTED MENTION HEAD SPAN: David
CONVERTED MENTION HEAD SPAN: Emma
CONVERTED MENTION HEAD SPAN: Michael
CONVERTED MENTION HEAD SPAN: Sarah
CONVERTED MENTION HEAD SPAN: Benjamin
CONVERTED MENTION HEAD SPAN: Olivia
CONVERTED MENTION HEAD SPAN: Daniel
CONVERTED MENTION HEAD SPAN: Ava
CONVERTED MENTION HEAD SPAN: Jacob
CONVERTED MENTION HEAD SPAN: Emily
CONVERTED MENTION HEAD SPAN: Ethan
CONVERTED MENTION HEAD SPAN: Sophia
CAUGHT EXCEPTION ON SENTENCE ID: 2 (-1 if not known)
CAUGHT EXCEPTION ON SENTENCE ID: 3 (-1 if not known)
CAUGHT EXCEPTION ON SENTENCE ID: 4 (-1 if not known)
CAUGHT EXCEPTION ON SENTENCE ID: 5 (-1 if not known)
CAUGHT EXCEPTION ON SENTENCE ID: 6 (-1 if not known)
CAUGHT EXCEPTION ON SENTENCE ID: 7 (-1 if not known)
CAUGHT EXCEPTION ON SENTENCE ID: 8 (-1 if not known)
CAUGHT EXCEPTION ON SENTENCE ID: 9 (-1 if not known)
CAUGHT EXCEPTION ON SENTENCE ID: 10 (-1 if not known)
CAUGHT EXCEPTION ON SENTENCE ID: 11 (-1 if not known)
CAUGHT EXCEPTION ON SENTENCE ID: 12 (-1 if not known)
CAUGHT EXCEPTION ON SENTENCE ID: 13 (-1 if not known)
CAUGHT EXCEPTION ON SENTENCE ID: 14 (-1 if not known)
Reading document: presentation.ppt
Reading document: manual.pdf
Reading document: spreadsheet.xlsx
Reading document: article.txt
Reading document: proposal.docx
Reading document: thesis.pdf
Reading document: contract.doc
Reading document: guide.pptx
Reading document: script.txt
Reading document: agenda.doc
Reading document: code.java
Reading document: specification.pdf
Reading document: policy.docx
LCCS:          longestCommonContiguousSubstring(s[0], s[2])
LCCS:          longestCommonContiguousSubstring(s[0], s[3])
LCCS:          longestCommonContiguousSubstring(s[1], s[2])
LCCS:          longestCommonContiguousSubstring(s[1], s[3])
LCCS:          longestCommonContiguousSubstring(s[2], s[3])
LCCS:          longestCommonContiguousSubstring(s[0], s[4])
LCCS:          longestCommonContiguousSubstring(s[1], s[4])
LCCS:          longestCommonContiguousSubstring(s[2], s[4])
LCCS:          longestCommonContiguousSubstring(s[3], s[4])
LCCS:          longestCommonContiguousSubstring(s[0], s[5])
LCCS:          longestCommonContiguousSubstring(s[1], s[5])
LCCS:          longestCommonContiguousSubstring(s[2], s[5])
LCCS:          longestCommonContiguousSubstring(s[3], s[5])
Loaded 250 entries for dictionary "Spanish".
Loaded 50 entries for dictionary "French".
Loaded 500 entries for dictionary "German".
Loaded 150 entries for dictionary "Italian".
Loaded 300 entries for dictionary "Chinese".
Loaded 200 entries for dictionary "Japanese".
Loaded 400 entries for dictionary "Russian".
Loaded 75 entries for dictionary "Arabic".
Loaded 450 entries for dictionary "Portuguese".
Loaded 80 entries for dictionary "Korean".
Loaded 175 entries for dictionary "Dutch".
Loaded 320 entries for dictionary "Swedish".
Loaded 60 entries for dictionary "Norwegian".
Error reading file document.docx or writing file report.pdf: NullPointer Exception
Error reading file data.txt or writing file results.csv: IO Exception
Error reading file image.jpg or writing file audio.wav: Security Exception
Error reading file input.csv or writing file output.xlsx: Format Exception
Error reading file code.java or writing file executable.jar: ClassNotFound Exception
Error reading file text.txt or writing file output.txt: IllegalArgument Exception
Error reading file data.xml or writing file output.html: NumberFormat Exception
Error reading file input.txt or writing file output.txt: ArrayIndex OutOfBounds Exception
Error reading file image.png or writing file audio.mp3: ConcurrentModification Exception
Error reading file input.txt or writing file output.txt: Arithmetic Exception
Error reading file document.docx or writing file report.pdf: FileAlreadyExist Exception
Error reading file data.txt or writing file results.csv: FileNotFoundException
Error reading file image.jpg or writing file audio.wav: EOF Exception
LCS:           longestCommonSubstring(s[1], s[2])
LCS:           longestCommonSubstring(s[2], s[3])
LCS:           longestCommonSubstring(s[3], s[4])
LCS:           longestCommonSubstring(s[4], s[5])
LCS:           longestCommonSubstring(s[5], s[6])
LCS:           longestCommonSubstring(s[6], s[7])
LCS:           longestCommonSubstring(s[7], s[8])
LCS:           longestCommonSubstring(s[8], s[9])
LCS:           longestCommonSubstring(s[9], s[10])
LCS:           longestCommonSubstring(s[10], s[11])
LCS:           longestCommonSubstring(s[11], s[12])
LCS:           longestCommonSubstring(s[12], s[13])
LCS:           longestCommonSubstring(s[13], s[14])
Error reading file data.csv: FileNotFoundException
Error reading file config.properties: AccessDeniedException
Error reading file script.js: ParseException
Error reading file image.png: NullPointerException
Error reading file log.txt: OutOfMemoryError
Error reading file document.docx: InvalidFormatException
Error reading file video.mp4: MediaFormatException
Error reading file audio.wav: FormatNotSupportedException
Error reading file presentation.pptx: CorruptionException
Error reading file report.xlsx: SheetNotFoundException
Error reading file code.java: CompilationError
Error reading file style.css: InvalidSyntaxException
Error reading file template.html: TemplateNotFoundException
Error reading file database.db: ConnectionTimeoutException
Warning: no parse found in ExhaustivePCFGParser.extractBestParse
Warning: no parse found in ExhaustivePCFGParser.extractBestParse
Warning: no parse found in ExhaustivePCFGParser.extractBestParse
Warning: no parse found in ExhaustivePCFGParser.extractBestParse
Warning: no parse found in ExhaustivePCFGParser.extractBestParse
Warning: no parse found in ExhaustivePCFGParser.extractBestParse
Warning: no parse found in ExhaustivePCFGParser.extractBestParse
Warning: no parse found in ExhaustivePCFGParser.extractBestParse
Warning: no parse found in ExhaustivePCFGParser.extractBestParse
Warning: no parse found in ExhaustivePCFGParser.extractBestParse
Warning: no parse found in ExhaustivePCFGParser.extractBestParse
Warning: no parse found in ExhaustivePCFGParser.extractBestParse
Warning: no parse found in ExhaustivePCFGParser.extractBestParse
Warning: no parse found in ExhaustivePCFGParser.extractBestParse
Warning: no parse found in ExhaustivePCFGParser.extractBestParse
edit distance: 7
edit distance: 2
edit distance: 1
edit distance: 5
edit distance: 3
edit distance: 6
edit distance: 8
edit distance: 9
edit distance: 0
edit distance: 10
edit distance: 12
edit distance: 15
edit distance: 11
Skipped dependency-path features.
Skipped dependency-path features.
Skipped dependency-path features.
Skipped dependency-path features.
Skipped dependency-path features.
Skipped dependency-path features.
Skipped dependency-path features.
Skipped dependency-path features.
Skipped dependency-path features.
Skipped dependency-path features.
Skipped dependency-path features.
Skipped dependency-path features.
Skipped dependency-path features.
Skipped dependency-path features.
Skipped dependency-path features.
WARNING: failed to generate dependencies from tree <tree2>
WARNING: failed to generate dependencies from tree <tree3>
WARNING: failed to generate dependencies from tree <tree4>
WARNING: failed to generate dependencies from tree <tree5>
WARNING: failed to generate dependencies from tree <tree6>
WARNING: failed to generate dependencies from tree <tree7>
WARNING: failed to generate dependencies from tree <tree8>
WARNING: failed to generate dependencies from tree <tree9>
WARNING: failed to generate dependencies from tree <tree10>
WARNING: failed to generate dependencies from tree <tree11>
WARNING: failed to generate dependencies from tree <tree12>
WARNING: failed to generate dependencies from tree <tree13>
WARNING: failed to generate dependencies from tree <tree14>
WARNING: found sentence without TreeAnnotation. Skipped dependency-path features.
WARNING: found sentence without TreeAnnotation. Skipped dependency-path features.
WARNING: found sentence without TreeAnnotation. Skipped dependency-path features.
WARNING: found sentence without TreeAnnotation. Skipped dependency-path features.
WARNING: found sentence without TreeAnnotation. Skipped dependency-path features.
WARNING: found sentence without TreeAnnotation. Skipped dependency-path features.
WARNING: found sentence without TreeAnnotation. Skipped dependency-path features.
WARNING: found sentence without TreeAnnotation. Skipped dependency-path features.
WARNING: found sentence without TreeAnnotation. Skipped dependency-path features.
WARNING: found sentence without TreeAnnotation. Skipped dependency-path features.
WARNING: found sentence without TreeAnnotation. Skipped dependency-path features.
WARNING: found sentence without TreeAnnotation. Skipped dependency-path features.
WARNING: found sentence without TreeAnnotation. Skipped dependency-path features.
WARNING: found sentence without TreeAnnotation. Skipped dependency-path features.
WARNING: found sentence without TreeAnnotation. Skipped dependency-path features.
Adding gender annotations!
Adding gender annotations!
Adding gender annotations!
Adding gender annotations!
Adding gender annotations!
Adding gender annotations!
Adding gender annotations!
Adding gender annotations!
Adding gender annotations!
Adding gender annotations!
Adding gender annotations!
Adding gender annotations!
Adding gender annotations!
Adding gender annotations!
Adding gender annotations!
Copying property: key2 value2
Copying property: key3 value3
Copying property: key4 value4
Copying property: key5 value5
Copying property: key6 value6
Copying property: key7 value7
Copying property: key8 value8
Copying property: key9 value9
Copying property: key10 value10
Copying property: key11 value11
Copying property: key12 value12
Copying property: key13 value13
Copying property: key14 value14
Second Poll.  Took 100 milliseconds.
Second Poll.  Took 150 milliseconds.
Second Poll.  Took 200 milliseconds.
Second Poll.  Took 250 milliseconds.
Second Poll.  Took 300 milliseconds.
Second Poll.  Took 350 milliseconds.
Second Poll.  Took 400 milliseconds.
Second Poll.  Took 450 milliseconds.
Second Poll.  Took 500 milliseconds.
Second Poll.  Took 550 milliseconds.
Second Poll.  Took 600 milliseconds.
Second Poll.  Took 650 milliseconds.
Second Poll.  Took 700 milliseconds.
Second Poll.  Took 750 milliseconds.
Training is ready.
Training is ready.
Training is ready.
Training is ready.
Training is ready.
Training is ready.
Training is ready.
Training is ready.
Training is ready.
Training is ready.
Training is ready.
Training is ready.
Training is ready.
Training is ready.
Training is ready.
RELATION TREE: Organization Hierarchy
RELATION TREE: Friendship Network
RELATION TREE: Genealogy Chart
RELATION TREE: Social Media Connections
RELATION TREE: Academic Genealogy
RELATION TREE: Company Ownership Structure
RELATION TREE: Family Genealogy
RELATION TREE: Political Power Structure
RELATION TREE: Project Team Structure
RELATION TREE: Ancestry Tree
RELATION TREE: Network of Collaborators
RELATION TREE: Departmental Organization
RELATION TREE: Cultural Genealogy
Created MemoryMonitor.  Took 250 milliseconds.
Created MemoryMonitor.  Took 80 milliseconds.
Created MemoryMonitor.  Took 300 milliseconds.
Created MemoryMonitor.  Took 150 milliseconds.
Created MemoryMonitor.  Took 200 milliseconds.
Created MemoryMonitor.  Took 180 milliseconds.
Created MemoryMonitor.  Took 90 milliseconds.
Created MemoryMonitor.  Took 280 milliseconds.
Created MemoryMonitor.  Took 110 milliseconds.
Created MemoryMonitor.  Took 220 milliseconds.
Created MemoryMonitor.  Took 190 milliseconds.
Created MemoryMonitor.  Took 240 milliseconds.
Created MemoryMonitor.  Took 130 milliseconds.
ARG1 SENTENCE: Please enter your password.
ARG1 SENTENCE: The system is shutting down.
ARG1 SENTENCE: Invalid input. Please try again.
ARG1 SENTENCE: The operation was successful.
ARG1 SENTENCE: Unable to establish connection.
ARG1 SENTENCE: File not found.
ARG1 SENTENCE: Access denied.
ARG1 SENTENCE: The server is offline.
ARG1 SENTENCE: Data processing in progress.
ARG1 SENTENCE: An unexpected error occurred.
ARG1 SENTENCE: Your session has expired.
ARG1 SENTENCE: The configuration has been updated.
ARG1 SENTENCE: Task completed successfully.
ARG1 SENTENCE: A new update is available.
Creating training set from train_02.txt
Creating training set from train_03.txt
Creating training set from train_04.txt
Creating training set from train_05.txt
Creating training set from train_06.txt
Creating training set from train_07.txt
Creating training set from train_08.txt
Creating training set from train_09.txt
Creating training set from train_10.txt
Creating training set from train_11.txt
Creating training set from train_12.txt
Creating training set from train_13.txt
Creating training set from train_14.txt
ARG1 HEAD: 5
ARG1 HEAD: 8
ARG1 HEAD: 12
ARG1 HEAD: 3
ARG1 HEAD: 9
ARG1 HEAD: 7
ARG1 HEAD: 2
ARG1 HEAD: 6
ARG1 HEAD: 11
ARG1 HEAD: 4
ARG1 HEAD: 1
ARG1 HEAD: 13
ARG1 HEAD: 15
Reading Multi-Word rules file ...
Reading Multi-Word rules file ...
Reading Multi-Word rules file ...
Reading Multi-Word rules file ...
Reading Multi-Word rules file ...
Reading Multi-Word rules file ...
Reading Multi-Word rules file ...
Reading Multi-Word rules file ...
Reading Multi-Word rules file ...
Reading Multi-Word rules file ...
Reading Multi-Word rules file ...
Reading Multi-Word rules file ...
Reading Multi-Word rules file ...
Reading Multi-Word rules file ...
Reading Multi-Word rules file ...
Conflicting properties. Multi-word rules file will be considered.
Conflicting properties. Multi-word rules file will be considered.
Conflicting properties. Multi-word rules file will be considered.
Conflicting properties. Multi-word rules file will be considered.
Conflicting properties. Multi-word rules file will be considered.
Conflicting properties. Multi-word rules file will be considered.
Conflicting properties. Multi-word rules file will be considered.
Conflicting properties. Multi-word rules file will be considered.
Conflicting properties. Multi-word rules file will be considered.
Conflicting properties. Multi-word rules file will be considered.
Conflicting properties. Multi-word rules file will be considered.
Conflicting properties. Multi-word rules file will be considered.
Conflicting properties. Multi-word rules file will be considered.
Conflicting properties. Multi-word rules file will be considered.
Conflicting properties. Multi-word rules file will be considered.
ARG1: parameter2
ARG1: parameter3
ARG1: parameter4
ARG1: parameter5
ARG1: parameter6
ARG1: parameter7
ARG1: parameter8
ARG1: parameter9
ARG1: parameter10
ARG1: parameter11
ARG1: parameter12
ARG1: parameter13
ARG1: parameter14
Inferring Multi-Word rules from training set ...
Inferring Multi-Word rules from training set ...
Inferring Multi-Word rules from training set ...
Inferring Multi-Word rules from training set ...
Inferring Multi-Word rules from training set ...
Inferring Multi-Word rules from training set ...
Inferring Multi-Word rules from training set ...
Inferring Multi-Word rules from training set ...
Inferring Multi-Word rules from training set ...
Inferring Multi-Word rules from training set ...
Inferring Multi-Word rules from training set ...
Inferring Multi-Word rules from training set ...
Inferring Multi-Word rules from training set ...
Inferring Multi-Word rules from training set ...
Inferring Multi-Word rules from training set ...
No multi-word rules provided. No inferMultiWordRules flag validated. Not inferring rules from training.
No multi-word rules provided. No inferMultiWordRules flag validated. Not inferring rules from training.
No multi-word rules provided. No inferMultiWordRules flag validated. Not inferring rules from training.
No multi-word rules provided. No inferMultiWordRules flag validated. Not inferring rules from training.
No multi-word rules provided. No inferMultiWordRules flag validated. Not inferring rules from training.
No multi-word rules provided. No inferMultiWordRules flag validated. Not inferring rules from training.
No multi-word rules provided. No inferMultiWordRules flag validated. Not inferring rules from training.
No multi-word rules provided. No inferMultiWordRules flag validated. Not inferring rules from training.
No multi-word rules provided. No inferMultiWordRules flag validated. Not inferring rules from training.
No multi-word rules provided. No inferMultiWordRules flag validated. Not inferring rules from training.
No multi-word rules provided. No inferMultiWordRules flag validated. Not inferring rules from training.
No multi-word rules provided. No inferMultiWordRules flag validated. Not inferring rules from training.
No multi-word rules provided. No inferMultiWordRules flag validated. Not inferring rules from training.
No multi-word rules provided. No inferMultiWordRules flag validated. Not inferring rules from training.
No multi-word rules provided. No inferMultiWordRules flag validated. Not inferring rules from training.
ARG0 SENTENCE: Please enter your username.
ARG0 SENTENCE: The file was successfully saved.
ARG0 SENTENCE: The operation cannot be completed.
ARG0 SENTENCE: Invalid input. Please try again.
ARG0 SENTENCE: Connection lost. Reconnecting...
ARG0 SENTENCE: Error: File not found.
ARG0 SENTENCE: Welcome to our website.
ARG0 SENTENCE: The package will be delivered tomorrow.
ARG0 SENTENCE: Unable to establish a connection.
ARG0 SENTENCE: New update available. Install now?
ARG0 SENTENCE: The payment was successful.
ARG0 SENTENCE: Access denied. Please log in.
ARG0 SENTENCE: Thank you for your feedback.
ARG0 SENTENCE: Searching for relevant results...
Command line arguments for training the StatTokSent model:   -trainFile <filename>              conllu file to train from   -testFile <filename>               filename for testing the trained model   -serializeTo <filename>            where to write the finished model   -loadClassifier <filename>         load an existing model   -crossValidationFolds N            use N fold cross-validation   -multiWordRulesFile <filename>     MWT rules   -inferMultiWordRules 1             infer MWT rules from training file
Command line arguments for training the StatTokSent model:   -trainFile <filename>              conllu file to train from   -testFile <filename>               filename for testing the trained model   -serializeTo <filename>            where to write the finished model   -loadClassifier <filename>         load an existing model   -crossValidationFolds N            use N fold cross-validation   -multiWordRulesFile <filename>     MWT rules   -inferMultiWordRules 1             infer MWT rules from training file
Command line arguments for training the StatTokSent model:   -trainFile <filename>              conllu file to train from   -testFile <filename>               filename for testing the trained model   -serializeTo <filename>            where to write the finished model   -loadClassifier <filename>         load an existing model   -crossValidationFolds N            use N fold cross-validation   -multiWordRulesFile <filename>     MWT rules   -inferMultiWordRules 1             infer MWT rules from training file
Command line arguments for training the StatTokSent model:   -trainFile <filename>              conllu file to train from   -testFile <filename>               filename for testing the trained model   -serializeTo <filename>            where to write the finished model   -loadClassifier <filename>         load an existing model   -crossValidationFolds N            use N fold cross-validation   -multiWordRulesFile <filename>     MWT rules   -inferMultiWordRules 1             infer MWT rules from training file
Command line arguments for training the StatTokSent model:   -trainFile <filename>              conllu file to train from   -testFile <filename>               filename for testing the trained model   -serializeTo <filename>            where to write the finished model   -loadClassifier <filename>         load an existing model   -crossValidationFolds N            use N fold cross-validation   -multiWordRulesFile <filename>     MWT rules   -inferMultiWordRules 1             infer MWT rules from training file
Command line arguments for training the StatTokSent model:   -trainFile <filename>              conllu file to train from   -testFile <filename>               filename for testing the trained model   -serializeTo <filename>            where to write the finished model   -loadClassifier <filename>         load an existing model   -crossValidationFolds N            use N fold cross-validation   -multiWordRulesFile <filename>     MWT rules   -inferMultiWordRules 1             infer MWT rules from training file
Command line arguments for training the StatTokSent model:   -trainFile <filename>              conllu file to train from   -testFile <filename>               filename for testing the trained model   -serializeTo <filename>            where to write the finished model   -loadClassifier <filename>         load an existing model   -crossValidationFolds N            use N fold cross-validation   -multiWordRulesFile <filename>     MWT rules   -inferMultiWordRules 1             infer MWT rules from training file
Command line arguments for training the StatTokSent model:   -trainFile <filename>              conllu file to train from   -testFile <filename>               filename for testing the trained model   -serializeTo <filename>            where to write the finished model   -loadClassifier <filename>         load an existing model   -crossValidationFolds N            use N fold cross-validation   -multiWordRulesFile <filename>     MWT rules   -inferMultiWordRules 1             infer MWT rules from training file
Command line arguments for training the StatTokSent model:   -trainFile <filename>              conllu file to train from   -testFile <filename>               filename for testing the trained model   -serializeTo <filename>            where to write the finished model   -loadClassifier <filename>         load an existing model   -crossValidationFolds N            use N fold cross-validation   -multiWordRulesFile <filename>     MWT rules   -inferMultiWordRules 1             infer MWT rules from training file
Command line arguments for training the StatTokSent model:   -trainFile <filename>              conllu file to train from   -testFile <filename>               filename for testing the trained model   -serializeTo <filename>            where to write the finished model   -loadClassifier <filename>         load an existing model   -crossValidationFolds N            use N fold cross-validation   -multiWordRulesFile <filename>     MWT rules   -inferMultiWordRules 1             infer MWT rules from training file
Command line arguments for training the StatTokSent model:   -trainFile <filename>              conllu file to train from   -testFile <filename>               filename for testing the trained model   -serializeTo <filename>            where to write the finished model   -loadClassifier <filename>         load an existing model   -crossValidationFolds N            use N fold cross-validation   -multiWordRulesFile <filename>     MWT rules   -inferMultiWordRules 1             infer MWT rules from training file
Command line arguments for training the StatTokSent model:   -trainFile <filename>              conllu file to train from   -testFile <filename>               filename for testing the trained model   -serializeTo <filename>            where to write the finished model   -loadClassifier <filename>         load an existing model   -crossValidationFolds N            use N fold cross-validation   -multiWordRulesFile <filename>     MWT rules   -inferMultiWordRules 1             infer MWT rules from training file
Command line arguments for training the StatTokSent model:   -trainFile <filename>              conllu file to train from   -testFile <filename>               filename for testing the trained model   -serializeTo <filename>            where to write the finished model   -loadClassifier <filename>         load an existing model   -crossValidationFolds N            use N fold cross-validation   -multiWordRulesFile <filename>     MWT rules   -inferMultiWordRules 1             infer MWT rules from training file
Command line arguments for training the StatTokSent model:   -trainFile <filename>              conllu file to train from   -testFile <filename>               filename for testing the trained model   -serializeTo <filename>            where to write the finished model   -loadClassifier <filename>         load an existing model   -crossValidationFolds N            use N fold cross-validation   -multiWordRulesFile <filename>     MWT rules   -inferMultiWordRules 1             infer MWT rules from training file
Command line arguments for training the StatTokSent model:   -trainFile <filename>              conllu file to train from   -testFile <filename>               filename for testing the trained model   -serializeTo <filename>            where to write the finished model   -loadClassifier <filename>         load an existing model   -crossValidationFolds N            use N fold cross-validation   -multiWordRulesFile <filename>     MWT rules   -inferMultiWordRules 1             infer MWT rules from training file
ARG0 HEAD: 3
ARG0 HEAD: 2
ARG0 HEAD: 1
ARG0 HEAD: 4
ARG0 HEAD: 6
ARG0 HEAD: 7
ARG0 HEAD: 8
ARG0 HEAD: 9
ARG0 HEAD: 10
ARG0 HEAD: 11
ARG0 HEAD: 12
ARG0 HEAD: 13
ARG0 HEAD: 14
Found 5 rules
Found 3 rules
Found 15 rules
Found 8 rules
Found 12 rules
Found 6 rules
Found 11 rules
Found 7 rules
Found 20 rules
Found 9 rules
Found 4 rules
Found 14 rules
Found 2 rules
ARG0: value2
ARG0: value3
ARG0: value4
ARG0: value5
ARG0: value6
ARG0: value7
ARG0: value8
ARG0: value9
ARG0: value10
ARG0: value11
ARG0: value12
ARG0: value13
ARG0: value14
Reading rules from config.json
Reading rules from data.csv
Reading rules from settings.xml
Reading rules from policy.yaml
Reading rules from security.ini
Reading rules from script.js
Reading rules from styles.css
Reading rules from template.html
Reading rules from log.txt
Reading rules from database.sql
Reading rules from ruleset.xml
Reading rules from schema.json
Reading rules from rules.py
WARNING: found weird argument offsets. Most likely because arguments appear in different sentences than the relation:
WARNING: found weird argument offsets. Most likely because arguments appear in different sentences than the relation:
WARNING: found weird argument offsets. Most likely because arguments appear in different sentences than the relation:
WARNING: found weird argument offsets. Most likely because arguments appear in different sentences than the relation:
WARNING: found weird argument offsets. Most likely because arguments appear in different sentences than the relation:
WARNING: found weird argument offsets. Most likely because arguments appear in different sentences than the relation:
WARNING: found weird argument offsets. Most likely because arguments appear in different sentences than the relation:
WARNING: found weird argument offsets. Most likely because arguments appear in different sentences than the relation:
WARNING: found weird argument offsets. Most likely because arguments appear in different sentences than the relation:
WARNING: found weird argument offsets. Most likely because arguments appear in different sentences than the relation:
WARNING: found weird argument offsets. Most likely because arguments appear in different sentences than the relation:
WARNING: found weird argument offsets. Most likely because arguments appear in different sentences than the relation:
WARNING: found weird argument offsets. Most likely because arguments appear in different sentences than the relation:
WARNING: found weird argument offsets. Most likely because arguments appear in different sentences than the relation:
WARNING: found weird argument offsets. Most likely because arguments appear in different sentences than the relation:
No multiWordRules file specified.
No multiWordRules file specified.
No multiWordRules file specified.
No multiWordRules file specified.
No multiWordRules file specified.
No multiWordRules file specified.
No multiWordRules file specified.
No multiWordRules file specified.
No multiWordRules file specified.
No multiWordRules file specified.
No multiWordRules file specified.
No multiWordRules file specified.
No multiWordRules file specified.
No multiWordRules file specified.
No multiWordRules file specified.
FileParser: Parse failed. Trying recovery parse...
HTMLParser: Parse failed. Trying recovery parse...
XMLParser: Parse failed. Trying recovery parse...
JSONParser: Parse failed. Trying recovery parse...
CSVParser: Parse failed. Trying recovery parse...
LogParser: Parse failed. Trying recovery parse...
DataParser: Parse failed. Trying recovery parse...
ConfigParser: Parse failed. Trying recovery parse...
RequestParser: Parse failed. Trying recovery parse...
ResponseParser: Parse failed. Trying recovery parse...
CommandParser: Parse failed. Trying recovery parse...
ParserUtil: Parse failed. Trying recovery parse...
XMLReader: Parse failed. Trying recovery parse...
JSONReader: Parse failed. Trying recovery parse...
You have not specified a window size. Use -windowSize option.
You have not specified a window size. Use -windowSize option.
You have not specified a window size. Use -windowSize option.
You have not specified a window size. Use -windowSize option.
You have not specified a window size. Use -windowSize option.
You have not specified a window size. Use -windowSize option.
You have not specified a window size. Use -windowSize option.
You have not specified a window size. Use -windowSize option.
You have not specified a window size. Use -windowSize option.
You have not specified a window size. Use -windowSize option.
You have not specified a window size. Use -windowSize option.
You have not specified a window size. Use -windowSize option.
You have not specified a window size. Use -windowSize option.
You have not specified a window size. Use -windowSize option.
You have not specified a window size. Use -windowSize option.
You have not specified a model. Use -model option.
You have not specified a model. Use -model option.
You have not specified a model. Use -model option.
You have not specified a model. Use -model option.
You have not specified a model. Use -model option.
You have not specified a model. Use -model option.
You have not specified a model. Use -model option.
You have not specified a model. Use -model option.
You have not specified a model. Use -model option.
You have not specified a model. Use -model option.
You have not specified a model. Use -model option.
You have not specified a model. Use -model option.
You have not specified a model. Use -model option.
You have not specified a model. Use -model option.
You have not specified a model. Use -model option.
PCFG 15 words (incl. stop) iScore 9.5
PCFG 20 words (incl. stop) iScore 7.8
PCFG 25 words (incl. stop) iScore 6.4
PCFG 30 words (incl. stop) iScore 8.9
PCFG 35 words (incl. stop) iScore 7.6
PCFG 40 words (incl. stop) iScore 8.1
PCFG 45 words (incl. stop) iScore 9.3
PCFG 50 words (incl. stop) iScore 6.7
PCFG 55 words (incl. stop) iScore 9.1
PCFG 60 words (incl. stop) iScore 7.5
PCFG 65 words (incl. stop) iScore 8.6
PCFG 70 words (incl. stop) iScore 6.9
PCFG 75 words (incl. stop) iScore 9.7
Arg0 sentence: Lorem ipsum dolor sit amet.
Arg0 sentence: The quick brown fox jumps over the lazy dog.
Arg0 sentence: Hello, world!
Arg0 sentence: Once upon a time, there was a princess.
Arg0 sentence: In a galaxy far, far away...
Arg0 sentence: What is your favorite color?
Arg0 sentence: Today is a beautiful day.
Arg0 sentence: It's raining cats and dogs outside.
Arg0 sentence: The answer to life, the universe, and everything is 42.
Arg0 sentence: I love ice cream!
Arg0 sentence: The sun will come out tomorrow.
Arg0 sentence: Are you ready to rumble?
Arg0 sentence: Time flies when you're having fun.
Arg0 sentence: To be or not to be, that is the question.
You have not specified a text file. Use -textFile option.
You have not specified a text file. Use -textFile option.
You have not specified a text file. Use -textFile option.
You have not specified a text file. Use -textFile option.
You have not specified a text file. Use -textFile option.
You have not specified a text file. Use -textFile option.
You have not specified a text file. Use -textFile option.
You have not specified a text file. Use -textFile option.
You have not specified a text file. Use -textFile option.
You have not specified a text file. Use -textFile option.
You have not specified a text file. Use -textFile option.
You have not specified a text file. Use -textFile option.
You have not specified a text file. Use -textFile option.
You have not specified a text file. Use -textFile option.
You have not specified a text file. Use -textFile option.
Found window size of 20
Found window size of 30
Found window size of 40
Found window size of 50
Found window size of 60
Found window size of 70
Found window size of 80
Found window size of 90
Found window size of 100
Found window size of 110
Found window size of 120
Found window size of 130
Found window size of 140
Relation sentence: The dog is chasing its tail.
Relation sentence: The bird is flying in the sky.
Relation sentence: The fish is swimming in the pond.
Relation sentence: The horse is galloping in the field.
Relation sentence: The tree is swaying in the wind.
Relation sentence: The flower is blooming in the garden.
Relation sentence: The car is speeding down the road.
Relation sentence: The train is arriving at the station.
Relation sentence: The plane is taking off from the runway.
Relation sentence: The book is lying on the table.
Relation sentence: The pen is writing on the paper.
Relation sentence: The phone is ringing on the desk.
Relation sentence: The clock is ticking on the wall.
Using default multi word rules
Using default multi word rules
Using default multi word rules
Using default multi word rules
Using default multi word rules
Using default multi word rules
Using default multi word rules
Using default multi word rules
Using default multi word rules
Using default multi word rules
Using default multi word rules
Using default multi word rules
Using default multi word rules
Using default multi word rules
Using default multi word rules
I'm going to exception soon (on purpose)
I'm going to exception soon (on purpose)
I'm going to exception soon (on purpose)
I'm going to exception soon (on purpose)
I'm going to exception soon (on purpose)
I'm going to exception soon (on purpose)
I'm going to exception soon (on purpose)
I'm going to exception soon (on purpose)
I'm going to exception soon (on purpose)
I'm going to exception soon (on purpose)
I'm going to exception soon (on purpose)
I'm going to exception soon (on purpose)
I'm going to exception soon (on purpose)
I'm going to exception soon (on purpose)
I'm going to exception soon (on purpose)
WARNING: Found relation with arg0 in a different sentence: hates
WARNING: Found relation with arg0 in a different sentence: admires
WARNING: Found relation with arg0 in a different sentence: fears
WARNING: Found relation with arg0 in a different sentence: supports
WARNING: Found relation with arg0 in a different sentence: opposes
WARNING: Found relation with arg0 in a different sentence: respects
WARNING: Found relation with arg0 in a different sentence: considers
WARNING: Found relation with arg0 in a different sentence: discusses
WARNING: Found relation with arg0 in a different sentence: disagrees
WARNING: Found relation with arg0 in a different sentence: likes
WARNING: Found relation with arg0 in a different sentence: disapproves
WARNING: Found relation with arg0 in a different sentence: argues
WARNING: Found relation with arg0 in a different sentence: supports
PCFG parsing 15 words (incl. stop): insideScore = 0.5867
PCFG parsing 20 words (incl. stop): insideScore = 0.8742
PCFG parsing 25 words (incl. stop): insideScore = 0.9123
PCFG parsing 30 words (incl. stop): insideScore = 0.6725
PCFG parsing 35 words (incl. stop): insideScore = 0.4321
PCFG parsing 40 words (incl. stop): insideScore = 0.5678
PCFG parsing 45 words (incl. stop): insideScore = 0.9012
PCFG parsing 50 words (incl. stop): insideScore = 0.2500
PCFG parsing 55 words (incl. stop): insideScore = 0.6879
PCFG parsing 60 words (incl. stop): insideScore = 0.9321
PCFG parsing 65 words (incl. stop): insideScore = 0.5362
PCFG parsing 70 words (incl. stop): insideScore = 0.7321
PCFG parsing 75 words (incl. stop): insideScore = 0.8512
Using multi word rules from rules.txt
Using multi word rules from phrases.txt
Using multi word rules from grammar.txt
Using multi word rules from words.txt
Using multi word rules from lexicon.txt
Using multi word rules from corpus.txt
Using multi word rules from dictionary.txt
Using multi word rules from expressions.txt
Using multi word rules from patterns.txt
Using multi word rules from samples.txt
Using multi word rules from data.txt
Using multi word rules from phrases_en.txt
Using multi word rules from grammar_fr.txt
Using multi word rules from lexicon_es.txt
Current sentence: I ate an apple for breakfast.
Current sentence: She is wearing a red dress.
Current sentence: The car drove down the street.
Current sentence: The sun is shining brightly.
Current sentence: He plays the guitar beautifully.
Current sentence: We went for a walk in the park.
Current sentence: The book is on the shelf.
Current sentence: She has blue eyes and blonde hair.
Current sentence: The dog barked loudly.
Current sentence: They are having dinner together.
Current sentence: The train arrived on time.
Current sentence: He is a talented dancer.
Current sentence: We are going on vacation next week.
Current sentence: The movie was suspenseful and thrilling.
Loading StatTokSent model from model2.pth
Loading StatTokSent model from model3.pth
Loading StatTokSent model from model4.pth
Loading StatTokSent model from model5.pth
Loading StatTokSent model from model6.pth
Loading StatTokSent model from model7.pth
Loading StatTokSent model from model8.pth
Loading StatTokSent model from model9.pth
Loading StatTokSent model from model10.pth
Loading StatTokSent model from model11.pth
Loading StatTokSent model from model12.pth
Loading StatTokSent model from model13.pth
Loading StatTokSent model from model14.pth
-parseInside regex|-fileList|-ioFileList|-untok
-parseInside regex|-fileList|-ioFileList|-untok
-parseInside regex|-fileList|-ioFileList|-untok
-parseInside regex|-fileList|-ioFileList|-untok
-parseInside regex|-fileList|-ioFileList|-untok
-parseInside regex|-fileList|-ioFileList|-untok
-parseInside regex|-fileList|-ioFileList|-untok
-parseInside regex|-fileList|-ioFileList|-untok
-parseInside regex|-fileList|-ioFileList|-untok
-parseInside regex|-fileList|-ioFileList|-untok
-parseInside regex|-fileList|-ioFileList|-untok
-parseInside regex|-fileList|-ioFileList|-untok
-parseInside regex|-fileList|-ioFileList|-untok
-parseInside regex|-fileList|-ioFileList|-untok
-parseInside regex|-fileList|-ioFileList|-untok
The predicted relation is: colleague
The predicted relation is: sibling
The predicted relation is: neighbor
The predicted relation is: partner
The predicted relation is: enemy
The predicted relation is: mentor
The predicted relation is: classmate
The predicted relation is: teammate
The predicted relation is: supervisor
The predicted relation is: customer
The predicted relation is: client
The predicted relation is: employee
The predicted relation is: student
-lowerCase|-preserveLines|-oneLinePerElement|-filter regex|
-lowerCase|-preserveLines|-oneLinePerElement|-filter regex|
-lowerCase|-preserveLines|-oneLinePerElement|-filter regex|
-lowerCase|-preserveLines|-oneLinePerElement|-filter regex|
-lowerCase|-preserveLines|-oneLinePerElement|-filter regex|
-lowerCase|-preserveLines|-oneLinePerElement|-filter regex|
-lowerCase|-preserveLines|-oneLinePerElement|-filter regex|
-lowerCase|-preserveLines|-oneLinePerElement|-filter regex|
-lowerCase|-preserveLines|-oneLinePerElement|-filter regex|
-lowerCase|-preserveLines|-oneLinePerElement|-filter regex|
-lowerCase|-preserveLines|-oneLinePerElement|-filter regex|
-lowerCase|-preserveLines|-oneLinePerElement|-filter regex|
-lowerCase|-preserveLines|-oneLinePerElement|-filter regex|
-lowerCase|-preserveLines|-oneLinePerElement|-filter regex|
-lowerCase|-preserveLines|-oneLinePerElement|-filter regex|
options: -h|-help|-options tokenizerOptions|-encoding encoding|-dump|
options: -h|-help|-options tokenizerOptions|-encoding encoding|-dump|
options: -h|-help|-options tokenizerOptions|-encoding encoding|-dump|
options: -h|-help|-options tokenizerOptions|-encoding encoding|-dump|
options: -h|-help|-options tokenizerOptions|-encoding encoding|-dump|
options: -h|-help|-options tokenizerOptions|-encoding encoding|-dump|
options: -h|-help|-options tokenizerOptions|-encoding encoding|-dump|
options: -h|-help|-options tokenizerOptions|-encoding encoding|-dump|
options: -h|-help|-options tokenizerOptions|-encoding encoding|-dump|
options: -h|-help|-options tokenizerOptions|-encoding encoding|-dump|
options: -h|-help|-options tokenizerOptions|-encoding encoding|-dump|
options: -h|-help|-options tokenizerOptions|-encoding encoding|-dump|
options: -h|-help|-options tokenizerOptions|-encoding encoding|-dump|
options: -h|-help|-options tokenizerOptions|-encoding encoding|-dump|
options: -h|-help|-options tokenizerOptions|-encoding encoding|-dump|
ataghello world
ataghello world
ataghello world
ataghello world
ataghello world
ataghello world
ataghello world
ataghello world
ataghello world
ataghello world
ataghello world
ataghello world
ataghello world
ataghello world
ataghello world
Classification: found similar type relation.getDepartment() for relation: department
Classification: found similar type relation.getManager() for relation: manager
Classification: found similar type relation.getAddress() for relation: address
Classification: found similar type relation.getSalary() for relation: salary
Classification: found similar type relation.getPhoneNumber() for relation: phoneNumber
Classification: found similar type relation.getEmail() for relation: email
Classification: found similar type relation.getProject() for relation: project
Classification: found similar type relation.getSkillSet() for relation: skillSet
Classification: found similar type relation.getEducation() for relation: education
Classification: found similar type relation.getExperience() for relation: experience
Classification: found similar type relation.getPerformance() for relation: performance
Classification: found similar type relation.getHireDate() for relation: hireDate
Classification: found similar type relation.getManagerName() for relation: managerName
Usage: java edu.stanford.nlp.process.PTBTokenizer [options]* filename*
Usage: java edu.stanford.nlp.process.PTBTokenizer [options]* filename*
Usage: java edu.stanford.nlp.process.PTBTokenizer [options]* filename*
Usage: java edu.stanford.nlp.process.PTBTokenizer [options]* filename*
Usage: java edu.stanford.nlp.process.PTBTokenizer [options]* filename*
Usage: java edu.stanford.nlp.process.PTBTokenizer [options]* filename*
Usage: java edu.stanford.nlp.process.PTBTokenizer [options]* filename*
Usage: java edu.stanford.nlp.process.PTBTokenizer [options]* filename*
Usage: java edu.stanford.nlp.process.PTBTokenizer [options]* filename*
Usage: java edu.stanford.nlp.process.PTBTokenizer [options]* filename*
Usage: java edu.stanford.nlp.process.PTBTokenizer [options]* filename*
Usage: java edu.stanford.nlp.process.PTBTokenizer [options]* filename*
Usage: java edu.stanford.nlp.process.PTBTokenizer [options]* filename*
Usage: java edu.stanford.nlp.process.PTBTokenizer [options]* filename*
Usage: java edu.stanford.nlp.process.PTBTokenizer [options]* filename*
Justification done.
Justification done.
Justification done.
Justification done.
Justification done.
Justification done.
Justification done.
Justification done.
Justification done.
Justification done.
Justification done.
Justification done.
Justification done.
Justification done.
Justification done.
WARNINGhello world
INFOhello world
ERRORhello world
DEBUGhello world
SUCCESShello world
FAILhello world
STARThello world
STOPhello world
CRITICALhello world
ALERThello world
NOTICEhello world
TRACEhello world
FATALhello world
PTBTokenizer tokenized 5000 tokens at 2500 tokens per second.
PTBTokenizer tokenized 8000 tokens at 3000 tokens per second.
PTBTokenizer tokenized 2000 tokens at 1500 tokens per second.
PTBTokenizer tokenized 6000 tokens at 2800 tokens per second.
PTBTokenizer tokenized 3000 tokens at 1800 tokens per second.
PTBTokenizer tokenized 4000 tokens at 2200 tokens per second.
PTBTokenizer tokenized 7000 tokens at 3200 tokens per second.
PTBTokenizer tokenized 9000 tokens at 4000 tokens per second.
PTBTokenizer tokenized 10000 tokens at 5000 tokens per second.
PTBTokenizer tokenized 2500 tokens at 2000 tokens per second.
PTBTokenizer tokenized 3500 tokens at 2300 tokens per second.
PTBTokenizer tokenized 4500 tokens at 2600 tokens per second.
PTBTokenizer tokenized 5500 tokens at 2800 tokens per second.
Using gazetteer: data/gazetteer2.txt
Using gazetteer: data/gazetteer3.txt
Using gazetteer: data/gazetteer4.txt
Using gazetteer: data/gazetteer5.txt
Using gazetteer: data/gazetteer6.txt
Using gazetteer: data/gazetteer7.txt
Using gazetteer: data/gazetteer8.txt
Using gazetteer: data/gazetteer9.txt
Using gazetteer: data/gazetteer10.txt
Using gazetteer: data/gazetteer11.txt
Using gazetteer: data/gazetteer12.txt
Using gazetteer: data/gazetteer13.txt
Using gazetteer: data/gazetteer14.txt
You shouldn't see any other messages or 'skipped tracks' here
You shouldn't see any other messages or 'skipped tracks' here
You shouldn't see any other messages or 'skipped tracks' here
You shouldn't see any other messages or 'skipped tracks' here
You shouldn't see any other messages or 'skipped tracks' here
You shouldn't see any other messages or 'skipped tracks' here
You shouldn't see any other messages or 'skipped tracks' here
You shouldn't see any other messages or 'skipped tracks' here
You shouldn't see any other messages or 'skipped tracks' here
You shouldn't see any other messages or 'skipped tracks' here
You shouldn't see any other messages or 'skipped tracks' here
You shouldn't see any other messages or 'skipped tracks' here
You shouldn't see any other messages or 'skipped tracks' here
You shouldn't see any other messages or 'skipped tracks' here
You shouldn't see any other messages or 'skipped tracks' here
True Positive:2
True Positive:3
True Positive:4
True Positive:5
True Positive:6
True Positive:7
True Positive:8
True Positive:9
True Positive:10
True Positive:11
True Positive:12
True Positive:13
True Positive:14
^middle 'fourth message' was forced
^middle 'fourth message' was forced
^middle 'fourth message' was forced
^middle 'fourth message' was forced
^middle 'fourth message' was forced
^middle 'fourth message' was forced
^middle 'fourth message' was forced
^middle 'fourth message' was forced
^middle 'fourth message' was forced
^middle 'fourth message' was forced
^middle 'fourth message' was forced
^middle 'fourth message' was forced
^middle 'fourth message' was forced
^middle 'fourth message' was forced
^middle 'fourth message' was forced
Invalid unary:
Invalid unary:
Invalid unary:
Invalid unary:
Invalid unary:
Invalid unary:
Invalid unary:
Invalid unary:
Invalid unary:
Invalid unary:
Invalid unary:
Invalid unary:
Invalid unary:
Invalid unary:
Invalid unary:
mentions are null
mentions are null
mentions are null
mentions are null
mentions are null
mentions are null
mentions are null
mentions are null
mentions are null
mentions are null
mentions are null
mentions are null
mentions are null
mentions are null
mentions are null
this is a fourth message
this is a fourth message
this is a fourth message
this is a fourth message
this is a fourth message
this is a fourth message
this is a fourth message
this is a fourth message
this is a fourth message
this is a fourth message
this is a fourth message
this is a fourth message
this is a fourth message
this is a fourth message
this is a fourth message
this is a third message
this is a third message
this is a third message
this is a third message
this is a third message
this is a third message
this is a third message
this is a third message
this is a third message
this is a third message
this is a third message
this is a third message
this is a third message
this is a third message
this is a third message
this is a another message
this is a another message
this is a another message
this is a another message
this is a another message
this is a another message
this is a another message
this is a another message
this is a another message
this is a another message
this is a another message
this is a another message
this is a another message
this is a another message
this is a another message
Used {ALEX} (2) to recognize token2
Used {ALEX} (2) to recognize token3
Used {ALEX} (2) to recognize token4
Used {ALEX} (2) to recognize token5
Used {ALEX} (2) to recognize token6
Used {ALEX} (2) to recognize token7
Used {ALEX} (2) to recognize token8
Used {ALEX} (2) to recognize token9
Used {ALEX} (2) to recognize token10
Used {ALEX} (2) to recognize token11
Used {ALEX} (2) to recognize token12
Used {ALEX} (2) to recognize token13
Used {ALEX} (2) to recognize token14
this is a message
this is a message
this is a message
this is a message
this is a message
this is a message
this is a message
this is a message
this is a message
this is a message
this is a message
this is a message
this is a message
this is a message
this is a message
Found entity: Location
Found entity: Organization
Found entity: Event
Found entity: Date
Found entity: Time
Found entity: Product
Found entity: Mention
Found entity: Term
Found entity: Concept
Found entity: Phrase
Found entity: Symbol
Found entity: Number
Found entity: URL
Used {SSN} to recognize doc
Used {SSN} to recognize pdf
Used {SSN} to recognize jpg
Used {SSN} to recognize png
Used {SSN} to recognize xls
Used {SSN} to recognize mp3
Used {SSN} to recognize mp4
Used {SSN} to recognize ppt
Used {SSN} to recognize html
Used {SSN} to recognize css
Used {SSN} to recognize js
Used {SSN} to recognize xml
Used {SSN} to recognize json
Word: banana
Word: orange
Word: kiwi
Word: pineapple
Word: mango
Word: peach
Word: pear
Word: strawberry
Word: watermelon
Word: grape
Word: lemon
Word: cherry
Word: blueberry
Tag: Verb
Tag: Adjective
Tag: Adverb
Tag: Pronoun
Tag: Preposition
Tag: Conjunction
Tag: Interjection
Tag: Numeral
Tag: Adverbial
Tag: Determiner
Tag: Auxiliary
Tag: Particle
Tag: Exclamation
Creating TokensRegexExtractor
Creating TokensRegexExtractor
Creating TokensRegexExtractor
Creating TokensRegexExtractor
Creating TokensRegexExtractor
Creating TokensRegexExtractor
Creating TokensRegexExtractor
Creating TokensRegexExtractor
Creating TokensRegexExtractor
Creating TokensRegexExtractor
Creating TokensRegexExtractor
Creating TokensRegexExtractor
Creating TokensRegexExtractor
Creating TokensRegexExtractor
Creating TokensRegexExtractor
Invalid tagging:
Invalid tagging:
Invalid tagging:
Invalid tagging:
Invalid tagging:
Invalid tagging:
Invalid tagging:
Invalid tagging:
Invalid tagging:
Invalid tagging:
Invalid tagging:
Invalid tagging:
Invalid tagging:
Invalid tagging:
Invalid tagging:
Training classifier:
Training classifier:
Training classifier:
Training classifier:
Training classifier:
Training classifier:
Training classifier:
Training classifier:
Training classifier:
Training classifier:
Training classifier:
Training classifier:
Training classifier:
Training classifier:
Training classifier:
Problem in ExhaustiveDependencyParser::extractBestParse
Problem in ExhaustiveDependencyParser::extractBestParse
Problem in ExhaustiveDependencyParser::extractBestParse
Problem in ExhaustiveDependencyParser::extractBestParse
Problem in ExhaustiveDependencyParser::extractBestParse
Problem in ExhaustiveDependencyParser::extractBestParse
Problem in ExhaustiveDependencyParser::extractBestParse
Problem in ExhaustiveDependencyParser::extractBestParse
Problem in ExhaustiveDependencyParser::extractBestParse
Problem in ExhaustiveDependencyParser::extractBestParse
Problem in ExhaustiveDependencyParser::extractBestParse
Problem in ExhaustiveDependencyParser::extractBestParse
Problem in ExhaustiveDependencyParser::extractBestParse
Problem in ExhaustiveDependencyParser::extractBestParse
Problem in ExhaustiveDependencyParser::extractBestParse
Used {LDOTS4} to recognize image.jpg as description
Used {LDOTS4} to recognize socket.jpg as name
Used {LDOTS4} to recognize document.docx as content
Used {LDOTS4} to recognize data.csv as dataset
Used {LDOTS4} to recognize info.txt as text
Used {LDOTS4} to recognize record.wav as audio
Used {LDOTS4} to recognize video.mp4 as video
Used {LDOTS4} to recognize code.py as script
Used {LDOTS4} to recognize config.ini as configuration
Used {LDOTS4} to recognize log.log as log
Used {LDOTS4} to recognize index.html as webpage
Used {LDOTS4} to recognize style.css as stylesheet
Used {LDOTS4} to recognize script.js as JavaScript code
Used {LDOTS4} to recognize database.db as database
[2.345] Featurized 200 / 1200 examples
[3.456] Featurized 300 / 1200 examples
[4.567] Featurized 400 / 1200 examples
[5.678] Featurized 500 / 1200 examples
[6.789] Featurized 600 / 1200 examples
[7.890] Featurized 700 / 1200 examples
[8.901] Featurized 800 / 1200 examples
[9.012] Featurized 900 / 1200 examples
[10.123] Featurized 1000 / 1200 examples
[11.234] Featurized 1100 / 1200 examples
[12.345] Featurized 1200 / 1200 examples
[13.456] Featurized 1300 / 1200 examples
[14.567] Featurized 1400 / 1200 examples
outside of a track
outside of a track
outside of a track
outside of a track
outside of a track
outside of a track
outside of a track
outside of a track
outside of a track
outside of a track
outside of a track
outside of a track
outside of a track
outside of a track
outside of a track
In an anonymous track
In an anonymous track
In an anonymous track
In an anonymous track
In an anonymous track
In an anonymous track
In an anonymous track
In an anonymous track
In an anonymous track
In an anonymous track
In an anonymous track
In an anonymous track
In an anonymous track
In an anonymous track
In an anonymous track
Read 20 examples
Read 5 examples
Read 15 examples
Read 8 examples
Read 12 examples
Read 25 examples
Read 3 examples
Read 7 examples
Read 30 examples
Read 18 examples
Read 2 examples
Read 13 examples
Read 6 examples
---- headScore matrix (head x dep, best tags) ----
---- headScore matrix (head x dep, best tags) ----
---- headScore matrix (head x dep, best tags) ----
---- headScore matrix (head x dep, best tags) ----
---- headScore matrix (head x dep, best tags) ----
---- headScore matrix (head x dep, best tags) ----
---- headScore matrix (head x dep, best tags) ----
---- headScore matrix (head x dep, best tags) ----
---- headScore matrix (head x dep, best tags) ----
---- headScore matrix (head x dep, best tags) ----
---- headScore matrix (head x dep, best tags) ----
---- headScore matrix (head x dep, best tags) ----
---- headScore matrix (head x dep, best tags) ----
---- headScore matrix (head x dep, best tags) ----
---- headScore matrix (head x dep, best tags) ----
Training accuracy:
Training accuracy:
Training accuracy:
Training accuracy:
Training accuracy:
Training accuracy:
Training accuracy:
Training accuracy:
Training accuracy:
Training accuracy:
Training accuracy:
Training accuracy:
Training accuracy:
Training accuracy:
Training accuracy:
Starting half-filters...
Starting half-filters...
Starting half-filters...
Starting half-filters...
Starting half-filters...
Starting half-filters...
Starting half-filters...
Starting half-filters...
Starting half-filters...
Starting half-filters...
Starting half-filters...
Starting half-filters...
Starting half-filters...
Starting half-filters...
Starting half-filters...
Starting outsides...
Starting outsides...
Starting outsides...
Starting outsides...
Starting outsides...
Starting outsides...
Starting outsides...
Starting outsides...
Starting outsides...
Starting outsides...
Starting outsides...
Starting outsides...
Starting outsides...
Starting outsides...
Starting outsides...
Dep  parsing 10 words (incl. stop): insideScore 0.92
Dep  parsing 10 words (incl. stop): insideScore 0.76
Dep  parsing 10 words (incl. stop): insideScore 0.81
Dep  parsing 10 words (incl. stop): insideScore 0.97
Dep  parsing 10 words (incl. stop): insideScore 0.84
Dep  parsing 10 words (incl. stop): insideScore 0.88
Dep  parsing 10 words (incl. stop): insideScore 0.93
Dep  parsing 10 words (incl. stop): insideScore 0.79
Dep  parsing 10 words (incl. stop): insideScore 0.86
Dep  parsing 10 words (incl. stop): insideScore 0.91
Dep  parsing 10 words (incl. stop): insideScore 0.95
Dep  parsing 10 words (incl. stop): insideScore 0.78
Dep  parsing 10 words (incl. stop): insideScore 0.89
Randomizing dataset...
Randomizing dataset...
Randomizing dataset...
Randomizing dataset...
Randomizing dataset...
Randomizing dataset...
Randomizing dataset...
Randomizing dataset...
Randomizing dataset...
Randomizing dataset...
Randomizing dataset...
Randomizing dataset...
Randomizing dataset...
Randomizing dataset...
Randomizing dataset...
^shown should have appeared above
^shown should have appeared above
^shown should have appeared above
^shown should have appeared above
^shown should have appeared above
^shown should have appeared above
^shown should have appeared above
^shown should have appeared above
^shown should have appeared above
^shown should have appeared above
^shown should have appeared above
^shown should have appeared above
^shown should have appeared above
^shown should have appeared above
^shown should have appeared above
Applying feature threshold (0.7)...
Applying feature threshold (0.3)...
Applying feature threshold (0.9)...
Applying feature threshold (0.2)...
Applying feature threshold (0.6)...
Applying feature threshold (0.8)...
Applying feature threshold (0.4)...
Applying feature threshold (0.1)...
Applying feature threshold (0.98)...
Applying feature threshold (0.25)...
Applying feature threshold (0.75)...
Applying feature threshold (0.6)...
Applying feature threshold (0.85)...
Found a relation with a missing slot: age
Found a relation with a missing slot: occupation
Found a relation with a missing slot: address
Found a relation with a missing slot: email
Found a relation with a missing slot: phone number
Found a relation with a missing slot: date of birth
Found a relation with a missing slot: nationality
Found a relation with a missing slot: gender
Found a relation with a missing slot: height
Found a relation with a missing slot: weight
Found a relation with a missing slot: education
Found a relation with a missing slot: skills
Found a relation with a missing slot: experience
Read 5 rules from rules.txt for relation S
Read 20 rules from rules.txt for relation T
Read 8 rules from rules.txt for relation U
Read 12 rules from rules.txt for relation V
Read 15 rules from rules.txt for relation W
Read 4 rules from rules.txt for relation X
Read 7 rules from rules.txt for relation Y
Read 9 rules from rules.txt for relation Z
Read 3 rules from rules.txt for relation A
Read 6 rules from rules.txt for relation B
Read 14 rules from rules.txt for relation C
Read 11 rules from rules.txt for relation D
Read 18 rules from rules.txt for relation E
Creating SemgrexRegexExtractor
Creating SemgrexRegexExtractor
Creating SemgrexRegexExtractor
Creating SemgrexRegexExtractor
Creating SemgrexRegexExtractor
Creating SemgrexRegexExtractor
Creating SemgrexRegexExtractor
Creating SemgrexRegexExtractor
Creating SemgrexRegexExtractor
Creating SemgrexRegexExtractor
Creating SemgrexRegexExtractor
Creating SemgrexRegexExtractor
Creating SemgrexRegexExtractor
Creating SemgrexRegexExtractor
Creating SemgrexRegexExtractor
Used 2022-10-02 to recognize cat as animal
Used 2022-10-03 to recognize car as vehicle
Used 2022-10-04 to recognize dog as animal
Used 2022-10-05 to recognize book as item
Used 2022-10-06 to recognize tree as plant
Used 2022-10-07 to recognize chair as furniture
Used 2022-10-08 to recognize pen as stationery
Used 2022-10-09 to recognize phone as device
Used 2022-10-10 to recognize table as furniture
Used 2022-10-11 to recognize laptop as device
Used 2022-10-12 to recognize banana as fruit
Used 2022-10-13 to recognize shirt as clothing
Used 2022-10-14 to recognize airplane as vehicle
RECALL    (macro average): 0.600%
RECALL    (macro average): 0.900%
RECALL    (macro average): 0.450%
RECALL    (macro average): 0.800%
RECALL    (macro average): 0.350%
RECALL    (macro average): 0.700%
RECALL    (macro average): 0.550%
RECALL    (macro average): 0.950%
RECALL    (macro average): 0.400%
RECALL    (macro average): 0.850%
RECALL    (macro average): 0.300%
RECALL    (macro average): 0.650%
RECALL    (macro average): 0.500%
Starting insides...
Starting insides...
Starting insides...
Starting insides...
Starting insides...
Starting insides...
Starting insides...
Starting insides...
Starting insides...
Starting insides...
Starting insides...
Starting insides...
Starting insides...
Starting insides...
Starting insides...
PRECISION (macro average): 94.320%
PRECISION (macro average): 76.890%
PRECISION (macro average): 89.120%
PRECISION (macro average): 79.550%
PRECISION (macro average): 91.670%
PRECISION (macro average): 85.130%
PRECISION (macro average): 92.850%
PRECISION (macro average): 77.780%
PRECISION (macro average): 88.240%
PRECISION (macro average): 80.010%
PRECISION (macro average): 93.450%
PRECISION (macro average): 84.390%
PRECISION (macro average): 95.200%
WARNING: Forgot to call finishThread() on thread Thread-2
WARNING: Forgot to call finishThread() on thread Thread-3
WARNING: Forgot to call finishThread() on thread Thread-4
WARNING: Forgot to call finishThread() on thread Thread-5
WARNING: Forgot to call finishThread() on thread Thread-6
WARNING: Forgot to call finishThread() on thread Thread-7
WARNING: Forgot to call finishThread() on thread Thread-8
WARNING: Forgot to call finishThread() on thread Thread-9
WARNING: Forgot to call finishThread() on thread Thread-10
WARNING: Forgot to call finishThread() on thread Thread-11
WARNING: Forgot to call finishThread() on thread Thread-12
WARNING: Forgot to call finishThread() on thread Thread-13
WARNING: Forgot to call finishThread() on thread Thread-14
F1        (micro average): 90.123%
F1        (micro average): 34.567%
F1        (micro average): 12.345%
F1        (micro average): 78.901%
F1        (micro average): 23.456%
F1        (micro average): 45.678%
F1        (micro average): 67.890%
F1        (micro average): 89.012%
F1        (micro average): 11.111%
F1        (micro average): 22.222%
F1        (micro average): 33.333%
F1        (micro average): 44.444%
F1        (micro average): 55.555%
RECALL    (micro average): 83.215%
RECALL    (micro average): 91.542%
RECALL    (micro average): 75.893%
RECALL    (micro average): 88.679%
RECALL    (micro average): 94.762%
RECALL    (micro average): 80.592%
RECALL    (micro average): 87.937%
RECALL    (micro average): 92.815%
RECALL    (micro average): 78.213%
RECALL    (micro average): 81.594%
RECALL    (micro average): 89.436%
RECALL    (micro average): 84.617%
RECALL    (micro average): 97.954%
PRECISION (micro average): 82.450%
PRECISION (micro average): 94.712%
PRECISION (micro average): 75.342%
PRECISION (micro average): 89.219%
PRECISION (micro average): 63.982%
PRECISION (micro average): 71.567%
PRECISION (micro average): 80.073%
PRECISION (micro average): 96.821%
PRECISION (micro average): 78.924%
PRECISION (micro average): 88.563%
PRECISION (micro average): 93.758%
PRECISION (micro average): 69.502%
PRECISION (micro average): 85.219%
DepParser accepted tagging: word2|tag2, got score 0.6
DepParser accepted tagging: word3|tag3, got score 0.9
DepParser accepted tagging: word4|tag4, got score 0.5
DepParser accepted tagging: word5|tag5, got score 0.7
DepParser accepted tagging: word6|tag6, got score 0.3
DepParser accepted tagging: word7|tag7, got score 0.2
DepParser accepted tagging: word8|tag8, got score 0.4
DepParser accepted tagging: word9|tag9, got score 0.1
DepParser accepted tagging: word10|tag10, got score 0.5
DepParser accepted tagging: word11|tag11, got score 0.7
DepParser accepted tagging: word12|tag12, got score 0.9
DepParser accepted tagging: word13|tag13, got score 0.4
DepParser accepted tagging: word14|tag14, got score 0.6
Per-relation Accuracy
Per-relation Accuracy
Per-relation Accuracy
Per-relation Accuracy
Per-relation Accuracy
Per-relation Accuracy
Per-relation Accuracy
Per-relation Accuracy
Per-relation Accuracy
Per-relation Accuracy
Per-relation Accuracy
Per-relation Accuracy
Per-relation Accuracy
Per-relation Accuracy
Per-relation Accuracy
No statistical model will be used.
No statistical model will be used.
No statistical model will be used.
No statistical model will be used.
No statistical model will be used.
No statistical model will be used.
No statistical model will be used.
No statistical model will be used.
No statistical model will be used.
No statistical model will be used.
No statistical model will be used.
No statistical model will be used.
No statistical model will be used.
No statistical model will be used.
No statistical model will be used.
Created dparser arrays of size 100
Created dparser arrays of size 1000
Created dparser arrays of size 10000
Created dparser arrays of size 100000
Created dparser arrays of size 50
Created dparser arrays of size 500
Created dparser arrays of size 5000
Created dparser arrays of size 50000
Created dparser arrays of size 500000
Created dparser arrays of size 20
Created dparser arrays of size 200
Created dparser arrays of size 2000
Created dparser arrays of size 20000
Read statistical model from model2
Read statistical model from model3
Read statistical model from model4
Read statistical model from model5
Read statistical model from model6
Read statistical model from model7
Read statistical model from model8
Read statistical model from model9
Read statistical model from model10
Read statistical model from model11
Read statistical model from model12
Read statistical model from model13
Read statistical model from model14
<ERROR> tagging is impossible.
<INFO> tagging is impossible.
<ALERT> tagging is impossible.
<FAIL> tagging is impossible.
<ABORT> tagging is impossible.
<INVALID> tagging is impossible.
<FATAL> tagging is impossible.
<CRITICAL> tagging is impossible.
<NOTICE> tagging is impossible.
<DEBUG> tagging is impossible.
<TRACE> tagging is impossible.
<EXCEPTION> tagging is impossible.
<PANIC> tagging is impossible.
splitting into 2 entities
splitting into 2 entities
splitting into 2 entities
splitting into 2 entities
splitting into 2 entities
splitting into 2 entities
splitting into 2 entities
splitting into 2 entities
splitting into 2 entities
splitting into 2 entities
splitting into 2 entities
splitting into 2 entities
splitting into 2 entities
splitting into 2 entities
splitting into 2 entities
Used {ASSIMILATIONS3} to recognize image; splitAssimilations=false
Used {ASSIMILATIONS3} to recognize audio; splitAssimilations=true
Used {ASSIMILATIONS3} to recognize video; splitAssimilations=false
Used {ASSIMILATIONS3} to recognize file; splitAssimilations=true
Used {ASSIMILATIONS3} to recognize folder; splitAssimilations=false
Used {ASSIMILATIONS3} to recognize input; splitAssimilations=true
Used {ASSIMILATIONS3} to recognize output; splitAssimilations=false
Used {ASSIMILATIONS3} to recognize document; splitAssimilations=true
Used {ASSIMILATIONS3} to recognize form; splitAssimilations=false
Used {ASSIMILATIONS3} to recognize command; splitAssimilations=true
Used {ASSIMILATIONS3} to recognize argument; splitAssimilations=false
Used {ASSIMILATIONS3} to recognize parameter; splitAssimilations=true
Used {ASSIMILATIONS3} to recognize result; splitAssimilations=false
Error: Unable to load HeadFinder; default HeadFinder will be used.
Error: Unable to load HeadFinder; default HeadFinder will be used.
Error: Unable to load HeadFinder; default HeadFinder will be used.
Error: Unable to load HeadFinder; default HeadFinder will be used.
Error: Unable to load HeadFinder; default HeadFinder will be used.
Error: Unable to load HeadFinder; default HeadFinder will be used.
Error: Unable to load HeadFinder; default HeadFinder will be used.
Error: Unable to load HeadFinder; default HeadFinder will be used.
Error: Unable to load HeadFinder; default HeadFinder will be used.
Error: Unable to load HeadFinder; default HeadFinder will be used.
Error: Unable to load HeadFinder; default HeadFinder will be used.
Error: Unable to load HeadFinder; default HeadFinder will be used.
Error: Unable to load HeadFinder; default HeadFinder will be used.
Error: Unable to load HeadFinder; default HeadFinder will be used.
Error: Unable to load HeadFinder; default HeadFinder will be used.
prepending entity
prepending entity
prepending entity
prepending entity
prepending entity
prepending entity
prepending entity
prepending entity
prepending entity
prepending entity
prepending entity
prepending entity
prepending entity
prepending entity
prepending entity
... and removing beginning of next entity.
... and removing beginning of next entity.
... and removing beginning of next entity.
... and removing beginning of next entity.
... and removing beginning of next entity.
... and removing beginning of next entity.
... and removing beginning of next entity.
... and removing beginning of next entity.
... and removing beginning of next entity.
... and removing beginning of next entity.
... and removing beginning of next entity.
... and removing beginning of next entity.
... and removing beginning of next entity.
... and removing beginning of next entity.
... and removing beginning of next entity.
... and removing end of previous entity.
... and removing end of previous entity.
... and removing end of previous entity.
... and removing end of previous entity.
... and removing end of previous entity.
... and removing end of previous entity.
... and removing end of previous entity.
... and removing end of previous entity.
... and removing end of previous entity.
... and removing end of previous entity.
... and removing end of previous entity.
... and removing end of previous entity.
... and removing end of previous entity.
... and removing end of previous entity.
... and removing end of previous entity.
The words of the sentence:
The words of the sentence:
The words of the sentence:
The words of the sentence:
The words of the sentence:
The words of the sentence:
The words of the sentence:
The words of the sentence:
The words of the sentence:
The words of the sentence:
The words of the sentence:
The words of the sentence:
The words of the sentence:
The words of the sentence:
The words of the sentence:
adding singleton entity
adding singleton entity
adding singleton entity
adding singleton entity
adding singleton entity
adding singleton entity
adding singleton entity
adding singleton entity
adding singleton entity
adding singleton entity
adding singleton entity
adding singleton entity
adding singleton entity
adding singleton entity
adding singleton entity
splitting off prev entity
splitting off prev entity
splitting off prev entity
splitting off prev entity
splitting off prev entity
splitting off prev entity
splitting off prev entity
splitting off prev entity
splitting off prev entity
splitting off prev entity
splitting off prev entity
splitting off prev entity
splitting off prev entity
splitting off prev entity
splitting off prev entity
splitting off next entity
splitting off next entity
splitting off next entity
splitting off next entity
splitting off next entity
splitting off next entity
splitting off next entity
splitting off next entity
splitting off next entity
splitting off next entity
splitting off next entity
splitting off next entity
splitting off next entity
splitting off next entity
splitting off next entity
FYI: Redwood rocks
FYI: Redwood rocks
FYI: Redwood rocks
FYI: Redwood rocks
FYI: Redwood rocks
FYI: Redwood rocks
FYI: Redwood rocks
FYI: Redwood rocks
FYI: Redwood rocks
FYI: Redwood rocks
FYI: Redwood rocks
FYI: Redwood rocks
FYI: Redwood rocks
FYI: Redwood rocks
FYI: Redwood rocks
I'm warning you in Redwood!
I'm warning you in Redwood!
I'm warning you in Redwood!
I'm warning you in Redwood!
I'm warning you in Redwood!
I'm warning you in Redwood!
I'm warning you in Redwood!
I'm warning you in Redwood!
I'm warning you in Redwood!
I'm warning you in Redwood!
I'm warning you in Redwood!
I'm warning you in Redwood!
I'm warning you in Redwood!
I'm warning you in Redwood!
I'm warning you in Redwood!
changing position 2 from C to D
changing position 3 from E to F
changing position 4 from G to H
changing position 5 from I to J
changing position 6 from K to L
changing position 7 from M to N
changing position 8 from O to P
changing position 9 from Q to R
changing position 10 from S to T
changing position 11 from U to V
changing position 12 from W to X
changing position 13 from Y to Z
changing position 14 from AA to BB
<13> word features removed due to thresholding.
<8> word features removed due to thresholding.
<5> word features removed due to thresholding.
<12> word features removed due to thresholding.
<10> word features removed due to thresholding.
<7> word features removed due to thresholding.
<2> word features removed due to thresholding.
<15> word features removed due to thresholding.
<4> word features removed due to thresholding.
<9> word features removed due to thresholding.
<6> word features removed due to thresholding.
<3> word features removed due to thresholding.
<18> word features removed due to thresholding.
Total affix features: 20
Total affix features: 15
Total affix features: 5
Total affix features: 12
Total affix features: 8
Total affix features: 27
Total affix features: 18
Total affix features: 6
Total affix features: 23
Total affix features: 14
Total affix features: 9
Total affix features: 16
Total affix features: 11
I'm warning you!
I'm warning you!
I'm warning you!
I'm warning you!
I'm warning you!
I'm warning you!
I'm warning you!
I'm warning you!
I'm warning you!
I'm warning you!
I'm warning you!
I'm warning you!
I'm warning you!
I'm warning you!
I'm warning you!
Using level 0
Using level 0
Using level 0
Using level 0
Using level 0
Using level 0
Using level 0
Using level 0
Using level 0
Using level 0
Using level 0
Using level 0
Using level 0
Using level 0
Using level 0
Per-token marginalized probabilities
Per-token marginalized probabilities
Per-token marginalized probabilities
Per-token marginalized probabilities
Per-token marginalized probabilities
Per-token marginalized probabilities
Per-token marginalized probabilities
Per-token marginalized probabilities
Per-token marginalized probabilities
Per-token marginalized probabilities
Per-token marginalized probabilities
Per-token marginalized probabilities
Per-token marginalized probabilities
Per-token marginalized probabilities
Per-token marginalized probabilities
Ten best entity labelings
Ten best entity labelings
Ten best entity labelings
Ten best entity labelings
Ten best entity labelings
Ten best entity labelings
Ten best entity labelings
Ten best entity labelings
Ten best entity labelings
Ten best entity labelings
Ten best entity labelings
Ten best entity labelings
Ten best entity labelings
Ten best entity labelings
Ten best entity labelings
Feature level 2 is not supported in ChineseWordFeatureExtractor.
Feature level 3 is not supported in ChineseWordFeatureExtractor.
Feature level 4 is not supported in ChineseWordFeatureExtractor.
Feature level 5 is not supported in ChineseWordFeatureExtractor.
Feature level 6 is not supported in ChineseWordFeatureExtractor.
Feature level 7 is not supported in ChineseWordFeatureExtractor.
Feature level 8 is not supported in ChineseWordFeatureExtractor.
Feature level 9 is not supported in ChineseWordFeatureExtractor.
Feature level 10 is not supported in ChineseWordFeatureExtractor.
Feature level 11 is not supported in ChineseWordFeatureExtractor.
Feature level 12 is not supported in ChineseWordFeatureExtractor.
Feature level 13 is not supported in ChineseWordFeatureExtractor.
Feature level 14 is not supported in ChineseWordFeatureExtractor.
WARNING: Hello a fourth time from the class logger
ERROR: Hello a fourth time from the class logger
DEBUG: Hello a fourth time from the class logger
INFO: Hello a fourth time from the class logger
WARNING: Hello a fourth time from the class logger
ERROR: Hello a fourth time from the class logger
DEBUG: Hello a fourth time from the class logger
INFO: Hello a fourth time from the class logger
WARNING: Hello a fourth time from the class logger
ERROR: Hello a fourth time from the class logger
DEBUG: Hello a fourth time from the class logger
INFO: Hello a fourth time from the class logger
WARNING: Hello a fourth time from the class logger
ChineseUWM: using Good-Turing smoothing for unknown words.
ChineseUWM: using Good-Turing smoothing for unknown words.
ChineseUWM: using Good-Turing smoothing for unknown words.
ChineseUWM: using Good-Turing smoothing for unknown words.
ChineseUWM: using Good-Turing smoothing for unknown words.
ChineseUWM: using Good-Turing smoothing for unknown words.
ChineseUWM: using Good-Turing smoothing for unknown words.
ChineseUWM: using Good-Turing smoothing for unknown words.
ChineseUWM: using Good-Turing smoothing for unknown words.
ChineseUWM: using Good-Turing smoothing for unknown words.
ChineseUWM: using Good-Turing smoothing for unknown words.
ChineseUWM: using Good-Turing smoothing for unknown words.
ChineseUWM: using Good-Turing smoothing for unknown words.
ChineseUWM: using Good-Turing smoothing for unknown words.
ChineseUWM: using Good-Turing smoothing for unknown words.
Inside a track
Inside a track
Inside a track
Inside a track
Inside a track
Inside a track
Inside a track
Inside a track
Inside a track
Inside a track
Inside a track
Inside a track
Inside a track
Inside a track
Inside a track
ChineseUWM: treating unknown word as the average of their equivalents by first-character identity. useUnicodeType: false
ChineseUWM: treating unknown word as the average of their equivalents by first-character identity. useUnicodeType: yes
ChineseUWM: treating unknown word as the average of their equivalents by first-character identity. useUnicodeType: no
ChineseUWM: treating unknown word as the average of their equivalents by first-character identity. useUnicodeType: on
ChineseUWM: treating unknown word as the average of their equivalents by first-character identity. useUnicodeType: off
ChineseUWM: treating unknown word as the average of their equivalents by first-character identity. useUnicodeType: y
ChineseUWM: treating unknown word as the average of their equivalents by first-character identity. useUnicodeType: n
ChineseUWM: treating unknown word as the average of their equivalents by first-character identity. useUnicodeType: 1
ChineseUWM: treating unknown word as the average of their equivalents by first-character identity. useUnicodeType: 0
ChineseUWM: treating unknown word as the average of their equivalents by first-character identity. useUnicodeType: enabled
ChineseUWM: treating unknown word as the average of their equivalents by first-character identity. useUnicodeType: disabled
ChineseUWM: treating unknown word as the average of their equivalents by first-character identity. useUnicodeType: true
ChineseUWM: treating unknown word as the average of their equivalents by first-character identity. useUnicodeType: false
ChineseUWM: treating unknown word as the average of their equivalents by first-character identity. useUnicodeType: yes
Testing tagged word
Testing tagged word
Testing tagged word
Testing tagged word
Testing tagged word
Testing tagged word
Testing tagged word
Testing tagged word
Testing tagged word
Testing tagged word
Testing tagged word
Testing tagged word
Testing tagged word
Testing tagged word
Testing tagged word
Couldn't find end pattern!
Couldn't find end pattern!
Couldn't find end pattern!
Couldn't find end pattern!
Couldn't find end pattern!
Couldn't find end pattern!
Couldn't find end pattern!
Couldn't find end pattern!
Couldn't find end pattern!
Couldn't find end pattern!
Couldn't find end pattern!
Couldn't find end pattern!
Couldn't find end pattern!
Couldn't find end pattern!
Couldn't find end pattern!
Hello from Redwood -> Java again!
Hello from Redwood -> Java again!
Hello from Redwood -> Java again!
Hello from Redwood -> Java again!
Hello from Redwood -> Java again!
Hello from Redwood -> Java again!
Hello from Redwood -> Java again!
Hello from Redwood -> Java again!
Hello from Redwood -> Java again!
Hello from Redwood -> Java again!
Hello from Redwood -> Java again!
Hello from Redwood -> Java again!
Hello from Redwood -> Java again!
Hello from Redwood -> Java again!
Hello from Redwood -> Java again!
hooray dates!
hooray dates!
hooray dates!
hooray dates!
hooray dates!
hooray dates!
hooray dates!
hooray dates!
hooray dates!
hooray dates!
hooray dates!
hooray dates!
hooray dates!
hooray dates!
hooray dates!
Hello from Redwood -> Java!
Hello from Redwood -> Java!
Hello from Redwood -> Java!
Hello from Redwood -> Java!
Hello from Redwood -> Java!
Hello from Redwood -> Java!
Hello from Redwood -> Java!
Hello from Redwood -> Java!
Hello from Redwood -> Java!
Hello from Redwood -> Java!
Hello from Redwood -> Java!
Hello from Redwood -> Java!
Hello from Redwood -> Java!
Hello from Redwood -> Java!
Hello from Redwood -> Java!
Failed to find end for tag2
Failed to find end for tag3
Failed to find end for tag4
Failed to find end for tag5
Failed to find end for tag6
Failed to find end for tag7
Failed to find end for tag8
Failed to find end for tag9
Failed to find end for tag10
Failed to find end for tag11
Failed to find end for tag12
Failed to find end for tag13
Failed to find end for tag14
Uh-oh dates!
Uh-oh dates!
Uh-oh dates!
Uh-oh dates!
Uh-oh dates!
Uh-oh dates!
Uh-oh dates!
Uh-oh dates!
Uh-oh dates!
Uh-oh dates!
Uh-oh dates!
Uh-oh dates!
Uh-oh dates!
Uh-oh dates!
Uh-oh dates!
Hello from Redwood!
Hello from Redwood!
Hello from Redwood!
Hello from Redwood!
Hello from Redwood!
Hello from Redwood!
Hello from Redwood!
Hello from Redwood!
Hello from Redwood!
Hello from Redwood!
Hello from Redwood!
Hello from Redwood!
Hello from Redwood!
Hello from Redwood!
Hello from Redwood!
hooray numbers!
hooray numbers!
hooray numbers!
hooray numbers!
hooray numbers!
hooray numbers!
hooray numbers!
hooray numbers!
hooray numbers!
hooray numbers!
hooray numbers!
hooray numbers!
hooray numbers!
hooray numbers!
hooray numbers!
Uh-oh numbers!
Uh-oh numbers!
Uh-oh numbers!
Uh-oh numbers!
Uh-oh numbers!
Uh-oh numbers!
Uh-oh numbers!
Uh-oh numbers!
Uh-oh numbers!
Uh-oh numbers!
Uh-oh numbers!
Uh-oh numbers!
Uh-oh numbers!
Uh-oh numbers!
Uh-oh numbers!
hooray names!
hooray names!
hooray names!
hooray names!
hooray names!
hooray names!
hooray names!
hooray names!
hooray names!
hooray names!
hooray names!
hooray names!
hooray names!
hooray names!
hooray names!
Starting test
Starting test
Starting test
Starting test
Starting test
Starting test
Starting test
Starting test
Starting test
Starting test
Starting test
Starting test
Starting test
Starting test
Starting test
Uh-oh names!
Uh-oh names!
Uh-oh names!
Uh-oh names!
Uh-oh names!
Uh-oh names!
Uh-oh names!
Uh-oh names!
Uh-oh names!
Uh-oh names!
Uh-oh names!
Uh-oh names!
Uh-oh names!
Uh-oh names!
Uh-oh names!
Testing unknown matching
Testing unknown matching
Testing unknown matching
Testing unknown matching
Testing unknown matching
Testing unknown matching
Testing unknown matching
Testing unknown matching
Testing unknown matching
Testing unknown matching
Testing unknown matching
Testing unknown matching
Testing unknown matching
Testing unknown matching
Testing unknown matching
ihs contains d? false
ihs contains d? true
ihs contains d? false
ihs contains d? true
ihs contains d? false
ihs contains d? true
ihs contains d? false
ihs contains d? true
ihs contains d? false
ihs contains d? true
ihs contains d? false
ihs contains d? true
ihs contains d? false
ihs contains c? false
ihs contains c? true
ihs contains c? false
ihs contains c? true
ihs contains c? false
ihs contains c? true
ihs contains c? false
ihs contains c? true
ihs contains c? false
ihs contains c? true
ihs contains c? false
ihs contains c? true
ihs contains c? false
Unknown word estimate for banana as NN: -3.256
Unknown word estimate for cherry as NN: -2.789
Unknown word estimate for dog as NN: -1.987
Unknown word estimate for elephant as NN: -2.123
Unknown word estimate for cat as NN: -3.543
Unknown word estimate for fish as NN: -3.998
Unknown word estimate for elephant as NN: -2.345
Unknown word estimate for dog as NN: -1.452
Unknown word estimate for apple as NN: -3.214
Unknown word estimate for cat as NN: -2.987
Unknown word estimate for banana as NN: -4.123
Unknown word estimate for cherry as NN: -3.098
Unknown word estimate for fish as NN: -2.789
Warning: proposed tag is unseen in training data!
Warning: proposed tag is unseen in training data!
Warning: proposed tag is unseen in training data!
Warning: proposed tag is unseen in training data!
Warning: proposed tag is unseen in training data!
Warning: proposed tag is unseen in training data!
Warning: proposed tag is unseen in training data!
Warning: proposed tag is unseen in training data!
Warning: proposed tag is unseen in training data!
Warning: proposed tag is unseen in training data!
Warning: proposed tag is unseen in training data!
Warning: proposed tag is unseen in training data!
Warning: proposed tag is unseen in training data!
Warning: proposed tag is unseen in training data!
Warning: proposed tag is unseen in training data!
ihs contains b? false
ihs contains b? true
ihs contains b? false
ihs contains b? true
ihs contains b? false
ihs contains b? true
ihs contains b? false
ihs contains b? true
ihs contains b? false
ihs contains b? true
ihs contains b? false
ihs contains b? true
ihs contains b? false
#constraints: 200
#constraints: 300
#constraints: 400
#constraints: 500
#constraints: 600
#constraints: 700
#constraints: 800
#constraints: 900
#constraints: 1000
#constraints: 1100
#constraints: 1200
#constraints: 1300
#constraints: 1400
ihs contains a? false
ihs contains a? true
ihs contains a? false
ihs contains a? true
ihs contains a? false
ihs contains a? true
ihs contains a? false
ihs contains a? true
ihs contains a? false
ihs contains a? true
ihs contains a? false
ihs contains a? true
ihs contains a? false
hs contains d? false
hs contains d? false
hs contains d? true
hs contains d? true
hs contains d? false
hs contains d? true
hs contains d? false
hs contains d? true
hs contains d? true
hs contains d? false
hs contains d? false
hs contains d? true
hs contains d? false
Label dictionary enabled
Label dictionary enabled
Label dictionary enabled
Label dictionary enabled
Label dictionary enabled
Label dictionary enabled
Label dictionary enabled
Label dictionary enabled
Label dictionary enabled
Label dictionary enabled
Label dictionary enabled
Label dictionary enabled
Label dictionary enabled
Label dictionary enabled
Label dictionary enabled
Used {EMAIL} to recognize token2
Used {EMAIL} to recognize token3
Used {EMAIL} to recognize token4
Used {EMAIL} to recognize token5
Used {EMAIL} to recognize token6
Used {EMAIL} to recognize token7
Used {EMAIL} to recognize token8
Used {EMAIL} to recognize token9
Used {EMAIL} to recognize token10
Used {EMAIL} to recognize token11
Used {EMAIL} to recognize token12
Used {EMAIL} to recognize token13
Used {EMAIL} to recognize token14
Scoring unknown word |banana| with tag VERB
Scoring unknown word |carrot| with tag ADJ
Scoring unknown word |dog| with tag NOUN
Scoring unknown word |elephant| with tag NOUN
Scoring unknown word |fox| with tag VERB
Scoring unknown word |grape| with tag NOUN
Scoring unknown word |horse| with tag NOUN
Scoring unknown word |ice cream| with tag NOUN
Scoring unknown word |jaguar| with tag NOUN
Scoring unknown word |kiwi| with tag NOUN
Scoring unknown word |lion| with tag NOUN
Scoring unknown word |mango| with tag NOUN
Scoring unknown word |nut| with tag NOUN
There were 10 trees.
There were 3 trees.
There were 20 trees.
There were 5 trees.
There were 16 trees.
There were 9 trees.
There were 13 trees.
There were 6 trees.
There were 11 trees.
There were 4 trees.
There were 8 trees.
There were 12 trees.
There were 2 trees.
hs contains a? false
hs contains a? true
hs contains a? false
hs contains a? true
hs contains a? false
hs contains a? true
hs contains a? false
hs contains a? true
hs contains a? false
hs contains a? true
hs contains a? false
hs contains a? true
hs contains a? false
error computing conditional log prob
error computing conditional log prob
error computing conditional log prob
error computing conditional log prob
error computing conditional log prob
error computing conditional log prob
error computing conditional log prob
error computing conditional log prob
error computing conditional log prob
error computing conditional log prob
error computing conditional log prob
error computing conditional log prob
error computing conditional log prob
error computing conditional log prob
error computing conditional log prob
Used {SGML2} to recognize input string as output string
Used {SGML2} to recognize content as data
Used {SGML2} to recognize old file as new file
Used {SGML2} to recognize raw data as formatted data
Used {SGML2} to recognize source code as compiled code
Used {SGML2} to recognize user input as processed data
Used {SGML2} to recognize HTML document as PDF file
Used {SGML2} to recognize image data as pixel values
Used {SGML2} to recognize audio file as waveform
Used {SGML2} to recognize video stream as frames
Used {SGML2} to recognize JSON object as XML representation
Used {SGML2} to recognize XML document as structured data
Used {SGML2} to recognize CSV file as tabular data
Used {SGML2} to recognize log entries as events
IdentityHashSet ihs contains a and b: false
IdentityHashSet ihs contains a and b: true
IdentityHashSet ihs contains a and b: false
IdentityHashSet ihs contains a and b: true
IdentityHashSet ihs contains a and b: false
IdentityHashSet ihs contains a and b: true
IdentityHashSet ihs contains a and b: false
IdentityHashSet ihs contains a and b: true
IdentityHashSet ihs contains a and b: false
IdentityHashSet ihs contains a and b: true
IdentityHashSet ihs contains a and b: false
IdentityHashSet ihs contains a and b: true
IdentityHashSet ihs contains a and b: false
IdentityHashSet ihs contains a and b: true
conditionalLogProbGivenFirst
conditionalLogProbGivenFirst
conditionalLogProbGivenFirst
conditionalLogProbGivenFirst
conditionalLogProbGivenFirst
conditionalLogProbGivenFirst
conditionalLogProbGivenFirst
conditionalLogProbGivenFirst
conditionalLogProbGivenFirst
conditionalLogProbGivenFirst
conditionalLogProbGivenFirst
conditionalLogProbGivenFirst
conditionalLogProbGivenFirst
conditionalLogProbGivenFirst
conditionalLogProbGivenFirst
Default encoding is: Latin-1
Default encoding is: ASCII
Default encoding is: UTF-16
Default encoding is: UTF-32
Default encoding is: UTF-8
Default encoding is: UTF-8
Default encoding is: Latin-1
Default encoding is: ASCII
Default encoding is: UTF-16
Default encoding is: UTF-32
Default encoding is: UTF-8
Default encoding is: UTF-8
Default encoding is: Latin-1
HashSet hs contains a and b: true
HashSet hs contains a and b: false
HashSet hs contains a and b: true
HashSet hs contains a and b: false
HashSet hs contains a and b: true
HashSet hs contains a and b: false
HashSet hs contains a and b: true
HashSet hs contains a and b: false
HashSet hs contains a and b: true
HashSet hs contains a and b: false
HashSet hs contains a and b: true
HashSet hs contains a and b: false
HashSet hs contains a and b: true
HashSet hs contains a and b: false
Using UTF-8 to construct MemoryTreebank
Using UTF-8 to construct MemoryTreebank
Using UTF-8 to construct MemoryTreebank
Using UTF-8 to construct MemoryTreebank
Using UTF-8 to construct MemoryTreebank
Using UTF-8 to construct MemoryTreebank
Using UTF-8 to construct MemoryTreebank
Using UTF-8 to construct MemoryTreebank
Using UTF-8 to construct MemoryTreebank
Using UTF-8 to construct MemoryTreebank
Using UTF-8 to construct MemoryTreebank
Using UTF-8 to construct MemoryTreebank
Using UTF-8 to construct MemoryTreebank
Using UTF-8 to construct MemoryTreebank
Using UTF-8 to construct MemoryTreebank
List d is empty
List d is [1, 2, 3]
List d is [a, b, c]
List d is [true, false]
List d is [apple, banana, orange]
List d is [Monday, Tuesday, Wednesday]
List d is [January, February, March]
List d is [red, green, blue]
List d is [dog, cat, rabbit]
List d is [male, female]
List d is [one, two, three]
List d is [happy, sad, excited]
List d is [Java, Python, C++]
NoDist score: 78
NoDist score: 86
NoDist score: 90
NoDist score: 83
NoDist score: 91
NoDist score: 88
NoDist score: 76
NoDist score: 82
NoDist score: 85
NoDist score: 93
NoDist score: 89
NoDist score: 80
NoDist score: 77
Normalization Z = 1
Normalization Z = 2
Normalization Z = 3
Normalization Z = 4
Normalization Z = 5
Normalization Z = 6
Normalization Z = 7
Normalization Z = 8
Normalization Z = 9
Normalization Z = 10
Normalization Z = 11
Normalization Z = 12
Normalization Z = 13
Creating factor table with 8 classes and window (clique) size 15
Creating factor table with 3 classes and window (clique) size 7
Creating factor table with 6 classes and window (clique) size 12
Creating factor table with 4 classes and window (clique) size 8
Creating factor table with 2 classes and window (clique) size 13
Creating factor table with 7 classes and window (clique) size 9
Creating factor table with 10 classes and window (clique) size 18
Creating factor table with 1 classes and window (clique) size 5
Creating factor table with 9 classes and window (clique) size 16
Creating factor table with 12 classes and window (clique) size 22
Creating factor table with 15 classes and window (clique) size 27
Creating factor table with 11 classes and window (clique) size 20
Creating factor table with 14 classes and window (clique) size 25
List b is [apple, banana, orange]
List b is [true, false, true]
List b is [red, green, blue]
List b is [dog, cat, bird]
List b is [Monday, Tuesday, Wednesday]
List b is [1.5, 2.3, 3.7]
List b is [John, Jane, James]
List b is [book, pen, notebook]
List b is [10, 20, 30]
List b is [January, February, March]
List b is [apple, orange, grape]
List b is [dog, horse, rabbit]
List b is [yes, no, maybe]
Rank of banana is 15
Rank of orange is 7
Rank of mango is 21
Rank of strawberry is 14
Rank of pineapple is 9
Rank of grape is 12
Rank of watermelon is 18
Rank of kiwi is 6
Rank of pear is 17
Rank of cherry is 11
Rank of lemon is 22
Rank of peach is 5
Rank of plum is 19
outputLayerWeights4Edge deriv(3,4) = 0.8 - 0.6 = 0.2
outputLayerWeights4Edge deriv(5,6) = 0.9 - 0.1 = 0.8
outputLayerWeights4Edge deriv(7,8) = 0.6 - 0.3 = 0.3
outputLayerWeights4Edge deriv(9,10) = 0.7 - 0.4 = 0.3
outputLayerWeights4Edge deriv(11,12) = 0.9 - 0.7 = 0.2
outputLayerWeights4Edge deriv(13,14) = 0.8 - 0.5 = 0.3
outputLayerWeights4Edge deriv(15,16) = 0.3 - 0.1 = 0.2
outputLayerWeights4Edge deriv(17,18) = 0.5 - 0.3 = 0.2
outputLayerWeights4Edge deriv(19,20) = 0.7 - 0.2 = 0.5
outputLayerWeights4Edge deriv(21,22) = 0.8 - 0.6 = 0.2
outputLayerWeights4Edge deriv(23,24) = 0.6 - 0.4 = 0.2
outputLayerWeights4Edge deriv(25,26) = 0.9 - 0.1 = 0.8
outputLayerWeights4Edge deriv(27,28) = 0.6 - 0.3 = 0.3
Used {THING3} to recognize report.pdf as normal token
Used {THING3} to recognize image.jpg as normal token
Used {THING3} to recognize data.csv as normal token
Used {THING3} to recognize code.py as normal token
Used {THING3} to recognize audio.mp3 as normal token
Used {THING3} to recognize video.mp4 as normal token
Used {THING3} to recognize archive.zip as normal token
Used {THING3} to recognize presentation.ppt as normal token
Used {THING3} to recognize spreadsheet.xlsx as normal token
Used {THING3} to recognize script.js as normal token
Used {THING3} to recognize executable.exe as normal token
Used {THING3} to recognize database.db as normal token
Used {THING3} to recognize configuration.conf as normal token
incorporating priors ...
incorporating priors ...
incorporating priors ...
incorporating priors ...
incorporating priors ...
incorporating priors ...
incorporating priors ...
incorporating priors ...
incorporating priors ...
incorporating priors ...
incorporating priors ...
incorporating priors ...
incorporating priors ...
incorporating priors ...
incorporating priors ...
usage: java EditDistance str1 str2
usage: java EditDistance str1 str2
usage: java EditDistance str1 str2
usage: java EditDistance str1 str2
usage: java EditDistance str1 str2
usage: java EditDistance str1 str2
usage: java EditDistance str1 str2
usage: java EditDistance str1 str2
usage: java EditDistance str1 str2
usage: java EditDistance str1 str2
usage: java EditDistance str1 str2
usage: java EditDistance str1 str2
usage: java EditDistance str1 str2
usage: java EditDistance str1 str2
usage: java EditDistance str1 str2
SimWordAvg stats:
SimWordAvg stats:
SimWordAvg stats:
SimWordAvg stats:
SimWordAvg stats:
SimWordAvg stats:
SimWordAvg stats:
SimWordAvg stats:
SimWordAvg stats:
SimWordAvg stats:
SimWordAvg stats:
SimWordAvg stats:
SimWordAvg stats:
SimWordAvg stats:
SimWordAvg stats:
7 correct out of 10 -- ACC: 0.7
13 correct out of 20 -- ACC: 0.65
20 correct out of 25 -- ACC: 0.8
18 correct out of 22 -- ACC: 0.8181818181818182
12 correct out of 18 -- ACC: 0.6666666666666666
25 correct out of 30 -- ACC: 0.8333333333333334
16 correct out of 20 -- ACC: 0.8
9 correct out of 12 -- ACC: 0.75
21 correct out of 25 -- ACC: 0.84
14 correct out of 16 -- ACC: 0.875
8 correct out of 10 -- ACC: 0.8
5 correct out of 7 -- ACC: 0.7142857142857143
19 correct out of 20 -- ACC: 0.95
cache stats: size: 200, hits: 180, misses: 20, puts: 50, hit % (using misses): 0.9, hit % (using puts): 0.782
cache stats: size: 150, hits: 120, misses: 30, puts: 40, hit % (using misses): 0.8, hit % (using puts): 0.75
cache stats: size: 250, hits: 200, misses: 50, puts: 80, hit % (using misses): 0.8, hit % (using puts): 0.714
cache stats: size: 180, hits: 150, misses: 30, puts: 60, hit % (using misses): 0.833, hit % (using puts): 0.714
cache stats: size: 120, hits: 100, misses: 20, puts: 40, hit % (using misses): 0.833, hit % (using puts): 0.714
cache stats: size: 300, hits: 240, misses: 60, puts: 100, hit % (using misses): 0.8, hit % (using puts): 0.706
cache stats: size: 220, hits: 180, misses: 40, puts: 70, hit % (using misses): 0.818, hit % (using puts): 0.72
cache stats: size: 160, hits: 130, misses: 30, puts: 50, hit % (using misses): 0.812, hit % (using puts): 0.722
cache stats: size: 280, hits: 230, misses: 50, puts: 90, hit % (using misses): 0.821, hit % (using puts): 0.719
cache stats: size: 190, hits: 150, misses: 40, puts: 60, hit % (using misses): 0.789, hit % (using puts): 0.714
cache stats: size: 130, hits: 100, misses: 30, puts: 40, hit % (using misses): 0.769, hit % (using puts): 0.714
cache stats: size: 320, hits: 260, misses: 60, puts: 110, hit % (using misses): 0.812, hit % (using puts): 0.702
cache stats: size: 240, hits: 190, misses: 50, puts: 80, hit % (using misses): 0.792, hit % (using puts): 0.703
Training lexicon...
Training lexicon...
Training lexicon...
Training lexicon...
Training lexicon...
Training lexicon...
Training lexicon...
Training lexicon...
Training lexicon...
Training lexicon...
Training lexicon...
Training lexicon...
Training lexicon...
Training lexicon...
Training lexicon...
Annotating trees...
Annotating trees...
Annotating trees...
Annotating trees...
Annotating trees...
Annotating trees...
Annotating trees...
Annotating trees...
Annotating trees...
Annotating trees...
Annotating trees...
Annotating trees...
Annotating trees...
Annotating trees...
Annotating trees...
Reading Trees...
Reading Trees...
Reading Trees...
Reading Trees...
Reading Trees...
Reading Trees...
Reading Trees...
Reading Trees...
Reading Trees...
Reading Trees...
Reading Trees...
Reading Trees...
Reading Trees...
Reading Trees...
Reading Trees...
usage: java edu.stanford.nlp.parser.lexparser.ChineseLexiconAndWordSegmenter-train trainFilesPath [start stop] serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.ChineseLexiconAndWordSegmenter-train trainFilesPath [start stop] serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.ChineseLexiconAndWordSegmenter-train trainFilesPath [start stop] serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.ChineseLexiconAndWordSegmenter-train trainFilesPath [start stop] serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.ChineseLexiconAndWordSegmenter-train trainFilesPath [start stop] serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.ChineseLexiconAndWordSegmenter-train trainFilesPath [start stop] serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.ChineseLexiconAndWordSegmenter-train trainFilesPath [start stop] serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.ChineseLexiconAndWordSegmenter-train trainFilesPath [start stop] serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.ChineseLexiconAndWordSegmenter-train trainFilesPath [start stop] serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.ChineseLexiconAndWordSegmenter-train trainFilesPath [start stop] serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.ChineseLexiconAndWordSegmenter-train trainFilesPath [start stop] serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.ChineseLexiconAndWordSegmenter-train trainFilesPath [start stop] serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.ChineseLexiconAndWordSegmenter-train trainFilesPath [start stop] serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.ChineseLexiconAndWordSegmenter-train trainFilesPath [start stop] serializedParserFilename
usage: java edu.stanford.nlp.parser.lexparser.ChineseLexiconAndWordSegmenter-train trainFilesPath [start stop] serializedParserFilename
calculating derivative
calculating derivative
calculating derivative
calculating derivative
calculating derivative
calculating derivative
calculating derivative
calculating derivative
calculating derivative
calculating derivative
calculating derivative
calculating derivative
calculating derivative
calculating derivative
calculating derivative
Usage: must specify a text segmenter data output path
Usage: must specify a text segmenter data output path
Usage: must specify a text segmenter data output path
Usage: must specify a text segmenter data output path
Usage: must specify a text segmenter data output path
Usage: must specify a text segmenter data output path
Usage: must specify a text segmenter data output path
Usage: must specify a text segmenter data output path
Usage: must specify a text segmenter data output path
Usage: must specify a text segmenter data output path
Usage: must specify a text segmenter data output path
Usage: must specify a text segmenter data output path
Usage: must specify a text segmenter data output path
Usage: must specify a text segmenter data output path
Usage: must specify a text segmenter data output path
calculating E[2]
calculating E[3]
calculating E[4]
calculating E[5]
calculating E[6]
calculating E[7]
calculating E[8]
calculating E[9]
calculating E[10]
calculating E[11]
calculating E[12]
calculating E[13]
calculating E[14]
Error loading segmenter, exiting...
Error loading segmenter, exiting...
Error loading segmenter, exiting...
Error loading segmenter, exiting...
Error loading segmenter, exiting...
Error loading segmenter, exiting...
Error loading segmenter, exiting...
Error loading segmenter, exiting...
Error loading segmenter, exiting...
Error loading segmenter, exiting...
Error loading segmenter, exiting...
Error loading segmenter, exiting...
Error loading segmenter, exiting...
Error loading segmenter, exiting...
Error loading segmenter, exiting...
calculating Ehat[2]
calculating Ehat[3]
calculating Ehat[4]
calculating Ehat[5]
calculating Ehat[6]
calculating Ehat[7]
calculating Ehat[8]
calculating Ehat[9]
calculating Ehat[10]
calculating Ehat[11]
calculating Ehat[12]
calculating Ehat[13]
calculating Ehat[14]
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
usage: java edu.stanford.nlp.parser.lexparser.LexicalizedParser parserFileOrUrl filename*
processing doc req_456
processing doc req_789
processing doc req_012
processing doc req_345
processing doc req_678
processing doc req_901
processing doc req_234
processing doc req_567
processing doc req_890
processing doc req_123
processing doc req_456
processing doc req_789
processing doc req_012
Loading segmenter from serialized file data.bin ...
Loading segmenter from serialized file segmenter.bin ...
Loading segmenter from serialized file model_url ...
Loading segmenter from serialized file model.dat ...
Loading segmenter from serialized file model.jar ...
Loading segmenter from serialized file segmenter.zip ...
Loading segmenter from serialized file segmenter_url ...
Loading segmenter from serialized file serialized_data ...
Loading segmenter from serialized file serialized_model.bin ...
Loading segmenter from serialized file serialized_segmenter ...
Loading segmenter from serialized file serialized_model_url ...
Loading segmenter from serialized file segmenter_data.bin ...
Loading segmenter from serialized file segmenter_model.dat ...
domainDimension: 250
domainDimension: 500
domainDimension: 1000
domainDimension: 2000
domainDimension: 5000
domainDimension: 10000
domainDimension: 20000
domainDimension: 50000
domainDimension: 100000
domainDimension: 200000
domainDimension: 500000
domainDimension: 1000000
domainDimension: 2000000
Error in placement in the heap!
Error in placement in the heap!
Error in placement in the heap!
Error in placement in the heap!
Error in placement in the heap!
Error in placement in the heap!
Error in placement in the heap!
Error in placement in the heap!
Error in placement in the heap!
Error in placement in the heap!
Error in placement in the heap!
Error in placement in the heap!
Error in placement in the heap!
Error in placement in the heap!
Error in placement in the heap!
Error in the ordering of the heap! (2)
Error in the ordering of the heap! (3)
Error in the ordering of the heap! (4)
Error in the ordering of the heap! (5)
Error in the ordering of the heap! (6)
Error in the ordering of the heap! (7)
Error in the ordering of the heap! (8)
Error in the ordering of the heap! (9)
Error in the ordering of the heap! (10)
Error in the ordering of the heap! (11)
Error in the ordering of the heap! (12)
Error in the ordering of the heap! (13)
Error in the ordering of the heap! (14)
Writing parser in text grammar format to file parser2.txt
Writing parser in text grammar format to file parser3.txt
Writing parser in text grammar format to file parser4.txt
Writing parser in text grammar format to file parser5.txt
Writing parser in text grammar format to file parser6.txt
Writing parser in text grammar format to file parser7.txt
Writing parser in text grammar format to file parser8.txt
Writing parser in text grammar format to file parser9.txt
Writing parser in text grammar format to file parser10.txt
Writing parser in text grammar format to file parser11.txt
Writing parser in text grammar format to file parser12.txt
Writing parser in text grammar format to file parser13.txt
Writing parser in text grammar format to file parser14.txt
Trouble saving segmenter data to ASCII format.
Trouble saving segmenter data to ASCII format.
Trouble saving segmenter data to ASCII format.
Trouble saving segmenter data to ASCII format.
Trouble saving segmenter data to ASCII format.
Trouble saving segmenter data to ASCII format.
Trouble saving segmenter data to ASCII format.
Trouble saving segmenter data to ASCII format.
Trouble saving segmenter data to ASCII format.
Trouble saving segmenter data to ASCII format.
Trouble saving segmenter data to ASCII format.
Trouble saving segmenter data to ASCII format.
Trouble saving segmenter data to ASCII format.
Trouble saving segmenter data to ASCII format.
Trouble saving segmenter data to ASCII format.
Output layer not activated, inputLayerSize must be equal to numClasses, setting it to 20
Output layer not activated, inputLayerSize must be equal to numClasses, setting it to 5
Output layer not activated, inputLayerSize must be equal to numClasses, setting it to 15
Output layer not activated, inputLayerSize must be equal to numClasses, setting it to 8
Output layer not activated, inputLayerSize must be equal to numClasses, setting it to 12
Output layer not activated, inputLayerSize must be equal to numClasses, setting it to 7
Output layer not activated, inputLayerSize must be equal to numClasses, setting it to 18
Output layer not activated, inputLayerSize must be equal to numClasses, setting it to 6
Output layer not activated, inputLayerSize must be equal to numClasses, setting it to 13
Output layer not activated, inputLayerSize must be equal to numClasses, setting it to 9
Output layer not activated, inputLayerSize must be equal to numClasses, setting it to 16
Output layer not activated, inputLayerSize must be equal to numClasses, setting it to 3
Output layer not activated, inputLayerSize must be equal to numClasses, setting it to 14
Used {TWITTER} to recognize tok2
Used {TWITTER} to recognize tok3
Used {TWITTER} to recognize tok4
Used {TWITTER} to recognize tok5
Used {TWITTER} to recognize tok6
Used {TWITTER} to recognize tok7
Used {TWITTER} to recognize tok8
Used {TWITTER} to recognize tok9
Used {TWITTER} to recognize tok10
Used {TWITTER} to recognize tok11
Used {TWITTER} to recognize tok12
Used {TWITTER} to recognize tok13
Used {TWITTER} to recognize tok14
numOfEdgeFeatures: 10
numOfEdgeFeatures: 15
numOfEdgeFeatures: 20
numOfEdgeFeatures: 25
numOfEdgeFeatures: 30
numOfEdgeFeatures: 35
numOfEdgeFeatures: 40
numOfEdgeFeatures: 45
numOfEdgeFeatures: 50
numOfEdgeFeatures: 55
numOfEdgeFeatures: 60
numOfEdgeFeatures: 65
numOfEdgeFeatures: 70
ChineseLexiconAndWordSegmenter invoked with arguments:
ChineseLexiconAndWordSegmenter invoked with arguments:
ChineseLexiconAndWordSegmenter invoked with arguments:
ChineseLexiconAndWordSegmenter invoked with arguments:
ChineseLexiconAndWordSegmenter invoked with arguments:
ChineseLexiconAndWordSegmenter invoked with arguments:
ChineseLexiconAndWordSegmenter invoked with arguments:
ChineseLexiconAndWordSegmenter invoked with arguments:
ChineseLexiconAndWordSegmenter invoked with arguments:
ChineseLexiconAndWordSegmenter invoked with arguments:
ChineseLexiconAndWordSegmenter invoked with arguments:
ChineseLexiconAndWordSegmenter invoked with arguments:
ChineseLexiconAndWordSegmenter invoked with arguments:
ChineseLexiconAndWordSegmenter invoked with arguments:
ChineseLexiconAndWordSegmenter invoked with arguments:
Used FAKEDUCKFEET to recognize token2
Used FAKEDUCKFEET to recognize token3
Used FAKEDUCKFEET to recognize token4
Used FAKEDUCKFEET to recognize token5
Used FAKEDUCKFEET to recognize token6
Used FAKEDUCKFEET to recognize token7
Used FAKEDUCKFEET to recognize token8
Used FAKEDUCKFEET to recognize token9
Used FAKEDUCKFEET to recognize token10
Used FAKEDUCKFEET to recognize token11
Used FAKEDUCKFEET to recognize token12
Used FAKEDUCKFEET to recognize token13
Used FAKEDUCKFEET to recognize token14
Parent post annotation split categories: category2
Parent post annotation split categories: category3
Parent post annotation split categories: category4
Parent post annotation split categories: category5
Parent post annotation split categories: category6
Parent post annotation split categories: category7
Parent post annotation split categories: category8
Parent post annotation split categories: category9
Parent post annotation split categories: category10
Parent post annotation split categories: category11
Parent post annotation split categories: category12
Parent post annotation split categories: category13
Parent post annotation split categories: category14
Used APOWORD to recognize image.jpg as JPG; quoteStyle=false; probablyLeft=true
Used APOWORD to recognize socket.png as PNG; quoteStyle=true; probablyLeft=true
Used APOWORD to recognize data.csv as CSV; quoteStyle=false; probablyLeft=true
Used APOWORD to recognize script.js as JS; quoteStyle=true; probablyLeft=false
Used APOWORD to recognize index.html as HTML; quoteStyle=false; probablyLeft=false
Used APOWORD to recognize config.ini as INI; quoteStyle=true; probablyLeft=true
Used APOWORD to recognize document.docx as DOCX; quoteStyle=false; probablyLeft=false
Used APOWORD to recognize video.mp4 as MP4; quoteStyle=true; probablyLeft=false
Used APOWORD to recognize audio.wav as WAV; quoteStyle=false; probablyLeft=true
Used APOWORD to recognize image.png as PNG; quoteStyle=true; probablyLeft=false
Used APOWORD to recognize data.json as JSON; quoteStyle=false; probablyLeft=true
Used APOWORD to recognize script.py as PY; quoteStyle=true; probablyLeft=false
Used APOWORD to recognize index.css as CSS; quoteStyle=false; probablyLeft=false
dropout took: 248 ms
dropout took: 312 ms
dropout took: 421 ms
dropout took: 563 ms
dropout took: 646 ms
dropout took: 711 ms
dropout took: 822 ms
dropout took: 954 ms
dropout took: 1078 ms
dropout took: 1196 ms
dropout took: 1275 ms
dropout took: 1368 ms
dropout took: 1487 ms
Could not scan class: com.example.ClassB (in jar: bar.jar)
Could not scan class: com.example.ClassC (in jar: baz.jar)
Could not scan class: com.example.ClassD (in jar: qux.jar)
Could not scan class: com.example.ClassE (in jar: xyz.jar)
Could not scan class: com.example.ClassF (in jar: abc.jar)
Could not scan class: com.example.ClassG (in jar: def.jar)
Could not scan class: com.example.ClassH (in jar: ghi.jar)
Could not scan class: com.example.ClassI (in jar: jkl.jar)
Could not scan class: com.example.ClassJ (in jar: mno.jar)
Could not scan class: com.example.ClassK (in jar: pqr.jar)
Could not scan class: com.example.ClassL (in jar: stu.jar)
Could not scan class: com.example.ClassM (in jar: vwx.jar)
Could not scan class: com.example.ClassN (in jar: yza.jar)
dropoutPriorGrad.keys:[
dropoutPriorGrad.keys:[
dropoutPriorGrad.keys:[
dropoutPriorGrad.keys:[
dropoutPriorGrad.keys:[
dropoutPriorGrad.keys:[
dropoutPriorGrad.keys:[
dropoutPriorGrad.keys:[
dropoutPriorGrad.keys:[
dropoutPriorGrad.keys:[
dropoutPriorGrad.keys:[
dropoutPriorGrad.keys:[
dropoutPriorGrad.keys:[
dropoutPriorGrad.keys:[
dropoutPriorGrad.keys:[
Skipping sentence; too long: 20
Skipping sentence; too long: 5
Skipping sentence; too long: 15
Skipping sentence; too long: 8
Skipping sentence; too long: 12
Skipping sentence; too long: 3
Skipping sentence; too long: 7
Skipping sentence; too long: 18
Skipping sentence; too long: 6
Skipping sentence; too long: 9
Skipping sentence; too long: 14
Skipping sentence; too long: 4
Skipping sentence; too long: 17
dropoutPriorGradFirstHalf.keys:[
dropoutPriorGradFirstHalf.keys:[
dropoutPriorGradFirstHalf.keys:[
dropoutPriorGradFirstHalf.keys:[
dropoutPriorGradFirstHalf.keys:[
dropoutPriorGradFirstHalf.keys:[
dropoutPriorGradFirstHalf.keys:[
dropoutPriorGradFirstHalf.keys:[
dropoutPriorGradFirstHalf.keys:[
dropoutPriorGradFirstHalf.keys:[
dropoutPriorGradFirstHalf.keys:[
dropoutPriorGradFirstHalf.keys:[
dropoutPriorGradFirstHalf.keys:[
dropoutPriorGradFirstHalf.keys:[
dropoutPriorGradFirstHalf.keys:[
Could not open jar file: file2.jar(are you sure the file exists?)
Could not open jar file: file3.jar(are you sure the file exists?)
Could not open jar file: file4.jar(are you sure the file exists?)
Could not open jar file: file5.jar(are you sure the file exists?)
Could not open jar file: file6.jar(are you sure the file exists?)
Could not open jar file: file7.jar(are you sure the file exists?)
Could not open jar file: file8.jar(are you sure the file exists?)
Could not open jar file: file9.jar(are you sure the file exists?)
Could not open jar file: file10.jar(are you sure the file exists?)
Could not open jar file: file11.jar(are you sure the file exists?)
Could not open jar file: file12.jar(are you sure the file exists?)
Could not open jar file: file13.jar(are you sure the file exists?)
Could not open jar file: file14.jar(are you sure the file exists?)
Processing sentence; length: 5
Processing sentence; length: 7
Processing sentence; length: 3
Processing sentence; length: 8
Processing sentence; length: 2
Processing sentence; length: 6
Processing sentence; length: 4
Processing sentence; length: 9
Processing sentence; length: 1
Processing sentence; length: 7
Processing sentence; length: 12
Processing sentence; length: 15
Processing sentence; length: 11
Used {DOLSIGN2} to recognize document.docx as normalized
Used {DOLSIGN2} to recognize image.jpg as tokenized
Used {DOLSIGN2} to recognize report.pdf as normalized
Used {DOLSIGN2} to recognize audio.wav as tokenized
Used {DOLSIGN2} to recognize video.mp4 as normalized
Used {DOLSIGN2} to recognize script.py as tokenized
Used {DOLSIGN2} to recognize data.json as normalized
Used {DOLSIGN2} to recognize code.c as tokenized
Used {DOLSIGN2} to recognize spreadsheet.xlsx as normalized
Used {DOLSIGN2} to recognize presentation.pptx as tokenized
Used {DOLSIGN2} to recognize email.msg as normalized
Used {DOLSIGN2} to recognize archive.zip as tokenized
Used {DOLSIGN2} to recognize database.sql as normalized
Class at path /var/www/html/index.class is unloadable
Class at path /usr/local/lib/test.class is unloadable
Class at path /opt/application/executor.class is unloadable
Class at path /etc/config/utility.class is unloadable
Class at path /usr/share/java/library.class is unloadable
Class at path /home/user/documents/report.class is unloadable
Class at path /var/log/error.class is unloadable
Class at path /usr/local/bin/tool.class is unloadable
Class at path /opt/project/manager.class is unloadable
Class at path /etc/module/parser.class is unloadable
Class at path /usr/share/library/util.class is unloadable
Class at path /home/user/backup/system.class is unloadable
Class at path /var/log/debug.class is unloadable
Done training lexicon.
Done training lexicon.
Done training lexicon.
Done training lexicon.
Done training lexicon.
Done training lexicon.
Done training lexicon.
Done training lexicon.
Done training lexicon.
Done training lexicon.
Done training lexicon.
Done training lexicon.
Done training lexicon.
Done training lexicon.
Done training lexicon.
Problem with graph:
Problem with graph:
Problem with graph:
Problem with graph:
Problem with graph:
Problem with graph:
Problem with graph:
Problem with graph:
Problem with graph:
Problem with graph:
Problem with graph:
Problem with graph:
Problem with graph:
Problem with graph:
Problem with graph:
Reading Lexicon from file lexicon.txt
Reading Lexicon from file words.txt
Reading Lexicon from file vocab.txt
Reading Lexicon from file synonyms.txt
Reading Lexicon from file thesaurus.txt
Reading Lexicon from file glossary.txt
Reading Lexicon from file terms.txt
Reading Lexicon from file phrases.txt
Reading Lexicon from file entries.txt
Reading Lexicon from file data.txt
Reading Lexicon from file lexemes.txt
Reading Lexicon from file terminology.txt
Reading Lexicon from file dictionary.dat
added edge2
added edge3
added edge4
added edge5
added edge6
added edge7
added edge8
added edge9
added edge10
added edge11
added edge12
added edge13
added edge14
removed edge2
removed edge3
removed edge4
removed edge5
removed edge6
removed edge7
removed edge8
removed edge9
removed edge10
removed edge11
removed edge12
removed edge13
removed edge14
Done annotating trees.
Done annotating trees.
Done annotating trees.
Done annotating trees.
Done annotating trees.
Done annotating trees.
Done annotating trees.
Done annotating trees.
Done annotating trees.
Done annotating trees.
Done annotating trees.
Done annotating trees.
Done annotating trees.
Done annotating trees.
Done annotating trees.
Done reading trees.
Done reading trees.
Done reading trees.
Done reading trees.
Done reading trees.
Done reading trees.
Done reading trees.
Done reading trees.
Done reading trees.
Done reading trees.
Done reading trees.
Done reading trees.
Done reading trees.
Done reading trees.
Done reading trees.
ChineseCharacterBasedLexicon called with args:
ChineseCharacterBasedLexicon called with args:
ChineseCharacterBasedLexicon called with args:
ChineseCharacterBasedLexicon called with args:
ChineseCharacterBasedLexicon called with args:
ChineseCharacterBasedLexicon called with args:
ChineseCharacterBasedLexicon called with args:
ChineseCharacterBasedLexicon called with args:
ChineseCharacterBasedLexicon called with args:
ChineseCharacterBasedLexicon called with args:
ChineseCharacterBasedLexicon called with args:
ChineseCharacterBasedLexicon called with args:
ChineseCharacterBasedLexicon called with args:
ChineseCharacterBasedLexicon called with args:
ChineseCharacterBasedLexicon called with args:
Distribution over word length:
Distribution over word length:
Distribution over word length:
Distribution over word length:
Distribution over word length:
Distribution over word length:
Distribution over word length:
Distribution over word length:
Distribution over word length:
Distribution over word length:
Distribution over word length:
Distribution over word length:
Distribution over word length:
Distribution over word length:
Distribution over word length:
Distribution over singleton char radicals:
Distribution over singleton char radicals:
Distribution over singleton char radicals:
Distribution over singleton char radicals:
Distribution over singleton char radicals:
Distribution over singleton char radicals:
Distribution over singleton char radicals:
Distribution over singleton char radicals:
Distribution over singleton char radicals:
Distribution over singleton char radicals:
Distribution over singleton char radicals:
Distribution over singleton char radicals:
Distribution over singleton char radicals:
Distribution over singleton char radicals:
Distribution over singleton char radicals:
Distribution over singleton word POS:
Distribution over singleton word POS:
Distribution over singleton word POS:
Distribution over singleton word POS:
Distribution over singleton word POS:
Distribution over singleton word POS:
Distribution over singleton word POS:
Distribution over singleton word POS:
Distribution over singleton word POS:
Distribution over singleton word POS:
Distribution over singleton word POS:
Distribution over singleton word POS:
Distribution over singleton word POS:
Distribution over singleton word POS:
Distribution over singleton word POS:
Thus singletonWords comprise 15.78% of tokens and 4.12% of types.
Thus singletonWords comprise 77.93% of tokens and 23.41% of types.
Thus singletonWords comprise 43.69% of tokens and 12.92% of types.
Thus singletonWords comprise 61.52% of tokens and 17.98% of types.
Thus singletonWords comprise 89.43% of tokens and 27.15% of types.
Thus singletonWords comprise 27.84% of tokens and 8.03% of types.
Thus singletonWords comprise 52.36% of tokens and 15.02% of types.
Thus singletonWords comprise 37.89% of tokens and 10.74% of types.
Thus singletonWords comprise 68.91% of tokens and 20.34% of types.
Thus singletonWords comprise 45.23% of tokens and 13.41% of types.
Thus singletonWords comprise 74.58% of tokens and 22.61% of types.
Thus singletonWords comprise 23.47% of tokens and 6.91% of types.
Thus singletonWords comprise 56.79% of tokens and 16.84% of types.
There are 32 singleton words out of 500 tokens and 24 types.
There are 58 singleton words out of 1200 tokens and 36 types.
There are 20 singleton words out of 800 tokens and 16 types.
There are 75 singleton words out of 1500 tokens and 45 types.
There are 11 singleton words out of 300 tokens and 12 types.
There are 39 singleton words out of 950 tokens and 30 types.
There are 64 singleton words out of 1100 tokens and 40 types.
There are 27 singleton words out of 650 tokens and 20 types.
There are 51 singleton words out of 850 tokens and 32 types.
There are 80 singleton words out of 1600 tokens and 48 types.
There are 15 singleton words out of 350 tokens and 14 types.
There are 48 singleton words out of 900 tokens and 29 types.
There are 36 singleton words out of 700 tokens and 22 types.
There are 23 singleton words out of 450 tokens and 18 types.
No input file specified!
No input file specified!
No input file specified!
No input file specified!
No input file specified!
No input file specified!
No input file specified!
No input file specified!
No input file specified!
No input file specified!
No input file specified!
No input file specified!
No input file specified!
No input file specified!
No input file specified!
Error running NER Combiner V2 on Tree!  Not applying NER tags!
Error running NER Combiner V3 on Tree!  Not applying NER tags!
Error running NER Combiner V4 on Tree!  Not applying NER tags!
Error running NER Combiner V5 on Tree!  Not applying NER tags!
Error running NER Combiner V6 on Tree!  Not applying NER tags!
Error running NER Combiner V7 on Tree!  Not applying NER tags!
Error running NER Combiner V8 on Tree!  Not applying NER tags!
Error running NER Combiner V9 on Tree!  Not applying NER tags!
Error running NER Combiner V10 on Tree!  Not applying NER tags!
Error running NER Combiner V11 on Tree!  Not applying NER tags!
Error running NER Combiner V12 on Tree!  Not applying NER tags!
Error running NER Combiner V13 on Tree!  Not applying NER tags!
Error running NER Combiner V14 on Tree!  Not applying NER tags!
Thus singletonChars comprise 5.67% of tokens and 1.43% of types.
Thus singletonChars comprise 9.83% of tokens and 2.34% of types.
Thus singletonChars comprise 15.59% of tokens and 3.77% of types.
Thus singletonChars comprise 21.32% of tokens and 4.98% of types.
Thus singletonChars comprise 26.96% of tokens and 6.01% of types.
Thus singletonChars comprise 32.81% of tokens and 7.13% of types.
Thus singletonChars comprise 39.04% of tokens and 8.34% of types.
Thus singletonChars comprise 44.75% of tokens and 9.61% of types.
Thus singletonChars comprise 50.24% of tokens and 10.86% of types.
Thus singletonChars comprise 55.67% of tokens and 12.07% of types.
Thus singletonChars comprise 60.98% of tokens and 13.54% of types.
Thus singletonChars comprise 65.91% of tokens and 14.78% of types.
Thus singletonChars comprise 71.33% of tokens and 16.05% of types.
There are 7 singleton chars out of 500 tokens and 9 types found in 25 trees.
There are 15 singleton chars out of 1500 tokens and 20 types found in 75 trees.
There are 5 singleton chars out of 300 tokens and 7 types found in 20 trees.
There are 12 singleton chars out of 800 tokens and 13 types found in 45 trees.
There are 9 singleton chars out of 700 tokens and 11 types found in 35 trees.
There are 8 singleton chars out of 600 tokens and 10 types found in 30 trees.
There are 6 singleton chars out of 400 tokens and 8 types found in 22 trees.
There are 11 singleton chars out of 900 tokens and 14 types found in 55 trees.
There are 14 singleton chars out of 1200 tokens and 18 types found in 70 trees.
There are 13 singleton chars out of 1100 tokens and 17 types found in 65 trees.
There are 4 singleton chars out of 200 tokens and 6 types found in 15 trees.
There are 3 singleton chars out of 100 tokens and 5 types found in 10 trees.
There are 2 singleton chars out of 50 tokens and 3 types found in 5 trees.
Unmatched Hooks: 7
Unmatched Hooks: 2
Unmatched Hooks: 9
Unmatched Hooks: 5
Unmatched Hooks: 1
Unmatched Hooks: 4
Unmatched Hooks: 8
Unmatched Hooks: 6
Unmatched Hooks: 10
Unmatched Hooks: 12
Unmatched Hooks: 15
Unmatched Hooks: 11
Unmatched Hooks: 13
--- Agenda Post-Mortem ---
--- Agenda Post-Mortem ---
--- Agenda Post-Mortem ---
--- Agenda Post-Mortem ---
--- Agenda Post-Mortem ---
--- Agenda Post-Mortem ---
--- Agenda Post-Mortem ---
--- Agenda Post-Mortem ---
--- Agenda Post-Mortem ---
--- Agenda Post-Mortem ---
--- Agenda Post-Mortem ---
--- Agenda Post-Mortem ---
--- Agenda Post-Mortem ---
--- Agenda Post-Mortem ---
--- Agenda Post-Mortem ---
FactoredParser: emptied agenda, no parse found!
FactoredParser: emptied agenda, no parse found!
FactoredParser: emptied agenda, no parse found!
FactoredParser: emptied agenda, no parse found!
FactoredParser: emptied agenda, no parse found!
FactoredParser: emptied agenda, no parse found!
FactoredParser: emptied agenda, no parse found!
FactoredParser: emptied agenda, no parse found!
FactoredParser: emptied agenda, no parse found!
FactoredParser: emptied agenda, no parse found!
FactoredParser: emptied agenda, no parse found!
FactoredParser: emptied agenda, no parse found!
FactoredParser: emptied agenda, no parse found!
FactoredParser: emptied agenda, no parse found!
FactoredParser: emptied agenda, no parse found!
DEBUG: aborting search because of empty agenda
DEBUG: aborting search because of empty agenda
DEBUG: aborting search because of empty agenda
DEBUG: aborting search because of empty agenda
DEBUG: aborting search because of empty agenda
DEBUG: aborting search because of empty agenda
DEBUG: aborting search because of empty agenda
DEBUG: aborting search because of empty agenda
DEBUG: aborting search because of empty agenda
DEBUG: aborting search because of empty agenda
DEBUG: aborting search because of empty agenda
DEBUG: aborting search because of empty agenda
DEBUG: aborting search because of empty agenda
DEBUG: aborting search because of empty agenda
DEBUG: aborting search because of empty agenda
Reading from standard input...
Reading from standard input...
Reading from standard input...
Reading from standard input...
Reading from standard input...
Reading from standard input...
Reading from standard input...
Reading from standard input...
Reading from standard input...
Reading from standard input...
Reading from standard input...
Reading from standard input...
Reading from standard input...
Reading from standard input...
Reading from standard input...
Used {)} to recognize image
Used {)} to recognize audio
Used {)} to recognize video
Used {)} to recognize document
Used {)} to recognize code
Used {)} to recognize archive
Used {)} to recognize folder
Used {)} to recognize link
Used {)} to recognize program
Used {)} to recognize database
Used {)} to recognize spreadsheet
Used {)} to recognize presentation
Used {)} to recognize PDF
FactoredParser: exceeded MAX_ITEMS work limit [100 items]; aborting.
FactoredParser: exceeded MAX_ITEMS work limit [200 items]; aborting.
FactoredParser: exceeded MAX_ITEMS work limit [500 items]; aborting.
FactoredParser: exceeded MAX_ITEMS work limit [1000 items]; aborting.
FactoredParser: exceeded MAX_ITEMS work limit [2000 items]; aborting.
FactoredParser: exceeded MAX_ITEMS work limit [5000 items]; aborting.
FactoredParser: exceeded MAX_ITEMS work limit [10000 items]; aborting.
FactoredParser: exceeded MAX_ITEMS work limit [20000 items]; aborting.
FactoredParser: exceeded MAX_ITEMS work limit [50000 items]; aborting.
FactoredParser: exceeded MAX_ITEMS work limit [100000 items]; aborting.
FactoredParser: exceeded MAX_ITEMS work limit [200000 items]; aborting.
FactoredParser: exceeded MAX_ITEMS work limit [500000 items]; aborting.
FactoredParser: exceeded MAX_ITEMS work limit [1000000 items]; aborting.
FactoredParser: exceeded MAX_ITEMS work limit [2000000 items]; aborting.
Warning: No trees specified to operate on.  Use -treeFile path option.
Warning: No trees specified to operate on.  Use -treeFile path option.
Warning: No trees specified to operate on.  Use -treeFile path option.
Warning: No trees specified to operate on.  Use -treeFile path option.
Warning: No trees specified to operate on.  Use -treeFile path option.
Warning: No trees specified to operate on.  Use -treeFile path option.
Warning: No trees specified to operate on.  Use -treeFile path option.
Warning: No trees specified to operate on.  Use -treeFile path option.
Warning: No trees specified to operate on.  Use -treeFile path option.
Warning: No trees specified to operate on.  Use -treeFile path option.
Warning: No trees specified to operate on.  Use -treeFile path option.
Warning: No trees specified to operate on.  Use -treeFile path option.
Warning: No trees specified to operate on.  Use -treeFile path option.
Warning: No trees specified to operate on.  Use -treeFile path option.
Warning: No trees specified to operate on.  Use -treeFile path option.
Usage: java edu.stanford.nlp.trees.tregex.tsurgeon.Tsurgeon [-s] -treeFile <file-with-trees> [-po <matching-pattern> <operation>] <operation-file-1> <operation-file-2> ... <operation-file-n>
Usage: java edu.stanford.nlp.trees.tregex.tsurgeon.Tsurgeon [-s] -treeFile <file-with-trees> [-po <matching-pattern> <operation>] <operation-file-1> <operation-file-2> ... <operation-file-n>
Usage: java edu.stanford.nlp.trees.tregex.tsurgeon.Tsurgeon [-s] -treeFile <file-with-trees> [-po <matching-pattern> <operation>] <operation-file-1> <operation-file-2> ... <operation-file-n>
Usage: java edu.stanford.nlp.trees.tregex.tsurgeon.Tsurgeon [-s] -treeFile <file-with-trees> [-po <matching-pattern> <operation>] <operation-file-1> <operation-file-2> ... <operation-file-n>
Usage: java edu.stanford.nlp.trees.tregex.tsurgeon.Tsurgeon [-s] -treeFile <file-with-trees> [-po <matching-pattern> <operation>] <operation-file-1> <operation-file-2> ... <operation-file-n>
Usage: java edu.stanford.nlp.trees.tregex.tsurgeon.Tsurgeon [-s] -treeFile <file-with-trees> [-po <matching-pattern> <operation>] <operation-file-1> <operation-file-2> ... <operation-file-n>
Usage: java edu.stanford.nlp.trees.tregex.tsurgeon.Tsurgeon [-s] -treeFile <file-with-trees> [-po <matching-pattern> <operation>] <operation-file-1> <operation-file-2> ... <operation-file-n>
Usage: java edu.stanford.nlp.trees.tregex.tsurgeon.Tsurgeon [-s] -treeFile <file-with-trees> [-po <matching-pattern> <operation>] <operation-file-1> <operation-file-2> ... <operation-file-n>
Usage: java edu.stanford.nlp.trees.tregex.tsurgeon.Tsurgeon [-s] -treeFile <file-with-trees> [-po <matching-pattern> <operation>] <operation-file-1> <operation-file-2> ... <operation-file-n>
Usage: java edu.stanford.nlp.trees.tregex.tsurgeon.Tsurgeon [-s] -treeFile <file-with-trees> [-po <matching-pattern> <operation>] <operation-file-1> <operation-file-2> ... <operation-file-n>
Usage: java edu.stanford.nlp.trees.tregex.tsurgeon.Tsurgeon [-s] -treeFile <file-with-trees> [-po <matching-pattern> <operation>] <operation-file-1> <operation-file-2> ... <operation-file-n>
Usage: java edu.stanford.nlp.trees.tregex.tsurgeon.Tsurgeon [-s] -treeFile <file-with-trees> [-po <matching-pattern> <operation>] <operation-file-1> <operation-file-2> ... <operation-file-n>
Usage: java edu.stanford.nlp.trees.tregex.tsurgeon.Tsurgeon [-s] -treeFile <file-with-trees> [-po <matching-pattern> <operation>] <operation-file-1> <operation-file-2> ... <operation-file-n>
Usage: java edu.stanford.nlp.trees.tregex.tsurgeon.Tsurgeon [-s] -treeFile <file-with-trees> [-po <matching-pattern> <operation>] <operation-file-1> <operation-file-2> ... <operation-file-n>
Usage: java edu.stanford.nlp.trees.tregex.tsurgeon.Tsurgeon [-s] -treeFile <file-with-trees> [-po <matching-pattern> <operation>] <operation-file-1> <operation-file-2> ... <operation-file-n>
DEBUG: aborting search because of reaching the MAX_ITEMS work limit [50 items]
DEBUG: aborting search because of reaching the MAX_ITEMS work limit [200 items]
DEBUG: aborting search because of reaching the MAX_ITEMS work limit [80 items]
DEBUG: aborting search because of reaching the MAX_ITEMS work limit [150 items]
DEBUG: aborting search because of reaching the MAX_ITEMS work limit [120 items]
DEBUG: aborting search because of reaching the MAX_ITEMS work limit [90 items]
DEBUG: aborting search because of reaching the MAX_ITEMS work limit [70 items]
DEBUG: aborting search because of reaching the MAX_ITEMS work limit [180 items]
DEBUG: aborting search because of reaching the MAX_ITEMS work limit [60 items]
DEBUG: aborting search because of reaching the MAX_ITEMS work limit [110 items]
DEBUG: aborting search because of reaching the MAX_ITEMS work limit [130 items]
DEBUG: aborting search because of reaching the MAX_ITEMS work limit [160 items]
DEBUG: aborting search because of reaching the MAX_ITEMS work limit [140 items]
FactoredParser: no consistent parse [hit A*-blocked edges, aborting].
FactoredParser: no consistent parse [hit A*-blocked edges, aborting].
FactoredParser: no consistent parse [hit A*-blocked edges, aborting].
FactoredParser: no consistent parse [hit A*-blocked edges, aborting].
FactoredParser: no consistent parse [hit A*-blocked edges, aborting].
FactoredParser: no consistent parse [hit A*-blocked edges, aborting].
FactoredParser: no consistent parse [hit A*-blocked edges, aborting].
FactoredParser: no consistent parse [hit A*-blocked edges, aborting].
FactoredParser: no consistent parse [hit A*-blocked edges, aborting].
FactoredParser: no consistent parse [hit A*-blocked edges, aborting].
FactoredParser: no consistent parse [hit A*-blocked edges, aborting].
FactoredParser: no consistent parse [hit A*-blocked edges, aborting].
FactoredParser: no consistent parse [hit A*-blocked edges, aborting].
FactoredParser: no consistent parse [hit A*-blocked edges, aborting].
FactoredParser: no consistent parse [hit A*-blocked edges, aborting].
activeFeatures size: 15, dataLen: 200
activeFeatures size: 8, dataLen: 50
activeFeatures size: 12, dataLen: 80
activeFeatures size: 5, dataLen: 150
activeFeatures size: 20, dataLen: 250
activeFeatures size: 14, dataLen: 120
activeFeatures size: 7, dataLen: 90
activeFeatures size: 18, dataLen: 180
activeFeatures size: 9, dataLen: 60
activeFeatures size: 16, dataLen: 220
activeFeatures size: 6, dataLen: 130
activeFeatures size: 13, dataLen: 100
activeFeatures size: 11, dataLen: 70
initializing data feature hash done!
initializing data feature hash done!
initializing data feature hash done!
initializing data feature hash done!
initializing data feature hash done!
initializing data feature hash done!
initializing data feature hash done!
initializing data feature hash done!
initializing data feature hash done!
initializing data feature hash done!
initializing data feature hash done!
initializing data feature hash done!
initializing data feature hash done!
initializing data feature hash done!
initializing data feature hash done!
Null node fetched by Tsurgeon operation for node: incorrect_label (either no node labeled this, or the labeled node didn't match anything)
Null node fetched by Tsurgeon operation for node: invalid_label (either no node labeled this, or the labeled node didn't match anything)
Null node fetched by Tsurgeon operation for node: null_label (either no node labeled this, or the labeled node didn't match anything)
Null node fetched by Tsurgeon operation for node: empty_label (either no node labeled this, or the labeled node didn't match anything)
Null node fetched by Tsurgeon operation for node: unknown_label (either no node labeled this, or the labeled node didn't match anything)
Null node fetched by Tsurgeon operation for node: unexpected_label (either no node labeled this, or the labeled node didn't match anything)
Null node fetched by Tsurgeon operation for node: missing_label2 (either no node labeled this, or the labeled node didn't match anything)
Null node fetched by Tsurgeon operation for node: incorrect_label2 (either no node labeled this, or the labeled node didn't match anything)
Null node fetched by Tsurgeon operation for node: invalid_label2 (either no node labeled this, or the labeled node didn't match anything)
Null node fetched by Tsurgeon operation for node: null_label2 (either no node labeled this, or the labeled node didn't match anything)
Null node fetched by Tsurgeon operation for node: empty_label2 (either no node labeled this, or the labeled node didn't match anything)
Null node fetched by Tsurgeon operation for node: unknown_label2 (either no node labeled this, or the labeled node didn't match anything)
Null node fetched by Tsurgeon operation for node: unexpected_label2 (either no node labeled this, or the labeled node didn't match anything)
Aborting kGood search because of an unacceptable (-Inf) item: data.txt
Aborting kGood search because of an unacceptable (-Inf) item: result.csv
Aborting kGood search because of an unacceptable (-Inf) item: log.txt
Aborting kGood search because of an unacceptable (-Inf) item: config.ini
Aborting kGood search because of an unacceptable (-Inf) item: user.db
Aborting kGood search because of an unacceptable (-Inf) item: index.html
Aborting kGood search because of an unacceptable (-Inf) item: code.py
Aborting kGood search because of an unacceptable (-Inf) item: video.mp4
Aborting kGood search because of an unacceptable (-Inf) item: audio.wav
Aborting kGood search because of an unacceptable (-Inf) item: script.sh
Aborting kGood search because of an unacceptable (-Inf) item: style.css
Aborting kGood search because of an unacceptable (-Inf) item: document.docx
Aborting kGood search because of an unacceptable (-Inf) item: presentation.pptx
Avg. condensed features per position: 0.173
Avg. condensed features per position: 0.421
Avg. condensed features per position: 0.084
Avg. condensed features per position: 0.345
Avg. condensed features per position: 0.613
Avg. condensed features per position: 0.291
Avg. condensed features per position: 0.135
Avg. condensed features per position: 0.512
Avg. condensed features per position: 0.378
Avg. condensed features per position: 0.219
Avg. condensed features per position: 0.466
Avg. condensed features per position: 0.097
Avg. condensed features per position: 0.394
Found parse! Number of remaining trees to find = 5
Found parse! Number of remaining trees to find = 3
Found parse! Number of remaining trees to find = 20
Found parse! Number of remaining trees to find = 7
Found parse! Number of remaining trees to find = 15
Found parse! Number of remaining trees to find = 8
Found parse! Number of remaining trees to find = 12
Found parse! Number of remaining trees to find = 4
Found parse! Number of remaining trees to find = 9
Found parse! Number of remaining trees to find = 6
Found parse! Number of remaining trees to find = 2
Found parse! Number of remaining trees to find = 18
Found parse! Number of remaining trees to find = 11
Avg. active features per position: 2.8
Avg. active features per position: 3.2
Avg. active features per position: 4.7
Avg. active features per position: 2.1
Avg. active features per position: 1.9
Avg. active features per position: 3.9
Avg. active features per position: 2.6
Avg. active features per position: 1.7
Avg. active features per position: 3.5
Avg. active features per position: 1.2
Avg. active features per position: 4.3
Avg. active features per position: 3.1
Avg. active features per position: 2.4
Found last parse!
Found last parse!
Found last parse!
Found last parse!
Found last parse!
Found last parse!
Found last parse!
Found last parse!
Found last parse!
Found last parse!
Found last parse!
Found last parse!
Found last parse!
Found last parse!
Found last parse!
After condense, activeFeatures = 15, condensedCount = 3
After condense, activeFeatures = 10, condensedCount = 2
After condense, activeFeatures = 25, condensedCount = 6
After condense, activeFeatures = 12, condensedCount = 4
After condense, activeFeatures = 18, condensedCount = 4
After condense, activeFeatures = 22, condensedCount = 6
After condense, activeFeatures = 13, condensedCount = 3
After condense, activeFeatures = 17, condensedCount = 4
After condense, activeFeatures = 23, condensedCount = 6
After condense, activeFeatures = 11, condensedCount = 3
After condense, activeFeatures = 19, condensedCount = 5
After condense, activeFeatures = 16, condensedCount = 4
After condense, activeFeatures = 21, condensedCount = 6
top: node2 bottom:leaf2
top: node3 bottom:leaf3
top: node4 bottom:leaf4
top: node5 bottom:leaf5
top: node6 bottom:leaf6
top: node7 bottom:leaf7
top: node8 bottom:leaf8
top: node9 bottom:leaf9
top: node10 bottom:leaf10
top: node11 bottom:leaf11
top: node12 bottom:leaf12
top: node13 bottom:leaf13
top: node14 bottom:leaf14
Excising...original tree:
Excising...original tree:
Excising...original tree:
Excising...original tree:
Excising...original tree:
Excising...original tree:
Excising...original tree:
Excising...original tree:
Excising...original tree:
Excising...original tree:
Excising...original tree:
Excising...original tree:
Excising...original tree:
Excising...original tree:
Excising...original tree:
Built edges:      20
Built edges:      30
Built edges:      40
Built edges:      50
Built edges:      60
Built edges:      70
Built edges:      80
Built edges:      90
Built edges:      100
Built edges:      110
Built edges:      120
Built edges:      130
Built edges:      140
Before condense, activeFeatures = 5
Before condense, activeFeatures = 10
Before condense, activeFeatures = 2
Before condense, activeFeatures = 7
Before condense, activeFeatures = 3
Before condense, activeFeatures = 9
Before condense, activeFeatures = 4
Before condense, activeFeatures = 8
Before condense, activeFeatures = 6
Before condense, activeFeatures = 1
Before condense, activeFeatures = 11
Before condense, activeFeatures = 13
Before condense, activeFeatures = 12
Warning: adjoin to foot for depth-1 auxiliary tree has no effect.
Warning: adjoin to foot for depth-1 auxiliary tree has no effect.
Warning: adjoin to foot for depth-1 auxiliary tree has no effect.
Warning: adjoin to foot for depth-1 auxiliary tree has no effect.
Warning: adjoin to foot for depth-1 auxiliary tree has no effect.
Warning: adjoin to foot for depth-1 auxiliary tree has no effect.
Warning: adjoin to foot for depth-1 auxiliary tree has no effect.
Warning: adjoin to foot for depth-1 auxiliary tree has no effect.
Warning: adjoin to foot for depth-1 auxiliary tree has no effect.
Warning: adjoin to foot for depth-1 auxiliary tree has no effect.
Warning: adjoin to foot for depth-1 auxiliary tree has no effect.
Warning: adjoin to foot for depth-1 auxiliary tree has no effect.
Warning: adjoin to foot for depth-1 auxiliary tree has no effect.
Warning: adjoin to foot for depth-1 auxiliary tree has no effect.
Warning: adjoin to foot for depth-1 auxiliary tree has no effect.
initializing data feature hash, sup-data size: 150, unsup data size: 150
initializing data feature hash, sup-data size: 80, unsup data size: 220
initializing data feature hash, sup-data size: 120, unsup data size: 180
initializing data feature hash, sup-data size: 90, unsup data size: 210
initializing data feature hash, sup-data size: 170, unsup data size: 130
initializing data feature hash, sup-data size: 110, unsup data size: 190
initializing data feature hash, sup-data size: 130, unsup data size: 170
initializing data feature hash, sup-data size: 160, unsup data size: 140
initializing data feature hash, sup-data size: 70, unsup data size: 230
initializing data feature hash, sup-data size: 140, unsup data size: 160
initializing data feature hash, sup-data size: 100, unsup data size: 200
initializing data feature hash, sup-data size: 190, unsup data size: 110
initializing data feature hash, sup-data size: 80, unsup data size: 220
Comb iScore 78
Comb iScore 92
Comb iScore 85
Comb iScore 96
Comb iScore 71
Comb iScore 89
Comb iScore 83
Comb iScore 94
Comb iScore 77
Comb iScore 88
Comb iScore 79
Comb iScore 91
Comb iScore 82
Dropout took: 456 ms
Dropout took: 789 ms
Dropout took: 987 ms
Dropout took: 654 ms
Dropout took: 321 ms
Dropout took: 543 ms
Dropout took: 210 ms
Dropout took: 876 ms
Dropout took: 357 ms
Dropout took: 951 ms
Dropout took: 468 ms
Dropout took: 789 ms
Dropout took: 852 ms
Here's the node you were interested in:
Here's the node you were interested in:
Here's the node you were interested in:
Here's the node you were interested in:
Here's the node you were interested in:
Here's the node you were interested in:
Here's the node you were interested in:
Here's the node you were interested in:
Here's the node you were interested in:
Here's the node you were interested in:
Here's the node you were interested in:
Here's the node you were interested in:
Here's the node you were interested in:
Here's the node you were interested in:
Here's the node you were interested in:
usage: java edu.stanford.nlp.process.PTBEscapingProcessor fileOrUrl
usage: java edu.stanford.nlp.process.PTBEscapingProcessor fileOrUrl
usage: java edu.stanford.nlp.process.PTBEscapingProcessor fileOrUrl
usage: java edu.stanford.nlp.process.PTBEscapingProcessor fileOrUrl
usage: java edu.stanford.nlp.process.PTBEscapingProcessor fileOrUrl
usage: java edu.stanford.nlp.process.PTBEscapingProcessor fileOrUrl
usage: java edu.stanford.nlp.process.PTBEscapingProcessor fileOrUrl
usage: java edu.stanford.nlp.process.PTBEscapingProcessor fileOrUrl
usage: java edu.stanford.nlp.process.PTBEscapingProcessor fileOrUrl
usage: java edu.stanford.nlp.process.PTBEscapingProcessor fileOrUrl
usage: java edu.stanford.nlp.process.PTBEscapingProcessor fileOrUrl
usage: java edu.stanford.nlp.process.PTBEscapingProcessor fileOrUrl
usage: java edu.stanford.nlp.process.PTBEscapingProcessor fileOrUrl
usage: java edu.stanford.nlp.process.PTBEscapingProcessor fileOrUrl
usage: java edu.stanford.nlp.process.PTBEscapingProcessor fileOrUrl
Expected count took: 456 ms
Expected count took: 789 ms
Expected count took: 234 ms
Expected count took: 567 ms
Expected count took: 890 ms
Expected count took: 345 ms
Expected count took: 678 ms
Expected count took: 901 ms
Expected count took: 456 ms
Expected count took: 789 ms
Expected count took: 234 ms
Expected count took: 567 ms
Expected count took: 890 ms
Matching tree:
Matching tree:
Matching tree:
Matching tree:
Matching tree:
Matching tree:
Matching tree:
Matching tree:
Matching tree:
Matching tree:
Matching tree:
Matching tree:
Matching tree:
Matching tree:
Matching tree:
java Morphology [-rebuildVerbTable file|-stem word+|file+]
java Morphology [-rebuildVerbTable file|-stem word+|file+]
java Morphology [-rebuildVerbTable file|-stem word+|file+]
java Morphology [-rebuildVerbTable file|-stem word+|file+]
java Morphology [-rebuildVerbTable file|-stem word+|file+]
java Morphology [-rebuildVerbTable file|-stem word+|file+]
java Morphology [-rebuildVerbTable file|-stem word+|file+]
java Morphology [-rebuildVerbTable file|-stem word+|file+]
java Morphology [-rebuildVerbTable file|-stem word+|file+]
java Morphology [-rebuildVerbTable file|-stem word+|file+]
java Morphology [-rebuildVerbTable file|-stem word+|file+]
java Morphology [-rebuildVerbTable file|-stem word+|file+]
java Morphology [-rebuildVerbTable file|-stem word+|file+]
java Morphology [-rebuildVerbTable file|-stem word+|file+]
java Morphology [-rebuildVerbTable file|-stem word+|file+]
Found a full match:
Found a full match:
Found a full match:
Found a full match:
Found a full match:
Found a full match:
Found a full match:
Found a full match:
Found a full match:
Found a full match:
Found a full match:
Found a full match:
Found a full match:
Found a full match:
Found a full match:
Calculate objective took: 25 ms
Calculate objective took: 50 ms
Calculate objective took: 100 ms
Calculate objective took: 200 ms
Calculate objective took: 500 ms
Calculate objective took: 1000 ms
Calculate objective took: 2000 ms
Calculate objective took: 5000 ms
Calculate objective took: 10000 ms
Calculate objective took: 20000 ms
Calculate objective took: 50000 ms
Calculate objective took: 100000 ms
Calculate objective took: 200000 ms
private static final String[] verbStems = {
private static final String[] verbStems = {
private static final String[] verbStems = {
private static final String[] verbStems = {
private static final String[] verbStems = {
private static final String[] verbStems = {
private static final String[] verbStems = {
private static final String[] verbStems = {
private static final String[] verbStems = {
private static final String[] verbStems = {
private static final String[] verbStems = {
private static final String[] verbStems = {
private static final String[] verbStems = {
private static final String[] verbStems = {
private static final String[] verbStems = {
Next tree read:
Next tree read:
Next tree read:
Next tree read:
Next tree read:
Next tree read:
Next tree read:
Next tree read:
Next tree read:
Next tree read:
Next tree read:
Next tree read:
Next tree read:
Next tree read:
Next tree read:
deriv(2) = 20 - 12 = 8
deriv(3) = 15 - 9 = 6
deriv(4) = 8 - 3 = 5
deriv(5) = 12 - 7 = 5
deriv(6) = 25 - 14 = 11
deriv(7) = 9 - 4 = 5
deriv(8) = 18 - 11 = 7
deriv(9) = 6 - 1 = 5
deriv(10) = 14 - 8 = 6
deriv(11) = 11 - 6 = 5
deriv(12) = 22 - 13 = 9
deriv(13) = 7 - 2 = 5
deriv(14) = 16 - 10 = 6
normalized conditionals:
normalized conditionals:
normalized conditionals:
normalized conditionals:
normalized conditionals:
normalized conditionals:
normalized conditionals:
normalized conditionals:
normalized conditionals:
normalized conditionals:
normalized conditionals:
normalized conditionals:
normalized conditionals:
normalized conditionals:
normalized conditionals:
Restoring forbidden chars
Restoring forbidden chars
Restoring forbidden chars
Restoring forbidden chars
Restoring forbidden chars
Restoring forbidden chars
Restoring forbidden chars
Restoring forbidden chars
Restoring forbidden chars
Restoring forbidden chars
Restoring forbidden chars
Restoring forbidden chars
Restoring forbidden chars
Restoring forbidden chars
Restoring forbidden chars
Terminals (# of tag edges in chart): 245
Terminals (# of tag edges in chart): 87
Terminals (# of tag edges in chart): 321
Terminals (# of tag edges in chart): 92
Terminals (# of tag edges in chart): 178
Terminals (# of tag edges in chart): 205
Terminals (# of tag edges in chart): 119
Terminals (# of tag edges in chart): 276
Terminals (# of tag edges in chart): 143
Terminals (# of tag edges in chart): 198
Terminals (# of tag edges in chart): 334
Terminals (# of tag edges in chart): 223
Terminals (# of tag edges in chart): 268
There were 20 matches in total.
There were 30 matches in total.
There were 40 matches in total.
There were 50 matches in total.
There were 60 matches in total.
There were 70 matches in total.
There were 80 matches in total.
There were 90 matches in total.
There were 100 matches in total.
There were 110 matches in total.
There were 120 matches in total.
There were 130 matches in total.
There were 140 matches in total.
using default tree
using default tree
using default tree
using default tree
using default tree
using default tree
using default tree
using default tree
using default tree
using default tree
using default tree
using default tree
using default tree
using default tree
using default tree
Adding to chart: edge2
Adding to chart: edge3
Adding to chart: edge4
Adding to chart: edge5
Adding to chart: edge6
Adding to chart: edge7
Adding to chart: edge8
Adding to chart: edge9
Adding to chart: edge10
Adding to chart: edge11
Adding to chart: edge12
Adding to chart: edge13
Adding to chart: edge14
nextGivenCurr[1]:
nextGivenCurr[2]:
nextGivenCurr[3]:
nextGivenCurr[4]:
nextGivenCurr[5]:
nextGivenCurr[6]:
nextGivenCurr[7]:
nextGivenCurr[8]:
nextGivenCurr[9]:
nextGivenCurr[10]:
nextGivenCurr[11]:
nextGivenCurr[12]:
nextGivenCurr[13]:
unnormalized conditionals:
unnormalized conditionals:
unnormalized conditionals:
unnormalized conditionals:
unnormalized conditionals:
unnormalized conditionals:
unnormalized conditionals:
unnormalized conditionals:
unnormalized conditionals:
unnormalized conditionals:
unnormalized conditionals:
unnormalized conditionals:
unnormalized conditionals:
unnormalized conditionals:
unnormalized conditionals:
Reading trees from file(s) file2.txt
Reading trees from file(s) file3.txt
Reading trees from file(s) file4.txt
Reading trees from file(s) file5.txt
Reading trees from file(s) file6.txt
Reading trees from file(s) file7.txt
Reading trees from file(s) file8.txt
Reading trees from file(s) file9.txt
Reading trees from file(s) file10.txt
Reading trees from file(s) file11.txt
Reading trees from file(s) file12.txt
Reading trees from file(s) file13.txt
Reading trees from file(s) file14.txt
Parsed representation:
Parsed representation:
Parsed representation:
Parsed representation:
Parsed representation:
Parsed representation:
Parsed representation:
Parsed representation:
Parsed representation:
Parsed representation:
Parsed representation:
Parsed representation:
Parsed representation:
Parsed representation:
Parsed representation:
value is 0.36787944117144233
value is 0.01831563888873418
value is 0.006737946999085467
value is 0.049787068367863944
value is 0.22313016014842982
value is 0.0013498588075760032
value is 0.00012340980408667956
value is 0.00033546262790251185
value is 0.036265577412911165
value is 0.4065696597405991
value is 0.011109002502155203
value is 0.0009118819655545162
value is 0.9999546000702375
value is 0.1353352832366127
P_-1(Background) = -0.678
P_-1(Background) = -1.234
P_-1(Background) = -0.876
P_-1(Background) = -0.432
P_-1(Background) = -0.987
P_-1(Background) = -0.543
P_-1(Background) = -0.654
P_-1(Background) = -1.345
P_-1(Background) = -0.897
P_-1(Background) = -0.432
P_-1(Background) = -1.098
P_-1(Background) = -0.432
P_-1(Background) = -0.867
Parse exception: NumberFormatException
Parse exception: FileNotFoundException
Parse exception: ArrayIndexOutOfBoundsException
Parse exception: IOException
Parse exception: IllegalArgumentException
Parse exception: ClassCastException
Parse exception: NoSuchMethodException
Parse exception: InterruptedException
Parse exception: SQLException
Parse exception: IllegalStateException
Parse exception: UnsupportedOperationException
Parse exception: NoSuchFieldException
Parse exception: SecurityException
Running gradient on 4 threads
Running gradient on 8 threads
Running gradient on 16 threads
Running gradient on 32 threads
Running gradient on 64 threads
Running gradient on 128 threads
Running gradient on 256 threads
Running gradient on 512 threads
Running gradient on 1024 threads
Running gradient on 2048 threads
Running gradient on 4096 threads
Running gradient on 8192 threads
Running gradient on 16384 threads
Using tree reader factory AvroTreeReaderFactory...
Using tree reader factory ORCTreeReaderFactory...
Using tree reader factory JsonTreeReaderFactory...
Using tree reader factory CsvTreeReaderFactory...
Using tree reader factory ThriftTreeReaderFactory...
Using tree reader factory ProtobufTreeReaderFactory...
Using tree reader factory XmlTreeReaderFactory...
Using tree reader factory BinaryTreeReaderFactory...
Using tree reader factory ExcelTreeReaderFactory...
Using tree reader factory YamlTreeReaderFactory...
Using tree reader factory PropertyTreeReaderFactory...
Using tree reader factory ConfigTreeReaderFactory...
Using tree reader factory IniTreeReaderFactory...
Read in 5 sentences.
Read in 7 sentences.
Read in 3 sentences.
Read in 8 sentences.
Read in 12 sentences.
Read in 6 sentences.
Read in 9 sentences.
Read in 4 sentences.
Read in 2 sentences.
Read in 11 sentences.
Read in 15 sentences.
Read in 13 sentences.
Read in 1 sentences.
Using head finder CollinsHeadFinder...
Using head finder SemanticHeadFinder...
Using head finder DepHeadFinder...
Using head finder LexicalizedHeadFinder...
Using head finder UniversalHeadFinder...
Using head finder MaxEntropyHeadFinder...
Using head finder GSFHeadFinder...
Using head finder MLEHeadFinder...
Using head finder AnnealedParserHeadFinder...
Using head finder PasiniHeadFinder...
Using head finder ShiftReduceParserHeadFinder...
Using head finder TurkishHeadFinder...
Using head finder TreeDepHeadFinder...
Using head finder DiscourseHeadFinder...
Usage: java edu.stanford.nlp.trees.tregex.TregexPattern [-T] [-C] [-w] [-W] [-f] [-o] [-n] [-s] [-filter]  [-hf class] [-trf class] [-h handle]* [-e ext] pattern [filepath]
Usage: java edu.stanford.nlp.trees.tregex.TregexPattern [-T] [-C] [-w] [-W] [-f] [-o] [-n] [-s] [-filter]  [-hf class] [-trf class] [-h handle]* [-e ext] pattern [filepath]
Usage: java edu.stanford.nlp.trees.tregex.TregexPattern [-T] [-C] [-w] [-W] [-f] [-o] [-n] [-s] [-filter]  [-hf class] [-trf class] [-h handle]* [-e ext] pattern [filepath]
Usage: java edu.stanford.nlp.trees.tregex.TregexPattern [-T] [-C] [-w] [-W] [-f] [-o] [-n] [-s] [-filter]  [-hf class] [-trf class] [-h handle]* [-e ext] pattern [filepath]
Usage: java edu.stanford.nlp.trees.tregex.TregexPattern [-T] [-C] [-w] [-W] [-f] [-o] [-n] [-s] [-filter]  [-hf class] [-trf class] [-h handle]* [-e ext] pattern [filepath]
Usage: java edu.stanford.nlp.trees.tregex.TregexPattern [-T] [-C] [-w] [-W] [-f] [-o] [-n] [-s] [-filter]  [-hf class] [-trf class] [-h handle]* [-e ext] pattern [filepath]
Usage: java edu.stanford.nlp.trees.tregex.TregexPattern [-T] [-C] [-w] [-W] [-f] [-o] [-n] [-s] [-filter]  [-hf class] [-trf class] [-h handle]* [-e ext] pattern [filepath]
Usage: java edu.stanford.nlp.trees.tregex.TregexPattern [-T] [-C] [-w] [-W] [-f] [-o] [-n] [-s] [-filter]  [-hf class] [-trf class] [-h handle]* [-e ext] pattern [filepath]
Usage: java edu.stanford.nlp.trees.tregex.TregexPattern [-T] [-C] [-w] [-W] [-f] [-o] [-n] [-s] [-filter]  [-hf class] [-trf class] [-h handle]* [-e ext] pattern [filepath]
Usage: java edu.stanford.nlp.trees.tregex.TregexPattern [-T] [-C] [-w] [-W] [-f] [-o] [-n] [-s] [-filter]  [-hf class] [-trf class] [-h handle]* [-e ext] pattern [filepath]
Usage: java edu.stanford.nlp.trees.tregex.TregexPattern [-T] [-C] [-w] [-W] [-f] [-o] [-n] [-s] [-filter]  [-hf class] [-trf class] [-h handle]* [-e ext] pattern [filepath]
Usage: java edu.stanford.nlp.trees.tregex.TregexPattern [-T] [-C] [-w] [-W] [-f] [-o] [-n] [-s] [-filter]  [-hf class] [-trf class] [-h handle]* [-e ext] pattern [filepath]
Usage: java edu.stanford.nlp.trees.tregex.TregexPattern [-T] [-C] [-w] [-W] [-f] [-o] [-n] [-s] [-filter]  [-hf class] [-trf class] [-h handle]* [-e ext] pattern [filepath]
Usage: java edu.stanford.nlp.trees.tregex.TregexPattern [-T] [-C] [-w] [-W] [-f] [-o] [-n] [-s] [-filter]  [-hf class] [-trf class] [-h handle]* [-e ext] pattern [filepath]
Usage: java edu.stanford.nlp.trees.tregex.TregexPattern [-T] [-C] [-w] [-W] [-f] [-o] [-n] [-s] [-filter]  [-hf class] [-trf class] [-h handle]* [-e ext] pattern [filepath]
Length: 5
Length: 7
Length: 3
Length: 8
Length: 2
Length: 6
Length: 4
Length: 9
Length: 12
Length: 1
Length: 11
Length: 15
Length: 13
-suppressEscaping, -tokenizerOptions, -printOriginalText, -whitespaceTokenization
-suppressEscaping, -tokenizerOptions, -printOriginalText, -whitespaceTokenization
-suppressEscaping, -tokenizerOptions, -printOriginalText, -whitespaceTokenization
-suppressEscaping, -tokenizerOptions, -printOriginalText, -whitespaceTokenization
-suppressEscaping, -tokenizerOptions, -printOriginalText, -whitespaceTokenization
-suppressEscaping, -tokenizerOptions, -printOriginalText, -whitespaceTokenization
-suppressEscaping, -tokenizerOptions, -printOriginalText, -whitespaceTokenization
-suppressEscaping, -tokenizerOptions, -printOriginalText, -whitespaceTokenization
-suppressEscaping, -tokenizerOptions, -printOriginalText, -whitespaceTokenization
-suppressEscaping, -tokenizerOptions, -printOriginalText, -whitespaceTokenization
-suppressEscaping, -tokenizerOptions, -printOriginalText, -whitespaceTokenization
-suppressEscaping, -tokenizerOptions, -printOriginalText, -whitespaceTokenization
-suppressEscaping, -tokenizerOptions, -printOriginalText, -whitespaceTokenization
-suppressEscaping, -tokenizerOptions, -printOriginalText, -whitespaceTokenization
-suppressEscaping, -tokenizerOptions, -printOriginalText, -whitespaceTokenization
Triggering on L: edge2
Triggering on L: edge3
Triggering on L: edge4
Triggering on L: edge5
Triggering on L: edge6
Triggering on L: edge7
Triggering on L: edge8
Triggering on L: edge9
Triggering on L: edge10
Triggering on L: edge11
Triggering on L: edge12
Triggering on L: edge13
Triggering on L: edge14
Only one tokenizer flag allowed at a time:
Only one tokenizer flag allowed at a time:
Only one tokenizer flag allowed at a time:
Only one tokenizer flag allowed at a time:
Only one tokenizer flag allowed at a time:
Only one tokenizer flag allowed at a time:
Only one tokenizer flag allowed at a time:
Only one tokenizer flag allowed at a time:
Only one tokenizer flag allowed at a time:
Only one tokenizer flag allowed at a time:
Only one tokenizer flag allowed at a time:
Only one tokenizer flag allowed at a time:
Only one tokenizer flag allowed at a time:
Only one tokenizer flag allowed at a time:
Only one tokenizer flag allowed at a time:
Please provide output file using -exportFeatures
Please provide output file using -exportFeatures
Please provide output file using -exportFeatures
Please provide output file using -exportFeatures
Please provide output file using -exportFeatures
Please provide output file using -exportFeatures
Please provide output file using -exportFeatures
Please provide output file using -exportFeatures
Please provide output file using -exportFeatures
Please provide output file using -exportFeatures
Please provide output file using -exportFeatures
Please provide output file using -exportFeatures
Please provide output file using -exportFeatures
Please provide output file using -exportFeatures
Please provide output file using -exportFeatures
Please provide input file using -trainFile
Please provide input file using -trainFile
Please provide input file using -trainFile
Please provide input file using -trainFile
Please provide input file using -trainFile
Please provide input file using -trainFile
Please provide input file using -trainFile
Please provide input file using -trainFile
Please provide input file using -trainFile
Please provide input file using -trainFile
Please provide input file using -trainFile
Please provide input file using -trainFile
Please provide input file using -trainFile
Please provide input file using -trainFile
Please provide input file using -trainFile
Split 8 segments, adding 3 sentences.
Split 6 segments, adding 7 sentences.
Split 2 segments, adding 12 sentences.
Split 3 segments, adding 4 sentences.
Split 9 segments, adding 7 sentences.
Split 7 segments, adding 11 sentences.
Split 4 segments, adding 6 sentences.
Split 1 segments, adding 9 sentences.
Split 10 segments, adding 3 sentences.
Split 5 segments, adding 8 sentences.
Split 3 segments, adding 5 sentences.
Split 8 segments, adding 9 sentences.
Split 6 segments, adding 4 sentences.
Error parsing Tsurgeon file
Error parsing Tsurgeon file
Error parsing Tsurgeon file
Error parsing Tsurgeon file
Error parsing Tsurgeon file
Error parsing Tsurgeon file
Error parsing Tsurgeon file
Error parsing Tsurgeon file
Error parsing Tsurgeon file
Error parsing Tsurgeon file
Error parsing Tsurgeon file
Error parsing Tsurgeon file
Error parsing Tsurgeon file
Error parsing Tsurgeon file
Error parsing Tsurgeon file
Exception in save
Exception in save
Exception in save
Exception in save
Exception in save
Exception in save
Exception in save
Exception in save
Exception in save
Exception in save
Exception in save
Exception in save
Exception in save
Exception in save
Exception in save
After SoftMax Transformation, learned scales are:
After SoftMax Transformation, learned scales are:
After SoftMax Transformation, learned scales are:
After SoftMax Transformation, learned scales are:
After SoftMax Transformation, learned scales are:
After SoftMax Transformation, learned scales are:
After SoftMax Transformation, learned scales are:
After SoftMax Transformation, learned scales are:
After SoftMax Transformation, learned scales are:
After SoftMax Transformation, learned scales are:
After SoftMax Transformation, learned scales are:
After SoftMax Transformation, learned scales are:
After SoftMax Transformation, learned scales are:
After SoftMax Transformation, learned scales are:
After SoftMax Transformation, learned scales are:
Exception while loading the OSXAdapter:
Exception while loading the OSXAdapter:
Exception while loading the OSXAdapter:
Exception while loading the OSXAdapter:
Exception while loading the OSXAdapter:
Exception while loading the OSXAdapter:
Exception while loading the OSXAdapter:
Exception while loading the OSXAdapter:
Exception while loading the OSXAdapter:
Exception while loading the OSXAdapter:
Exception while loading the OSXAdapter:
Exception while loading the OSXAdapter:
Exception while loading the OSXAdapter:
Exception while loading the OSXAdapter:
Exception while loading the OSXAdapter:
Reading initial LOP scales from file settings.ini
Reading initial LOP scales from file data.json
Reading initial LOP scales from file parameters.xml
Reading initial LOP scales from file preferences.cfg
Reading initial LOP scales from file options.properties
Reading initial LOP scales from file config.ini
Reading initial LOP scales from file setup.conf
Reading initial LOP scales from file defaults.yaml
Reading initial LOP scales from file info.txt
Reading initial LOP scales from file settings.xml
Reading initial LOP scales from file data.ini
Reading initial LOP scales from file properties.cfg
Reading initial LOP scales from file config.json
usage: java ChineseDocumentToSentenceProcessor [-segmentIBM] -file filename [-encoding encoding]
usage: java ChineseDocumentToSentenceProcessor [-segmentIBM] -file filename [-encoding encoding]
usage: java ChineseDocumentToSentenceProcessor [-segmentIBM] -file filename [-encoding encoding]
usage: java ChineseDocumentToSentenceProcessor [-segmentIBM] -file filename [-encoding encoding]
usage: java ChineseDocumentToSentenceProcessor [-segmentIBM] -file filename [-encoding encoding]
usage: java ChineseDocumentToSentenceProcessor [-segmentIBM] -file filename [-encoding encoding]
usage: java ChineseDocumentToSentenceProcessor [-segmentIBM] -file filename [-encoding encoding]
usage: java ChineseDocumentToSentenceProcessor [-segmentIBM] -file filename [-encoding encoding]
usage: java ChineseDocumentToSentenceProcessor [-segmentIBM] -file filename [-encoding encoding]
usage: java ChineseDocumentToSentenceProcessor [-segmentIBM] -file filename [-encoding encoding]
usage: java ChineseDocumentToSentenceProcessor [-segmentIBM] -file filename [-encoding encoding]
usage: java ChineseDocumentToSentenceProcessor [-segmentIBM] -file filename [-encoding encoding]
usage: java ChineseDocumentToSentenceProcessor [-segmentIBM] -file filename [-encoding encoding]
usage: java ChineseDocumentToSentenceProcessor [-segmentIBM] -file filename [-encoding encoding]
usage: java ChineseDocumentToSentenceProcessor [-segmentIBM] -file filename [-encoding encoding]
Didn't match: line2
Didn't match: line3
Didn't match: line4
Didn't match: line5
Didn't match: line6
Didn't match: line7
Didn't match: line8
Didn't match: line9
Didn't match: line10
Didn't match: line11
Didn't match: line12
Didn't match: line13
Didn't match: line14
Reading initial LOP weights from file initial_weights.txt ...
Reading initial LOP weights from file default_weights.txt ...
Reading initial LOP weights from file custom_weights.txt ...
Reading initial LOP weights from file weights.dat ...
Reading initial LOP weights from file model_weights.txt ...
Reading initial LOP weights from file latest_weights.bin ...
Reading initial LOP weights from file trained_weights.json ...
Reading initial LOP weights from file weights_backup.txt ...
Reading initial LOP weights from file pretrained_weights.h5 ...
Reading initial LOP weights from file lop_weights.pickle ...
Reading initial LOP weights from file weights_archive.zip ...
Reading initial LOP weights from file lop_weights_restore.bak ...
Reading initial LOP weights from file weights_old.txt ...
Unable to read file!
Unable to read file!
Unable to read file!
Unable to read file!
Unable to read file!
Unable to read file!
Unable to read file!
Unable to read file!
Unable to read file!
Unable to read file!
Unable to read file!
Unable to read file!
Unable to read file!
Unable to read file!
Unable to read file!
Tokens are: banana
Tokens are: orange
Tokens are: mango
Tokens are: pineapple
Tokens are: cherry
Tokens are: grape
Tokens are: watermelon
Tokens are: strawberry
Tokens are: kiwi
Tokens are: peach
Tokens are: lemon
Tokens are: blueberry
Tokens are: raspberry
Time to read unsupervised dropout data: 0.187 seconds, read 3 files
Time to read unsupervised dropout data: 0.312 seconds, read 7 files
Time to read unsupervised dropout data: 0.512 seconds, read 9 files
Time to read unsupervised dropout data: 0.125 seconds, read 2 files
Time to read unsupervised dropout data: 0.396 seconds, read 6 files
Time to read unsupervised dropout data: 0.078 seconds, read 1 files
Time to read unsupervised dropout data: 0.643 seconds, read 10 files
Time to read unsupervised dropout data: 0.449 seconds, read 8 files
Time to read unsupervised dropout data: 0.289 seconds, read 4 files
Time to read unsupervised dropout data: 0.623 seconds, read 9 files
Time to read unsupervised dropout data: 0.338 seconds, read 5 files
Time to read unsupervised dropout data: 0.163 seconds, read 3 files
Time to read unsupervised dropout data: 0.271 seconds, read 2 files
File not found!
File not found!
File not found!
File not found!
File not found!
File not found!
File not found!
File not found!
File not found!
File not found!
File not found!
File not found!
File not found!
File not found!
File not found!
Multiple WordsToSentencesAnnotator or other sentence splitters are operating on this document!
Multiple WordsToSentencesAnnotator or other sentence splitters are operating on this document!
Multiple WordsToSentencesAnnotator or other sentence splitters are operating on this document!
Multiple WordsToSentencesAnnotator or other sentence splitters are operating on this document!
Multiple WordsToSentencesAnnotator or other sentence splitters are operating on this document!
Multiple WordsToSentencesAnnotator or other sentence splitters are operating on this document!
Multiple WordsToSentencesAnnotator or other sentence splitters are operating on this document!
Multiple WordsToSentencesAnnotator or other sentence splitters are operating on this document!
Multiple WordsToSentencesAnnotator or other sentence splitters are operating on this document!
Multiple WordsToSentencesAnnotator or other sentence splitters are operating on this document!
Multiple WordsToSentencesAnnotator or other sentence splitters are operating on this document!
Multiple WordsToSentencesAnnotator or other sentence splitters are operating on this document!
Multiple WordsToSentencesAnnotator or other sentence splitters are operating on this document!
Multiple WordsToSentencesAnnotator or other sentence splitters are operating on this document!
Multiple WordsToSentencesAnnotator or other sentence splitters are operating on this document!
Usage: java Tdiff tree1 tree2
Usage: java Tdiff tree1 tree2
Usage: java Tdiff tree1 tree2
Usage: java Tdiff tree1 tree2
Usage: java Tdiff tree1 tree2
Usage: java Tdiff tree1 tree2
Usage: java Tdiff tree1 tree2
Usage: java Tdiff tree1 tree2
Usage: java Tdiff tree1 tree2
Usage: java Tdiff tree1 tree2
Usage: java Tdiff tree1 tree2
Usage: java Tdiff tree1 tree2
Usage: java Tdiff tree1 tree2
Usage: java Tdiff tree1 tree2
Usage: java Tdiff tree1 tree2
Time to read: : 3.2 seconds
Time to read: : 1.7 seconds
Time to read: : 4.6 seconds
Time to read: : 2.1 seconds
Time to read: : 3.9 seconds
Time to read: : 1.4 seconds
Time to read: : 2.8 seconds
Time to read: : 3.6 seconds
Time to read: : 1.9 seconds
Time to read: : 4.2 seconds
Time to read: : 2.3 seconds
Time to read: : 3.3 seconds
Time to read: : 1.6 seconds
encoding null!!
encoding null!!
encoding null!!
encoding null!!
encoding null!!
encoding null!!
encoding null!!
encoding null!!
encoding null!!
encoding null!!
encoding null!!
encoding null!!
encoding null!!
encoding null!!
encoding null!!
Error in reading tree.
Error in reading tree.
Error in reading tree.
Error in reading tree.
Error in reading tree.
Error in reading tree.
Error in reading tree.
Error in reading tree.
Error in reading tree.
Error in reading tree.
Error in reading tree.
Error in reading tree.
Error in reading tree.
Error in reading tree.
Error in reading tree.
We expected a much higher fraction of key reuse in the dictionary.  It is possible the dictionary was recreated and then not sorted.  Please sort the dictionary by the second column and update the dictionary creation code to sort this way before writing.  This will save quite a bit of time loading without sacrificing memory performance.
We expected a much higher fraction of key reuse in the dictionary.  It is possible the dictionary was recreated and then not sorted.  Please sort the dictionary by the second column and update the dictionary creation code to sort this way before writing.  This will save quite a bit of time loading without sacrificing memory performance.
We expected a much higher fraction of key reuse in the dictionary.  It is possible the dictionary was recreated and then not sorted.  Please sort the dictionary by the second column and update the dictionary creation code to sort this way before writing.  This will save quite a bit of time loading without sacrificing memory performance.
We expected a much higher fraction of key reuse in the dictionary.  It is possible the dictionary was recreated and then not sorted.  Please sort the dictionary by the second column and update the dictionary creation code to sort this way before writing.  This will save quite a bit of time loading without sacrificing memory performance.
We expected a much higher fraction of key reuse in the dictionary.  It is possible the dictionary was recreated and then not sorted.  Please sort the dictionary by the second column and update the dictionary creation code to sort this way before writing.  This will save quite a bit of time loading without sacrificing memory performance.
We expected a much higher fraction of key reuse in the dictionary.  It is possible the dictionary was recreated and then not sorted.  Please sort the dictionary by the second column and update the dictionary creation code to sort this way before writing.  This will save quite a bit of time loading without sacrificing memory performance.
We expected a much higher fraction of key reuse in the dictionary.  It is possible the dictionary was recreated and then not sorted.  Please sort the dictionary by the second column and update the dictionary creation code to sort this way before writing.  This will save quite a bit of time loading without sacrificing memory performance.
We expected a much higher fraction of key reuse in the dictionary.  It is possible the dictionary was recreated and then not sorted.  Please sort the dictionary by the second column and update the dictionary creation code to sort this way before writing.  This will save quite a bit of time loading without sacrificing memory performance.
We expected a much higher fraction of key reuse in the dictionary.  It is possible the dictionary was recreated and then not sorted.  Please sort the dictionary by the second column and update the dictionary creation code to sort this way before writing.  This will save quite a bit of time loading without sacrificing memory performance.
We expected a much higher fraction of key reuse in the dictionary.  It is possible the dictionary was recreated and then not sorted.  Please sort the dictionary by the second column and update the dictionary creation code to sort this way before writing.  This will save quite a bit of time loading without sacrificing memory performance.
We expected a much higher fraction of key reuse in the dictionary.  It is possible the dictionary was recreated and then not sorted.  Please sort the dictionary by the second column and update the dictionary creation code to sort this way before writing.  This will save quite a bit of time loading without sacrificing memory performance.
We expected a much higher fraction of key reuse in the dictionary.  It is possible the dictionary was recreated and then not sorted.  Please sort the dictionary by the second column and update the dictionary creation code to sort this way before writing.  This will save quite a bit of time loading without sacrificing memory performance.
We expected a much higher fraction of key reuse in the dictionary.  It is possible the dictionary was recreated and then not sorted.  Please sort the dictionary by the second column and update the dictionary creation code to sort this way before writing.  This will save quite a bit of time loading without sacrificing memory performance.
We expected a much higher fraction of key reuse in the dictionary.  It is possible the dictionary was recreated and then not sorted.  Please sort the dictionary by the second column and update the dictionary creation code to sort this way before writing.  This will save quite a bit of time loading without sacrificing memory performance.
We expected a much higher fraction of key reuse in the dictionary.  It is possible the dictionary was recreated and then not sorted.  Please sort the dictionary by the second column and update the dictionary creation code to sort this way before writing.  This will save quite a bit of time loading without sacrificing memory performance.
Done reading Wikidict (5000 links read; 4500 unique entities; 1.67 seconds elapsed)
Done reading Wikidict (15000 links read; 13000 unique entities; 3.89 seconds elapsed)
Done reading Wikidict (8000 links read; 7000 unique entities; 2.01 seconds elapsed)
Done reading Wikidict (20000 links read; 17000 unique entities; 4.52 seconds elapsed)
Done reading Wikidict (12000 links read; 10500 unique entities; 2.85 seconds elapsed)
Done reading Wikidict (25000 links read; 21000 unique entities; 5.71 seconds elapsed)
Done reading Wikidict (14000 links read; 12000 unique entities; 3.46 seconds elapsed)
Done reading Wikidict (18000 links read; 15000 unique entities; 4.12 seconds elapsed)
Done reading Wikidict (9000 links read; 8000 unique entities; 2.19 seconds elapsed)
Done reading Wikidict (30000 links read; 25000 unique entities; 6.34 seconds elapsed)
Done reading Wikidict (16000 links read; 14000 unique entities; 3.68 seconds elapsed)
Done reading Wikidict (21000 links read; 18000 unique entities; 4.76 seconds elapsed)
Done reading Wikidict (11000 links read; 9500 unique entities; 2.58 seconds elapsed)
Loaded 5 entries from Wikidict [2MB memory used; 250ms elapsed]
Loaded 20 entries from Wikidict [8MB memory used; 800ms elapsed]
Loaded 15 entries from Wikidict [7MB memory used; 700ms elapsed]
Loaded 8 entries from Wikidict [4MB memory used; 400ms elapsed]
Loaded 12 entries from Wikidict [6MB memory used; 600ms elapsed]
Loaded 3 entries from Wikidict [1MB memory used; 100ms elapsed]
Loaded 18 entries from Wikidict [9MB memory used; 900ms elapsed]
Loaded 6 entries from Wikidict [3MB memory used; 300ms elapsed]
Loaded 14 entries from Wikidict [7MB memory used; 700ms elapsed]
Loaded 9 entries from Wikidict [4MB memory used; 400ms elapsed]
Loaded 11 entries from Wikidict [6MB memory used; 600ms elapsed]
Loaded 7 entries from Wikidict [3MB memory used; 300ms elapsed]
Loaded 16 entries from Wikidict [8MB memory used; 800ms elapsed]
WordNetConnection unavailable for collocations.
WordNetConnection unavailable for collocations.
WordNetConnection unavailable for collocations.
WordNetConnection unavailable for collocations.
WordNetConnection unavailable for collocations.
WordNetConnection unavailable for collocations.
WordNetConnection unavailable for collocations.
WordNetConnection unavailable for collocations.
WordNetConnection unavailable for collocations.
WordNetConnection unavailable for collocations.
WordNetConnection unavailable for collocations.
WordNetConnection unavailable for collocations.
WordNetConnection unavailable for collocations.
WordNetConnection unavailable for collocations.
WordNetConnection unavailable for collocations.
Reading Wikidict from /path/to/wikidict2
Reading Wikidict from /path/to/wikidict3
Reading Wikidict from /path/to/wikidict4
Reading Wikidict from /path/to/wikidict5
Reading Wikidict from /path/to/wikidict6
Reading Wikidict from /path/to/wikidict7
Reading Wikidict from /path/to/wikidict8
Reading Wikidict from /path/to/wikidict9
Reading Wikidict from /path/to/wikidict10
Reading Wikidict from /path/to/wikidict11
Reading Wikidict from /path/to/wikidict12
Reading Wikidict from /path/to/wikidict13
Reading Wikidict from /path/to/wikidict14
inputLayerWeights4Edge.length=	10
inputLayerWeights4Edge.length=	15
inputLayerWeights4Edge.length=	20
inputLayerWeights4Edge.length=	25
inputLayerWeights4Edge.length=	30
inputLayerWeights4Edge.length=	35
inputLayerWeights4Edge.length=	40
inputLayerWeights4Edge.length=	45
inputLayerWeights4Edge.length=	50
inputLayerWeights4Edge.length=	55
inputLayerWeights4Edge.length=	60
inputLayerWeights4Edge.length=	65
inputLayerWeights4Edge.length=	70
inputLayerWeights4Edge.length=	75
Couldn't open WordNet Connection.  Aborting collocation detection.
Couldn't open WordNet Connection.  Aborting collocation detection.
Couldn't open WordNet Connection.  Aborting collocation detection.
Couldn't open WordNet Connection.  Aborting collocation detection.
Couldn't open WordNet Connection.  Aborting collocation detection.
Couldn't open WordNet Connection.  Aborting collocation detection.
Couldn't open WordNet Connection.  Aborting collocation detection.
Couldn't open WordNet Connection.  Aborting collocation detection.
Couldn't open WordNet Connection.  Aborting collocation detection.
Couldn't open WordNet Connection.  Aborting collocation detection.
Couldn't open WordNet Connection.  Aborting collocation detection.
Couldn't open WordNet Connection.  Aborting collocation detection.
Couldn't open WordNet Connection.  Aborting collocation detection.
Couldn't open WordNet Connection.  Aborting collocation detection.
Couldn't open WordNet Connection.  Aborting collocation detection.
Using Good-Turing smoothing for unknown words.
Using Good-Turing smoothing for unknown words.
Using Good-Turing smoothing for unknown words.
Using Good-Turing smoothing for unknown words.
Using Good-Turing smoothing for unknown words.
Using Good-Turing smoothing for unknown words.
Using Good-Turing smoothing for unknown words.
Using Good-Turing smoothing for unknown words.
Using Good-Turing smoothing for unknown words.
Using Good-Turing smoothing for unknown words.
Using Good-Turing smoothing for unknown words.
Using Good-Turing smoothing for unknown words.
Using Good-Turing smoothing for unknown words.
Using Good-Turing smoothing for unknown words.
Using Good-Turing smoothing for unknown words.
linearWeights.length=	15
linearWeights.length=	7
linearWeights.length=	12
linearWeights.length=	4
linearWeights.length=	9
linearWeights.length=	6
linearWeights.length=	19
linearWeights.length=	8
linearWeights.length=	13
linearWeights.length=	5
linearWeights.length=	11
linearWeights.length=	14
linearWeights.length=	16
linearWeights.length=	3
Error: problem with running stop command for WebServiceAnnotator
Error: problem with running stop command for WebServiceAnnotator
Error: problem with running stop command for WebServiceAnnotator
Error: problem with running stop command for WebServiceAnnotator
Error: problem with running stop command for WebServiceAnnotator
Error: problem with running stop command for WebServiceAnnotator
Error: problem with running stop command for WebServiceAnnotator
Error: problem with running stop command for WebServiceAnnotator
Error: problem with running stop command for WebServiceAnnotator
Error: problem with running stop command for WebServiceAnnotator
Error: problem with running stop command for WebServiceAnnotator
Error: problem with running stop command for WebServiceAnnotator
Error: problem with running stop command for WebServiceAnnotator
Error: problem with running stop command for WebServiceAnnotator
Error: problem with running stop command for WebServiceAnnotator
The "conll2007" and "conllStyleDependencies" formats are ignored in xml.
The "conll2007" and "conllStyleDependencies" formats are ignored in xml.
The "conll2007" and "conllStyleDependencies" formats are ignored in xml.
The "conll2007" and "conllStyleDependencies" formats are ignored in xml.
The "conll2007" and "conllStyleDependencies" formats are ignored in xml.
The "conll2007" and "conllStyleDependencies" formats are ignored in xml.
The "conll2007" and "conllStyleDependencies" formats are ignored in xml.
The "conll2007" and "conllStyleDependencies" formats are ignored in xml.
The "conll2007" and "conllStyleDependencies" formats are ignored in xml.
The "conll2007" and "conllStyleDependencies" formats are ignored in xml.
The "conll2007" and "conllStyleDependencies" formats are ignored in xml.
The "conll2007" and "conllStyleDependencies" formats are ignored in xml.
The "conll2007" and "conllStyleDependencies" formats are ignored in xml.
The "conll2007" and "conllStyleDependencies" formats are ignored in xml.
The "conll2007" and "conllStyleDependencies" formats are ignored in xml.
Classing unknown word as the average of their equivalents by identity of last 5 letters.
Classing unknown word as the average of their equivalents by identity of last 8 letters.
Classing unknown word as the average of their equivalents by identity of last 2 letters.
Classing unknown word as the average of their equivalents by identity of last 6 letters.
Classing unknown word as the average of their equivalents by identity of last 4 letters.
Classing unknown word as the average of their equivalents by identity of last 7 letters.
Classing unknown word as the average of their equivalents by identity of last 9 letters.
Classing unknown word as the average of their equivalents by identity of last 10 letters.
Classing unknown word as the average of their equivalents by identity of last 1 letters.
Classing unknown word as the average of their equivalents by identity of last 12 letters.
Classing unknown word as the average of their equivalents by identity of last 11 letters.
Classing unknown word as the average of their equivalents by identity of last 15 letters.
Classing unknown word as the average of their equivalents by identity of last 14 letters.
edgeFeatureIndicesMap.size()= 10
edgeFeatureIndicesMap.size()= 5
edgeFeatureIndicesMap.size()= 2
edgeFeatureIndicesMap.size()= 7
edgeFeatureIndicesMap.size()= 3
edgeFeatureIndicesMap.size()= 8
edgeFeatureIndicesMap.size()= 15
edgeFeatureIndicesMap.size()= 20
edgeFeatureIndicesMap.size()= 12
edgeFeatureIndicesMap.size()= 6
edgeFeatureIndicesMap.size()= 1
edgeFeatureIndicesMap.size()= 4
edgeFeatureIndicesMap.size()= 14
Including whether first letter is capitalized for unknown words
Including whether first letter is capitalized for unknown words
Including whether first letter is capitalized for unknown words
Including whether first letter is capitalized for unknown words
Including whether first letter is capitalized for unknown words
Including whether first letter is capitalized for unknown words
Including whether first letter is capitalized for unknown words
Including whether first letter is capitalized for unknown words
Including whether first letter is capitalized for unknown words
Including whether first letter is capitalized for unknown words
Including whether first letter is capitalized for unknown words
Including whether first letter is capitalized for unknown words
Including whether first letter is capitalized for unknown words
Including whether first letter is capitalized for unknown words
Including whether first letter is capitalized for unknown words
<dependencies style="untyped">
<dependencies style="untyped">
<dependencies style="untyped">
<dependencies style="untyped">
<dependencies style="untyped">
<dependencies style="untyped">
<dependencies style="untyped">
<dependencies style="untyped">
<dependencies style="untyped">
<dependencies style="untyped">
<dependencies style="untyped">
<dependencies style="untyped">
<dependencies style="untyped">
<dependencies style="untyped">
<dependencies style="untyped">
Including first letter for unknown words.
Including first letter for unknown words.
Including first letter for unknown words.
Including first letter for unknown words.
Including first letter for unknown words.
Including first letter for unknown words.
Including first letter for unknown words.
Including first letter for unknown words.
Including first letter for unknown words.
Including first letter for unknown words.
Including first letter for unknown words.
Including first letter for unknown words.
Including first letter for unknown words.
Including first letter for unknown words.
Including first letter for unknown words.
nodeFeatureIndicesMap.size()= 20
nodeFeatureIndicesMap.size()= 5
nodeFeatureIndicesMap.size()= 15
nodeFeatureIndicesMap.size()= 30
nodeFeatureIndicesMap.size()= 8
nodeFeatureIndicesMap.size()= 12
nodeFeatureIndicesMap.size()= 25
nodeFeatureIndicesMap.size()= 3
nodeFeatureIndicesMap.size()= 18
nodeFeatureIndicesMap.size()= 7
nodeFeatureIndicesMap.size()= 22
nodeFeatureIndicesMap.size()= 13
nodeFeatureIndicesMap.size()= 28
UWM.addTagging: Shouldn't call with seen word!
UWM.addTagging: Shouldn't call with seen word!
UWM.addTagging: Shouldn't call with seen word!
UWM.addTagging: Shouldn't call with seen word!
UWM.addTagging: Shouldn't call with seen word!
UWM.addTagging: Shouldn't call with seen word!
UWM.addTagging: Shouldn't call with seen word!
UWM.addTagging: Shouldn't call with seen word!
UWM.addTagging: Shouldn't call with seen word!
UWM.addTagging: Shouldn't call with seen word!
UWM.addTagging: Shouldn't call with seen word!
UWM.addTagging: Shouldn't call with seen word!
UWM.addTagging: Shouldn't call with seen word!
UWM.addTagging: Shouldn't call with seen word!
UWM.addTagging: Shouldn't call with seen word!
Testing complete... exiting
Testing complete... exiting
Testing complete... exiting
Testing complete... exiting
Testing complete... exiting
Testing complete... exiting
Testing complete... exiting
Testing complete... exiting
Testing complete... exiting
Testing complete... exiting
Testing complete... exiting
Testing complete... exiting
Testing complete... exiting
Testing complete... exiting
Testing complete... exiting
Unmounting server: server-02
Unmounting server: server-03
Unmounting server: server-04
Unmounting server: server-05
Unmounting server: server-06
Unmounting server: server-07
Unmounting server: server-08
Unmounting server: server-09
Unmounting server: server-10
Unmounting server: server-11
Unmounting server: server-12
Unmounting server: server-13
Unmounting server: server-14
<tree style="xml">
<tree style="xml">
<tree style="xml">
<tree style="xml">
<tree style="xml">
<tree style="xml">
<tree style="xml">
<tree style="xml">
<tree style="xml">
<tree style="xml">
<tree style="xml">
<tree style="xml">
<tree style="xml">
<tree style="xml">
<tree style="xml">
Testing failed....exiting
Testing failed....exiting
Testing failed....exiting
Testing failed....exiting
Testing failed....exiting
Testing failed....exiting
Testing failed....exiting
Testing failed....exiting
Testing failed....exiting
Testing failed....exiting
Testing failed....exiting
Testing failed....exiting
Testing failed....exiting
Testing failed....exiting
Testing failed....exiting
<tree style="latexTrees">
<tree style="latexTrees">
<tree style="latexTrees">
<tree style="latexTrees">
<tree style="latexTrees">
<tree style="latexTrees">
<tree style="latexTrees">
<tree style="latexTrees">
<tree style="latexTrees">
<tree style="latexTrees">
<tree style="latexTrees">
<tree style="latexTrees">
<tree style="latexTrees">
<tree style="latexTrees">
<tree style="latexTrees">
Error Matrix P(Observed|Truth):
Error Matrix P(Observed|Truth):
Error Matrix P(Observed|Truth):
Error Matrix P(Observed|Truth):
Error Matrix P(Observed|Truth):
Error Matrix P(Observed|Truth):
Error Matrix P(Observed|Truth):
Error Matrix P(Observed|Truth):
Error Matrix P(Observed|Truth):
Error Matrix P(Observed|Truth):
Error Matrix P(Observed|Truth):
Error Matrix P(Observed|Truth):
Error Matrix P(Observed|Truth):
Error Matrix P(Observed|Truth):
Error Matrix P(Observed|Truth):
Warning: proposed tag is unseen in training data:	unknownTag2
Warning: proposed tag is unseen in training data:	unknownTag3
Warning: proposed tag is unseen in training data:	unknownTag4
Warning: proposed tag is unseen in training data:	unknownTag5
Warning: proposed tag is unseen in training data:	unknownTag6
Warning: proposed tag is unseen in training data:	unknownTag7
Warning: proposed tag is unseen in training data:	unknownTag8
Warning: proposed tag is unseen in training data:	unknownTag9
Warning: proposed tag is unseen in training data:	unknownTag10
Warning: proposed tag is unseen in training data:	unknownTag11
Warning: proposed tag is unseen in training data:	unknownTag12
Warning: proposed tag is unseen in training data:	unknownTag13
Warning: proposed tag is unseen in training data:	unknownTag14
<tree style="penn">
<tree style="penn">
<tree style="penn">
<tree style="penn">
<tree style="penn">
<tree style="penn">
<tree style="penn">
<tree style="penn">
<tree style="penn">
<tree style="penn">
<tree style="penn">
<tree style="penn">
<tree style="penn">
<tree style="penn">
<tree style="penn">
(Sun Dec 23 00:59:39 2007) (pichuan)
(Sun Dec 23 00:59:39 2007) (pichuan)
(Sun Dec 23 00:59:39 2007) (pichuan)
(Sun Dec 23 00:59:39 2007) (pichuan)
(Sun Dec 23 00:59:39 2007) (pichuan)
(Sun Dec 23 00:59:39 2007) (pichuan)
(Sun Dec 23 00:59:39 2007) (pichuan)
(Sun Dec 23 00:59:39 2007) (pichuan)
(Sun Dec 23 00:59:39 2007) (pichuan)
(Sun Dec 23 00:59:39 2007) (pichuan)
(Sun Dec 23 00:59:39 2007) (pichuan)
(Sun Dec 23 00:59:39 2007) (pichuan)
(Sun Dec 23 00:59:39 2007) (pichuan)
(Sun Dec 23 00:59:39 2007) (pichuan)
(Sun Dec 23 00:59:39 2007) (pichuan)
Warning: this is now only tested for Chinese Segmenter
Warning: this is now only tested for Chinese Segmenter
Warning: this is now only tested for Chinese Segmenter
Warning: this is now only tested for Chinese Segmenter
Warning: this is now only tested for Chinese Segmenter
Warning: this is now only tested for Chinese Segmenter
Warning: this is now only tested for Chinese Segmenter
Warning: this is now only tested for Chinese Segmenter
Warning: this is now only tested for Chinese Segmenter
Warning: this is now only tested for Chinese Segmenter
Warning: this is now only tested for Chinese Segmenter
Warning: this is now only tested for Chinese Segmenter
Warning: this is now only tested for Chinese Segmenter
Warning: this is now only tested for Chinese Segmenter
Warning: this is now only tested for Chinese Segmenter
Found 20 matching embeddings of dimension 200
Found 15 matching embeddings of dimension 150
Found 5 matching embeddings of dimension 50
Found 8 matching embeddings of dimension 80
Found 12 matching embeddings of dimension 120
Found 25 matching embeddings of dimension 250
Found 18 matching embeddings of dimension 180
Found 7 matching embeddings of dimension 70
Found 13 matching embeddings of dimension 130
Found 22 matching embeddings of dimension 220
Found 17 matching embeddings of dimension 170
Found 9 matching embeddings of dimension 90
Found 16 matching embeddings of dimension 160
Inconsistent vector lengths: 8 vs. 12
Inconsistent vector lengths: 5 vs. 9
Inconsistent vector lengths: 14 vs. 20
Inconsistent vector lengths: 3 vs. 7
Inconsistent vector lengths: 9 vs. 13
Inconsistent vector lengths: 6 vs. 11
Inconsistent vector lengths: 12 vs. 16
Inconsistent vector lengths: 2 vs. 6
Inconsistent vector lengths: 7 vs. 10
Inconsistent vector lengths: 11 vs. 17
Inconsistent vector lengths: 4 vs. 8
Inconsistent vector lengths: 13 vs. 18
Inconsistent vector lengths: 1 vs. 4
Found a dictionary of size 100
Found a dictionary of size 200
Found a dictionary of size 500
Found a dictionary of size 1000
Found a dictionary of size 1500
Found a dictionary of size 2000
Found a dictionary of size 2500
Found a dictionary of size 3000
Found a dictionary of size 3500
Found a dictionary of size 4000
Found a dictionary of size 4500
Found a dictionary of size 5000
Found a dictionary of size 5500
java BaseLexicon treebankPath fileRange unknownWordModel words*
java BaseLexicon treebankPath fileRange unknownWordModel words*
java BaseLexicon treebankPath fileRange unknownWordModel words*
java BaseLexicon treebankPath fileRange unknownWordModel words*
java BaseLexicon treebankPath fileRange unknownWordModel words*
java BaseLexicon treebankPath fileRange unknownWordModel words*
java BaseLexicon treebankPath fileRange unknownWordModel words*
java BaseLexicon treebankPath fileRange unknownWordModel words*
java BaseLexicon treebankPath fileRange unknownWordModel words*
java BaseLexicon treebankPath fileRange unknownWordModel words*
java BaseLexicon treebankPath fileRange unknownWordModel words*
java BaseLexicon treebankPath fileRange unknownWordModel words*
java BaseLexicon treebankPath fileRange unknownWordModel words*
java BaseLexicon treebankPath fileRange unknownWordModel words*
java BaseLexicon treebankPath fileRange unknownWordModel words*
TreePrint: can't remove top bracket: not unary
TreePrint: can't remove top bracket: not unary
TreePrint: can't remove top bracket: not unary
TreePrint: can't remove top bracket: not unary
TreePrint: can't remove top bracket: not unary
TreePrint: can't remove top bracket: not unary
TreePrint: can't remove top bracket: not unary
TreePrint: can't remove top bracket: not unary
TreePrint: can't remove top bracket: not unary
TreePrint: can't remove top bracket: not unary
TreePrint: can't remove top bracket: not unary
TreePrint: can't remove top bracket: not unary
TreePrint: can't remove top bracket: not unary
TreePrint: can't remove top bracket: not unary
TreePrint: can't remove top bracket: not unary
Tagging probabilities log P(word|tag)
Tagging probabilities log P(word|tag)
Tagging probabilities log P(word|tag)
Tagging probabilities log P(word|tag)
Tagging probabilities log P(word|tag)
Tagging probabilities log P(word|tag)
Tagging probabilities log P(word|tag)
Tagging probabilities log P(word|tag)
Tagging probabilities log P(word|tag)
Tagging probabilities log P(word|tag)
Tagging probabilities log P(word|tag)
Tagging probabilities log P(word|tag)
Tagging probabilities log P(word|tag)
Tagging probabilities log P(word|tag)
Tagging probabilities log P(word|tag)
Unseen counter: 187
Unseen counter: 312
Unseen counter: 425
Unseen counter: 539
Unseen counter: 674
Unseen counter: 722
Unseen counter: 843
Unseen counter: 926
Unseen counter: 1035
Unseen counter: 1148
Unseen counter: 1222
Unseen counter: 1318
Unseen counter: 1426
<h>Visualisation provided using the <a href="http://brat.nlplab.org/">brat visualisation/annotation software</a>.</h>
<h>Visualisation provided using the <a href="http://brat.nlplab.org/">brat visualisation/annotation software</a>.</h>
<h>Visualisation provided using the <a href="http://brat.nlplab.org/">brat visualisation/annotation software</a>.</h>
<h>Visualisation provided using the <a href="http://brat.nlplab.org/">brat visualisation/annotation software</a>.</h>
<h>Visualisation provided using the <a href="http://brat.nlplab.org/">brat visualisation/annotation software</a>.</h>
<h>Visualisation provided using the <a href="http://brat.nlplab.org/">brat visualisation/annotation software</a>.</h>
<h>Visualisation provided using the <a href="http://brat.nlplab.org/">brat visualisation/annotation software</a>.</h>
<h>Visualisation provided using the <a href="http://brat.nlplab.org/">brat visualisation/annotation software</a>.</h>
<h>Visualisation provided using the <a href="http://brat.nlplab.org/">brat visualisation/annotation software</a>.</h>
<h>Visualisation provided using the <a href="http://brat.nlplab.org/">brat visualisation/annotation software</a>.</h>
<h>Visualisation provided using the <a href="http://brat.nlplab.org/">brat visualisation/annotation software</a>.</h>
<h>Visualisation provided using the <a href="http://brat.nlplab.org/">brat visualisation/annotation software</a>.</h>
<h>Visualisation provided using the <a href="http://brat.nlplab.org/">brat visualisation/annotation software</a>.</h>
<h>Visualisation provided using the <a href="http://brat.nlplab.org/">brat visualisation/annotation software</a>.</h>
<h>Visualisation provided using the <a href="http://brat.nlplab.org/">brat visualisation/annotation software</a>.</h>
<script type="text/javascript" src="brat.js"></script>
<script type="text/javascript" src="brat.js"></script>
<script type="text/javascript" src="brat.js"></script>
<script type="text/javascript" src="brat.js"></script>
<script type="text/javascript" src="brat.js"></script>
<script type="text/javascript" src="brat.js"></script>
<script type="text/javascript" src="brat.js"></script>
<script type="text/javascript" src="brat.js"></script>
<script type="text/javascript" src="brat.js"></script>
<script type="text/javascript" src="brat.js"></script>
<script type="text/javascript" src="brat.js"></script>
<script type="text/javascript" src="brat.js"></script>
<script type="text/javascript" src="brat.js"></script>
<script type="text/javascript" src="brat.js"></script>
<script type="text/javascript" src="brat.js"></script>
Stats on how many taggings for how many words
Stats on how many taggings for how many words
Stats on how many taggings for how many words
Stats on how many taggings for how many words
Stats on how many taggings for how many words
Stats on how many taggings for how many words
Stats on how many taggings for how many words
Stats on how many taggings for how many words
Stats on how many taggings for how many words
Stats on how many taggings for how many words
Stats on how many taggings for how many words
Stats on how many taggings for how many words
Stats on how many taggings for how many words
Stats on how many taggings for how many words
Stats on how many taggings for how many words
rulesWithWord length: 15 [should be sum of words + unknown sigs]
rulesWithWord length: 8 [should be sum of words + unknown sigs]
rulesWithWord length: 12 [should be sum of words + unknown sigs]
rulesWithWord length: 5 [should be sum of words + unknown sigs]
rulesWithWord length: 7 [should be sum of words + unknown sigs]
rulesWithWord length: 20 [should be sum of words + unknown sigs]
rulesWithWord length: 14 [should be sum of words + unknown sigs]
rulesWithWord length: 9 [should be sum of words + unknown sigs]
rulesWithWord length: 18 [should be sum of words + unknown sigs]
rulesWithWord length: 6 [should be sum of words + unknown sigs]
rulesWithWord length: 13 [should be sum of words + unknown sigs]
rulesWithWord length: 11 [should be sum of words + unknown sigs]
rulesWithWord length: 16 [should be sum of words + unknown sigs]
Words size: 15
Words size: 20
Words size: 25
Words size: 30
Words size: 35
Words size: 40
Words size: 45
Words size: 50
Words size: 55
Words size: 60
Words size: 65
Words size: 70
Words size: 75
Tags size: 5
Tags size: 20
Tags size: 3
Tags size: 8
Tags size: 15
Tags size: 12
Tags size: 7
Tags size: 6
Tags size: 9
Tags size: 2
Tags size: 14
Tags size: 18
Tags size: 4
windowSize=8
windowSize=16
windowSize=32
windowSize=64
windowSize=128
windowSize=256
windowSize=512
windowSize=1024
windowSize=2048
windowSize=4096
windowSize=8192
windowSize=16384
windowSize=32768
Sum of rulesWithWord: 250
Sum of rulesWithWord: 500
Sum of rulesWithWord: 150
Sum of rulesWithWord: 300
Sum of rulesWithWord: 450
Sum of rulesWithWord: 200
Sum of rulesWithWord: 350
Sum of rulesWithWord: 400
Sum of rulesWithWord: 50
Sum of rulesWithWord: 150
Sum of rulesWithWord: 700
Sum of rulesWithWord: 550
Sum of rulesWithWord: 600
Reading FeatureIndex from path2... done.
Reading FeatureIndex from path3... done.
Reading FeatureIndex from path4... done.
Reading FeatureIndex from path5... done.
Reading FeatureIndex from path6... done.
Reading FeatureIndex from path7... done.
Reading FeatureIndex from path8... done.
Reading FeatureIndex from path9... done.
Reading FeatureIndex from path10... done.
Reading FeatureIndex from path11... done.
Reading FeatureIndex from path12... done.
Reading FeatureIndex from path13... done.
Reading FeatureIndex from path14... done.
unknownLevel is 1
unknownLevel is 2
unknownLevel is 3
unknownLevel is 4
unknownLevel is 5
unknownLevel is 6
unknownLevel is 7
unknownLevel is 8
unknownLevel is 9
unknownLevel is 10
unknownLevel is 11
unknownLevel is 12
unknownLevel is 13
BaseLexicon statistics
BaseLexicon statistics
BaseLexicon statistics
BaseLexicon statistics
BaseLexicon statistics
BaseLexicon statistics
BaseLexicon statistics
BaseLexicon statistics
BaseLexicon statistics
BaseLexicon statistics
BaseLexicon statistics
BaseLexicon statistics
BaseLexicon statistics
BaseLexicon statistics
BaseLexicon statistics
skipped="true"/>
skipped="true"/>
skipped="true"/>
skipped="true"/>
skipped="true"/>
skipped="true"/>
skipped="true"/>
skipped="true"/>
skipped="true"/>
skipped="true"/>
skipped="true"/>
skipped="true"/>
skipped="true"/>
skipped="true"/>
skipped="true"/>
SENTENCE_SKIPPED_OR_UNPARSABLE
SENTENCE_SKIPPED_OR_UNPARSABLE
SENTENCE_SKIPPED_OR_UNPARSABLE
SENTENCE_SKIPPED_OR_UNPARSABLE
SENTENCE_SKIPPED_OR_UNPARSABLE
SENTENCE_SKIPPED_OR_UNPARSABLE
SENTENCE_SKIPPED_OR_UNPARSABLE
SENTENCE_SKIPPED_OR_UNPARSABLE
SENTENCE_SKIPPED_OR_UNPARSABLE
SENTENCE_SKIPPED_OR_UNPARSABLE
SENTENCE_SKIPPED_OR_UNPARSABLE
SENTENCE_SKIPPED_OR_UNPARSABLE
SENTENCE_SKIPPED_OR_UNPARSABLE
SENTENCE_SKIPPED_OR_UNPARSABLE
SENTENCE_SKIPPED_OR_UNPARSABLE
Serializing FeatureIndex to feature_index_2... done.
Serializing FeatureIndex to feature_index_3... done.
Serializing FeatureIndex to feature_index_4... done.
Serializing FeatureIndex to feature_index_5... done.
Serializing FeatureIndex to feature_index_6... done.
Serializing FeatureIndex to feature_index_7... done.
Serializing FeatureIndex to feature_index_8... done.
Serializing FeatureIndex to feature_index_9... done.
Serializing FeatureIndex to feature_index_10... done.
Serializing FeatureIndex to feature_index_11... done.
Serializing FeatureIndex to feature_index_12... done.
Serializing FeatureIndex to feature_index_13... done.
Serializing FeatureIndex to feature_index_14... done.
|intersect|: 15
|intersect|: 5
|intersect|: 8
|intersect|: 12
|intersect|: 3
|intersect|: 7
|intersect|: 14
|intersect|: 9
|intersect|: 6
|intersect|: 11
|intersect|: 2
|intersect|: 4
|intersect|: 13
smooth[1] = 1.8
smooth[2] = 3.2
smooth[3] = 2.1
smooth[4] = 2.9
smooth[5] = 2.7
smooth[6] = 2.2
smooth[7] = 2.6
smooth[8] = 3.5
smooth[9] = 2.4
smooth[10] = 1.9
smooth[11] = 3.1
smooth[12] = 2.8
smooth[13] = 2.3
Reading weights from model2.ser... done.
Reading weights from model3.ser... done.
Reading weights from model4.ser... done.
Reading weights from model5.ser... done.
Reading weights from model6.ser... done.
Reading weights from model7.ser... done.
Reading weights from model8.ser... done.
Reading weights from model9.ser... done.
Reading weights from model10.ser... done.
Reading weights from model11.ser... done.
Reading weights from model12.ser... done.
Reading weights from model13.ser... done.
Reading weights from model14.ser... done.
Longest sentence is of length: 15
Longest sentence is of length: 8
Longest sentence is of length: 12
Longest sentence is of length: 6
Longest sentence is of length: 14
Longest sentence is of length: 9
Longest sentence is of length: 11
Longest sentence is of length: 7
Longest sentence is of length: 13
Longest sentence is of length: 16
Longest sentence is of length: 5
Longest sentence is of length: 18
Longest sentence is of length: 17
Serializing weights to weights2.bin... done.
Serializing weights to weights3.bin... done.
Serializing weights to weights4.bin... done.
Serializing weights to weights5.bin... done.
Serializing weights to weights6.bin... done.
Serializing weights to weights7.bin... done.
Serializing weights to weights8.bin... done.
Serializing weights to weights9.bin... done.
Serializing weights to weights10.bin... done.
Serializing weights to weights11.bin... done.
Serializing weights to weights12.bin... done.
Serializing weights to weights13.bin... done.
Serializing weights to weights14.bin... done.
Average length: 32.78; median length: 16.23
Average length: 55.43; median length: 32.00
Average length: 29.81; median length: 15.10
Average length: 38.95; median length: 19.23
Average length: 41.77; median length: 23.65
Average length: 27.64; median length: 12.80
Average length: 36.29; median length: 17.43
Average length: 49.13; median length: 28.90
Average length: 31.56; median length: 14.70
Average length: 45.23; median length: 26.30
Average length: 23.48; median length: 10.67
Average length: 27.92; median length: 13.00
Average length: 38.74; median length: 21.98
Serializing weights to /path/to/weights2... FAILED. Encountered a serialization error.
Serializing weights to /path/to/weights3... FAILED. Unable to complete the serialization process.
Serializing weights to /path/to/weights4... FAILED. Serialization failed due to an unknown error.
Serializing weights to /path/to/weights5... FAILED. Serialization process terminated unexpectedly.
Serializing weights to /path/to/weights6... FAILED. An error occurred while saving the weights.
Serializing weights to /path/to/weights7... FAILED. Failed to write the weights to the specified path.
Serializing weights to /path/to/weights8... FAILED. Error encountered during the weights serialization.
Serializing weights to /path/to/weights9... FAILED. Serialization operation unsuccessful.
Serializing weights to /path/to/weights10... FAILED. Serialization failed with an error message.
Serializing weights to /path/to/weights11... FAILED. Unable to serialize the weights properly.
Serializing weights to /path/to/weights12... FAILED. Serialization process encountered an unexpected issue.
Serializing weights to /path/to/weights13... FAILED. Failed to complete the serialization due to an error.
Serializing weights to /path/to/weights14... FAILED. Error occurred while saving the serialized weights.
consists of 8 sentences
consists of 3 sentences
consists of 10 sentences
consists of 7 sentences
consists of 2 sentences
consists of 9 sentences
consists of 6 sentences
consists of 4 sentences
consists of 11 sentences
consists of 1 sentences
consists of 13 sentences
consists of 14 sentences
consists of 15 sentences
<div>This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/corenlp.html">publicly released distribution</a>.</div>
<div>This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/corenlp.html">publicly released distribution</a>.</div>
<div>This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/corenlp.html">publicly released distribution</a>.</div>
<div>This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/corenlp.html">publicly released distribution</a>.</div>
<div>This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/corenlp.html">publicly released distribution</a>.</div>
<div>This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/corenlp.html">publicly released distribution</a>.</div>
<div>This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/corenlp.html">publicly released distribution</a>.</div>
<div>This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/corenlp.html">publicly released distribution</a>.</div>
<div>This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/corenlp.html">publicly released distribution</a>.</div>
<div>This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/corenlp.html">publicly released distribution</a>.</div>
<div>This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/corenlp.html">publicly released distribution</a>.</div>
<div>This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/corenlp.html">publicly released distribution</a>.</div>
<div>This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/corenlp.html">publicly released distribution</a>.</div>
<div>This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/corenlp.html">publicly released distribution</a>.</div>
<div>This query is too long.  If you want to run very long queries, please download and use our <a href="http://nlp.stanford.edu/software/corenlp.html">publicly released distribution</a>.</div>
Adding true-case annotation...
Adding true-case annotation...
Adding true-case annotation...
Adding true-case annotation...
Adding true-case annotation...
Adding true-case annotation...
Adding true-case annotation...
Adding true-case annotation...
Adding true-case annotation...
Adding true-case annotation...
Adding true-case annotation...
Adding true-case annotation...
Adding true-case annotation...
Adding true-case annotation...
Adding true-case annotation...
This treebank contains 200 trees.
This treebank contains 300 trees.
This treebank contains 400 trees.
This treebank contains 500 trees.
This treebank contains 600 trees.
This treebank contains 700 trees.
This treebank contains 800 trees.
This treebank contains 900 trees.
This treebank contains 1000 trees.
This treebank contains 1100 trees.
This treebank contains 1200 trees.
This treebank contains 1300 trees.
This treebank contains 1400 trees.
Serializing class index to path2... done.
Serializing class index to path3... done.
Serializing class index to path4... done.
Serializing class index to path5... done.
Serializing class index to path6... done.
Serializing class index to path7... done.
Serializing class index to path8... done.
Serializing class index to path9... done.
Serializing class index to path10... done.
Serializing class index to path11... done.
Serializing class index to path12... done.
Serializing class index to path13... done.
Serializing class index to path14... done.
INFO: Read 50 unique entries out of 1000 from database.csv, false TokensRegex patterns.
INFO: Read 200 unique entries out of 800 from input.txt, true TokensRegex patterns.
INFO: Read 80 unique entries out of 300 from config.json, false TokensRegex patterns.
INFO: Read 150 unique entries out of 600 from output.csv, true TokensRegex patterns.
INFO: Read 120 unique entries out of 400 from log.txt, false TokensRegex patterns.
INFO: Read 250 unique entries out of 1000 from data.db, true TokensRegex patterns.
INFO: Read 40 unique entries out of 200 from sample.json, false TokensRegex patterns.
INFO: Read 180 unique entries out of 700 from input.csv, true TokensRegex patterns.
INFO: Read 90 unique entries out of 600 from config.txt, false TokensRegex patterns.
INFO: Read 220 unique entries out of 800 from output.json, true TokensRegex patterns.
INFO: Read 70 unique entries out of 400 from log.csv, false TokensRegex patterns.
INFO: Read 130 unique entries out of 900 from data.xml, true TokensRegex patterns.
INFO: Read 60 unique entries out of 300 from sample.csv, false TokensRegex patterns.
INFO: Read 210 unique entries out of 1000 from input.json, true TokensRegex patterns.
There were 2000 words in the treebank.
There were 3000 words in the treebank.
There were 4000 words in the treebank.
There were 5000 words in the treebank.
There were 6000 words in the treebank.
There were 7000 words in the treebank.
There were 8000 words in the treebank.
There were 9000 words in the treebank.
There were 10000 words in the treebank.
There were 11000 words in the treebank.
There were 12000 words in the treebank.
There were 13000 words in the treebank.
There were 14000 words in the treebank.
c_W is 0.5 mle = 0.11 smoothInUnknownsThresh is 0.9 base p_T_U is 1.2/0.3 = 0.4
c_W is 0.75 mle = 0.63 smoothInUnknownsThresh is 0.6 base p_T_U is 0.4/0.5 = 0.9
c_W is 1.0 mle = 0.84 smoothInUnknownsThresh is 0.3 base p_T_U is 1.1/0.2 = 0.6
c_W is 1.25 mle = 0.99 smoothInUnknownsThresh is 0.1 base p_T_U is 0.8/0.9 = 0.3
c_W is 1.5 mle = 0.22 smoothInUnknownsThresh is 0.8 base p_T_U is 0.6/0.7 = 0.7
c_W is 1.75 mle = 0.44 smoothInUnknownsThresh is 0.5 base p_T_U is 1.2/0.3 = 1.0
c_W is 2.0 mle = 0.66 smoothInUnknownsThresh is 0.9 base p_T_U is 0.4/0.5 = 0.5
c_W is 2.25 mle = 0.88 smoothInUnknownsThresh is 0.6 base p_T_U is 1.1/0.2 = 0.8
c_W is 2.5 mle = 0.11 smoothInUnknownsThresh is 0.3 base p_T_U is 0.8/0.9 = 0.1
c_W is 2.75 mle = 0.33 smoothInUnknownsThresh is 0.1 base p_T_U is 0.6/0.7 = 0.4
c_W is 3.0 mle = 0.55 smoothInUnknownsThresh is 0.8 base p_T_U is 1.2/0.3 = 0.7
c_W is 3.25 mle = 0.77 smoothInUnknownsThresh is 0.5 base p_T_U is 0.4/0.5 = 1.0
c_W is 3.5 mle = 0.99 smoothInUnknownsThresh is 0.9 base p_T_U is 1.1/0.2 = 0.3
The -punct option requires you to specify -tlp
The -punct option requires you to specify -tlp
The -punct option requires you to specify -tlp
The -punct option requires you to specify -tlp
The -punct option requires you to specify -tlp
The -punct option requires you to specify -tlp
The -punct option requires you to specify -tlp
The -punct option requires you to specify -tlp
The -punct option requires you to specify -tlp
The -punct option requires you to specify -tlp
The -punct option requires you to specify -tlp
The -punct option requires you to specify -tlp
The -punct option requires you to specify -tlp
The -punct option requires you to specify -tlp
The -punct option requires you to specify -tlp
annotationOptions not yet implemented
annotationOptions not yet implemented
annotationOptions not yet implemented
annotationOptions not yet implemented
annotationOptions not yet implemented
annotationOptions not yet implemented
annotationOptions not yet implemented
annotationOptions not yet implemented
annotationOptions not yet implemented
annotationOptions not yet implemented
annotationOptions not yet implemented
annotationOptions not yet implemented
annotationOptions not yet implemented
annotationOptions not yet implemented
annotationOptions not yet implemented
Serializing Text classifier to model2.ser... done.
Serializing Text classifier to model3.ser... done.
Serializing Text classifier to model4.ser... done.
Serializing Text classifier to model5.ser... done.
Serializing Text classifier to model6.ser... done.
Serializing Text classifier to model7.ser... done.
Serializing Text classifier to model8.ser... done.
Serializing Text classifier to model9.ser... done.
Serializing Text classifier to model10.ser... done.
Serializing Text classifier to model11.ser... done.
Serializing Text classifier to model12.ser... done.
Serializing Text classifier to model13.ser... done.
Serializing Text classifier to model14.ser... done.
Couldn't instantiate as TreebankLanguagePack: image.jpg
Couldn't instantiate as TreebankLanguagePack: socket.jpg
Couldn't instantiate as TreebankLanguagePack: user.xml
Couldn't instantiate as TreebankLanguagePack: config.properties
Couldn't instantiate as TreebankLanguagePack: data.txt
Couldn't instantiate as TreebankLanguagePack: log.log
Couldn't instantiate as TreebankLanguagePack: index.html
Couldn't instantiate as TreebankLanguagePack: script.js
Couldn't instantiate as TreebankLanguagePack: style.css
Couldn't instantiate as TreebankLanguagePack: picture.jpg
Couldn't instantiate as TreebankLanguagePack: document.doc
Couldn't instantiate as TreebankLanguagePack: video.mp4
Couldn't instantiate as TreebankLanguagePack: audio.wav
WARNING: Entry doesn't have overwriteable types SecondEntry, but entry type is in noDefaultOverwriteLabels
WARNING: Entry doesn't have overwriteable types ThirdEntry, but entry type is in noDefaultOverwriteLabels
WARNING: Entry doesn't have overwriteable types FourthEntry, but entry type is in noDefaultOverwriteLabels
WARNING: Entry doesn't have overwriteable types FifthEntry, but entry type is in noDefaultOverwriteLabels
WARNING: Entry doesn't have overwriteable types SixthEntry, but entry type is in noDefaultOverwriteLabels
WARNING: Entry doesn't have overwriteable types SeventhEntry, but entry type is in noDefaultOverwriteLabels
WARNING: Entry doesn't have overwriteable types EighthEntry, but entry type is in noDefaultOverwriteLabels
WARNING: Entry doesn't have overwriteable types NinthEntry, but entry type is in noDefaultOverwriteLabels
WARNING: Entry doesn't have overwriteable types TenthEntry, but entry type is in noDefaultOverwriteLabels
WARNING: Entry doesn't have overwriteable types EleventhEntry, but entry type is in noDefaultOverwriteLabels
WARNING: Entry doesn't have overwriteable types TwelfthEntry, but entry type is in noDefaultOverwriteLabels
WARNING: Entry doesn't have overwriteable types ThirteenthEntry, but entry type is in noDefaultOverwriteLabels
WARNING: Entry doesn't have overwriteable types FourteenthEntry, but entry type is in noDefaultOverwriteLabels
Serializing Text classifier to serialized_model2.pkl... FAILED. File path does not exist.
Serializing Text classifier to serialized_model3.pkl... FAILED. Serialization error.
Serializing Text classifier to serialized_model4.pkl... FAILED. File is already in use.
Serializing Text classifier to serialized_model5.pkl... FAILED. Invalid file format.
Serializing Text classifier to serialized_model6.pkl... FAILED. Insufficient disk space.
Serializing Text classifier to serialized_model7.pkl... FAILED. Permission denied.
Serializing Text classifier to serialized_model8.pkl... FAILED. File system error.
Serializing Text classifier to serialized_model9.pkl... FAILED. Out of memory.
Serializing Text classifier to serialized_model10.pkl... FAILED. Invalid serialization path.
Serializing Text classifier to serialized_model11.pkl... FAILED. File access error.
Serializing Text classifier to serialized_model12.pkl... FAILED. Serialization timeout.
Serializing Text classifier to serialized_model13.pkl... FAILED. Serialization data corrupted.
Serializing Text classifier to serialized_model14.pkl... FAILED. File size exceeded maximum limit.
Couldn't instantiate as TreeReaderFactory: image.jpg
Couldn't instantiate as TreeReaderFactory: socket.jpg
Couldn't instantiate as TreeReaderFactory: folder
Couldn't instantiate as TreeReaderFactory: database
Couldn't instantiate as TreeReaderFactory: config.txt
Couldn't instantiate as TreeReaderFactory: log.log
Couldn't instantiate as TreeReaderFactory: script.sh
Couldn't instantiate as TreeReaderFactory: index.html
Couldn't instantiate as TreeReaderFactory: style.css
Couldn't instantiate as TreeReaderFactory: template.html
Couldn't instantiate as TreeReaderFactory: data.csv
Couldn't instantiate as TreeReaderFactory: user.txt
Couldn't instantiate as TreeReaderFactory: output.pdf
filter <class>: class implements Predicate<Tree>, this filters trees which return false
filter <class>: class implements Predicate<Tree>, this filters trees which return false
filter <class>: class implements Predicate<Tree>, this filters trees which return false
filter <class>: class implements Predicate<Tree>, this filters trees which return false
filter <class>: class implements Predicate<Tree>, this filters trees which return false
filter <class>: class implements Predicate<Tree>, this filters trees which return false
filter <class>: class implements Predicate<Tree>, this filters trees which return false
filter <class>: class implements Predicate<Tree>, this filters trees which return false
filter <class>: class implements Predicate<Tree>, this filters trees which return false
filter <class>: class implements Predicate<Tree>, this filters trees which return false
filter <class>: class implements Predicate<Tree>, this filters trees which return false
filter <class>: class implements Predicate<Tree>, this filters trees which return false
filter <class>: class implements Predicate<Tree>, this filters trees which return false
filter <class>: class implements Predicate<Tree>, this filters trees which return false
filter <class>: class implements Predicate<Tree>, this filters trees which return false
weights.length= 8
weights.length= 5
weights.length= 15
weights.length= 13
weights.length= 6
weights.length= 9
weights.length= 12
weights.length= 3
weights.length= 7
weights.length= 11
weights.length= 14
weights.length= 4
weights.length= 2
-oneLine	-words	-taggedWords	-annotate options
-oneLine	-words	-taggedWords	-annotate options
-oneLine	-words	-taggedWords	-annotate options
-oneLine	-words	-taggedWords	-annotate options
-oneLine	-words	-taggedWords	-annotate options
-oneLine	-words	-taggedWords	-annotate options
-oneLine	-words	-taggedWords	-annotate options
-oneLine	-words	-taggedWords	-annotate options
-oneLine	-words	-taggedWords	-annotate options
-oneLine	-words	-taggedWords	-annotate options
-oneLine	-words	-taggedWords	-annotate options
-oneLine	-words	-taggedWords	-annotate options
-oneLine	-words	-taggedWords	-annotate options
-oneLine	-words	-taggedWords	-annotate options
-oneLine	-words	-taggedWords	-annotate options
-summary	-decimate	-yield	-correct	-punct
-summary	-decimate	-yield	-correct	-punct
-summary	-decimate	-yield	-correct	-punct
-summary	-decimate	-yield	-correct	-punct
-summary	-decimate	-yield	-correct	-punct
-summary	-decimate	-yield	-correct	-punct
-summary	-decimate	-yield	-correct	-punct
-summary	-decimate	-yield	-correct	-punct
-summary	-decimate	-yield	-correct	-punct
-summary	-decimate	-yield	-correct	-punct
-summary	-decimate	-yield	-correct	-punct
-summary	-decimate	-yield	-correct	-punct
-summary	-decimate	-yield	-correct	-punct
-summary	-decimate	-yield	-correct	-punct
-summary	-decimate	-yield	-correct	-punct
-pennPrint	-encoding enc	-tlp class	-sentenceLengths
-pennPrint	-encoding enc	-tlp class	-sentenceLengths
-pennPrint	-encoding enc	-tlp class	-sentenceLengths
-pennPrint	-encoding enc	-tlp class	-sentenceLengths
-pennPrint	-encoding enc	-tlp class	-sentenceLengths
-pennPrint	-encoding enc	-tlp class	-sentenceLengths
-pennPrint	-encoding enc	-tlp class	-sentenceLengths
-pennPrint	-encoding enc	-tlp class	-sentenceLengths
-pennPrint	-encoding enc	-tlp class	-sentenceLengths
-pennPrint	-encoding enc	-tlp class	-sentenceLengths
-pennPrint	-encoding enc	-tlp class	-sentenceLengths
-pennPrint	-encoding enc	-tlp class	-sentenceLengths
-pennPrint	-encoding enc	-tlp class	-sentenceLengths
-pennPrint	-encoding enc	-tlp class	-sentenceLengths
-pennPrint	-encoding enc	-tlp class	-sentenceLengths
-maxLength n	-suffix ext	-treeReaderFactory class
-maxLength n	-suffix ext	-treeReaderFactory class
-maxLength n	-suffix ext	-treeReaderFactory class
-maxLength n	-suffix ext	-treeReaderFactory class
-maxLength n	-suffix ext	-treeReaderFactory class
-maxLength n	-suffix ext	-treeReaderFactory class
-maxLength n	-suffix ext	-treeReaderFactory class
-maxLength n	-suffix ext	-treeReaderFactory class
-maxLength n	-suffix ext	-treeReaderFactory class
-maxLength n	-suffix ext	-treeReaderFactory class
-maxLength n	-suffix ext	-treeReaderFactory class
-maxLength n	-suffix ext	-treeReaderFactory class
-maxLength n	-suffix ext	-treeReaderFactory class
-maxLength n	-suffix ext	-treeReaderFactory class
-maxLength n	-suffix ext	-treeReaderFactory class
Useful flags include:
Useful flags include:
Useful flags include:
Useful flags include:
Useful flags include:
Useful flags include:
Useful flags include:
Useful flags include:
Useful flags include:
Useful flags include:
Useful flags include:
Useful flags include:
Useful flags include:
Useful flags include:
Useful flags include:
Train.openClassTypesThreshold is 0.8
Train.openClassTypesThreshold is 0.2
Train.openClassTypesThreshold is 0.3
Train.openClassTypesThreshold is 0.6
Train.openClassTypesThreshold is 0.9
Train.openClassTypesThreshold is 0.1
Train.openClassTypesThreshold is 0.4
Train.openClassTypesThreshold is 0.7
Train.openClassTypesThreshold is 0.95
Train.openClassTypesThreshold is 0.15
Train.openClassTypesThreshold is 0.25
Train.openClassTypesThreshold is 0.85
Train.openClassTypesThreshold is 0.55
Usage: java Treebanks [-flags]* treebankPath [fileRanges]
Usage: java Treebanks [-flags]* treebankPath [fileRanges]
Usage: java Treebanks [-flags]* treebankPath [fileRanges]
Usage: java Treebanks [-flags]* treebankPath [fileRanges]
Usage: java Treebanks [-flags]* treebankPath [fileRanges]
Usage: java Treebanks [-flags]* treebankPath [fileRanges]
Usage: java Treebanks [-flags]* treebankPath [fileRanges]
Usage: java Treebanks [-flags]* treebankPath [fileRanges]
Usage: java Treebanks [-flags]* treebankPath [fileRanges]
Usage: java Treebanks [-flags]* treebankPath [fileRanges]
Usage: java Treebanks [-flags]* treebankPath [fileRanges]
Usage: java Treebanks [-flags]* treebankPath [fileRanges]
Usage: java Treebanks [-flags]* treebankPath [fileRanges]
Usage: java Treebanks [-flags]* treebankPath [fileRanges]
Usage: java Treebanks [-flags]* treebankPath [fileRanges]
This main method will let you variously manipulate and view a treebank.
This main method will let you variously manipulate and view a treebank.
This main method will let you variously manipulate and view a treebank.
This main method will let you variously manipulate and view a treebank.
This main method will let you variously manipulate and view a treebank.
This main method will let you variously manipulate and view a treebank.
This main method will let you variously manipulate and view a treebank.
This main method will let you variously manipulate and view a treebank.
This main method will let you variously manipulate and view a treebank.
This main method will let you variously manipulate and view a treebank.
This main method will let you variously manipulate and view a treebank.
This main method will let you variously manipulate and view a treebank.
This main method will let you variously manipulate and view a treebank.
This main method will let you variously manipulate and view a treebank.
This main method will let you variously manipulate and view a treebank.
embeddings.size()= 25\n
embeddings.size()= 9\n
embeddings.size()= 42\n
embeddings.size()= 37\n
embeddings.size()= 12\n
embeddings.size()= 22\n
embeddings.size()= 6\n
embeddings.size()= 18\n
embeddings.size()= 34\n
embeddings.size()= 27\n
embeddings.size()= 13\n
embeddings.size()= 8\n
embeddings.size()= 31\n
INFO: Read 250 unique entries from 20 files
INFO: Read 500 unique entries from 30 files
INFO: Read 750 unique entries from 40 files
INFO: Read 1000 unique entries from 50 files
INFO: Read 1250 unique entries from 60 files
INFO: Read 1500 unique entries from 70 files
INFO: Read 1750 unique entries from 80 files
INFO: Read 2000 unique entries from 90 files
INFO: Read 2250 unique entries from 100 files
INFO: Read 2500 unique entries from 110 files
INFO: Read 2750 unique entries from 120 files
INFO: Read 3000 unique entries from 130 files
INFO: Read 3250 unique entries from 140 files
Initializing lexicon scores ...
Initializing lexicon scores ...
Initializing lexicon scores ...
Initializing lexicon scores ...
Initializing lexicon scores ...
Initializing lexicon scores ...
Initializing lexicon scores ...
Initializing lexicon scores ...
Initializing lexicon scores ...
Initializing lexicon scores ...
Initializing lexicon scores ...
Initializing lexicon scores ...
Initializing lexicon scores ...
Initializing lexicon scores ...
Initializing lexicon scores ...
classIndex.size()= 45
classIndex.size()= 62
classIndex.size()= 23
classIndex.size()= 8
classIndex.size()= 39
classIndex.size()= 17
classIndex.size()= 56
classIndex.size()= 12
classIndex.size()= 28
classIndex.size()= 41
classIndex.size()= 35
classIndex.size()= 19
classIndex.size()= 71
Adding TokensRegexNER annotations ...
Adding TokensRegexNER annotations ...
Adding TokensRegexNER annotations ...
Adding TokensRegexNER annotations ...
Adding TokensRegexNER annotations ...
Adding TokensRegexNER annotations ...
Adding TokensRegexNER annotations ...
Adding TokensRegexNER annotations ...
Adding TokensRegexNER annotations ...
Adding TokensRegexNER annotations ...
Adding TokensRegexNER annotations ...
Adding TokensRegexNER annotations ...
Adding TokensRegexNER annotations ...
Adding TokensRegexNER annotations ...
Adding TokensRegexNER annotations ...
DEBUG: float[5][] weights loaded
DEBUG: float[20][] weights loaded
DEBUG: float[8][] weights loaded
DEBUG: float[15][] weights loaded
DEBUG: float[12][] weights loaded
DEBUG: float[7][] weights loaded
DEBUG: float[3][] weights loaded
DEBUG: float[18][] weights loaded
DEBUG: float[6][] weights loaded
DEBUG: float[9][] weights loaded
DEBUG: float[13][] weights loaded
DEBUG: float[4][] weights loaded
DEBUG: float[11][] weights loaded
using case insensitive!
using case insensitive!
using case insensitive!
using case insensitive!
using case insensitive!
using case insensitive!
using case insensitive!
using case insensitive!
using case insensitive!
using case insensitive!
using case insensitive!
using case insensitive!
using case insensitive!
using case insensitive!
using case insensitive!
Loading processed data from serialized file ... done. Got 250 datums.
Loading processed data from serialized file ... done. Got 500 datums.
Loading processed data from serialized file ... done. Got 1000 datums.
Loading processed data from serialized file ... done. Got 2000 datums.
Loading processed data from serialized file ... done. Got 5000 datums.
Loading processed data from serialized file ... done. Got 10000 datums.
Loading processed data from serialized file ... done. Got 20000 datums.
Loading processed data from serialized file ... done. Got 50000 datums.
Loading processed data from serialized file ... done. Got 100000 datums.
Loading processed data from serialized file ... done. Got 200000 datums.
Loading processed data from serialized file ... done. Got 500000 datums.
Loading processed data from serialized file ... done. Got 1000000 datums.
Loading processed data from serialized file ... done. Got 2000000 datums.
Beginning tokenization
Beginning tokenization
Beginning tokenization
Beginning tokenization
Beginning tokenization
Beginning tokenization
Beginning tokenization
Beginning tokenization
Beginning tokenization
Beginning tokenization
Beginning tokenization
Beginning tokenization
Beginning tokenization
Beginning tokenization
Beginning tokenization
No tokenizer type provided. Defaulting to PTBTokenizer.
No tokenizer type provided. Defaulting to PTBTokenizer.
No tokenizer type provided. Defaulting to PTBTokenizer.
No tokenizer type provided. Defaulting to PTBTokenizer.
No tokenizer type provided. Defaulting to PTBTokenizer.
No tokenizer type provided. Defaulting to PTBTokenizer.
No tokenizer type provided. Defaulting to PTBTokenizer.
No tokenizer type provided. Defaulting to PTBTokenizer.
No tokenizer type provided. Defaulting to PTBTokenizer.
No tokenizer type provided. Defaulting to PTBTokenizer.
No tokenizer type provided. Defaulting to PTBTokenizer.
No tokenizer type provided. Defaulting to PTBTokenizer.
No tokenizer type provided. Defaulting to PTBTokenizer.
No tokenizer type provided. Defaulting to PTBTokenizer.
No tokenizer type provided. Defaulting to PTBTokenizer.
20 phrasal category types, 200 tag types, and 2000 word types
30 phrasal category types, 300 tag types, and 3000 word types
40 phrasal category types, 400 tag types, and 4000 word types
50 phrasal category types, 500 tag types, and 5000 word types
60 phrasal category types, 600 tag types, and 6000 word types
70 phrasal category types, 700 tag types, and 7000 word types
80 phrasal category types, 800 tag types, and 8000 word types
90 phrasal category types, 900 tag types, and 9000 word types
100 phrasal category types, 1000 tag types, and 10000 word types
110 phrasal category types, 1100 tag types, and 11000 word types
120 phrasal category types, 1200 tag types, and 12000 word types
130 phrasal category types, 1300 tag types, and 13000 word types
140 phrasal category types, 1400 tag types, and 14000 word types
Saving processed data of size 250 to serialized file...
Saving processed data of size 500 to serialized file...
Saving processed data of size 1000 to serialized file...
Saving processed data of size 2000 to serialized file...
Saving processed data of size 5000 to serialized file...
Saving processed data of size 10000 to serialized file...
Saving processed data of size 20000 to serialized file...
Saving processed data of size 50000 to serialized file...
Saving processed data of size 100000 to serialized file...
Saving processed data of size 200000 to serialized file...
Saving processed data of size 500000 to serialized file...
Saving processed data of size 1000000 to serialized file...
Saving processed data of size 2000000 to serialized file...
Feature is NULL!
Feature is NULL!
Feature is NULL!
Feature is NULL!
Feature is NULL!
Feature is NULL!
Feature is NULL!
Feature is NULL!
Feature is NULL!
Feature is NULL!
Feature is NULL!
Feature is NULL!
Feature is NULL!
Feature is NULL!
Feature is NULL!
Warning: desired encoding ISO-8859-1 not accepted. IOException
Warning: desired encoding UTF-16 not accepted. IllegalArgumentException
Warning: desired encoding ASCII not accepted. InvalidEncodingException
Warning: desired encoding UTF-32 not accepted. MalformedEncodingException
Warning: desired encoding UTF-8 not accepted. SocketTimeoutException
Warning: desired encoding ISO-8859-1 not accepted. EOFException
Warning: desired encoding UTF-16 not accepted. NullPointerException
Warning: desired encoding ASCII not accepted. FileNotFoundException
Warning: desired encoding UTF-32 not accepted. SQLException
Warning: desired encoding UTF-8 not accepted. SSLHandshakeException
Warning: desired encoding ISO-8859-1 not accepted. XPathExpressionException
Warning: desired encoding UTF-16 not accepted. ClassCastException
Warning: desired encoding ASCII not accepted. NoSuchAlgorithmException
Using UTF-8 to construct PrintWriter
Using UTF-8 to construct PrintWriter
Using UTF-8 to construct PrintWriter
Using UTF-8 to construct PrintWriter
Using UTF-8 to construct PrintWriter
Using UTF-8 to construct PrintWriter
Using UTF-8 to construct PrintWriter
Using UTF-8 to construct PrintWriter
Using UTF-8 to construct PrintWriter
Using UTF-8 to construct PrintWriter
Using UTF-8 to construct PrintWriter
Using UTF-8 to construct PrintWriter
Using UTF-8 to construct PrintWriter
Using UTF-8 to construct PrintWriter
Using UTF-8 to construct PrintWriter
Extracted quotes:
Extracted quotes:
Extracted quotes:
Extracted quotes:
Extracted quotes:
Extracted quotes:
Extracted quotes:
Extracted quotes:
Extracted quotes:
Extracted quotes:
Extracted quotes:
Extracted quotes:
Extracted quotes:
Extracted quotes:
Extracted quotes:
initTagBins: tags 20 bins 10
initTagBins: tags 15 bins 7
initTagBins: tags 8 bins 4
initTagBins: tags 13 bins 6
initTagBins: tags 25 bins 12
initTagBins: tags 18 bins 9
initTagBins: tags 30 bins 15
initTagBins: tags 12 bins 6
initTagBins: tags 16 bins 8
initTagBins: tags 22 bins 11
initTagBins: tags 9 bins 4
initTagBins: tags 14 bins 7
initTagBins: tags 28 bins 14
Extracted the following KBP triples:
Extracted the following KBP triples:
Extracted the following KBP triples:
Extracted the following KBP triples:
Extracted the following KBP triples:
Extracted the following KBP triples:
Extracted the following KBP triples:
Extracted the following KBP triples:
Extracted the following KBP triples:
Extracted the following KBP triples:
Extracted the following KBP triples:
Extracted the following KBP triples:
Extracted the following KBP triples:
Extracted the following KBP triples:
Extracted the following KBP triples:
data[dataIndex].length         512
data[dataIndex].length         1024
data[dataIndex].length         128
data[dataIndex].length         64
data[dataIndex].length         256
data[dataIndex].length         512
data[dataIndex].length         512
data[dataIndex].length         128
data[dataIndex].length         1024
data[dataIndex].length         256
data[dataIndex].length         128
data[dataIndex].length         64
data[dataIndex].length         512
Extracted the following Open IE triples:
Extracted the following Open IE triples:
Extracted the following Open IE triples:
Extracted the following Open IE triples:
Extracted the following Open IE triples:
Extracted the following Open IE triples:
Extracted the following Open IE triples:
Extracted the following Open IE triples:
Extracted the following Open IE triples:
Extracted the following Open IE triples:
Extracted the following Open IE triples:
Extracted the following Open IE triples:
Extracted the following Open IE triples:
Extracted the following Open IE triples:
Extracted the following Open IE triples:
Example bad root rewrites as leaf: banana
Example bad root rewrites as leaf: cherry
Example bad root rewrites as leaf: date
Example bad root rewrites as leaf: eggplant
Example bad root rewrites as leaf: fig
Example bad root rewrites as leaf: grape
Example bad root rewrites as leaf: honeydew
Example bad root rewrites as leaf: kiwi
Example bad root rewrites as leaf: lemon
Example bad root rewrites as leaf: mango
Example bad root rewrites as leaf: nectarine
Example bad root rewrites as leaf: orange
Example bad root rewrites as leaf: pear
Extracted the following MachineReading relation mentions:
Extracted the following MachineReading relation mentions:
Extracted the following MachineReading relation mentions:
Extracted the following MachineReading relation mentions:
Extracted the following MachineReading relation mentions:
Extracted the following MachineReading relation mentions:
Extracted the following MachineReading relation mentions:
Extracted the following MachineReading relation mentions:
Extracted the following MachineReading relation mentions:
Extracted the following MachineReading relation mentions:
Extracted the following MachineReading relation mentions:
Extracted the following MachineReading relation mentions:
Extracted the following MachineReading relation mentions:
Extracted the following MachineReading relation mentions:
Extracted the following MachineReading relation mentions:
Extracted the following MachineReading entity mentions:
Extracted the following MachineReading entity mentions:
Extracted the following MachineReading entity mentions:
Extracted the following MachineReading entity mentions:
Extracted the following MachineReading entity mentions:
Extracted the following MachineReading entity mentions:
Extracted the following MachineReading entity mentions:
Extracted the following MachineReading entity mentions:
Extracted the following MachineReading entity mentions:
Extracted the following MachineReading entity mentions:
Extracted the following MachineReading entity mentions:
Extracted the following MachineReading entity mentions:
Extracted the following MachineReading entity mentions:
Extracted the following MachineReading entity mentions:
Extracted the following MachineReading entity mentions:
Extracted the following NER entity mentions:
Extracted the following NER entity mentions:
Extracted the following NER entity mentions:
Extracted the following NER entity mentions:
Extracted the following NER entity mentions:
Extracted the following NER entity mentions:
Extracted the following NER entity mentions:
Extracted the following NER entity mentions:
Extracted the following NER entity mentions:
Extracted the following NER entity mentions:
Extracted the following NER entity mentions:
Extracted the following NER entity mentions:
Extracted the following NER entity mentions:
Extracted the following NER entity mentions:
Extracted the following NER entity mentions:
There are 5 tags.
There are 3 tags.
There are 20 tags.
There are 7 tags.
There are 12 tags.
There are 8 tags.
There are 15 tags.
There are 6 tags.
There are 9 tags.
There are 4 tags.
There are 2 tags.
There are 18 tags.
There are 14 tags.
Could not load split!
Could not load split!
Could not load split!
Could not load split!
Could not load split!
Could not load split!
Could not load split!
Could not load split!
Could not load split!
Could not load split!
Could not load split!
Could not load split!
Could not load split!
Could not load split!
Could not load split!
Dependency Parse (graph2):
Dependency Parse (graph3):
Dependency Parse (graph4):
Dependency Parse (graph5):
Dependency Parse (graph6):
Dependency Parse (graph7):
Dependency Parse (graph8):
Dependency Parse (graph9):
Dependency Parse (graph10):
Dependency Parse (graph11):
Dependency Parse (graph12):
Dependency Parse (graph13):
Dependency Parse (graph14):
Add processed data failed.Exception: Invalid input data format.
Add processed data failed.Exception: Data exceeds maximum size limit.
Add processed data failed.Exception: Server timeout.
Add processed data failed.Exception: Database connection error.
Add processed data failed.Exception: Unauthorized access.
Add processed data failed.Exception: Required fields missing.
Add processed data failed.Exception: Data validation failed.
Add processed data failed.Exception: Duplicate entry found.
Add processed data failed.Exception: Unable to process request.
Add processed data failed.Exception: Invalid API key.
Add processed data failed.Exception: Insufficient permissions.
Add processed data failed.Exception: File not found.
Add processed data failed.Exception: Network connection lost.
>>> OOV word types
>>> OOV word types
>>> OOV word types
>>> OOV word types
>>> OOV word types
>>> OOV word types
>>> OOV word types
>>> OOV word types
>>> OOV word types
>>> OOV word types
>>> OOV word types
>>> OOV word types
>>> OOV word types
>>> OOV word types
>>> OOV word types
Sentiment-annotated binary tree:
Sentiment-annotated binary tree:
Sentiment-annotated binary tree:
Sentiment-annotated binary tree:
Sentiment-annotated binary tree:
Sentiment-annotated binary tree:
Sentiment-annotated binary tree:
Sentiment-annotated binary tree:
Sentiment-annotated binary tree:
Sentiment-annotated binary tree:
Sentiment-annotated binary tree:
Sentiment-annotated binary tree:
Sentiment-annotated binary tree:
Sentiment-annotated binary tree:
Sentiment-annotated binary tree:
gradient check passed
gradient check passed
gradient check passed
gradient check passed
gradient check passed
gradient check passed
gradient check passed
gradient check passed
gradient check passed
gradient check passed
gradient check passed
gradient check passed
gradient check passed
gradient check passed
gradient check passed
>>> Word counts
>>> Word counts
>>> Word counts
>>> Word counts
>>> Word counts
>>> Word counts
>>> Word counts
>>> Word counts
>>> Word counts
>>> Word counts
>>> Word counts
>>> Word counts
>>> Word counts
>>> Word counts
>>> Word counts
Binary Constituency parse:
Binary Constituency parse:
Binary Constituency parse:
Binary Constituency parse:
Binary Constituency parse:
Binary Constituency parse:
Binary Constituency parse:
Binary Constituency parse:
Binary Constituency parse:
Binary Constituency parse:
Binary Constituency parse:
Binary Constituency parse:
Binary Constituency parse:
Binary Constituency parse:
Binary Constituency parse:
Successfully tested stochastic objective function.
Successfully tested stochastic objective function.
Successfully tested stochastic objective function.
Successfully tested stochastic objective function.
Successfully tested stochastic objective function.
Successfully tested stochastic objective function.
Successfully tested stochastic objective function.
Successfully tested stochastic objective function.
Successfully tested stochastic objective function.
Successfully tested stochastic objective function.
Successfully tested stochastic objective function.
Successfully tested stochastic objective function.
Successfully tested stochastic objective function.
Successfully tested stochastic objective function.
Successfully tested stochastic objective function.
>>> POS tag counts
>>> POS tag counts
>>> POS tag counts
>>> POS tag counts
>>> POS tag counts
>>> POS tag counts
>>> POS tag counts
>>> POS tag counts
>>> POS tag counts
>>> POS tag counts
>>> POS tag counts
>>> POS tag counts
>>> POS tag counts
>>> POS tag counts
>>> POS tag counts
numWeights: 15
numWeights: 20
numWeights: 25
numWeights: 30
numWeights: 35
numWeights: 40
numWeights: 45
numWeights: 50
numWeights: 55
numWeights: 60
numWeights: 65
numWeights: 70
numWeights: 75
>>> Phrasal tag counts
>>> Phrasal tag counts
>>> Phrasal tag counts
>>> Phrasal tag counts
>>> Phrasal tag counts
>>> Phrasal tag counts
>>> Phrasal tag counts
>>> Phrasal tag counts
>>> Phrasal tag counts
>>> Phrasal tag counts
>>> Phrasal tag counts
>>> Phrasal tag counts
>>> Phrasal tag counts
>>> Phrasal tag counts
>>> Phrasal tag counts
After feature grouping, total of 7 groups
After feature grouping, total of 11 groups
After feature grouping, total of 2 groups
After feature grouping, total of 9 groups
After feature grouping, total of 6 groups
After feature grouping, total of 5 groups
After feature grouping, total of 3 groups
After feature grouping, total of 8 groups
After feature grouping, total of 12 groups
After feature grouping, total of 1 groups
After feature grouping, total of 10 groups
After feature grouping, total of 15 groups
After feature grouping, total of 13 groups
Document Source Type: Word
Document Source Type: Excel
Document Source Type: Image
Document Source Type: Presentation
Document Source Type: Text
Document Source Type: HTML
Document Source Type: CSV
Document Source Type: XML
Document Source Type: JSON
Document Source Type: Audio
Document Source Type: Video
Document Source Type: Database
Document Source Type: Web
>>> Branching factor means by phrasal tag:
>>> Branching factor means by phrasal tag:
>>> Branching factor means by phrasal tag:
>>> Branching factor means by phrasal tag:
>>> Branching factor means by phrasal tag:
>>> Branching factor means by phrasal tag:
>>> Branching factor means by phrasal tag:
>>> Branching factor means by phrasal tag:
>>> Branching factor means by phrasal tag:
>>> Branching factor means by phrasal tag:
>>> Branching factor means by phrasal tag:
>>> Branching factor means by phrasal tag:
>>> Branching factor means by phrasal tag:
>>> Branching factor means by phrasal tag:
>>> Branching factor means by phrasal tag:
Removing features with weight below 0.7 and retraining...
Removing features with weight below 0.3 and retraining...
Removing features with weight below 0.9 and retraining...
Removing features with weight below 0.2 and retraining...
Removing features with weight below 0.4 and retraining...
Removing features with weight below 0.6 and retraining...
Removing features with weight below 0.8 and retraining...
Removing features with weight below 0.1 and retraining...
Removing features with weight below 0.3 and retraining...
Removing features with weight below 0.7 and retraining...
Removing features with weight below 0.9 and retraining...
Removing features with weight below 0.5 and retraining...
Removing features with weight below 0.2 and retraining...
Reading temporary feature index file.
Reading temporary feature index file.
Reading temporary feature index file.
Reading temporary feature index file.
Reading temporary feature index file.
Reading temporary feature index file.
Reading temporary feature index file.
Reading temporary feature index file.
Reading temporary feature index file.
Reading temporary feature index file.
Reading temporary feature index file.
Reading temporary feature index file.
Reading temporary feature index file.
Reading temporary feature index file.
Reading temporary feature index file.
Writing feature index to temporary file.
Writing feature index to temporary file.
Writing feature index to temporary file.
Writing feature index to temporary file.
Writing feature index to temporary file.
Writing feature index to temporary file.
Writing feature index to temporary file.
Writing feature index to temporary file.
Writing feature index to temporary file.
Writing feature index to temporary file.
Writing feature index to temporary file.
Writing feature index to temporary file.
Writing feature index to temporary file.
Writing feature index to temporary file.
Writing feature index to temporary file.
Warning! numTags differs and is 10
Warning! numTags differs and is 15
Warning! numTags differs and is 20
Warning! numTags differs and is 25
Warning! numTags differs and is 30
Warning! numTags differs and is 35
Warning! numTags differs and is 40
Warning! numTags differs and is 45
Warning! numTags differs and is 50
Warning! numTags differs and is 55
Warning! numTags differs and is 60
Warning! numTags differs and is 65
Warning! numTags differs and is 70
Document Type: Receipt
Document Type: Contract
Document Type: Report
Document Type: Letter
Document Type: Proposal
Document Type: Agreement
Document Type: Application
Document Type: Statement
Document Type: Form
Document Type: Memo
Document Type: Notice
Document Type: Policy
Document Type: Resume
Time to export class index : 1.78 seconds
Time to export class index : 3.21 seconds
Time to export class index : 0.98 seconds
Time to export class index : 4.57 seconds
Time to export class index : 2.13 seconds
Time to export class index : 1.45 seconds
Time to export class index : 3.87 seconds
Time to export class index : 1.99 seconds
Time to export class index : 0.74 seconds
Time to export class index : 2.89 seconds
Time to export class index : 3.76 seconds
Time to export class index : 2.65 seconds
Time to export class index : 1.24 seconds
Treebank has 2000 trees (1000 of length <= 40) and 100000 words (tokens)
Treebank has 3000 trees (1500 of length <= 40) and 150000 words (tokens)
Treebank has 4000 trees (2000 of length <= 40) and 200000 words (tokens)
Treebank has 5000 trees (2500 of length <= 40) and 250000 words (tokens)
Treebank has 6000 trees (3000 of length <= 40) and 300000 words (tokens)
Treebank has 7000 trees (3500 of length <= 40) and 350000 words (tokens)
Treebank has 8000 trees (4000 of length <= 40) and 400000 words (tokens)
Treebank has 9000 trees (4500 of length <= 40) and 450000 words (tokens)
Treebank has 10000 trees (5000 of length <= 40) and 500000 words (tokens)
Treebank has 11000 trees (5500 of length <= 40) and 550000 words (tokens)
Treebank has 12000 trees (6000 of length <= 40) and 600000 words (tokens)
Treebank has 13000 trees (6500 of length <= 40) and 650000 words (tokens)
Treebank has 14000 trees (7000 of length <= 40) and 700000 words (tokens)
-->distribution package details
-->distribution package details
-->distribution package details
-->distribution package details
-->distribution package details
-->distribution package details
-->distribution package details
-->distribution package details
-->distribution package details
-->distribution package details
-->distribution package details
-->distribution package details
-->distribution package details
-->distribution package details
-->distribution package details
Current memory used: 2048 MB
Current memory used: 512 MB
Current memory used: 4096 MB
Current memory used: 8192 MB
Current memory used: 256 MB
Current memory used: 1536 MB
Current memory used: 3072 MB
Current memory used: 6144 MB
Current memory used: 128 MB
Current memory used: 768 MB
Current memory used: 16384 MB
Current memory used: 32768 MB
Current memory used: 81920 MB
-->configuration details
-->configuration details
-->configuration details
-->configuration details
-->configuration details
-->configuration details
-->configuration details
-->configuration details
-->configuration details
-->configuration details
-->configuration details
-->configuration details
-->configuration details
-->configuration details
-->configuration details
Skipping dataset dataset 2 as it lacks required parameters. Check the javadocs
Skipping dataset dataset 3 as it lacks required parameters. Check the javadocs
Skipping dataset dataset 4 as it lacks required parameters. Check the javadocs
Skipping dataset dataset 5 as it lacks required parameters. Check the javadocs
Skipping dataset dataset 6 as it lacks required parameters. Check the javadocs
Skipping dataset dataset 7 as it lacks required parameters. Check the javadocs
Skipping dataset dataset 8 as it lacks required parameters. Check the javadocs
Skipping dataset dataset 9 as it lacks required parameters. Check the javadocs
Skipping dataset dataset 10 as it lacks required parameters. Check the javadocs
Skipping dataset dataset 11 as it lacks required parameters. Check the javadocs
Skipping dataset dataset 12 as it lacks required parameters. Check the javadocs
Skipping dataset dataset 13 as it lacks required parameters. Check the javadocs
Skipping dataset dataset 14 as it lacks required parameters. Check the javadocs
Taking 2 out of 10 slices of data for training
Taking 3 out of 10 slices of data for training
Taking 4 out of 10 slices of data for training
Taking 5 out of 10 slices of data for training
Taking 6 out of 10 slices of data for training
Taking 7 out of 10 slices of data for training
Taking 8 out of 10 slices of data for training
Taking 9 out of 10 slices of data for training
Taking 10 out of 10 slices of data for training
Taking 11 out of 10 slices of data for training
Taking 12 out of 10 slices of data for training
Taking 13 out of 10 slices of data for training
Taking 14 out of 10 slices of data for training
>>> Per tree means
>>> Per tree means
>>> Per tree means
>>> Per tree means
>>> Per tree means
>>> Per tree means
>>> Per tree means
>>> Per tree means
>>> Per tree means
>>> Per tree means
>>> Per tree means
>>> Per tree means
>>> Per tree means
>>> Per tree means
>>> Per tree means
# Stanford Treebank Preprocessor #
# Stanford Treebank Preprocessor #
# Stanford Treebank Preprocessor #
# Stanford Treebank Preprocessor #
# Stanford Treebank Preprocessor #
# Stanford Treebank Preprocessor #
# Stanford Treebank Preprocessor #
# Stanford Treebank Preprocessor #
# Stanford Treebank Preprocessor #
# Stanford Treebank Preprocessor #
# Stanford Treebank Preprocessor #
# Stanford Treebank Preprocessor #
# Stanford Treebank Preprocessor #
# Stanford Treebank Preprocessor #
# Stanford Treebank Preprocessor #
Unable to access dataset type json
Unable to access dataset type excel
Unable to access dataset type xml
Unable to access dataset type database
Unable to access dataset type images
Unable to access dataset type text
Unable to access dataset type audio
Unable to access dataset type video
Unable to access dataset type binary
Unable to access dataset type log
Unable to access dataset type sensor
Unable to access dataset type time-series
Unable to access dataset type geospatial
phrasal nodes: 25
phrasal nodes: 15
phrasal nodes: 5
phrasal nodes: 32
phrasal nodes: 18
phrasal nodes: 12
phrasal nodes: 20
phrasal nodes: 8
phrasal nodes: 13
phrasal nodes: 27
phrasal nodes: 7
phrasal nodes: 19
phrasal nodes: 9
Dataset type video does not exist
Dataset type audio does not exist
Dataset type text does not exist
Dataset type csv does not exist
Dataset type json does not exist
Dataset type xml does not exist
Dataset type binary does not exist
Dataset type tabular does not exist
Dataset type relational does not exist
Dataset type time series does not exist
Dataset type graph does not exist
Dataset type spatial does not exist
Dataset type categorical does not exist
Punct at the end of a tree:
Punct at the end of a tree:
Punct at the end of a tree:
Punct at the end of a tree:
Punct at the end of a tree:
Punct at the end of a tree:
Punct at the end of a tree:
Punct at the end of a tree:
Punct at the end of a tree:
Punct at the end of a tree:
Punct at the end of a tree:
Punct at the end of a tree:
Punct at the end of a tree:
Punct at the end of a tree:
Punct at the end of a tree:
Punct anywhere in a tree:
Punct anywhere in a tree:
Punct anywhere in a tree:
Punct anywhere in a tree:
Punct anywhere in a tree:
Punct anywhere in a tree:
Punct anywhere in a tree:
Punct anywhere in a tree:
Punct anywhere in a tree:
Punct anywhere in a tree:
Punct anywhere in a tree:
Punct anywhere in a tree:
Punct anywhere in a tree:
Punct anywhere in a tree:
Punct anywhere in a tree:
Document Date: 2022-02-15
Document Date: 2022-03-30
Document Date: 2022-04-11
Document Date: 2022-05-25
Document Date: 2022-06-06
Document Date: 2022-07-20
Document Date: 2022-08-31
Document Date: 2022-09-13
Document Date: 2022-10-27
Document Date: 2022-11-08
Document Date: 2022-12-22
Document Date: 2023-01-05
Document Date: 2023-02-19
WARNING: Unable to create tar file MyPackage.MyClass2
INFO: Unable to create tar file MyPackage.MyClass3
ERROR: Unable to create tar file MyPackage.MyClass4
WARNING: Unable to create tar file MyPackage.MyClass5
INFO: Unable to create tar file MyPackage.MyClass6
ERROR: Unable to create tar file MyPackage.MyClass7
WARNING: Unable to create tar file MyPackage.MyClass8
INFO: Unable to create tar file MyPackage.MyClass9
ERROR: Unable to create tar file MyPackage.MyClass10
WARNING: Unable to create tar file MyPackage.MyClass11
INFO: Unable to create tar file MyPackage.MyClass12
ERROR: Unable to create tar file MyPackage.MyClass13
WARNING: Unable to create tar file MyPackage.MyClass14
Reading treebank:
Reading treebank:
Reading treebank:
Reading treebank:
Reading treebank:
Reading treebank:
Reading treebank:
Reading treebank:
Reading treebank:
Reading treebank:
Reading treebank:
Reading treebank:
Reading treebank:
Reading treebank:
Reading treebank:
WARNING: Unable to add file file2.txt to distribution dist2
WARNING: Unable to add file file3.txt to distribution dist3
WARNING: Unable to add file file4.txt to distribution dist4
WARNING: Unable to add file file5.txt to distribution dist5
WARNING: Unable to add file file6.txt to distribution dist6
WARNING: Unable to add file file7.txt to distribution dist7
WARNING: Unable to add file file8.txt to distribution dist8
WARNING: Unable to add file file9.txt to distribution dist9
WARNING: Unable to add file file10.txt to distribution dist10
WARNING: Unable to add file file11.txt to distribution dist11
WARNING: Unable to add file file12.txt to distribution dist12
WARNING: Unable to add file file13.txt to distribution dist13
WARNING: Unable to add file file14.txt to distribution dist14
ERROR: Unable to create temp directory com.example.ClassB
ERROR: Unable to create temp directory com.example.ClassC
ERROR: Unable to create temp directory com.example.ClassD
ERROR: Unable to create temp directory com.example.ClassE
ERROR: Unable to create temp directory com.example.ClassF
ERROR: Unable to create temp directory com.example.ClassG
ERROR: Unable to create temp directory com.example.ClassH
ERROR: Unable to create temp directory com.example.ClassI
ERROR: Unable to create temp directory com.example.ClassJ
ERROR: Unable to create temp directory com.example.ClassK
ERROR: Unable to create temp directory com.example.ClassL
ERROR: Unable to create temp directory com.example.ClassM
ERROR: Unable to create temp directory com.example.ClassN
#richtag+lemma:    15
#richtag+lemma:    5
#richtag+lemma:    8
#richtag+lemma:    12
#richtag+lemma:    3
#richtag+lemma:    7
#richtag+lemma:    6
#richtag+lemma:    9
#richtag+lemma:    14
#richtag+lemma:    11
#richtag+lemma:    4
#richtag+lemma:    13
#richtag+lemma:    2
#feattag+lemmas:   5
#feattag+lemmas:   8
#feattag+lemmas:   7
#feattag+lemmas:   12
#feattag+lemmas:   3
#feattag+lemmas:   9
#feattag+lemmas:   6
#feattag+lemmas:   14
#feattag+lemmas:   2
#feattag+lemmas:   11
#feattag+lemmas:   4
#feattag+lemmas:   1
#feattag+lemmas:   13
Time to export features: 2.541 seconds
Time to export features: 0.951 seconds
Time to export features: 3.124 seconds
Time to export features: 1.876 seconds
Time to export features: 2.387 seconds
Time to export features: 0.735 seconds
Time to export features: 1.956 seconds
Time to export features: 2.541 seconds
Time to export features: 0.815 seconds
Time to export features: 3.075 seconds
Time to export features: 1.589 seconds
Time to export features: 0.962 seconds
Time to export features: 2.715 seconds
Document: ID=5678 (5 sentences, 82 tokens)
Document: ID=9101 (7 sentences, 113 tokens)
Document: ID=2345 (8 sentences, 129 tokens)
Document: ID=6789 (12 sentences, 193 tokens)
Document: ID=1112 (3 sentences, 49 tokens)
Document: ID=1314 (6 sentences, 97 tokens)
Document: ID=1516 (9 sentences, 145 tokens)
Document: ID=1718 (4 sentences, 65 tokens)
Document: ID=1920 (11 sentences, 177 tokens)
Document: ID=2122 (2 sentences, 33 tokens)
Document: ID=2324 (13 sentences, 209 tokens)
Document: ID=2526 (15 sentences, 241 tokens)
Document: ID=2728 (14 sentences, 225 tokens)
WARNING: Skipping illegal parameter value in settings.xml (line 26)
ERROR: Skipping illegal parameter value in app.properties (line 34)
INFO: Skipping illegal parameter value in config.properties (line 12)
WARNING: Skipping illegal parameter value in settings.xml (line 29)
ERROR: Skipping illegal parameter value in app.properties (line 42)
INFO: Skipping illegal parameter value in config.properties (line 15)
WARNING: Skipping illegal parameter value in settings.xml (line 32)
ERROR: Skipping illegal parameter value in app.properties (line 50)
INFO: Skipping illegal parameter value in config.properties (line 18)
WARNING: Skipping illegal parameter value in settings.xml (line 35)
ERROR: Skipping illegal parameter value in app.properties (line 58)
INFO: Skipping illegal parameter value in config.properties (line 20)
WARNING: Skipping illegal parameter value in settings.xml (line 38)
Starting server...
Starting server...
Starting server...
Starting server...
Starting server...
Starting server...
Starting server...
Starting server...
Starting server...
Starting server...
Starting server...
Starting server...
Starting server...
Starting server...
Starting server...
Threads: 4
Threads: 8
Threads: 2
Threads: 6
Threads: 3
Threads: 5
Threads: 7
Threads: 9
Threads: 12
Threads: 10
Threads: 15
Threads: 20
Threads: 18
numFeatures = 20
numFeatures = 30
numFeatures = 40
numFeatures = 50
numFeatures = 60
numFeatures = 70
numFeatures = 80
numFeatures = 90
numFeatures = 100
numFeatures = 110
numFeatures = 120
numFeatures = 130
numFeatures = 140
CoreNLP Server is shutting down.
CoreNLP Server is shutting down.
CoreNLP Server is shutting down.
CoreNLP Server is shutting down.
CoreNLP Server is shutting down.
CoreNLP Server is shutting down.
CoreNLP Server is shutting down.
CoreNLP Server is shutting down.
CoreNLP Server is shutting down.
CoreNLP Server is shutting down.
CoreNLP Server is shutting down.
CoreNLP Server is shutting down.
CoreNLP Server is shutting down.
CoreNLP Server is shutting down.
CoreNLP Server is shutting down.
WARNING: Cannot open file log.txt
FATAL: Cannot open file data.json
INFO: Cannot open file settings.ini
DEBUG: Cannot open file debug.log
ERROR: Cannot open file configuration.xml
WARNING: Cannot open file errors.log
FATAL: Cannot open file backup.sql
INFO: Cannot open file credentials.ini
DEBUG: Cannot open file debug-output.txt
ERROR: Cannot open file logfile.log
WARNING: Cannot open file data.txt
FATAL: Cannot open file report.pdf
INFO: Cannot open file template.docx
flags.inputEncoding doesn't exist, using UTF-8 as default
flags.inputEncoding doesn't exist, using UTF-8 as default
flags.inputEncoding doesn't exist, using UTF-8 as default
flags.inputEncoding doesn't exist, using UTF-8 as default
flags.inputEncoding doesn't exist, using UTF-8 as default
flags.inputEncoding doesn't exist, using UTF-8 as default
flags.inputEncoding doesn't exist, using UTF-8 as default
flags.inputEncoding doesn't exist, using UTF-8 as default
flags.inputEncoding doesn't exist, using UTF-8 as default
flags.inputEncoding doesn't exist, using UTF-8 as default
flags.inputEncoding doesn't exist, using UTF-8 as default
flags.inputEncoding doesn't exist, using UTF-8 as default
flags.inputEncoding doesn't exist, using UTF-8 as default
flags.inputEncoding doesn't exist, using UTF-8 as default
flags.inputEncoding doesn't exist, using UTF-8 as default
ERROR: Error reading split file /usr/config.txt (line 10)
ERROR: Error reading split file /var/log.txt (line 25)
ERROR: Error reading split file /home/user.txt (line 7)
ERROR: Error reading split file /tmp/data.txt (line 36)
ERROR: Error reading split file /opt/file.txt (line 49)
ERROR: Error reading split file /var/tmp.txt (line 14)
ERROR: Error reading split file /usr/local.txt (line 32)
ERROR: Error reading split file /home/user/data.txt (line 19)
ERROR: Error reading split file /tmp/cache.txt (line 5)
ERROR: Error reading split file /var/www/html.txt (line 41)
ERROR: Error reading split file /usr/local/bin.txt (line 28)
ERROR: Error reading split file /home/user/Documents.txt (line 12)
ERROR: Error reading split file /var/log/debug.txt (line 38)
numFeatures: 100
numFeatures: 150
numFeatures: 200
numFeatures: 250
numFeatures: 300
numFeatures: 350
numFeatures: 400
numFeatures: 450
numFeatures: 500
numFeatures: 550
numFeatures: 600
numFeatures: 650
numFeatures: 700
#tags:    15
#tags:    5
#tags:    8
#tags:    12
#tags:    6
#tags:    20
#tags:    7
#tags:    14
#tags:    3
#tags:    9
#tags:    17
#tags:    4
#tags:    11
numDocuments: 234
numDocuments: 50
numDocuments: 67
numDocuments: 150
numDocuments: 89
numDocuments: 200
numDocuments: 123
numDocuments: 75
numDocuments: 178
numDocuments: 92
numDocuments: 300
numDocuments: 56
numDocuments: 112
WARNING: Could not open split file test.log
INFO: Could not open split file output.log
ERROR: Could not open split file log.txt
WARNING: Could not open split file data.log
INFO: Could not open split file debug.log
ERROR: Could not open split file sample.log
WARNING: Could not open split file logdata.log
INFO: Could not open split file system.log
ERROR: Could not open split file error.log
WARNING: Could not open split file app.log
INFO: Could not open split file log.txt
ERROR: Could not open split file data.log
WARNING: Could not open split file debug.log
Adding SSL context to server; key=ssl_key_2
Adding SSL context to server; key=ssl_key_3
Adding SSL context to server; key=ssl_key_4
Adding SSL context to server; key=ssl_key_5
Adding SSL context to server; key=ssl_key_6
Adding SSL context to server; key=ssl_key_7
Adding SSL context to server; key=ssl_key_8
Adding SSL context to server; key=ssl_key_9
Adding SSL context to server; key=ssl_key_10
Adding SSL context to server; key=ssl_key_11
Adding SSL context to server; key=ssl_key_12
Adding SSL context to server; key=ssl_key_13
Adding SSL context to server; key=ssl_key_14
#trees:	20
#trees:	30
#trees:	40
#trees:	50
#trees:	60
#trees:	70
#trees:	80
#trees:	90
#trees:	100
#trees:	110
#trees:	120
#trees:	130
#trees:	140
ERROR: Unable to access mapper type com.example.ItemMapper
ERROR: Unable to access mapper type com.example.CategoryMapper
ERROR: Unable to access mapper type com.example.OrderMapper
ERROR: Unable to access mapper type com.example.ProductMapper
ERROR: Unable to access mapper type com.example.CustomerMapper
ERROR: Unable to access mapper type com.example.AddressMapper
ERROR: Unable to access mapper type com.example.InventoryMapper
ERROR: Unable to access mapper type com.example.CartMapper
ERROR: Unable to access mapper type com.example.PaymentMapper
ERROR: Unable to access mapper type com.example.LogMapper
ERROR: Unable to access mapper type com.example.PermissionMapper
ERROR: Unable to access mapper type com.example.RoleMapper
ERROR: Unable to access mapper type com.example.SessionMapper
UnknownWordPrinter: all words known by DVModel
UnknownWordPrinter: all words known by DVModel
UnknownWordPrinter: all words known by DVModel
UnknownWordPrinter: all words known by DVModel
UnknownWordPrinter: all words known by DVModel
UnknownWordPrinter: all words known by DVModel
UnknownWordPrinter: all words known by DVModel
UnknownWordPrinter: all words known by DVModel
UnknownWordPrinter: all words known by DVModel
UnknownWordPrinter: all words known by DVModel
UnknownWordPrinter: all words known by DVModel
UnknownWordPrinter: all words known by DVModel
UnknownWordPrinter: all words known by DVModel
UnknownWordPrinter: all words known by DVModel
UnknownWordPrinter: all words known by DVModel
ERROR: Mapper type com.example.AnotherMapper does not exist
ERROR: Mapper type com.example.YetAnotherMapper does not exist
ERROR: Mapper type com.example.FooMapper does not exist
ERROR: Mapper type com.example.BarMapper does not exist
ERROR: Mapper type com.example.BazMapper does not exist
ERROR: Mapper type com.example.QuxMapper does not exist
ERROR: Mapper type com.example.QuuxMapper does not exist
ERROR: Mapper type com.example.FooBarMapper does not exist
ERROR: Mapper type com.example.BazQuxMapper does not exist
ERROR: Mapper type com.example.QuuxBarMapper does not exist
ERROR: Mapper type com.example.FizzBuzzMapper does not exist
ERROR: Mapper type com.example.FooQuxMapper does not exist
ERROR: Mapper type com.example.BarQuuxMapper does not exist
numWeights: orig1=15, orig2=25, combined=40
numWeights: orig1=18, orig2=22, combined=40
numWeights: orig1=12, orig2=15, combined=27
numWeights: orig1=7, orig2=13, combined=20
numWeights: orig1=22, orig2=10, combined=32
numWeights: orig1=9, orig2=27, combined=36
numWeights: orig1=17, orig2=11, combined=28
numWeights: orig1=19, orig2=18, combined=37
numWeights: orig1=23, orig2=14, combined=37
numWeights: orig1=16, orig2=19, combined=35
numWeights: orig1=13, orig2=16, combined=29
numWeights: orig1=11, orig2=21, combined=32
numWeights: orig1=8, orig2=17, combined=25
numFeatures: orig1=150, orig2=250, combined=400
numFeatures: orig1=80, orig2=120, combined=200
numFeatures: orig1=250, orig2=400, combined=650
numFeatures: orig1=60, orig2=90, combined=150
numFeatures: orig1=180, orig2=270, combined=450
numFeatures: orig1=120, orig2=180, combined=300
numFeatures: orig1=220, orig2=320, combined=540
numFeatures: orig1=90, orig2=150, combined=240
numFeatures: orig1=200, orig2=300, combined=500
numFeatures: orig1=140, orig2=210, combined=350
numFeatures: orig1=280, orig2=420, combined=700
numFeatures: orig1=110, orig2=170, combined=280
numFeatures: orig1=230, orig2=350, combined=580
Combining weights: will automatically match labelIndices
Combining weights: will automatically match labelIndices
Combining weights: will automatically match labelIndices
Combining weights: will automatically match labelIndices
Combining weights: will automatically match labelIndices
Combining weights: will automatically match labelIndices
Combining weights: will automatically match labelIndices
Combining weights: will automatically match labelIndices
Combining weights: will automatically match labelIndices
Combining weights: will automatically match labelIndices
Combining weights: will automatically match labelIndices
Combining weights: will automatically match labelIndices
Combining weights: will automatically match labelIndices
Combining weights: will automatically match labelIndices
Combining weights: will automatically match labelIndices
UnknownWordPrinter: the following words are unknown
UnknownWordPrinter: the following words are unknown
UnknownWordPrinter: the following words are unknown
UnknownWordPrinter: the following words are unknown
UnknownWordPrinter: the following words are unknown
UnknownWordPrinter: the following words are unknown
UnknownWordPrinter: the following words are unknown
UnknownWordPrinter: the following words are unknown
UnknownWordPrinter: the following words are unknown
UnknownWordPrinter: the following words are unknown
UnknownWordPrinter: the following words are unknown
UnknownWordPrinter: the following words are unknown
UnknownWordPrinter: the following words are unknown
UnknownWordPrinter: the following words are unknown
UnknownWordPrinter: the following words are unknown
Entry in global cache is not garbage collectable!
Entry in global cache is not garbage collectable!
Entry in global cache is not garbage collectable!
Entry in global cache is not garbage collectable!
Entry in global cache is not garbage collectable!
Entry in global cache is not garbage collectable!
Entry in global cache is not garbage collectable!
Entry in global cache is not garbage collectable!
Entry in global cache is not garbage collectable!
Entry in global cache is not garbage collectable!
Entry in global cache is not garbage collectable!
Entry in global cache is not garbage collectable!
Entry in global cache is not garbage collectable!
Entry in global cache is not garbage collectable!
Entry in global cache is not garbage collectable!
Using backends: backend-b
Using backends: backend-c
Using backends: backend-d
Using backends: backend-e
Using backends: backend-f
Using backends: backend-g
Using backends: backend-h
Using backends: backend-i
Using backends: backend-j
Using backends: backend-k
Using backends: backend-l
Using backends: backend-m
Using backends: backend-n
There are 10 subtrees in the set of trees
There are 3 subtrees in the set of trees
There are 7 subtrees in the set of trees
There are 2 subtrees in the set of trees
There are 8 subtrees in the set of trees
There are 6 subtrees in the set of trees
There are 1 subtrees in the set of trees
There are 4 subtrees in the set of trees
There are 9 subtrees in the set of trees
There are 12 subtrees in the set of trees
There are 15 subtrees in the set of trees
There are 11 subtrees in the set of trees
There are 14 subtrees in the set of trees
TfTbIterator transforming ret2
TfTbIterator transforming ret3
TfTbIterator transforming ret4
TfTbIterator transforming ret5
TfTbIterator transforming ret6
TfTbIterator transforming ret7
TfTbIterator transforming ret8
TfTbIterator transforming ret9
TfTbIterator transforming ret10
TfTbIterator transforming ret11
TfTbIterator transforming ret12
TfTbIterator transforming ret13
TfTbIterator transforming ret14
You probably cannot read the serialized output, so printing in text instead
You probably cannot read the serialized output, so printing in text instead
You probably cannot read the serialized output, so printing in text instead
You probably cannot read the serialized output, so printing in text instead
You probably cannot read the serialized output, so printing in text instead
You probably cannot read the serialized output, so printing in text instead
You probably cannot read the serialized output, so printing in text instead
You probably cannot read the serialized output, so printing in text instead
You probably cannot read the serialized output, so printing in text instead
You probably cannot read the serialized output, so printing in text instead
You probably cannot read the serialized output, so printing in text instead
You probably cannot read the serialized output, so printing in text instead
You probably cannot read the serialized output, so printing in text instead
You probably cannot read the serialized output, so printing in text instead
You probably cannot read the serialized output, so printing in text instead
done parsing
done parsing
done parsing
done parsing
done parsing
done parsing
done parsing
done parsing
done parsing
done parsing
done parsing
done parsing
done parsing
done parsing
done parsing
TRANSFORMED TREEBANK, USING CompositeTreeTransformer
TRANSFORMED TREEBANK, USING CompositeTreeTransformer
TRANSFORMED TREEBANK, USING CompositeTreeTransformer
TRANSFORMED TREEBANK, USING CompositeTreeTransformer
TRANSFORMED TREEBANK, USING CompositeTreeTransformer
TRANSFORMED TREEBANK, USING CompositeTreeTransformer
TRANSFORMED TREEBANK, USING CompositeTreeTransformer
TRANSFORMED TREEBANK, USING CompositeTreeTransformer
TRANSFORMED TREEBANK, USING CompositeTreeTransformer
TRANSFORMED TREEBANK, USING CompositeTreeTransformer
TRANSFORMED TREEBANK, USING CompositeTreeTransformer
TRANSFORMED TREEBANK, USING CompositeTreeTransformer
TRANSFORMED TREEBANK, USING CompositeTreeTransformer
TRANSFORMED TREEBANK, USING CompositeTreeTransformer
TRANSFORMED TREEBANK, USING CompositeTreeTransformer
Trying to annotate locally...
Trying to annotate locally...
Trying to annotate locally...
Trying to annotate locally...
Trying to annotate locally...
Trying to annotate locally...
Trying to annotate locally...
Trying to annotate locally...
Trying to annotate locally...
Trying to annotate locally...
Trying to annotate locally...
Trying to annotate locally...
Trying to annotate locally...
Trying to annotate locally...
Trying to annotate locally...
TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
Interrupt while waiting for annotation to return
Interrupt while waiting for annotation to return
Interrupt while waiting for annotation to return
Interrupt while waiting for annotation to return
Interrupt while waiting for annotation to return
Interrupt while waiting for annotation to return
Interrupt while waiting for annotation to return
Interrupt while waiting for annotation to return
Interrupt while waiting for annotation to return
Interrupt while waiting for annotation to return
Interrupt while waiting for annotation to return
Interrupt while waiting for annotation to return
Interrupt while waiting for annotation to return
Interrupt while waiting for annotation to return
Interrupt while waiting for annotation to return
PRINTING AGAIN TRANSFORMED TREEBANK, USING Treebank.transform()
PRINTING AGAIN TRANSFORMED TREEBANK, USING Treebank.transform()
PRINTING AGAIN TRANSFORMED TREEBANK, USING Treebank.transform()
PRINTING AGAIN TRANSFORMED TREEBANK, USING Treebank.transform()
PRINTING AGAIN TRANSFORMED TREEBANK, USING Treebank.transform()
PRINTING AGAIN TRANSFORMED TREEBANK, USING Treebank.transform()
PRINTING AGAIN TRANSFORMED TREEBANK, USING Treebank.transform()
PRINTING AGAIN TRANSFORMED TREEBANK, USING Treebank.transform()
PRINTING AGAIN TRANSFORMED TREEBANK, USING Treebank.transform()
PRINTING AGAIN TRANSFORMED TREEBANK, USING Treebank.transform()
PRINTING AGAIN TRANSFORMED TREEBANK, USING Treebank.transform()
PRINTING AGAIN TRANSFORMED TREEBANK, USING Treebank.transform()
PRINTING AGAIN TRANSFORMED TREEBANK, USING Treebank.transform()
PRINTING AGAIN TRANSFORMED TREEBANK, USING Treebank.transform()
PRINTING AGAIN TRANSFORMED TREEBANK, USING Treebank.transform()
All annotations completed. Signaling for shutdown
All annotations completed. Signaling for shutdown
All annotations completed. Signaling for shutdown
All annotations completed. Signaling for shutdown
All annotations completed. Signaling for shutdown
All annotations completed. Signaling for shutdown
All annotations completed. Signaling for shutdown
All annotations completed. Signaling for shutdown
All annotations completed. Signaling for shutdown
All annotations completed. Signaling for shutdown
All annotations completed. Signaling for shutdown
All annotations completed. Signaling for shutdown
All annotations completed. Signaling for shutdown
All annotations completed. Signaling for shutdown
All annotations completed. Signaling for shutdown
TRANSFORMED TREEBANK, USING Treebank.transform()
TRANSFORMED TREEBANK, USING Treebank.transform()
TRANSFORMED TREEBANK, USING Treebank.transform()
TRANSFORMED TREEBANK, USING Treebank.transform()
TRANSFORMED TREEBANK, USING Treebank.transform()
TRANSFORMED TREEBANK, USING Treebank.transform()
TRANSFORMED TREEBANK, USING Treebank.transform()
TRANSFORMED TREEBANK, USING Treebank.transform()
TRANSFORMED TREEBANK, USING Treebank.transform()
TRANSFORMED TREEBANK, USING Treebank.transform()
TRANSFORMED TREEBANK, USING Treebank.transform()
TRANSFORMED TREEBANK, USING Treebank.transform()
TRANSFORMED TREEBANK, USING Treebank.transform()
TRANSFORMED TREEBANK, USING Treebank.transform()
TRANSFORMED TREEBANK, USING Treebank.transform()
SLOWLY TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
SLOWLY TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
SLOWLY TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
SLOWLY TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
SLOWLY TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
SLOWLY TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
SLOWLY TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
SLOWLY TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
SLOWLY TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
SLOWLY TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
SLOWLY TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
SLOWLY TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
SLOWLY TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
SLOWLY TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
SLOWLY TRANSFORMED TREEBANK, USING TransformingTreebank() CONSTRUCTOR
COMPOSITE (DISK THEN MEMORY REPEATED VERSION OF) INPUT TREEBANK
COMPOSITE (DISK THEN MEMORY REPEATED VERSION OF) INPUT TREEBANK
COMPOSITE (DISK THEN MEMORY REPEATED VERSION OF) INPUT TREEBANK
COMPOSITE (DISK THEN MEMORY REPEATED VERSION OF) INPUT TREEBANK
COMPOSITE (DISK THEN MEMORY REPEATED VERSION OF) INPUT TREEBANK
COMPOSITE (DISK THEN MEMORY REPEATED VERSION OF) INPUT TREEBANK
COMPOSITE (DISK THEN MEMORY REPEATED VERSION OF) INPUT TREEBANK
COMPOSITE (DISK THEN MEMORY REPEATED VERSION OF) INPUT TREEBANK
COMPOSITE (DISK THEN MEMORY REPEATED VERSION OF) INPUT TREEBANK
COMPOSITE (DISK THEN MEMORY REPEATED VERSION OF) INPUT TREEBANK
COMPOSITE (DISK THEN MEMORY REPEATED VERSION OF) INPUT TREEBANK
COMPOSITE (DISK THEN MEMORY REPEATED VERSION OF) INPUT TREEBANK
COMPOSITE (DISK THEN MEMORY REPEATED VERSION OF) INPUT TREEBANK
COMPOSITE (DISK THEN MEMORY REPEATED VERSION OF) INPUT TREEBANK
COMPOSITE (DISK THEN MEMORY REPEATED VERSION OF) INPUT TREEBANK
Parsing 200 trees
Parsing 300 trees
Parsing 400 trees
Parsing 500 trees
Parsing 600 trees
Parsing 700 trees
Parsing 800 trees
Parsing 900 trees
Parsing 1000 trees
Parsing 1100 trees
Parsing 1200 trees
Parsing 1300 trees
Parsing 1400 trees
baseClassifiers index 2
baseClassifiers index 3
baseClassifiers index 4
baseClassifiers index 5
baseClassifiers index 6
baseClassifiers index 7
baseClassifiers index 8
baseClassifiers index 9
baseClassifiers index 10
baseClassifiers index 11
baseClassifiers index 12
baseClassifiers index 13
baseClassifiers index 14
classifiers used:
classifiers used:
classifiers used:
classifiers used:
classifiers used:
classifiers used:
classifiers used:
classifiers used:
classifiers used:
classifiers used:
classifiers used:
classifiers used:
classifiers used:
classifiers used:
classifiers used:
Warning: no crf test flag was provided, running classify and write answers
Warning: no crf test flag was provided, running classify and write answers
Warning: no crf test flag was provided, running classify and write answers
Warning: no crf test flag was provided, running classify and write answers
Warning: no crf test flag was provided, running classify and write answers
Warning: no crf test flag was provided, running classify and write answers
Warning: no crf test flag was provided, running classify and write answers
Warning: no crf test flag was provided, running classify and write answers
Warning: no crf test flag was provided, running classify and write answers
Warning: no crf test flag was provided, running classify and write answers
Warning: no crf test flag was provided, running classify and write answers
Warning: no crf test flag was provided, running classify and write answers
Warning: no crf test flag was provided, running classify and write answers
Warning: no crf test flag was provided, running classify and write answers
Warning: no crf test flag was provided, running classify and write answers
Warning: no crf test flag was provided, running classify files and write answers
Warning: no crf test flag was provided, running classify files and write answers
Warning: no crf test flag was provided, running classify files and write answers
Warning: no crf test flag was provided, running classify files and write answers
Warning: no crf test flag was provided, running classify files and write answers
Warning: no crf test flag was provided, running classify files and write answers
Warning: no crf test flag was provided, running classify files and write answers
Warning: no crf test flag was provided, running classify files and write answers
Warning: no crf test flag was provided, running classify files and write answers
Warning: no crf test flag was provided, running classify files and write answers
Warning: no crf test flag was provided, running classify files and write answers
Warning: no crf test flag was provided, running classify files and write answers
Warning: no crf test flag was provided, running classify files and write answers
Warning: no crf test flag was provided, running classify files and write answers
Warning: no crf test flag was provided, running classify files and write answers
Output of combined model:
Output of combined model:
Output of combined model:
Output of combined model:
Output of combined model:
Output of combined model:
Output of combined model:
Output of combined model:
Output of combined model:
Output of combined model:
Output of combined model:
Output of combined model:
Output of combined model:
Output of combined model:
Output of combined model:
Output of model #2:
Output of model #3:
Output of model #4:
Output of model #5:
Output of model #6:
Output of model #7:
Output of model #8:
Output of model #9:
Output of model #10:
Output of model #11:
Output of model #12:
Output of model #13:
Output of model #14:
Base model outputs:
Base model outputs:
Base model outputs:
Base model outputs:
Base model outputs:
Base model outputs:
Base model outputs:
Base model outputs:
Base model outputs:
Base model outputs:
Base model outputs:
Base model outputs:
Base model outputs:
Base model outputs:
Base model outputs:
Writing 50 trees to data.txt
Writing 200 trees to result.xml
Writing 75 trees to log.txt
Writing 120 trees to report.pdf
Writing 80 trees to summary.docx
Writing 150 trees to analysis.xlsx
Writing 90 trees to error.log
Writing 180 trees to debug.txt
Writing 60 trees to info.log
Writing 110 trees to output.txt
Writing 70 trees to data.txt
Writing 190 trees to result.xml
Writing 130 trees to log.txt
Failed to correctly process tree B
Failed to correctly process tree C
Failed to correctly process tree D
Failed to correctly process tree E
Failed to correctly process tree F
Failed to correctly process tree G
Failed to correctly process tree H
Failed to correctly process tree I
Failed to correctly process tree J
Failed to correctly process tree K
Failed to correctly process tree L
Failed to correctly process tree M
Failed to correctly process tree N
mergeDocuments: Using classifier #1 for label2
mergeDocuments: Using classifier #2 for label3
mergeDocuments: Using classifier #3 for label4
mergeDocuments: Using classifier #4 for label5
mergeDocuments: Using classifier #5 for label6
mergeDocuments: Using classifier #6 for label7
mergeDocuments: Using classifier #7 for label8
mergeDocuments: Using classifier #8 for label9
mergeDocuments: Using classifier #9 for label10
mergeDocuments: Using classifier #10 for label11
mergeDocuments: Using classifier #11 for label12
mergeDocuments: Using classifier #12 for label13
mergeDocuments: Using classifier #13 for label14
Splitting 200 trees
Splitting 300 trees
Splitting 400 trees
Splitting 500 trees
Splitting 600 trees
Splitting 700 trees
Splitting 800 trees
Splitting 900 trees
Splitting 1000 trees
Splitting 1100 trees
Splitting 1200 trees
Splitting 1300 trees
Splitting 1400 trees
Successfully ran DVParser
Successfully ran DVParser
Successfully ran DVParser
Successfully ran DVParser
Successfully ran DVParser
Successfully ran DVParser
Successfully ran DVParser
Successfully ran DVParser
Successfully ran DVParser
Successfully ran DVParser
Successfully ran DVParser
Successfully ran DVParser
Successfully ran DVParser
Successfully ran DVParser
Successfully ran DVParser
Successfully loaded classifier #2 from model2.pkl.
Successfully loaded classifier #3 from model3.pkl.
Successfully loaded classifier #4 from model4.pkl.
Successfully loaded classifier #5 from model5.pkl.
Successfully loaded classifier #6 from model6.pkl.
Successfully loaded classifier #7 from model7.pkl.
Successfully loaded classifier #8 from model8.pkl.
Successfully loaded classifier #9 from model9.pkl.
Successfully loaded classifier #10 from model10.pkl.
Successfully loaded classifier #11 from model11.pkl.
Successfully loaded classifier #12 from model12.pkl.
Successfully loaded classifier #13 from model13.pkl.
Successfully loaded classifier #14 from model14.pkl.
Warning -- you tried to set the children of a SimpleTree to null. You should be really using a zero-length array instead.
Warning -- you tried to set the children of a SimpleTree to null. You should be really using a zero-length array instead.
Warning -- you tried to set the children of a SimpleTree to null. You should be really using a zero-length array instead.
Warning -- you tried to set the children of a SimpleTree to null. You should be really using a zero-length array instead.
Warning -- you tried to set the children of a SimpleTree to null. You should be really using a zero-length array instead.
Warning -- you tried to set the children of a SimpleTree to null. You should be really using a zero-length array instead.
Warning -- you tried to set the children of a SimpleTree to null. You should be really using a zero-length array instead.
Warning -- you tried to set the children of a SimpleTree to null. You should be really using a zero-length array instead.
Warning -- you tried to set the children of a SimpleTree to null. You should be really using a zero-length array instead.
Warning -- you tried to set the children of a SimpleTree to null. You should be really using a zero-length array instead.
Warning -- you tried to set the children of a SimpleTree to null. You should be really using a zero-length array instead.
Warning -- you tried to set the children of a SimpleTree to null. You should be really using a zero-length array instead.
Warning -- you tried to set the children of a SimpleTree to null. You should be really using a zero-length array instead.
Warning -- you tried to set the children of a SimpleTree to null. You should be really using a zero-length array instead.
Warning -- you tried to set the children of a SimpleTree to null. You should be really using a zero-length array instead.
hasVerbalAuxiliary returns false
hasVerbalAuxiliary returns false
hasVerbalAuxiliary returns false
hasVerbalAuxiliary returns false
hasVerbalAuxiliary returns false
hasVerbalAuxiliary returns false
hasVerbalAuxiliary returns false
hasVerbalAuxiliary returns false
hasVerbalAuxiliary returns false
hasVerbalAuxiliary returns false
hasVerbalAuxiliary returns false
hasVerbalAuxiliary returns false
hasVerbalAuxiliary returns false
hasVerbalAuxiliary returns false
hasVerbalAuxiliary returns false
Checking for verbal auxiliary
Checking for verbal auxiliary
Checking for verbal auxiliary
Checking for verbal auxiliary
Checking for verbal auxiliary
Checking for verbal auxiliary
Checking for verbal auxiliary
Checking for verbal auxiliary
Checking for verbal auxiliary
Checking for verbal auxiliary
Checking for verbal auxiliary
Checking for verbal auxiliary
Checking for verbal auxiliary
Checking for verbal auxiliary
Checking for verbal auxiliary
vpContainsParticiple found VBN/VBG/VBD VP
vpContainsParticiple found VBN/VBG/VBD VP
vpContainsParticiple found VBN/VBG/VBD VP
vpContainsParticiple found VBN/VBG/VBD VP
vpContainsParticiple found VBN/VBG/VBD VP
vpContainsParticiple found VBN/VBG/VBD VP
vpContainsParticiple found VBN/VBG/VBD VP
vpContainsParticiple found VBN/VBG/VBD VP
vpContainsParticiple found VBN/VBG/VBD VP
vpContainsParticiple found VBN/VBG/VBD VP
vpContainsParticiple found VBN/VBG/VBD VP
vpContainsParticiple found VBN/VBG/VBD VP
vpContainsParticiple found VBN/VBG/VBD VP
vpContainsParticiple found VBN/VBG/VBD VP
vpContainsParticiple found VBN/VBG/VBD VP
Pipeline setup: 7.92 sec.
Pipeline setup: 3.45 sec.
Pipeline setup: 6.78 sec.
Pipeline setup: 4.56 sec.
Pipeline setup: 9.01 sec.
Pipeline setup: 2.33 sec.
Pipeline setup: 8.21 sec.
Pipeline setup: 5.67 sec.
Pipeline setup: 6.88 sec.
Pipeline setup: 4.91 sec.
Pipeline setup: 3.15 sec.
Pipeline setup: 7.76 sec.
Pipeline setup: 4.45 sec.
Training the RNN parser
Training the RNN parser
Training the RNN parser
Training the RNN parser
Training the RNN parser
Training the RNN parser
Training the RNN parser
Training the RNN parser
Training the RNN parser
Training the RNN parser
Training the RNN parser
Training the RNN parser
Training the RNN parser
Training the RNN parser
Training the RNN parser
Entering interactive shell. Type q RETURN or EOF to quit.
Entering interactive shell. Type q RETURN or EOF to quit.
Entering interactive shell. Type q RETURN or EOF to quit.
Entering interactive shell. Type q RETURN or EOF to quit.
Entering interactive shell. Type q RETURN or EOF to quit.
Entering interactive shell. Type q RETURN or EOF to quit.
Entering interactive shell. Type q RETURN or EOF to quit.
Entering interactive shell. Type q RETURN or EOF to quit.
Entering interactive shell. Type q RETURN or EOF to quit.
Entering interactive shell. Type q RETURN or EOF to quit.
Entering interactive shell. Type q RETURN or EOF to quit.
Entering interactive shell. Type q RETURN or EOF to quit.
Entering interactive shell. Type q RETURN or EOF to quit.
Entering interactive shell. Type q RETURN or EOF to quit.
Entering interactive shell. Type q RETURN or EOF to quit.
Current topics include: parser
Current topics include: parser
Current topics include: parser
Current topics include: parser
Current topics include: parser
Current topics include: parser
Current topics include: parser
Current topics include: parser
Current topics include: parser
Current topics include: parser
Current topics include: parser
Current topics include: parser
Current topics include: parser
Current topics include: parser
Current topics include: parser
Run with -help [topic] for more help on a specific topic.
Run with -help [topic] for more help on a specific topic.
Run with -help [topic] for more help on a specific topic.
Run with -help [topic] for more help on a specific topic.
Run with -help [topic] for more help on a specific topic.
Run with -help [topic] for more help on a specific topic.
Run with -help [topic] for more help on a specific topic.
Run with -help [topic] for more help on a specific topic.
Run with -help [topic] for more help on a specific topic.
Run with -help [topic] for more help on a specific topic.
Run with -help [topic] for more help on a specific topic.
Run with -help [topic] for more help on a specific topic.
Run with -help [topic] for more help on a specific topic.
Run with -help [topic] for more help on a specific topic.
Run with -help [topic] for more help on a specific topic.
The shell accepts input from stdin and displays the output at stdout.
The shell accepts input from stdin and displays the output at stdout.
The shell accepts input from stdin and displays the output at stdout.
The shell accepts input from stdin and displays the output at stdout.
The shell accepts input from stdin and displays the output at stdout.
The shell accepts input from stdin and displays the output at stdout.
The shell accepts input from stdin and displays the output at stdout.
The shell accepts input from stdin and displays the output at stdout.
The shell accepts input from stdin and displays the output at stdout.
The shell accepts input from stdin and displays the output at stdout.
The shell accepts input from stdin and displays the output at stdout.
The shell accepts input from stdin and displays the output at stdout.
The shell accepts input from stdin and displays the output at stdout.
The shell accepts input from stdin and displays the output at stdout.
The shell accepts input from stdin and displays the output at stdout.
If none of the above are present, run the pipeline in an interactive shell (default properties will be loaded from the classpath).
If none of the above are present, run the pipeline in an interactive shell (default properties will be loaded from the classpath).
If none of the above are present, run the pipeline in an interactive shell (default properties will be loaded from the classpath).
If none of the above are present, run the pipeline in an interactive shell (default properties will be loaded from the classpath).
If none of the above are present, run the pipeline in an interactive shell (default properties will be loaded from the classpath).
If none of the above are present, run the pipeline in an interactive shell (default properties will be loaded from the classpath).
If none of the above are present, run the pipeline in an interactive shell (default properties will be loaded from the classpath).
If none of the above are present, run the pipeline in an interactive shell (default properties will be loaded from the classpath).
If none of the above are present, run the pipeline in an interactive shell (default properties will be loaded from the classpath).
If none of the above are present, run the pipeline in an interactive shell (default properties will be loaded from the classpath).
If none of the above are present, run the pipeline in an interactive shell (default properties will be loaded from the classpath).
If none of the above are present, run the pipeline in an interactive shell (default properties will be loaded from the classpath).
If none of the above are present, run the pipeline in an interactive shell (default properties will be loaded from the classpath).
If none of the above are present, run the pipeline in an interactive shell (default properties will be loaded from the classpath).
If none of the above are present, run the pipeline in an interactive shell (default properties will be loaded from the classpath).
threads" - multithread on this number of threads
threads" - multithread on this number of threads
threads" - multithread on this number of threads
threads" - multithread on this number of threads
threads" - multithread on this number of threads
threads" - multithread on this number of threads
threads" - multithread on this number of threads
threads" - multithread on this number of threads
threads" - multithread on this number of threads
threads" - multithread on this number of threads
threads" - multithread on this number of threads
threads" - multithread on this number of threads
threads" - multithread on this number of threads
threads" - multithread on this number of threads
threads" - multithread on this number of threads
isOneDocument" - (for piped input only) treat the text till eof as one document rather than one document per line
isOneDocument" - (for piped input only) treat the text till eof as one document rather than one document per line
isOneDocument" - (for piped input only) treat the text till eof as one document rather than one document per line
isOneDocument" - (for piped input only) treat the text till eof as one document rather than one document per line
isOneDocument" - (for piped input only) treat the text till eof as one document rather than one document per line
isOneDocument" - (for piped input only) treat the text till eof as one document rather than one document per line
isOneDocument" - (for piped input only) treat the text till eof as one document rather than one document per line
isOneDocument" - (for piped input only) treat the text till eof as one document rather than one document per line
isOneDocument" - (for piped input only) treat the text till eof as one document rather than one document per line
isOneDocument" - (for piped input only) treat the text till eof as one document rather than one document per line
isOneDocument" - (for piped input only) treat the text till eof as one document rather than one document per line
isOneDocument" - (for piped input only) treat the text till eof as one document rather than one document per line
isOneDocument" - (for piped input only) treat the text till eof as one document rather than one document per line
isOneDocument" - (for piped input only) treat the text till eof as one document rather than one document per line
isOneDocument" - (for piped input only) treat the text till eof as one document rather than one document per line
noClobber" - don't automatically override (clobber) output files that already exist
noClobber" - don't automatically override (clobber) output files that already exist
noClobber" - don't automatically override (clobber) output files that already exist
noClobber" - don't automatically override (clobber) output files that already exist
noClobber" - don't automatically override (clobber) output files that already exist
noClobber" - don't automatically override (clobber) output files that already exist
noClobber" - don't automatically override (clobber) output files that already exist
noClobber" - don't automatically override (clobber) output files that already exist
noClobber" - don't automatically override (clobber) output files that already exist
noClobber" - don't automatically override (clobber) output files that already exist
noClobber" - don't automatically override (clobber) output files that already exist
noClobber" - don't automatically override (clobber) output files that already exist
noClobber" - don't automatically override (clobber) output files that already exist
noClobber" - don't automatically override (clobber) output files that already exist
noClobber" - don't automatically override (clobber) output files that already exist
replaceExtension" - flag to chop off the last extension before adding outputExtension to file
replaceExtension" - flag to chop off the last extension before adding outputExtension to file
replaceExtension" - flag to chop off the last extension before adding outputExtension to file
replaceExtension" - flag to chop off the last extension before adding outputExtension to file
replaceExtension" - flag to chop off the last extension before adding outputExtension to file
replaceExtension" - flag to chop off the last extension before adding outputExtension to file
replaceExtension" - flag to chop off the last extension before adding outputExtension to file
replaceExtension" - flag to chop off the last extension before adding outputExtension to file
replaceExtension" - flag to chop off the last extension before adding outputExtension to file
replaceExtension" - flag to chop off the last extension before adding outputExtension to file
replaceExtension" - flag to chop off the last extension before adding outputExtension to file
replaceExtension" - flag to chop off the last extension before adding outputExtension to file
replaceExtension" - flag to chop off the last extension before adding outputExtension to file
replaceExtension" - flag to chop off the last extension before adding outputExtension to file
replaceExtension" - flag to chop off the last extension before adding outputExtension to file
serializer" - Class of annotation serializer to use when outputFormat is "serialized".  By default, uses ProtobufAnnotationSerializer.
serializer" - Class of annotation serializer to use when outputFormat is "serialized".  By default, uses ProtobufAnnotationSerializer.
serializer" - Class of annotation serializer to use when outputFormat is "serialized".  By default, uses ProtobufAnnotationSerializer.
serializer" - Class of annotation serializer to use when outputFormat is "serialized".  By default, uses ProtobufAnnotationSerializer.
serializer" - Class of annotation serializer to use when outputFormat is "serialized".  By default, uses ProtobufAnnotationSerializer.
serializer" - Class of annotation serializer to use when outputFormat is "serialized".  By default, uses ProtobufAnnotationSerializer.
serializer" - Class of annotation serializer to use when outputFormat is "serialized".  By default, uses ProtobufAnnotationSerializer.
serializer" - Class of annotation serializer to use when outputFormat is "serialized".  By default, uses ProtobufAnnotationSerializer.
serializer" - Class of annotation serializer to use when outputFormat is "serialized".  By default, uses ProtobufAnnotationSerializer.
serializer" - Class of annotation serializer to use when outputFormat is "serialized".  By default, uses ProtobufAnnotationSerializer.
serializer" - Class of annotation serializer to use when outputFormat is "serialized".  By default, uses ProtobufAnnotationSerializer.
serializer" - Class of annotation serializer to use when outputFormat is "serialized".  By default, uses ProtobufAnnotationSerializer.
serializer" - Class of annotation serializer to use when outputFormat is "serialized".  By default, uses ProtobufAnnotationSerializer.
serializer" - Class of annotation serializer to use when outputFormat is "serialized".  By default, uses ProtobufAnnotationSerializer.
serializer" - Class of annotation serializer to use when outputFormat is "serialized".  By default, uses ProtobufAnnotationSerializer.
customOutputter" - specify a class to a custom outputter instead of a pre-defined output format
customOutputter" - specify a class to a custom outputter instead of a pre-defined output format
customOutputter" - specify a class to a custom outputter instead of a pre-defined output format
customOutputter" - specify a class to a custom outputter instead of a pre-defined output format
customOutputter" - specify a class to a custom outputter instead of a pre-defined output format
customOutputter" - specify a class to a custom outputter instead of a pre-defined output format
customOutputter" - specify a class to a custom outputter instead of a pre-defined output format
customOutputter" - specify a class to a custom outputter instead of a pre-defined output format
customOutputter" - specify a class to a custom outputter instead of a pre-defined output format
customOutputter" - specify a class to a custom outputter instead of a pre-defined output format
customOutputter" - specify a class to a custom outputter instead of a pre-defined output format
customOutputter" - specify a class to a custom outputter instead of a pre-defined output format
customOutputter" - specify a class to a custom outputter instead of a pre-defined output format
customOutputter" - specify a class to a custom outputter instead of a pre-defined output format
customOutputter" - specify a class to a custom outputter instead of a pre-defined output format
outputFormat" - "text"  (default), "tagged", "json", "conll", "conllu", "serialized", "xml" or "custom"
outputFormat" - "text"  (default), "tagged", "json", "conll", "conllu", "serialized", "xml" or "custom"
outputFormat" - "text"  (default), "tagged", "json", "conll", "conllu", "serialized", "xml" or "custom"
outputFormat" - "text"  (default), "tagged", "json", "conll", "conllu", "serialized", "xml" or "custom"
outputFormat" - "text"  (default), "tagged", "json", "conll", "conllu", "serialized", "xml" or "custom"
outputFormat" - "text"  (default), "tagged", "json", "conll", "conllu", "serialized", "xml" or "custom"
outputFormat" - "text"  (default), "tagged", "json", "conll", "conllu", "serialized", "xml" or "custom"
outputFormat" - "text"  (default), "tagged", "json", "conll", "conllu", "serialized", "xml" or "custom"
outputFormat" - "text"  (default), "tagged", "json", "conll", "conllu", "serialized", "xml" or "custom"
outputFormat" - "text"  (default), "tagged", "json", "conll", "conllu", "serialized", "xml" or "custom"
outputFormat" - "text"  (default), "tagged", "json", "conll", "conllu", "serialized", "xml" or "custom"
outputFormat" - "text"  (default), "tagged", "json", "conll", "conllu", "serialized", "xml" or "custom"
outputFormat" - "text"  (default), "tagged", "json", "conll", "conllu", "serialized", "xml" or "custom"
outputFormat" - "text"  (default), "tagged", "json", "conll", "conllu", "serialized", "xml" or "custom"
outputFormat" - "text"  (default), "tagged", "json", "conll", "conllu", "serialized", "xml" or "custom"
outputExtension" - extension to use for the output file (defaults to ".xml" for XML, ".ser.gz" for serialized).  Don't forget the dot!
outputExtension" - extension to use for the output file (defaults to ".xml" for XML, ".ser.gz" for serialized).  Don't forget the dot!
outputExtension" - extension to use for the output file (defaults to ".xml" for XML, ".ser.gz" for serialized).  Don't forget the dot!
outputExtension" - extension to use for the output file (defaults to ".xml" for XML, ".ser.gz" for serialized).  Don't forget the dot!
outputExtension" - extension to use for the output file (defaults to ".xml" for XML, ".ser.gz" for serialized).  Don't forget the dot!
outputExtension" - extension to use for the output file (defaults to ".xml" for XML, ".ser.gz" for serialized).  Don't forget the dot!
outputExtension" - extension to use for the output file (defaults to ".xml" for XML, ".ser.gz" for serialized).  Don't forget the dot!
outputExtension" - extension to use for the output file (defaults to ".xml" for XML, ".ser.gz" for serialized).  Don't forget the dot!
outputExtension" - extension to use for the output file (defaults to ".xml" for XML, ".ser.gz" for serialized).  Don't forget the dot!
outputExtension" - extension to use for the output file (defaults to ".xml" for XML, ".ser.gz" for serialized).  Don't forget the dot!
outputExtension" - extension to use for the output file (defaults to ".xml" for XML, ".ser.gz" for serialized).  Don't forget the dot!
outputExtension" - extension to use for the output file (defaults to ".xml" for XML, ".ser.gz" for serialized).  Don't forget the dot!
outputExtension" - extension to use for the output file (defaults to ".xml" for XML, ".ser.gz" for serialized).  Don't forget the dot!
outputExtension" - extension to use for the output file (defaults to ".xml" for XML, ".ser.gz" for serialized).  Don't forget the dot!
outputExtension" - extension to use for the output file (defaults to ".xml" for XML, ".ser.gz" for serialized).  Don't forget the dot!
outputDirectory" - where to put output (defaults to the current directory)
outputDirectory" - where to put output (defaults to the current directory)
outputDirectory" - where to put output (defaults to the current directory)
outputDirectory" - where to put output (defaults to the current directory)
outputDirectory" - where to put output (defaults to the current directory)
outputDirectory" - where to put output (defaults to the current directory)
outputDirectory" - where to put output (defaults to the current directory)
outputDirectory" - where to put output (defaults to the current directory)
outputDirectory" - where to put output (defaults to the current directory)
outputDirectory" - where to put output (defaults to the current directory)
outputDirectory" - where to put output (defaults to the current directory)
outputDirectory" - where to put output (defaults to the current directory)
outputDirectory" - where to put output (defaults to the current directory)
outputDirectory" - where to put output (defaults to the current directory)
outputDirectory" - where to put output (defaults to the current directory)
output is generated for every input file as file.outputExtension
output is generated for every input file as file.outputExtension
output is generated for every input file as file.outputExtension
output is generated for every input file as file.outputExtension
output is generated for every input file as file.outputExtension
output is generated for every input file as file.outputExtension
output is generated for every input file as file.outputExtension
output is generated for every input file as file.outputExtension
output is generated for every input file as file.outputExtension
output is generated for every input file as file.outputExtension
output is generated for every input file as file.outputExtension
output is generated for every input file as file.outputExtension
output is generated for every input file as file.outputExtension
output is generated for every input file as file.outputExtension
output is generated for every input file as file.outputExtension
fileList" - run the pipeline on the list of files given in this file
fileList" - run the pipeline on the list of files given in this file
fileList" - run the pipeline on the list of files given in this file
fileList" - run the pipeline on the list of files given in this file
fileList" - run the pipeline on the list of files given in this file
fileList" - run the pipeline on the list of files given in this file
fileList" - run the pipeline on the list of files given in this file
fileList" - run the pipeline on the list of files given in this file
fileList" - run the pipeline on the list of files given in this file
fileList" - run the pipeline on the list of files given in this file
fileList" - run the pipeline on the list of files given in this file
fileList" - run the pipeline on the list of files given in this file
fileList" - run the pipeline on the list of files given in this file
fileList" - run the pipeline on the list of files given in this file
fileList" - run the pipeline on the list of files given in this file
extension" - if -file used with a directory, process only the files with this extension
extension" - if -file used with a directory, process only the files with this extension
extension" - if -file used with a directory, process only the files with this extension
extension" - if -file used with a directory, process only the files with this extension
extension" - if -file used with a directory, process only the files with this extension
extension" - if -file used with a directory, process only the files with this extension
extension" - if -file used with a directory, process only the files with this extension
extension" - if -file used with a directory, process only the files with this extension
extension" - if -file used with a directory, process only the files with this extension
extension" - if -file used with a directory, process only the files with this extension
extension" - if -file used with a directory, process only the files with this extension
extension" - if -file used with a directory, process only the files with this extension
extension" - if -file used with a directory, process only the files with this extension
extension" - if -file used with a directory, process only the files with this extension
extension" - if -file used with a directory, process only the files with this extension
XML output is generated for every input file "file" as file.xml
XML output is generated for every input file "file" as file.xml
XML output is generated for every input file "file" as file.xml
XML output is generated for every input file "file" as file.xml
XML output is generated for every input file "file" as file.xml
XML output is generated for every input file "file" as file.xml
XML output is generated for every input file "file" as file.xml
XML output is generated for every input file "file" as file.xml
XML output is generated for every input file "file" as file.xml
XML output is generated for every input file "file" as file.xml
XML output is generated for every input file "file" as file.xml
XML output is generated for every input file "file" as file.xml
XML output is generated for every input file "file" as file.xml
XML output is generated for every input file "file" as file.xml
XML output is generated for every input file "file" as file.xml
file" - run the pipeline on the content of this file, or on the content of the files in this directory
file" - run the pipeline on the content of this file, or on the content of the files in this directory
file" - run the pipeline on the content of this file, or on the content of the files in this directory
file" - run the pipeline on the content of this file, or on the content of the files in this directory
file" - run the pipeline on the content of this file, or on the content of the files in this directory
file" - run the pipeline on the content of this file, or on the content of the files in this directory
file" - run the pipeline on the content of this file, or on the content of the files in this directory
file" - run the pipeline on the content of this file, or on the content of the files in this directory
file" - run the pipeline on the content of this file, or on the content of the files in this directory
file" - run the pipeline on the content of this file, or on the content of the files in this directory
file" - run the pipeline on the content of this file, or on the content of the files in this directory
file" - run the pipeline on the content of this file, or on the content of the files in this directory
file" - run the pipeline on the content of this file, or on the content of the files in this directory
file" - run the pipeline on the content of this file, or on the content of the files in this directory
file" - run the pipeline on the content of this file, or on the content of the files in this directory
Command line properties:
Command line properties:
Command line properties:
Command line properties:
Command line properties:
Command line properties:
Command line properties:
Command line properties:
Command line properties:
Command line properties:
Command line properties:
Command line properties:
Command line properties:
Command line properties:
Command line properties:
parse.model" - path towards the PCFG parser model
parse.model" - path towards the PCFG parser model
parse.model" - path towards the PCFG parser model
parse.model" - path towards the PCFG parser model
parse.model" - path towards the PCFG parser model
parse.model" - path towards the PCFG parser model
parse.model" - path towards the PCFG parser model
parse.model" - path towards the PCFG parser model
parse.model" - path towards the PCFG parser model
parse.model" - path towards the PCFG parser model
parse.model" - path towards the PCFG parser model
parse.model" - path towards the PCFG parser model
parse.model" - path towards the PCFG parser model
parse.model" - path towards the PCFG parser model
parse.model" - path towards the PCFG parser model
If annotator "parse" is defined:
If annotator "parse" is defined:
If annotator "parse" is defined:
If annotator "parse" is defined:
If annotator "parse" is defined:
If annotator "parse" is defined:
If annotator "parse" is defined:
If annotator "parse" is defined:
If annotator "parse" is defined:
If annotator "parse" is defined:
If annotator "parse" is defined:
If annotator "parse" is defined:
If annotator "parse" is defined:
If annotator "parse" is defined:
If annotator "parse" is defined:
sup.relation.model" - path towards the relation extraction model
sup.relation.model" - path towards the relation extraction model
sup.relation.model" - path towards the relation extraction model
sup.relation.model" - path towards the relation extraction model
sup.relation.model" - path towards the relation extraction model
sup.relation.model" - path towards the relation extraction model
sup.relation.model" - path towards the relation extraction model
sup.relation.model" - path towards the relation extraction model
sup.relation.model" - path towards the relation extraction model
sup.relation.model" - path towards the relation extraction model
sup.relation.model" - path towards the relation extraction model
sup.relation.model" - path towards the relation extraction model
sup.relation.model" - path towards the relation extraction model
sup.relation.model" - path towards the relation extraction model
sup.relation.model" - path towards the relation extraction model
sup.relation.verbose" - whether verbose or not
sup.relation.verbose" - whether verbose or not
sup.relation.verbose" - whether verbose or not
sup.relation.verbose" - whether verbose or not
sup.relation.verbose" - whether verbose or not
sup.relation.verbose" - whether verbose or not
sup.relation.verbose" - whether verbose or not
sup.relation.verbose" - whether verbose or not
sup.relation.verbose" - whether verbose or not
sup.relation.verbose" - whether verbose or not
sup.relation.verbose" - whether verbose or not
sup.relation.verbose" - whether verbose or not
sup.relation.verbose" - whether verbose or not
sup.relation.verbose" - whether verbose or not
sup.relation.verbose" - whether verbose or not
If annotator "relation" is defined:
If annotator "relation" is defined:
If annotator "relation" is defined:
If annotator "relation" is defined:
If annotator "relation" is defined:
If annotator "relation" is defined:
If annotator "relation" is defined:
If annotator "relation" is defined:
If annotator "relation" is defined:
If annotator "relation" is defined:
If annotator "relation" is defined:
If annotator "relation" is defined:
If annotator "relation" is defined:
If annotator "relation" is defined:
If annotator "relation" is defined:
hasPassiveAuxiliary found VBN/VBG/VBD VP
hasPassiveAuxiliary found VBN/VBG/VBD VP
hasPassiveAuxiliary found VBN/VBG/VBD VP
hasPassiveAuxiliary found VBN/VBG/VBD VP
hasPassiveAuxiliary found VBN/VBG/VBD VP
hasPassiveAuxiliary found VBN/VBG/VBD VP
hasPassiveAuxiliary found VBN/VBG/VBD VP
hasPassiveAuxiliary found VBN/VBG/VBD VP
hasPassiveAuxiliary found VBN/VBG/VBD VP
hasPassiveAuxiliary found VBN/VBG/VBD VP
hasPassiveAuxiliary found VBN/VBG/VBD VP
hasPassiveAuxiliary found VBN/VBG/VBD VP
hasPassiveAuxiliary found VBN/VBG/VBD VP
hasPassiveAuxiliary found VBN/VBG/VBD VP
hasPassiveAuxiliary found VBN/VBG/VBD VP
Filtering rules for the given training set
Filtering rules for the given training set
Filtering rules for the given training set
Filtering rules for the given training set
Filtering rules for the given training set
Filtering rules for the given training set
Filtering rules for the given training set
Filtering rules for the given training set
Filtering rules for the given training set
Filtering rules for the given training set
Filtering rules for the given training set
Filtering rules for the given training set
Filtering rules for the given training set
Filtering rules for the given training set
Filtering rules for the given training set
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CC
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CC
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CC
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CC
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CC
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CC
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CC
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CC
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CC
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CC
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CC
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CC
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CC
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CC
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CC
hasPassiveAuxiliary found (VP (VP)), recursing
hasPassiveAuxiliary found (VP (VP)), recursing
hasPassiveAuxiliary found (VP (VP)), recursing
hasPassiveAuxiliary found (VP (VP)), recursing
hasPassiveAuxiliary found (VP (VP)), recursing
hasPassiveAuxiliary found (VP (VP)), recursing
hasPassiveAuxiliary found (VP (VP)), recursing
hasPassiveAuxiliary found (VP (VP)), recursing
hasPassiveAuxiliary found (VP (VP)), recursing
hasPassiveAuxiliary found (VP (VP)), recursing
hasPassiveAuxiliary found (VP (VP)), recursing
hasPassiveAuxiliary found (VP (VP)), recursing
hasPassiveAuxiliary found (VP (VP)), recursing
hasPassiveAuxiliary found (VP (VP)), recursing
hasPassiveAuxiliary found (VP (VP)), recursing
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CONJP
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CONJP
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CONJP
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CONJP
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CONJP
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CONJP
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CONJP
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CONJP
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CONJP
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CONJP
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CONJP
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CONJP
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CONJP
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CONJP
hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CONJP
Finished parsing 200 trees, getting 20 hypotheses each
Finished parsing 300 trees, getting 30 hypotheses each
Finished parsing 400 trees, getting 40 hypotheses each
Finished parsing 500 trees, getting 50 hypotheses each
Finished parsing 600 trees, getting 60 hypotheses each
Finished parsing 700 trees, getting 70 hypotheses each
Finished parsing 800 trees, getting 80 hypotheses each
Finished parsing 900 trees, getting 90 hypotheses each
Finished parsing 1000 trees, getting 100 hypotheses each
Finished parsing 1100 trees, getting 110 hypotheses each
Finished parsing 1200 trees, getting 120 hypotheses each
Finished parsing 1300 trees, getting 130 hypotheses each
Finished parsing 1400 trees, getting 140 hypotheses each
hasPassiveProgressiveAuxiliary found VP
hasPassiveProgressiveAuxiliary found VP
hasPassiveProgressiveAuxiliary found VP
hasPassiveProgressiveAuxiliary found VP
hasPassiveProgressiveAuxiliary found VP
hasPassiveProgressiveAuxiliary found VP
hasPassiveProgressiveAuxiliary found VP
hasPassiveProgressiveAuxiliary found VP
hasPassiveProgressiveAuxiliary found VP
hasPassiveProgressiveAuxiliary found VP
hasPassiveProgressiveAuxiliary found VP
hasPassiveProgressiveAuxiliary found VP
hasPassiveProgressiveAuxiliary found VP
hasPassiveProgressiveAuxiliary found VP
hasPassiveProgressiveAuxiliary found VP
If annotator "truecase" is defined:
If annotator "truecase" is defined:
If annotator "truecase" is defined:
If annotator "truecase" is defined:
If annotator "truecase" is defined:
If annotator "truecase" is defined:
If annotator "truecase" is defined:
If annotator "truecase" is defined:
If annotator "truecase" is defined:
If annotator "truecase" is defined:
If annotator "truecase" is defined:
If annotator "truecase" is defined:
If annotator "truecase" is defined:
If annotator "truecase" is defined:
If annotator "truecase" is defined:
ner.additional.regexner.mapping" - additional regex rules to use for NER recognition
ner.additional.regexner.mapping" - additional regex rules to use for NER recognition
ner.additional.regexner.mapping" - additional regex rules to use for NER recognition
ner.additional.regexner.mapping" - additional regex rules to use for NER recognition
ner.additional.regexner.mapping" - additional regex rules to use for NER recognition
ner.additional.regexner.mapping" - additional regex rules to use for NER recognition
ner.additional.regexner.mapping" - additional regex rules to use for NER recognition
ner.additional.regexner.mapping" - additional regex rules to use for NER recognition
ner.additional.regexner.mapping" - additional regex rules to use for NER recognition
ner.additional.regexner.mapping" - additional regex rules to use for NER recognition
ner.additional.regexner.mapping" - additional regex rules to use for NER recognition
ner.additional.regexner.mapping" - additional regex rules to use for NER recognition
ner.additional.regexner.mapping" - additional regex rules to use for NER recognition
ner.additional.regexner.mapping" - additional regex rules to use for NER recognition
ner.additional.regexner.mapping" - additional regex rules to use for NER recognition
ner.additional.tokensregex.rules" - additional tokensregex rules to use for NER recognition
ner.additional.tokensregex.rules" - additional tokensregex rules to use for NER recognition
ner.additional.tokensregex.rules" - additional tokensregex rules to use for NER recognition
ner.additional.tokensregex.rules" - additional tokensregex rules to use for NER recognition
ner.additional.tokensregex.rules" - additional tokensregex rules to use for NER recognition
ner.additional.tokensregex.rules" - additional tokensregex rules to use for NER recognition
ner.additional.tokensregex.rules" - additional tokensregex rules to use for NER recognition
ner.additional.tokensregex.rules" - additional tokensregex rules to use for NER recognition
ner.additional.tokensregex.rules" - additional tokensregex rules to use for NER recognition
ner.additional.tokensregex.rules" - additional tokensregex rules to use for NER recognition
ner.additional.tokensregex.rules" - additional tokensregex rules to use for NER recognition
ner.additional.tokensregex.rules" - additional tokensregex rules to use for NER recognition
ner.additional.tokensregex.rules" - additional tokensregex rules to use for NER recognition
ner.additional.tokensregex.rules" - additional tokensregex rules to use for NER recognition
ner.additional.tokensregex.rules" - additional tokensregex rules to use for NER recognition
ner.applyFineGrained" - whether or not to apply fine grained regex NER annotation (English specific)
ner.applyFineGrained" - whether or not to apply fine grained regex NER annotation (English specific)
ner.applyFineGrained" - whether or not to apply fine grained regex NER annotation (English specific)
ner.applyFineGrained" - whether or not to apply fine grained regex NER annotation (English specific)
ner.applyFineGrained" - whether or not to apply fine grained regex NER annotation (English specific)
ner.applyFineGrained" - whether or not to apply fine grained regex NER annotation (English specific)
ner.applyFineGrained" - whether or not to apply fine grained regex NER annotation (English specific)
ner.applyFineGrained" - whether or not to apply fine grained regex NER annotation (English specific)
ner.applyFineGrained" - whether or not to apply fine grained regex NER annotation (English specific)
ner.applyFineGrained" - whether or not to apply fine grained regex NER annotation (English specific)
ner.applyFineGrained" - whether or not to apply fine grained regex NER annotation (English specific)
ner.applyFineGrained" - whether or not to apply fine grained regex NER annotation (English specific)
ner.applyFineGrained" - whether or not to apply fine grained regex NER annotation (English specific)
ner.applyFineGrained" - whether or not to apply fine grained regex NER annotation (English specific)
ner.applyFineGrained" - whether or not to apply fine grained regex NER annotation (English specific)
ner.applyNumericClassifiers" - whether or not to use any numeric classifiers (English specific)
ner.applyNumericClassifiers" - whether or not to use any numeric classifiers (English specific)
ner.applyNumericClassifiers" - whether or not to use any numeric classifiers (English specific)
ner.applyNumericClassifiers" - whether or not to use any numeric classifiers (English specific)
ner.applyNumericClassifiers" - whether or not to use any numeric classifiers (English specific)
ner.applyNumericClassifiers" - whether or not to use any numeric classifiers (English specific)
ner.applyNumericClassifiers" - whether or not to use any numeric classifiers (English specific)
ner.applyNumericClassifiers" - whether or not to use any numeric classifiers (English specific)
ner.applyNumericClassifiers" - whether or not to use any numeric classifiers (English specific)
ner.applyNumericClassifiers" - whether or not to use any numeric classifiers (English specific)
ner.applyNumericClassifiers" - whether or not to use any numeric classifiers (English specific)
ner.applyNumericClassifiers" - whether or not to use any numeric classifiers (English specific)
ner.applyNumericClassifiers" - whether or not to use any numeric classifiers (English specific)
ner.applyNumericClassifiers" - whether or not to use any numeric classifiers (English specific)
ner.applyNumericClassifiers" - whether or not to use any numeric classifiers (English specific)
ner.useSUTime" - Whether or not to use sutime (English specific)
ner.useSUTime" - Whether or not to use sutime (English specific)
ner.useSUTime" - Whether or not to use sutime (English specific)
ner.useSUTime" - Whether or not to use sutime (English specific)
ner.useSUTime" - Whether or not to use sutime (English specific)
ner.useSUTime" - Whether or not to use sutime (English specific)
ner.useSUTime" - Whether or not to use sutime (English specific)
ner.useSUTime" - Whether or not to use sutime (English specific)
ner.useSUTime" - Whether or not to use sutime (English specific)
ner.useSUTime" - Whether or not to use sutime (English specific)
ner.useSUTime" - Whether or not to use sutime (English specific)
ner.useSUTime" - Whether or not to use sutime (English specific)
ner.useSUTime" - Whether or not to use sutime (English specific)
ner.useSUTime" - Whether or not to use sutime (English specific)
ner.useSUTime" - Whether or not to use sutime (English specific)
ner.model" - paths for the ner models.  By default, the English 3 class, 7 class, and 4 class models are used.
ner.model" - paths for the ner models.  By default, the English 3 class, 7 class, and 4 class models are used.
ner.model" - paths for the ner models.  By default, the English 3 class, 7 class, and 4 class models are used.
ner.model" - paths for the ner models.  By default, the English 3 class, 7 class, and 4 class models are used.
ner.model" - paths for the ner models.  By default, the English 3 class, 7 class, and 4 class models are used.
ner.model" - paths for the ner models.  By default, the English 3 class, 7 class, and 4 class models are used.
ner.model" - paths for the ner models.  By default, the English 3 class, 7 class, and 4 class models are used.
ner.model" - paths for the ner models.  By default, the English 3 class, 7 class, and 4 class models are used.
ner.model" - paths for the ner models.  By default, the English 3 class, 7 class, and 4 class models are used.
ner.model" - paths for the ner models.  By default, the English 3 class, 7 class, and 4 class models are used.
ner.model" - paths for the ner models.  By default, the English 3 class, 7 class, and 4 class models are used.
ner.model" - paths for the ner models.  By default, the English 3 class, 7 class, and 4 class models are used.
ner.model" - paths for the ner models.  By default, the English 3 class, 7 class, and 4 class models are used.
ner.model" - paths for the ner models.  By default, the English 3 class, 7 class, and 4 class models are used.
ner.model" - paths for the ner models.  By default, the English 3 class, 7 class, and 4 class models are used.
If annotator "ner" is defined:
If annotator "ner" is defined:
If annotator "ner" is defined:
If annotator "ner" is defined:
If annotator "ner" is defined:
If annotator "ner" is defined:
If annotator "ner" is defined:
If annotator "ner" is defined:
If annotator "ner" is defined:
If annotator "ner" is defined:
If annotator "ner" is defined:
If annotator "ner" is defined:
If annotator "ner" is defined:
If annotator "ner" is defined:
If annotator "ner" is defined:
pos.model" - path towards the POS tagger model
pos.model" - path towards the POS tagger model
pos.model" - path towards the POS tagger model
pos.model" - path towards the POS tagger model
pos.model" - path towards the POS tagger model
pos.model" - path towards the POS tagger model
pos.model" - path towards the POS tagger model
pos.model" - path towards the POS tagger model
pos.model" - path towards the POS tagger model
pos.model" - path towards the POS tagger model
pos.model" - path towards the POS tagger model
pos.model" - path towards the POS tagger model
pos.model" - path towards the POS tagger model
pos.model" - path towards the POS tagger model
pos.model" - path towards the POS tagger model
pos.maxlen" - maximum length of sentence to POS tag
pos.maxlen" - maximum length of sentence to POS tag
pos.maxlen" - maximum length of sentence to POS tag
pos.maxlen" - maximum length of sentence to POS tag
pos.maxlen" - maximum length of sentence to POS tag
pos.maxlen" - maximum length of sentence to POS tag
pos.maxlen" - maximum length of sentence to POS tag
pos.maxlen" - maximum length of sentence to POS tag
pos.maxlen" - maximum length of sentence to POS tag
pos.maxlen" - maximum length of sentence to POS tag
pos.maxlen" - maximum length of sentence to POS tag
pos.maxlen" - maximum length of sentence to POS tag
pos.maxlen" - maximum length of sentence to POS tag
pos.maxlen" - maximum length of sentence to POS tag
pos.maxlen" - maximum length of sentence to POS tag
If annotator "pos" is defined:
If annotator "pos" is defined:
If annotator "pos" is defined:
If annotator "pos" is defined:
If annotator "pos" is defined:
If annotator "pos" is defined:
If annotator "pos" is defined:
If annotator "pos" is defined:
If annotator "pos" is defined:
If annotator "pos" is defined:
If annotator "pos" is defined:
If annotator "pos" is defined:
If annotator "pos" is defined:
If annotator "pos" is defined:
If annotator "pos" is defined:
clean.allowflawedxml" - if set to true, don't complain about XML errors
clean.allowflawedxml" - if set to true, don't complain about XML errors
clean.allowflawedxml" - if set to true, don't complain about XML errors
clean.allowflawedxml" - if set to true, don't complain about XML errors
clean.allowflawedxml" - if set to true, don't complain about XML errors
clean.allowflawedxml" - if set to true, don't complain about XML errors
clean.allowflawedxml" - if set to true, don't complain about XML errors
clean.allowflawedxml" - if set to true, don't complain about XML errors
clean.allowflawedxml" - if set to true, don't complain about XML errors
clean.allowflawedxml" - if set to true, don't complain about XML errors
clean.allowflawedxml" - if set to true, don't complain about XML errors
clean.allowflawedxml" - if set to true, don't complain about XML errors
clean.allowflawedxml" - if set to true, don't complain about XML errors
clean.allowflawedxml" - if set to true, don't complain about XML errors
clean.allowflawedxml" - if set to true, don't complain about XML errors
clean.sentenceendingtags" - regex of tags which mark sentence endings
clean.sentenceendingtags" - regex of tags which mark sentence endings
clean.sentenceendingtags" - regex of tags which mark sentence endings
clean.sentenceendingtags" - regex of tags which mark sentence endings
clean.sentenceendingtags" - regex of tags which mark sentence endings
clean.sentenceendingtags" - regex of tags which mark sentence endings
clean.sentenceendingtags" - regex of tags which mark sentence endings
clean.sentenceendingtags" - regex of tags which mark sentence endings
clean.sentenceendingtags" - regex of tags which mark sentence endings
clean.sentenceendingtags" - regex of tags which mark sentence endings
clean.sentenceendingtags" - regex of tags which mark sentence endings
clean.sentenceendingtags" - regex of tags which mark sentence endings
clean.sentenceendingtags" - regex of tags which mark sentence endings
clean.sentenceendingtags" - regex of tags which mark sentence endings
clean.sentenceendingtags" - regex of tags which mark sentence endings
clean.xmltags" - regex of tags to extract text from
clean.xmltags" - regex of tags to extract text from
clean.xmltags" - regex of tags to extract text from
clean.xmltags" - regex of tags to extract text from
clean.xmltags" - regex of tags to extract text from
clean.xmltags" - regex of tags to extract text from
clean.xmltags" - regex of tags to extract text from
clean.xmltags" - regex of tags to extract text from
clean.xmltags" - regex of tags to extract text from
clean.xmltags" - regex of tags to extract text from
clean.xmltags" - regex of tags to extract text from
clean.xmltags" - regex of tags to extract text from
clean.xmltags" - regex of tags to extract text from
clean.xmltags" - regex of tags to extract text from
clean.xmltags" - regex of tags to extract text from
If annotator "cleanxml" is defined:
If annotator "cleanxml" is defined:
If annotator "cleanxml" is defined:
If annotator "cleanxml" is defined:
If annotator "cleanxml" is defined:
If annotator "cleanxml" is defined:
If annotator "cleanxml" is defined:
If annotator "cleanxml" is defined:
If annotator "cleanxml" is defined:
If annotator "cleanxml" is defined:
If annotator "cleanxml" is defined:
If annotator "cleanxml" is defined:
If annotator "cleanxml" is defined:
If annotator "cleanxml" is defined:
If annotator "cleanxml" is defined:
tokenize.codepoint" - If true, add codepoint offsets for counting non-BMP characters
tokenize.codepoint" - If true, add codepoint offsets for counting non-BMP characters
tokenize.codepoint" - If true, add codepoint offsets for counting non-BMP characters
tokenize.codepoint" - If true, add codepoint offsets for counting non-BMP characters
tokenize.codepoint" - If true, add codepoint offsets for counting non-BMP characters
tokenize.codepoint" - If true, add codepoint offsets for counting non-BMP characters
tokenize.codepoint" - If true, add codepoint offsets for counting non-BMP characters
tokenize.codepoint" - If true, add codepoint offsets for counting non-BMP characters
tokenize.codepoint" - If true, add codepoint offsets for counting non-BMP characters
tokenize.codepoint" - If true, add codepoint offsets for counting non-BMP characters
tokenize.codepoint" - If true, add codepoint offsets for counting non-BMP characters
tokenize.codepoint" - If true, add codepoint offsets for counting non-BMP characters
tokenize.codepoint" - If true, add codepoint offsets for counting non-BMP characters
tokenize.codepoint" - If true, add codepoint offsets for counting non-BMP characters
tokenize.codepoint" - If true, add codepoint offsets for counting non-BMP characters
tokenize.whitespace" - If true, just use whitespace tokenization
tokenize.whitespace" - If true, just use whitespace tokenization
tokenize.whitespace" - If true, just use whitespace tokenization
tokenize.whitespace" - If true, just use whitespace tokenization
tokenize.whitespace" - If true, just use whitespace tokenization
tokenize.whitespace" - If true, just use whitespace tokenization
tokenize.whitespace" - If true, just use whitespace tokenization
tokenize.whitespace" - If true, just use whitespace tokenization
tokenize.whitespace" - If true, just use whitespace tokenization
tokenize.whitespace" - If true, just use whitespace tokenization
tokenize.whitespace" - If true, just use whitespace tokenization
tokenize.whitespace" - If true, just use whitespace tokenization
tokenize.whitespace" - If true, just use whitespace tokenization
tokenize.whitespace" - If true, just use whitespace tokenization
tokenize.whitespace" - If true, just use whitespace tokenization
tokenize.options" - PTBTokenizer options (see edu.stanford.nlp.process.PTBTokenizer for details)
tokenize.options" - PTBTokenizer options (see edu.stanford.nlp.process.PTBTokenizer for details)
tokenize.options" - PTBTokenizer options (see edu.stanford.nlp.process.PTBTokenizer for details)
tokenize.options" - PTBTokenizer options (see edu.stanford.nlp.process.PTBTokenizer for details)
tokenize.options" - PTBTokenizer options (see edu.stanford.nlp.process.PTBTokenizer for details)
tokenize.options" - PTBTokenizer options (see edu.stanford.nlp.process.PTBTokenizer for details)
tokenize.options" - PTBTokenizer options (see edu.stanford.nlp.process.PTBTokenizer for details)
tokenize.options" - PTBTokenizer options (see edu.stanford.nlp.process.PTBTokenizer for details)
tokenize.options" - PTBTokenizer options (see edu.stanford.nlp.process.PTBTokenizer for details)
tokenize.options" - PTBTokenizer options (see edu.stanford.nlp.process.PTBTokenizer for details)
tokenize.options" - PTBTokenizer options (see edu.stanford.nlp.process.PTBTokenizer for details)
tokenize.options" - PTBTokenizer options (see edu.stanford.nlp.process.PTBTokenizer for details)
tokenize.options" - PTBTokenizer options (see edu.stanford.nlp.process.PTBTokenizer for details)
tokenize.options" - PTBTokenizer options (see edu.stanford.nlp.process.PTBTokenizer for details)
tokenize.options" - PTBTokenizer options (see edu.stanford.nlp.process.PTBTokenizer for details)
If annotator "tokenize" is defined:
If annotator "tokenize" is defined:
If annotator "tokenize" is defined:
If annotator "tokenize" is defined:
If annotator "tokenize" is defined:
If annotator "tokenize" is defined:
If annotator "tokenize" is defined:
If annotator "tokenize" is defined:
If annotator "tokenize" is defined:
If annotator "tokenize" is defined:
If annotator "tokenize" is defined:
If annotator "tokenize" is defined:
If annotator "tokenize" is defined:
If annotator "tokenize" is defined:
If annotator "tokenize" is defined:
The following annotators are supported: cleanxml, tokenize, quote, ssplit, pos, lemma, ner, truecase, parse, hcoref, relation
The following annotators are supported: cleanxml, tokenize, quote, ssplit, pos, lemma, ner, truecase, parse, hcoref, relation
The following annotators are supported: cleanxml, tokenize, quote, ssplit, pos, lemma, ner, truecase, parse, hcoref, relation
The following annotators are supported: cleanxml, tokenize, quote, ssplit, pos, lemma, ner, truecase, parse, hcoref, relation
The following annotators are supported: cleanxml, tokenize, quote, ssplit, pos, lemma, ner, truecase, parse, hcoref, relation
The following annotators are supported: cleanxml, tokenize, quote, ssplit, pos, lemma, ner, truecase, parse, hcoref, relation
The following annotators are supported: cleanxml, tokenize, quote, ssplit, pos, lemma, ner, truecase, parse, hcoref, relation
The following annotators are supported: cleanxml, tokenize, quote, ssplit, pos, lemma, ner, truecase, parse, hcoref, relation
The following annotators are supported: cleanxml, tokenize, quote, ssplit, pos, lemma, ner, truecase, parse, hcoref, relation
The following annotators are supported: cleanxml, tokenize, quote, ssplit, pos, lemma, ner, truecase, parse, hcoref, relation
The following annotators are supported: cleanxml, tokenize, quote, ssplit, pos, lemma, ner, truecase, parse, hcoref, relation
The following annotators are supported: cleanxml, tokenize, quote, ssplit, pos, lemma, ner, truecase, parse, hcoref, relation
The following annotators are supported: cleanxml, tokenize, quote, ssplit, pos, lemma, ner, truecase, parse, hcoref, relation
The following annotators are supported: cleanxml, tokenize, quote, ssplit, pos, lemma, ner, truecase, parse, hcoref, relation
The following annotators are supported: cleanxml, tokenize, quote, ssplit, pos, lemma, ner, truecase, parse, hcoref, relation
Read in 200 trees from /data/forest2
Read in 300 trees from /data/forest3
Read in 400 trees from /data/forest4
Read in 500 trees from /data/forest5
Read in 600 trees from /data/forest6
Read in 700 trees from /data/forest7
Read in 800 trees from /data/forest8
Read in 900 trees from /data/forest9
Read in 1000 trees from /data/forest10
Read in 1100 trees from /data/forest11
Read in 1200 trees from /data/forest12
Read in 1300 trees from /data/forest13
Read in 1400 trees from /data/forest14
annotators" - comma separated list of annotators
annotators" - comma separated list of annotators
annotators" - comma separated list of annotators
annotators" - comma separated list of annotators
annotators" - comma separated list of annotators
annotators" - comma separated list of annotators
annotators" - comma separated list of annotators
annotators" - comma separated list of annotators
annotators" - comma separated list of annotators
annotators" - comma separated list of annotators
annotators" - comma separated list of annotators
annotators" - comma separated list of annotators
annotators" - comma separated list of annotators
annotators" - comma separated list of annotators
annotators" - comma separated list of annotators
Running DVParser with arguments:
Running DVParser with arguments:
Running DVParser with arguments:
Running DVParser with arguments:
Running DVParser with arguments:
Running DVParser with arguments:
Running DVParser with arguments:
Running DVParser with arguments:
Running DVParser with arguments:
Running DVParser with arguments:
Running DVParser with arguments:
Running DVParser with arguments:
Running DVParser with arguments:
Running DVParser with arguments:
Running DVParser with arguments:
Checking for passive/progressive auxiliary
Checking for passive/progressive auxiliary
Checking for passive/progressive auxiliary
Checking for passive/progressive auxiliary
Checking for passive/progressive auxiliary
Checking for passive/progressive auxiliary
Checking for passive/progressive auxiliary
Checking for passive/progressive auxiliary
Checking for passive/progressive auxiliary
Checking for passive/progressive auxiliary
Checking for passive/progressive auxiliary
Checking for passive/progressive auxiliary
Checking for passive/progressive auxiliary
Checking for passive/progressive auxiliary
Checking for passive/progressive auxiliary
props" - path to file with configuration properties
props" - path to file with configuration properties
props" - path to file with configuration properties
props" - path to file with configuration properties
props" - path to file with configuration properties
props" - path to file with configuration properties
props" - path to file with configuration properties
props" - path to file with configuration properties
props" - path to file with configuration properties
props" - path to file with configuration properties
props" - path to file with configuration properties
props" - path to file with configuration properties
props" - path to file with configuration properties
props" - path to file with configuration properties
props" - path to file with configuration properties
-(no)trainWordVectors: whether or not to train the word vectors along with the matrices.  True by default
-(no)trainWordVectors: whether or not to train the word vectors along with the matrices.  True by default
-(no)trainWordVectors: whether or not to train the word vectors along with the matrices.  True by default
-(no)trainWordVectors: whether or not to train the word vectors along with the matrices.  True by default
-(no)trainWordVectors: whether or not to train the word vectors along with the matrices.  True by default
-(no)trainWordVectors: whether or not to train the word vectors along with the matrices.  True by default
-(no)trainWordVectors: whether or not to train the word vectors along with the matrices.  True by default
-(no)trainWordVectors: whether or not to train the word vectors along with the matrices.  True by default
-(no)trainWordVectors: whether or not to train the word vectors along with the matrices.  True by default
-(no)trainWordVectors: whether or not to train the word vectors along with the matrices.  True by default
-(no)trainWordVectors: whether or not to train the word vectors along with the matrices.  True by default
-(no)trainWordVectors: whether or not to train the word vectors along with the matrices.  True by default
-(no)trainWordVectors: whether or not to train the word vectors along with the matrices.  True by default
-(no)trainWordVectors: whether or not to train the word vectors along with the matrices.  True by default
-(no)trainWordVectors: whether or not to train the word vectors along with the matrices.  True by default
isAuxiliary found desired type of aux
isAuxiliary found desired type of aux
isAuxiliary found desired type of aux
isAuxiliary found desired type of aux
isAuxiliary found desired type of aux
isAuxiliary found desired type of aux
isAuxiliary found desired type of aux
isAuxiliary found desired type of aux
isAuxiliary found desired type of aux
isAuxiliary found desired type of aux
isAuxiliary found desired type of aux
isAuxiliary found desired type of aux
isAuxiliary found desired type of aux
isAuxiliary found desired type of aux
isAuxiliary found desired type of aux
(if -props or -annotators is not passed in, default properties will be loaded via the classpath)
(if -props or -annotators is not passed in, default properties will be loaded via the classpath)
(if -props or -annotators is not passed in, default properties will be loaded via the classpath)
(if -props or -annotators is not passed in, default properties will be loaded via the classpath)
(if -props or -annotators is not passed in, default properties will be loaded via the classpath)
(if -props or -annotators is not passed in, default properties will be loaded via the classpath)
(if -props or -annotators is not passed in, default properties will be loaded via the classpath)
(if -props or -annotators is not passed in, default properties will be loaded via the classpath)
(if -props or -annotators is not passed in, default properties will be loaded via the classpath)
(if -props or -annotators is not passed in, default properties will be loaded via the classpath)
(if -props or -annotators is not passed in, default properties will be loaded via the classpath)
(if -props or -annotators is not passed in, default properties will be loaded via the classpath)
(if -props or -annotators is not passed in, default properties will be loaded via the classpath)
(if -props or -annotators is not passed in, default properties will be loaded via the classpath)
(if -props or -annotators is not passed in, default properties will be loaded via the classpath)
-transformMatrixType: A couple different methods for initializing transform matrices
-transformMatrixType: A couple different methods for initializing transform matrices
-transformMatrixType: A couple different methods for initializing transform matrices
-transformMatrixType: A couple different methods for initializing transform matrices
-transformMatrixType: A couple different methods for initializing transform matrices
-transformMatrixType: A couple different methods for initializing transform matrices
-transformMatrixType: A couple different methods for initializing transform matrices
-transformMatrixType: A couple different methods for initializing transform matrices
-transformMatrixType: A couple different methods for initializing transform matrices
-transformMatrixType: A couple different methods for initializing transform matrices
-transformMatrixType: A couple different methods for initializing transform matrices
-transformMatrixType: A couple different methods for initializing transform matrices
-transformMatrixType: A couple different methods for initializing transform matrices
-transformMatrixType: A couple different methods for initializing transform matrices
-transformMatrixType: A couple different methods for initializing transform matrices
The following properties can be defined:
The following properties can be defined:
The following properties can be defined:
The following properties can be defined:
The following properties can be defined:
The following properties can be defined:
The following properties can be defined:
The following properties can be defined:
The following properties can be defined:
The following properties can be defined:
The following properties can be defined:
The following properties can be defined:
The following properties can be defined:
The following properties can be defined:
The following properties can be defined:
-unkWord: The vector representing unknown word in the word vectors file
-unkWord: The vector representing unknown word in the word vectors file
-unkWord: The vector representing unknown word in the word vectors file
-unkWord: The vector representing unknown word in the word vectors file
-unkWord: The vector representing unknown word in the word vectors file
-unkWord: The vector representing unknown word in the word vectors file
-unkWord: The vector representing unknown word in the word vectors file
-unkWord: The vector representing unknown word in the word vectors file
-unkWord: The vector representing unknown word in the word vectors file
-unkWord: The vector representing unknown word in the word vectors file
-unkWord: The vector representing unknown word in the word vectors file
-unkWord: The vector representing unknown word in the word vectors file
-unkWord: The vector representing unknown word in the word vectors file
-unkWord: The vector representing unknown word in the word vectors file
-unkWord: The vector representing unknown word in the word vectors file
parse.executable - path to the parseIt binary or parse.sh script
parse.executable - path to the parseIt binary or parse.sh script
parse.executable - path to the parseIt binary or parse.sh script
parse.executable - path to the parseIt binary or parse.sh script
parse.executable - path to the parseIt binary or parse.sh script
parse.executable - path to the parseIt binary or parse.sh script
parse.executable - path to the parseIt binary or parse.sh script
parse.executable - path to the parseIt binary or parse.sh script
parse.executable - path to the parseIt binary or parse.sh script
parse.executable - path to the parseIt binary or parse.sh script
parse.executable - path to the parseIt binary or parse.sh script
parse.executable - path to the parseIt binary or parse.sh script
parse.executable - path to the parseIt binary or parse.sh script
parse.executable - path to the parseIt binary or parse.sh script
parse.executable - path to the parseIt binary or parse.sh script
-baseParserWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well for English)
-baseParserWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well for English)
-baseParserWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well for English)
-baseParserWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well for English)
-baseParserWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well for English)
-baseParserWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well for English)
-baseParserWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well for English)
-baseParserWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well for English)
-baseParserWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well for English)
-baseParserWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well for English)
-baseParserWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well for English)
-baseParserWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well for English)
-baseParserWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well for English)
-baseParserWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well for English)
-baseParserWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well for English)
Charniak and Johnson parser-specific options:
Charniak and Johnson parser-specific options:
Charniak and Johnson parser-specific options:
Charniak and Johnson parser-specific options:
Charniak and Johnson parser-specific options:
Charniak and Johnson parser-specific options:
Charniak and Johnson parser-specific options:
Charniak and Johnson parser-specific options:
Charniak and Johnson parser-specific options:
Charniak and Johnson parser-specific options:
Charniak and Johnson parser-specific options:
Charniak and Johnson parser-specific options:
Charniak and Johnson parser-specific options:
Charniak and Johnson parser-specific options:
Charniak and Johnson parser-specific options:
-scalingForInit: How much to scale matrices when creating a new DVModel
-scalingForInit: How much to scale matrices when creating a new DVModel
-scalingForInit: How much to scale matrices when creating a new DVModel
-scalingForInit: How much to scale matrices when creating a new DVModel
-scalingForInit: How much to scale matrices when creating a new DVModel
-scalingForInit: How much to scale matrices when creating a new DVModel
-scalingForInit: How much to scale matrices when creating a new DVModel
-scalingForInit: How much to scale matrices when creating a new DVModel
-scalingForInit: How much to scale matrices when creating a new DVModel
-scalingForInit: How much to scale matrices when creating a new DVModel
-scalingForInit: How much to scale matrices when creating a new DVModel
-scalingForInit: How much to scale matrices when creating a new DVModel
-scalingForInit: How much to scale matrices when creating a new DVModel
-scalingForInit: How much to scale matrices when creating a new DVModel
-scalingForInit: How much to scale matrices when creating a new DVModel
parse.debug - set to true to make the parser slightly more verbose
parse.debug - set to true to make the parser slightly more verbose
parse.debug - set to true to make the parser slightly more verbose
parse.debug - set to true to make the parser slightly more verbose
parse.debug - set to true to make the parser slightly more verbose
parse.debug - set to true to make the parser slightly more verbose
parse.debug - set to true to make the parser slightly more verbose
parse.debug - set to true to make the parser slightly more verbose
parse.debug - set to true to make the parser slightly more verbose
parse.debug - set to true to make the parser slightly more verbose
parse.debug - set to true to make the parser slightly more verbose
parse.debug - set to true to make the parser slightly more verbose
parse.debug - set to true to make the parser slightly more verbose
parse.debug - set to true to make the parser slightly more verbose
parse.debug - set to true to make the parser slightly more verbose
-dvSimplifiedModel: Use a greatly dumbed down DVModel
-dvSimplifiedModel: Use a greatly dumbed down DVModel
-dvSimplifiedModel: Use a greatly dumbed down DVModel
-dvSimplifiedModel: Use a greatly dumbed down DVModel
-dvSimplifiedModel: Use a greatly dumbed down DVModel
-dvSimplifiedModel: Use a greatly dumbed down DVModel
-dvSimplifiedModel: Use a greatly dumbed down DVModel
-dvSimplifiedModel: Use a greatly dumbed down DVModel
-dvSimplifiedModel: Use a greatly dumbed down DVModel
-dvSimplifiedModel: Use a greatly dumbed down DVModel
-dvSimplifiedModel: Use a greatly dumbed down DVModel
-dvSimplifiedModel: Use a greatly dumbed down DVModel
-dvSimplifiedModel: Use a greatly dumbed down DVModel
-dvSimplifiedModel: Use a greatly dumbed down DVModel
-dvSimplifiedModel: Use a greatly dumbed down DVModel
parse.flags - extra flags to the parser (default: -retainTmpSubcategories)
parse.flags - extra flags to the parser (default: -retainTmpSubcategories)
parse.flags - extra flags to the parser (default: -retainTmpSubcategories)
parse.flags - extra flags to the parser (default: -retainTmpSubcategories)
parse.flags - extra flags to the parser (default: -retainTmpSubcategories)
parse.flags - extra flags to the parser (default: -retainTmpSubcategories)
parse.flags - extra flags to the parser (default: -retainTmpSubcategories)
parse.flags - extra flags to the parser (default: -retainTmpSubcategories)
parse.flags - extra flags to the parser (default: -retainTmpSubcategories)
parse.flags - extra flags to the parser (default: -retainTmpSubcategories)
parse.flags - extra flags to the parser (default: -retainTmpSubcategories)
parse.flags - extra flags to the parser (default: -retainTmpSubcategories)
parse.flags - extra flags to the parser (default: -retainTmpSubcategories)
parse.flags - extra flags to the parser (default: -retainTmpSubcategories)
parse.flags - extra flags to the parser (default: -retainTmpSubcategories)
-(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals
-(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals
-(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals
-(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals
-(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals
-(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals
-(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals
-(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals
-(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals
-(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals
-(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals
-(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals
-(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals
-(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals
-(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals
(In general, you shouldn't need to set this flags)
(In general, you shouldn't need to set this flags)
(In general, you shouldn't need to set this flags)
(In general, you shouldn't need to set this flags)
(In general, you shouldn't need to set this flags)
(In general, you shouldn't need to set this flags)
(In general, you shouldn't need to set this flags)
(In general, you shouldn't need to set this flags)
(In general, you shouldn't need to set this flags)
(In general, you shouldn't need to set this flags)
(In general, you shouldn't need to set this flags)
(In general, you shouldn't need to set this flags)
(In general, you shouldn't need to set this flags)
(In general, you shouldn't need to set this flags)
(In general, you shouldn't need to set this flags)
-(no)unknownDashedWordVectors: Whether or not to split unknown dashed words
-(no)unknownDashedWordVectors: Whether or not to split unknown dashed words
-(no)unknownDashedWordVectors: Whether or not to split unknown dashed words
-(no)unknownDashedWordVectors: Whether or not to split unknown dashed words
-(no)unknownDashedWordVectors: Whether or not to split unknown dashed words
-(no)unknownDashedWordVectors: Whether or not to split unknown dashed words
-(no)unknownDashedWordVectors: Whether or not to split unknown dashed words
-(no)unknownDashedWordVectors: Whether or not to split unknown dashed words
-(no)unknownDashedWordVectors: Whether or not to split unknown dashed words
-(no)unknownDashedWordVectors: Whether or not to split unknown dashed words
-(no)unknownDashedWordVectors: Whether or not to split unknown dashed words
-(no)unknownDashedWordVectors: Whether or not to split unknown dashed words
-(no)unknownDashedWordVectors: Whether or not to split unknown dashed words
-(no)unknownDashedWordVectors: Whether or not to split unknown dashed words
-(no)unknownDashedWordVectors: Whether or not to split unknown dashed words
Stanford Parser-specific options:
Stanford Parser-specific options:
Stanford Parser-specific options:
Stanford Parser-specific options:
Stanford Parser-specific options:
Stanford Parser-specific options:
Stanford Parser-specific options:
Stanford Parser-specific options:
Stanford Parser-specific options:
Stanford Parser-specific options:
Stanford Parser-specific options:
Stanford Parser-specific options:
Stanford Parser-specific options:
Stanford Parser-specific options:
Stanford Parser-specific options:
-(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers
-(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers
-(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers
-(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers
-(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers
-(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers
-(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers
-(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers
-(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers
-(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers
-(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers
-(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers
-(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers
-(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers
-(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers
parse.maxlen - maximum sentence length
parse.maxlen - maximum sentence length
parse.maxlen - maximum sentence length
parse.maxlen - maximum sentence length
parse.maxlen - maximum sentence length
parse.maxlen - maximum sentence length
parse.maxlen - maximum sentence length
parse.maxlen - maximum sentence length
parse.maxlen - maximum sentence length
parse.maxlen - maximum sentence length
parse.maxlen - maximum sentence length
parse.maxlen - maximum sentence length
parse.maxlen - maximum sentence length
parse.maxlen - maximum sentence length
parse.maxlen - maximum sentence length
-deltaMargin: How much we punish trees for being incorrect when training
-deltaMargin: How much we punish trees for being incorrect when training
-deltaMargin: How much we punish trees for being incorrect when training
-deltaMargin: How much we punish trees for being incorrect when training
-deltaMargin: How much we punish trees for being incorrect when training
-deltaMargin: How much we punish trees for being incorrect when training
-deltaMargin: How much we punish trees for being incorrect when training
-deltaMargin: How much we punish trees for being incorrect when training
-deltaMargin: How much we punish trees for being incorrect when training
-deltaMargin: How much we punish trees for being incorrect when training
-deltaMargin: How much we punish trees for being incorrect when training
-deltaMargin: How much we punish trees for being incorrect when training
-deltaMargin: How much we punish trees for being incorrect when training
-deltaMargin: How much we punish trees for being incorrect when training
-deltaMargin: How much we punish trees for being incorrect when training
parse.model - path to model file for parser
parse.model - path to model file for parser
parse.model - path to model file for parser
parse.model - path to model file for parser
parse.model - path to model file for parser
parse.model - path to model file for parser
parse.model - path to model file for parser
parse.model - path to model file for parser
parse.model - path to model file for parser
parse.model - path to model file for parser
parse.model - path to model file for parser
parse.model - path to model file for parser
parse.model - path to model file for parser
parse.model - path to model file for parser
parse.model - path to model file for parser
-learningRate: The rate of optimization when training
-learningRate: The rate of optimization when training
-learningRate: The rate of optimization when training
-learningRate: The rate of optimization when training
-learningRate: The rate of optimization when training
-learningRate: The rate of optimization when training
-learningRate: The rate of optimization when training
-learningRate: The rate of optimization when training
-learningRate: The rate of optimization when training
-learningRate: The rate of optimization when training
-learningRate: The rate of optimization when training
-learningRate: The rate of optimization when training
-learningRate: The rate of optimization when training
-learningRate: The rate of optimization when training
-learningRate: The rate of optimization when training
parse.type - selects the parser to use
parse.type - selects the parser to use
parse.type - selects the parser to use
parse.type - selects the parser to use
parse.type - selects the parser to use
parse.type - selects the parser to use
parse.type - selects the parser to use
parse.type - selects the parser to use
parse.type - selects the parser to use
parse.type - selects the parser to use
parse.type - selects the parser to use
parse.type - selects the parser to use
parse.type - selects the parser to use
parse.type - selects the parser to use
parse.type - selects the parser to use
-numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.
-numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.
-numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.
-numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.
-numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.
-numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.
-numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.
-numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.
-numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.
-numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.
-numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.
-numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.
-numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.
-numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.
-numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.
General options: (all parsers)
General options: (all parsers)
General options: (all parsers)
General options: (all parsers)
General options: (all parsers)
General options: (all parsers)
General options: (all parsers)
General options: (all parsers)
General options: (all parsers)
General options: (all parsers)
General options: (all parsers)
General options: (all parsers)
General options: (all parsers)
General options: (all parsers)
General options: (all parsers)
-wordVectorFile <name>: A filename to load word vectors from.
-wordVectorFile <name>: A filename to load word vectors from.
-wordVectorFile <name>: A filename to load word vectors from.
-wordVectorFile <name>: A filename to load word vectors from.
-wordVectorFile <name>: A filename to load word vectors from.
-wordVectorFile <name>: A filename to load word vectors from.
-wordVectorFile <name>: A filename to load word vectors from.
-wordVectorFile <name>: A filename to load word vectors from.
-wordVectorFile <name>: A filename to load word vectors from.
-wordVectorFile <name>: A filename to load word vectors from.
-wordVectorFile <name>: A filename to load word vectors from.
-wordVectorFile <name>: A filename to load word vectors from.
-wordVectorFile <name>: A filename to load word vectors from.
-wordVectorFile <name>: A filename to load word vectors from.
-wordVectorFile <name>: A filename to load word vectors from.
charniak - Charniak and Johnson reranking parser (sold separately)
charniak - Charniak and Johnson reranking parser (sold separately)
charniak - Charniak and Johnson reranking parser (sold separately)
charniak - Charniak and Johnson reranking parser (sold separately)
charniak - Charniak and Johnson reranking parser (sold separately)
charniak - Charniak and Johnson reranking parser (sold separately)
charniak - Charniak and Johnson reranking parser (sold separately)
charniak - Charniak and Johnson reranking parser (sold separately)
charniak - Charniak and Johnson reranking parser (sold separately)
charniak - Charniak and Johnson reranking parser (sold separately)
charniak - Charniak and Johnson reranking parser (sold separately)
charniak - Charniak and Johnson reranking parser (sold separately)
charniak - Charniak and Johnson reranking parser (sold separately)
charniak - Charniak and Johnson reranking parser (sold separately)
charniak - Charniak and Johnson reranking parser (sold separately)
-randomSeed <long>: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.
-randomSeed <long>: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.
-randomSeed <long>: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.
-randomSeed <long>: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.
-randomSeed <long>: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.
-randomSeed <long>: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.
-randomSeed <long>: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.
-randomSeed <long>: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.
-randomSeed <long>: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.
-randomSeed <long>: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.
-randomSeed <long>: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.
-randomSeed <long>: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.
-randomSeed <long>: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.
-randomSeed <long>: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.
-randomSeed <long>: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.
stanford - Stanford lexicalized parser (default)
stanford - Stanford lexicalized parser (default)
stanford - Stanford lexicalized parser (default)
stanford - Stanford lexicalized parser (default)
stanford - Stanford lexicalized parser (default)
stanford - Stanford lexicalized parser (default)
stanford - Stanford lexicalized parser (default)
stanford - Stanford lexicalized parser (default)
stanford - Stanford lexicalized parser (default)
stanford - Stanford lexicalized parser (default)
stanford - Stanford lexicalized parser (default)
stanford - Stanford lexicalized parser (default)
stanford - Stanford lexicalized parser (default)
stanford - Stanford lexicalized parser (default)
stanford - Stanford lexicalized parser (default)
-maxTrainTimeSeconds <int>: How long to train before terminating.
-maxTrainTimeSeconds <int>: How long to train before terminating.
-maxTrainTimeSeconds <int>: How long to train before terminating.
-maxTrainTimeSeconds <int>: How long to train before terminating.
-maxTrainTimeSeconds <int>: How long to train before terminating.
-maxTrainTimeSeconds <int>: How long to train before terminating.
-maxTrainTimeSeconds <int>: How long to train before terminating.
-maxTrainTimeSeconds <int>: How long to train before terminating.
-maxTrainTimeSeconds <int>: How long to train before terminating.
-maxTrainTimeSeconds <int>: How long to train before terminating.
-maxTrainTimeSeconds <int>: How long to train before terminating.
-maxTrainTimeSeconds <int>: How long to train before terminating.
-maxTrainTimeSeconds <int>: How long to train before terminating.
-maxTrainTimeSeconds <int>: How long to train before terminating.
-maxTrainTimeSeconds <int>: How long to train before terminating.
StanfordCoreNLP currently supports the following parsers:
StanfordCoreNLP currently supports the following parsers:
StanfordCoreNLP currently supports the following parsers:
StanfordCoreNLP currently supports the following parsers:
StanfordCoreNLP currently supports the following parsers:
StanfordCoreNLP currently supports the following parsers:
StanfordCoreNLP currently supports the following parsers:
StanfordCoreNLP currently supports the following parsers:
StanfordCoreNLP currently supports the following parsers:
StanfordCoreNLP currently supports the following parsers:
StanfordCoreNLP currently supports the following parsers:
StanfordCoreNLP currently supports the following parsers:
StanfordCoreNLP currently supports the following parsers:
StanfordCoreNLP currently supports the following parsers:
StanfordCoreNLP currently supports the following parsers:
-debugOutputFrequency <int>: How frequently to score a model when training and write out intermediate models.
-debugOutputFrequency <int>: How frequently to score a model when training and write out intermediate models.
-debugOutputFrequency <int>: How frequently to score a model when training and write out intermediate models.
-debugOutputFrequency <int>: How frequently to score a model when training and write out intermediate models.
-debugOutputFrequency <int>: How frequently to score a model when training and write out intermediate models.
-debugOutputFrequency <int>: How frequently to score a model when training and write out intermediate models.
-debugOutputFrequency <int>: How frequently to score a model when training and write out intermediate models.
-debugOutputFrequency <int>: How frequently to score a model when training and write out intermediate models.
-debugOutputFrequency <int>: How frequently to score a model when training and write out intermediate models.
-debugOutputFrequency <int>: How frequently to score a model when training and write out intermediate models.
-debugOutputFrequency <int>: How frequently to score a model when training and write out intermediate models.
-debugOutputFrequency <int>: How frequently to score a model when training and write out intermediate models.
-debugOutputFrequency <int>: How frequently to score a model when training and write out intermediate models.
-debugOutputFrequency <int>: How frequently to score a model when training and write out intermediate models.
-debugOutputFrequency <int>: How frequently to score a model when training and write out intermediate models.
See -help for a list of all help topics.
See -help for a list of all help topics.
See -help for a list of all help topics.
See -help for a list of all help topics.
See -help for a list of all help topics.
See -help for a list of all help topics.
See -help for a list of all help topics.
See -help for a list of all help topics.
See -help for a list of all help topics.
See -help for a list of all help topics.
See -help for a list of all help topics.
See -help for a list of all help topics.
See -help for a list of all help topics.
See -help for a list of all help topics.
See -help for a list of all help topics.
-qnTolerance <double>: Tolerance for early exit when optimizing a batch.
-qnTolerance <double>: Tolerance for early exit when optimizing a batch.
-qnTolerance <double>: Tolerance for early exit when optimizing a batch.
-qnTolerance <double>: Tolerance for early exit when optimizing a batch.
-qnTolerance <double>: Tolerance for early exit when optimizing a batch.
-qnTolerance <double>: Tolerance for early exit when optimizing a batch.
-qnTolerance <double>: Tolerance for early exit when optimizing a batch.
-qnTolerance <double>: Tolerance for early exit when optimizing a batch.
-qnTolerance <double>: Tolerance for early exit when optimizing a batch.
-qnTolerance <double>: Tolerance for early exit when optimizing a batch.
-qnTolerance <double>: Tolerance for early exit when optimizing a batch.
-qnTolerance <double>: Tolerance for early exit when optimizing a batch.
-qnTolerance <double>: Tolerance for early exit when optimizing a batch.
-qnTolerance <double>: Tolerance for early exit when optimizing a batch.
-qnTolerance <double>: Tolerance for early exit when optimizing a batch.
-qnEstimates <int>: Parameter for qn optimization.
-qnEstimates <int>: Parameter for qn optimization.
-qnEstimates <int>: Parameter for qn optimization.
-qnEstimates <int>: Parameter for qn optimization.
-qnEstimates <int>: Parameter for qn optimization.
-qnEstimates <int>: Parameter for qn optimization.
-qnEstimates <int>: Parameter for qn optimization.
-qnEstimates <int>: Parameter for qn optimization.
-qnEstimates <int>: Parameter for qn optimization.
-qnEstimates <int>: Parameter for qn optimization.
-qnEstimates <int>: Parameter for qn optimization.
-qnEstimates <int>: Parameter for qn optimization.
-qnEstimates <int>: Parameter for qn optimization.
-qnEstimates <int>: Parameter for qn optimization.
-qnEstimates <int>: Parameter for qn optimization.
-qnIterationsPerBatch <int>: How many steps to take per batch.
-qnIterationsPerBatch <int>: How many steps to take per batch.
-qnIterationsPerBatch <int>: How many steps to take per batch.
-qnIterationsPerBatch <int>: How many steps to take per batch.
-qnIterationsPerBatch <int>: How many steps to take per batch.
-qnIterationsPerBatch <int>: How many steps to take per batch.
-qnIterationsPerBatch <int>: How many steps to take per batch.
-qnIterationsPerBatch <int>: How many steps to take per batch.
-qnIterationsPerBatch <int>: How many steps to take per batch.
-qnIterationsPerBatch <int>: How many steps to take per batch.
-qnIterationsPerBatch <int>: How many steps to take per batch.
-qnIterationsPerBatch <int>: How many steps to take per batch.
-qnIterationsPerBatch <int>: How many steps to take per batch.
-qnIterationsPerBatch <int>: How many steps to take per batch.
-qnIterationsPerBatch <int>: How many steps to take per batch.
-batchSize <int>: How many trees to use in each batch of the training.
-batchSize <int>: How many trees to use in each batch of the training.
-batchSize <int>: How many trees to use in each batch of the training.
-batchSize <int>: How many trees to use in each batch of the training.
-batchSize <int>: How many trees to use in each batch of the training.
-batchSize <int>: How many trees to use in each batch of the training.
-batchSize <int>: How many trees to use in each batch of the training.
-batchSize <int>: How many trees to use in each batch of the training.
-batchSize <int>: How many trees to use in each batch of the training.
-batchSize <int>: How many trees to use in each batch of the training.
-batchSize <int>: How many trees to use in each batch of the training.
-batchSize <int>: How many trees to use in each batch of the training.
-batchSize <int>: How many trees to use in each batch of the training.
-batchSize <int>: How many trees to use in each batch of the training.
-batchSize <int>: How many trees to use in each batch of the training.
-regCost <double>: How large of a cost to put on regularization.
-regCost <double>: How large of a cost to put on regularization.
-regCost <double>: How large of a cost to put on regularization.
-regCost <double>: How large of a cost to put on regularization.
-regCost <double>: How large of a cost to put on regularization.
-regCost <double>: How large of a cost to put on regularization.
-regCost <double>: How large of a cost to put on regularization.
-regCost <double>: How large of a cost to put on regularization.
-regCost <double>: How large of a cost to put on regularization.
-regCost <double>: How large of a cost to put on regularization.
-regCost <double>: How large of a cost to put on regularization.
-regCost <double>: How large of a cost to put on regularization.
-regCost <double>: How large of a cost to put on regularization.
-regCost <double>: How large of a cost to put on regularization.
-regCost <double>: How large of a cost to put on regularization.
-trainingIterations <int>: When training, how many times to go through the train set.
-trainingIterations <int>: When training, how many times to go through the train set.
-trainingIterations <int>: When training, how many times to go through the train set.
-trainingIterations <int>: When training, how many times to go through the train set.
-trainingIterations <int>: When training, how many times to go through the train set.
-trainingIterations <int>: When training, how many times to go through the train set.
-trainingIterations <int>: When training, how many times to go through the train set.
-trainingIterations <int>: When training, how many times to go through the train set.
-trainingIterations <int>: When training, how many times to go through the train set.
-trainingIterations <int>: When training, how many times to go through the train set.
-trainingIterations <int>: When training, how many times to go through the train set.
-trainingIterations <int>: When training, how many times to go through the train set.
-trainingIterations <int>: When training, how many times to go through the train set.
-trainingIterations <int>: When training, how many times to go through the train set.
-trainingIterations <int>: When training, how many times to go through the train set.
-dvKBest <int>: How many hypotheses to use from the underlying parser.
-dvKBest <int>: How many hypotheses to use from the underlying parser.
-dvKBest <int>: How many hypotheses to use from the underlying parser.
-dvKBest <int>: How many hypotheses to use from the underlying parser.
-dvKBest <int>: How many hypotheses to use from the underlying parser.
-dvKBest <int>: How many hypotheses to use from the underlying parser.
-dvKBest <int>: How many hypotheses to use from the underlying parser.
-dvKBest <int>: How many hypotheses to use from the underlying parser.
-dvKBest <int>: How many hypotheses to use from the underlying parser.
-dvKBest <int>: How many hypotheses to use from the underlying parser.
-dvKBest <int>: How many hypotheses to use from the underlying parser.
-dvKBest <int>: How many hypotheses to use from the underlying parser.
-dvKBest <int>: How many hypotheses to use from the underlying parser.
-dvKBest <int>: How many hypotheses to use from the underlying parser.
-dvKBest <int>: How many hypotheses to use from the underlying parser.
-trainingThreads <int>: How many threads to use when training.
-trainingThreads <int>: How many threads to use when training.
-trainingThreads <int>: How many threads to use when training.
-trainingThreads <int>: How many threads to use when training.
-trainingThreads <int>: How many threads to use when training.
-trainingThreads <int>: How many threads to use when training.
-trainingThreads <int>: How many threads to use when training.
-trainingThreads <int>: How many threads to use when training.
-trainingThreads <int>: How many threads to use when training.
-trainingThreads <int>: How many threads to use when training.
-trainingThreads <int>: How many threads to use when training.
-trainingThreads <int>: How many threads to use when training.
-trainingThreads <int>: How many threads to use when training.
-trainingThreads <int>: How many threads to use when training.
-trainingThreads <int>: How many threads to use when training.
----------- non-collapsed dependencies (basic + extra) -----------
----------- non-collapsed dependencies (basic + extra) -----------
----------- non-collapsed dependencies (basic + extra) -----------
----------- non-collapsed dependencies (basic + extra) -----------
----------- non-collapsed dependencies (basic + extra) -----------
----------- non-collapsed dependencies (basic + extra) -----------
----------- non-collapsed dependencies (basic + extra) -----------
----------- non-collapsed dependencies (basic + extra) -----------
----------- non-collapsed dependencies (basic + extra) -----------
----------- non-collapsed dependencies (basic + extra) -----------
----------- non-collapsed dependencies (basic + extra) -----------
----------- non-collapsed dependencies (basic + extra) -----------
----------- non-collapsed dependencies (basic + extra) -----------
----------- non-collapsed dependencies (basic + extra) -----------
----------- non-collapsed dependencies (basic + extra) -----------
not use discourse salience
not use discourse salience
not use discourse salience
not use discourse salience
not use discourse salience
not use discourse salience
not use discourse salience
not use discourse salience
not use discourse salience
not use discourse salience
not use discourse salience
not use discourse salience
not use discourse salience
not use discourse salience
not use discourse salience
------------- basic dependencies ---------------
------------- basic dependencies ---------------
------------- basic dependencies ---------------
------------- basic dependencies ---------------
------------- basic dependencies ---------------
------------- basic dependencies ---------------
------------- basic dependencies ---------------
------------- basic dependencies ---------------
------------- basic dependencies ---------------
------------- basic dependencies ---------------
------------- basic dependencies ---------------
------------- basic dependencies ---------------
------------- basic dependencies ---------------
------------- basic dependencies ---------------
------------- basic dependencies ---------------
USE_ANIMACY_LIST on
USE_ANIMACY_LIST on
USE_ANIMACY_LIST on
USE_ANIMACY_LIST on
USE_ANIMACY_LIST on
USE_ANIMACY_LIST on
USE_ANIMACY_LIST on
USE_ANIMACY_LIST on
USE_ANIMACY_LIST on
USE_ANIMACY_LIST on
USE_ANIMACY_LIST on
USE_ANIMACY_LIST on
USE_ANIMACY_LIST on
USE_ANIMACY_LIST on
USE_ANIMACY_LIST on
============= parse tree =======================
============= parse tree =======================
============= parse tree =======================
============= parse tree =======================
============= parse tree =======================
============= parse tree =======================
============= parse tree =======================
============= parse tree =======================
============= parse tree =======================
============= parse tree =======================
============= parse tree =======================
============= parse tree =======================
============= parse tree =======================
============= parse tree =======================
============= parse tree =======================
USE_ANIMACY_LIST off
USE_ANIMACY_LIST off
USE_ANIMACY_LIST off
USE_ANIMACY_LIST off
USE_ANIMACY_LIST off
USE_ANIMACY_LIST off
USE_ANIMACY_LIST off
USE_ANIMACY_LIST off
USE_ANIMACY_LIST off
USE_ANIMACY_LIST off
USE_ANIMACY_LIST off
USE_ANIMACY_LIST off
USE_ANIMACY_LIST off
USE_ANIMACY_LIST off
USE_ANIMACY_LIST off
Optimal so far is:  batch size: 64   gain:  0.24
Optimal so far is:  batch size: 128   gain:  0.36
Optimal so far is:  batch size: 256   gain:  0.48
Optimal so far is:  batch size: 512   gain:  0.60
Optimal so far is:  batch size: 1024   gain:  0.72
Optimal so far is:  batch size: 2048   gain:  0.84
Optimal so far is:  batch size: 4096   gain:  0.96
Optimal so far is:  batch size: 8192   gain:  1.08
Optimal so far is:  batch size: 16384   gain:  1.20
Optimal so far is:  batch size: 32768   gain:  1.32
Optimal so far is:  batch size: 65536   gain:  1.44
Optimal so far is:  batch size: 131072   gain:  1.56
Optimal so far is:  batch size: 262144   gain:  1.68
The possible actions are [ACTION_3, ACTION_4, ACTION_1, ACTION_2].Enter a line with only a period to quit
The possible actions are [ACTION_2, ACTION_3, ACTION_4, ACTION_1].Enter a line with only a period to quit
The possible actions are [ACTION_4, ACTION_1, ACTION_2, ACTION_3].Enter a line with only a period to quit
The possible actions are [ACTION_1, ACTION_2, ACTION_3, ACTION_4].Enter a line with only a period to quit
The possible actions are [ACTION_2, ACTION_3, ACTION_1, ACTION_4].Enter a line with only a period to quit
The possible actions are [ACTION_3, ACTION_4, ACTION_1, ACTION_2].Enter a line with only a period to quit
The possible actions are [ACTION_4, ACTION_1, ACTION_3, ACTION_2].Enter a line with only a period to quit
The possible actions are [ACTION_1, ACTION_4, ACTION_2, ACTION_3].Enter a line with only a period to quit
The possible actions are [ACTION_3, ACTION_1, ACTION_2, ACTION_4].Enter a line with only a period to quit
The possible actions are [ACTION_2, ACTION_1, ACTION_3, ACTION_4].Enter a line with only a period to quit
The possible actions are [ACTION_4, ACTION_2, ACTION_3, ACTION_1].Enter a line with only a period to quit
The possible actions are [ACTION_2, ACTION_4, ACTION_3, ACTION_1].Enter a line with only a period to quit
The possible actions are [ACTION_3, ACTION_1, ACTION_4, ACTION_2].Enter a line with only a period to quit
Test cannot check for cycles in tree format (classes not available)
Test cannot check for cycles in tree format (classes not available)
Test cannot check for cycles in tree format (classes not available)
Test cannot check for cycles in tree format (classes not available)
Test cannot check for cycles in tree format (classes not available)
Test cannot check for cycles in tree format (classes not available)
Test cannot check for cycles in tree format (classes not available)
Test cannot check for cycles in tree format (classes not available)
Test cannot check for cycles in tree format (classes not available)
Test cannot check for cycles in tree format (classes not available)
Test cannot check for cycles in tree format (classes not available)
Test cannot check for cycles in tree format (classes not available)
Test cannot check for cycles in tree format (classes not available)
Test cannot check for cycles in tree format (classes not available)
Test cannot check for cycles in tree format (classes not available)
-language [en|zh|en-sd|zh-sd]:	 (Universal English Dependencies, Universal Chinese Dependencies, English Stanford Dependencies, Chinese Stanford Dependencies)
-language [en|zh|en-sd|zh-sd]:	 (Universal English Dependencies, Universal Chinese Dependencies, English Stanford Dependencies, Chinese Stanford Dependencies)
-language [en|zh|en-sd|zh-sd]:	 (Universal English Dependencies, Universal Chinese Dependencies, English Stanford Dependencies, Chinese Stanford Dependencies)
-language [en|zh|en-sd|zh-sd]:	 (Universal English Dependencies, Universal Chinese Dependencies, English Stanford Dependencies, Chinese Stanford Dependencies)
-language [en|zh|en-sd|zh-sd]:	 (Universal English Dependencies, Universal Chinese Dependencies, English Stanford Dependencies, Chinese Stanford Dependencies)
-language [en|zh|en-sd|zh-sd]:	 (Universal English Dependencies, Universal Chinese Dependencies, English Stanford Dependencies, Chinese Stanford Dependencies)
-language [en|zh|en-sd|zh-sd]:	 (Universal English Dependencies, Universal Chinese Dependencies, English Stanford Dependencies, Chinese Stanford Dependencies)
-language [en|zh|en-sd|zh-sd]:	 (Universal English Dependencies, Universal Chinese Dependencies, English Stanford Dependencies, Chinese Stanford Dependencies)
-language [en|zh|en-sd|zh-sd]:	 (Universal English Dependencies, Universal Chinese Dependencies, English Stanford Dependencies, Chinese Stanford Dependencies)
-language [en|zh|en-sd|zh-sd]:	 (Universal English Dependencies, Universal Chinese Dependencies, English Stanford Dependencies, Chinese Stanford Dependencies)
-language [en|zh|en-sd|zh-sd]:	 (Universal English Dependencies, Universal Chinese Dependencies, English Stanford Dependencies, Chinese Stanford Dependencies)
-language [en|zh|en-sd|zh-sd]:	 (Universal English Dependencies, Universal Chinese Dependencies, English Stanford Dependencies, Chinese Stanford Dependencies)
-language [en|zh|en-sd|zh-sd]:	 (Universal English Dependencies, Universal Chinese Dependencies, English Stanford Dependencies, Chinese Stanford Dependencies)
-language [en|zh|en-sd|zh-sd]:	 (Universal English Dependencies, Universal Chinese Dependencies, English Stanford Dependencies, Chinese Stanford Dependencies)
-language [en|zh|en-sd|zh-sd]:	 (Universal English Dependencies, Universal Chinese Dependencies, English Stanford Dependencies, Chinese Stanford Dependencies)
New connection with client# 2 at 2022-01-01 12:30:21
New connection with client# 3 at 2022-01-02 09:15:42
New connection with client# 4 at 2022-01-03 14:20:33
New connection with client# 5 at 2022-01-04 16:45:18
New connection with client# 6 at 2022-01-05 08:10:55
New connection with client# 7 at 2022-01-06 17:55:29
New connection with client# 8 at 2022-01-07 13:25:40
New connection with client# 9 at 2022-01-08 11:40:12
New connection with client# 10 at 2022-01-09 14:05:51
New connection with client# 11 at 2022-01-10 15:30:03
New connection with client# 12 at 2022-01-11 09:55:24
New connection with client# 13 at 2022-01-12 12:18:09
New connection with client# 14 at 2022-01-13 16:47:58
-conllx:		Output dependencies in CoNLL format.
-conllx:		Output dependencies in CoNLL format.
-conllx:		Output dependencies in CoNLL format.
-conllx:		Output dependencies in CoNLL format.
-conllx:		Output dependencies in CoNLL format.
-conllx:		Output dependencies in CoNLL format.
-conllx:		Output dependencies in CoNLL format.
-conllx:		Output dependencies in CoNLL format.
-conllx:		Output dependencies in CoNLL format.
-conllx:		Output dependencies in CoNLL format.
-conllx:		Output dependencies in CoNLL format.
-conllx:		Output dependencies in CoNLL format.
-conllx:		Output dependencies in CoNLL format.
-conllx:		Output dependencies in CoNLL format.
-conllx:		Output dependencies in CoNLL format.
-sentFile <FILE>:	Parse and convert sentences from <FILE>. Only implemented for English.
-sentFile <FILE>:	Parse and convert sentences from <FILE>. Only implemented for English.
-sentFile <FILE>:	Parse and convert sentences from <FILE>. Only implemented for English.
-sentFile <FILE>:	Parse and convert sentences from <FILE>. Only implemented for English.
-sentFile <FILE>:	Parse and convert sentences from <FILE>. Only implemented for English.
-sentFile <FILE>:	Parse and convert sentences from <FILE>. Only implemented for English.
-sentFile <FILE>:	Parse and convert sentences from <FILE>. Only implemented for English.
-sentFile <FILE>:	Parse and convert sentences from <FILE>. Only implemented for English.
-sentFile <FILE>:	Parse and convert sentences from <FILE>. Only implemented for English.
-sentFile <FILE>:	Parse and convert sentences from <FILE>. Only implemented for English.
-sentFile <FILE>:	Parse and convert sentences from <FILE>. Only implemented for English.
-sentFile <FILE>:	Parse and convert sentences from <FILE>. Only implemented for English.
-sentFile <FILE>:	Parse and convert sentences from <FILE>. Only implemented for English.
-sentFile <FILE>:	Parse and convert sentences from <FILE>. Only implemented for English.
-sentFile <FILE>:	Parse and convert sentences from <FILE>. Only implemented for English.
adding new phrases
adding new phrases
adding new phrases
adding new phrases
adding new phrases
adding new phrases
adding new phrases
adding new phrases
adding new phrases
adding new phrases
adding new phrases
adding new phrases
adding new phrases
adding new phrases
adding new phrases
Testing with batch size:  2
Testing with batch size:  3
Testing with batch size:  4
Testing with batch size:  5
Testing with batch size:  6
Testing with batch size:  7
Testing with batch size:  8
Testing with batch size:  9
Testing with batch size:  10
Testing with batch size:  11
Testing with batch size:  12
Testing with batch size:  13
Testing with batch size:  14
-treeFile <FILE>:	Convert from constituency trees in <FILE>
-treeFile <FILE>:	Convert from constituency trees in <FILE>
-treeFile <FILE>:	Convert from constituency trees in <FILE>
-treeFile <FILE>:	Convert from constituency trees in <FILE>
-treeFile <FILE>:	Convert from constituency trees in <FILE>
-treeFile <FILE>:	Convert from constituency trees in <FILE>
-treeFile <FILE>:	Convert from constituency trees in <FILE>
-treeFile <FILE>:	Convert from constituency trees in <FILE>
-treeFile <FILE>:	Convert from constituency trees in <FILE>
-treeFile <FILE>:	Convert from constituency trees in <FILE>
-treeFile <FILE>:	Convert from constituency trees in <FILE>
-treeFile <FILE>:	Convert from constituency trees in <FILE>
-treeFile <FILE>:	Convert from constituency trees in <FILE>
-treeFile <FILE>:	Convert from constituency trees in <FILE>
-treeFile <FILE>:	Convert from constituency trees in <FILE>
Finished processing text
Finished processing text
Finished processing text
Finished processing text
Finished processing text
Finished processing text
Finished processing text
Finished processing text
Finished processing text
Finished processing text
Finished processing text
Finished processing text
Finished processing text
Finished processing text
Finished processing text
-collapsedTree:	Generate collapsed-tree dependencies, deprecated.
-collapsedTree:	Generate collapsed-tree dependencies, deprecated.
-collapsedTree:	Generate collapsed-tree dependencies, deprecated.
-collapsedTree:	Generate collapsed-tree dependencies, deprecated.
-collapsedTree:	Generate collapsed-tree dependencies, deprecated.
-collapsedTree:	Generate collapsed-tree dependencies, deprecated.
-collapsedTree:	Generate collapsed-tree dependencies, deprecated.
-collapsedTree:	Generate collapsed-tree dependencies, deprecated.
-collapsedTree:	Generate collapsed-tree dependencies, deprecated.
-collapsedTree:	Generate collapsed-tree dependencies, deprecated.
-collapsedTree:	Generate collapsed-tree dependencies, deprecated.
-collapsedTree:	Generate collapsed-tree dependencies, deprecated.
-collapsedTree:	Generate collapsed-tree dependencies, deprecated.
-collapsedTree:	Generate collapsed-tree dependencies, deprecated.
-collapsedTree:	Generate collapsed-tree dependencies, deprecated.
written the output to result.csv
written the output to report.docx
written the output to data.json
written the output to output.txt
written the output to output.xml
written the output to summary.txt
written the output to output.xlsx
written the output to output.png
written the output to output.jpg
written the output to log.txt
written the output to result.csv
written the output to report.docx
written the output to data.json
-CCprocessed:	Generate CC-processed dependencies, deprecated.
-CCprocessed:	Generate CC-processed dependencies, deprecated.
-CCprocessed:	Generate CC-processed dependencies, deprecated.
-CCprocessed:	Generate CC-processed dependencies, deprecated.
-CCprocessed:	Generate CC-processed dependencies, deprecated.
-CCprocessed:	Generate CC-processed dependencies, deprecated.
-CCprocessed:	Generate CC-processed dependencies, deprecated.
-CCprocessed:	Generate CC-processed dependencies, deprecated.
-CCprocessed:	Generate CC-processed dependencies, deprecated.
-CCprocessed:	Generate CC-processed dependencies, deprecated.
-CCprocessed:	Generate CC-processed dependencies, deprecated.
-CCprocessed:	Generate CC-processed dependencies, deprecated.
-CCprocessed:	Generate CC-processed dependencies, deprecated.
-CCprocessed:	Generate CC-processed dependencies, deprecated.
-CCprocessed:	Generate CC-processed dependencies, deprecated.
val    ,    function after 500 ms
val    ,    function after 2000 ms
val    ,    function after 750 ms
val    ,    function after 1500 ms
val    ,    function after 3000 ms
val    ,    function after 1250 ms
val    ,    function after 2500 ms
val    ,    function after 1750 ms
val    ,    function after 2250 ms
val    ,    function after 2750 ms
val    ,    function after 3500 ms
val    ,    function after 1750 ms
val    ,    function after 4000 ms
-collapsed:		Generate collapsed dependencies, deprecated.
-collapsed:		Generate collapsed dependencies, deprecated.
-collapsed:		Generate collapsed dependencies, deprecated.
-collapsed:		Generate collapsed dependencies, deprecated.
-collapsed:		Generate collapsed dependencies, deprecated.
-collapsed:		Generate collapsed dependencies, deprecated.
-collapsed:		Generate collapsed dependencies, deprecated.
-collapsed:		Generate collapsed dependencies, deprecated.
-collapsed:		Generate collapsed dependencies, deprecated.
-collapsed:		Generate collapsed dependencies, deprecated.
-collapsed:		Generate collapsed dependencies, deprecated.
-collapsed:		Generate collapsed dependencies, deprecated.
-collapsed:		Generate collapsed dependencies, deprecated.
-collapsed:		Generate collapsed dependencies, deprecated.
-collapsed:		Generate collapsed dependencies, deprecated.
all seed words are banana
all seed words are cherry
all seed words are date
all seed words are elderberry
all seed words are fig
all seed words are grape
all seed words are honeydew
all seed words are mango
all seed words are nectarine
all seed words are orange
all seed words are peach
all seed words are quince
all seed words are raspberry
-enhanced++:	Generate enhanced++ dependencies (default), currently only implemented for English UD.
-enhanced++:	Generate enhanced++ dependencies (default), currently only implemented for English UD.
-enhanced++:	Generate enhanced++ dependencies (default), currently only implemented for English UD.
-enhanced++:	Generate enhanced++ dependencies (default), currently only implemented for English UD.
-enhanced++:	Generate enhanced++ dependencies (default), currently only implemented for English UD.
-enhanced++:	Generate enhanced++ dependencies (default), currently only implemented for English UD.
-enhanced++:	Generate enhanced++ dependencies (default), currently only implemented for English UD.
-enhanced++:	Generate enhanced++ dependencies (default), currently only implemented for English UD.
-enhanced++:	Generate enhanced++ dependencies (default), currently only implemented for English UD.
-enhanced++:	Generate enhanced++ dependencies (default), currently only implemented for English UD.
-enhanced++:	Generate enhanced++ dependencies (default), currently only implemented for English UD.
-enhanced++:	Generate enhanced++ dependencies (default), currently only implemented for English UD.
-enhanced++:	Generate enhanced++ dependencies (default), currently only implemented for English UD.
-enhanced++:	Generate enhanced++ dependencies (default), currently only implemented for English UD.
-enhanced++:	Generate enhanced++ dependencies (default), currently only implemented for English UD.
Starting to process text
Starting to process text
Starting to process text
Starting to process text
Starting to process text
Starting to process text
Starting to process text
Starting to process text
Starting to process text
Starting to process text
Starting to process text
Starting to process text
Starting to process text
Starting to process text
Starting to process text
-enhanced:		Generate enhanced dependencies, currently only implemented for English UD.
-enhanced:		Generate enhanced dependencies, currently only implemented for English UD.
-enhanced:		Generate enhanced dependencies, currently only implemented for English UD.
-enhanced:		Generate enhanced dependencies, currently only implemented for English UD.
-enhanced:		Generate enhanced dependencies, currently only implemented for English UD.
-enhanced:		Generate enhanced dependencies, currently only implemented for English UD.
-enhanced:		Generate enhanced dependencies, currently only implemented for English UD.
-enhanced:		Generate enhanced dependencies, currently only implemented for English UD.
-enhanced:		Generate enhanced dependencies, currently only implemented for English UD.
-enhanced:		Generate enhanced dependencies, currently only implemented for English UD.
-enhanced:		Generate enhanced dependencies, currently only implemented for English UD.
-enhanced:		Generate enhanced dependencies, currently only implemented for English UD.
-enhanced:		Generate enhanced dependencies, currently only implemented for English UD.
-enhanced:		Generate enhanced dependencies, currently only implemented for English UD.
-enhanced:		Generate enhanced dependencies, currently only implemented for English UD.
scoref-trainOn test example 2/25 = 8.0%
scoref-trainOn test example 3/25 = 12.0%
scoref-trainOn test example 4/25 = 16.0%
scoref-trainOn test example 5/25 = 20.0%
scoref-trainOn test example 6/25 = 24.0%
scoref-trainOn test example 7/25 = 28.0%
scoref-trainOn test example 8/25 = 32.0%
scoref-trainOn test example 9/25 = 36.0%
scoref-trainOn test example 10/25 = 40.0%
scoref-trainOn test example 11/25 = 44.0%
scoref-trainOn test example 12/25 = 48.0%
scoref-trainOn test example 13/25 = 52.0%
scoref-trainOn test example 14/25 = 56.0%
-basic:		Generate basic dependencies.
-basic:		Generate basic dependencies.
-basic:		Generate basic dependencies.
-basic:		Generate basic dependencies.
-basic:		Generate basic dependencies.
-basic:		Generate basic dependencies.
-basic:		Generate basic dependencies.
-basic:		Generate basic dependencies.
-basic:		Generate basic dependencies.
-basic:		Generate basic dependencies.
-basic:		Generate basic dependencies.
-basic:		Generate basic dependencies.
-basic:		Generate basic dependencies.
-basic:		Generate basic dependencies.
-basic:		Generate basic dependencies.
scoref-trainTesting...
scoref-trainTesting...
scoref-trainTesting...
scoref-trainTesting...
scoref-trainTesting...
scoref-trainTesting...
scoref-trainTesting...
scoref-trainTesting...
scoref-trainTesting...
scoref-trainTesting...
scoref-trainTesting...
scoref-trainTesting...
scoref-trainTesting...
scoref-trainTesting...
scoref-trainTesting...
scoref-trainBuilding test set...
scoref-trainBuilding test set...
scoref-trainBuilding test set...
scoref-trainBuilding test set...
scoref-trainBuilding test set...
scoref-trainBuilding test set...
scoref-trainBuilding test set...
scoref-trainBuilding test set...
scoref-trainBuilding test set...
scoref-trainBuilding test set...
scoref-trainBuilding test set...
scoref-trainBuilding test set...
scoref-trainBuilding test set...
scoref-trainBuilding test set...
scoref-trainBuilding test set...
additional seed words are orange
additional seed words are banana
additional seed words are cherry
additional seed words are grape
additional seed words are pineapple
additional seed words are watermelon
additional seed words are strawberry
additional seed words are lemon
additional seed words are peach
additional seed words are mango
additional seed words are plum
additional seed words are kiwi
additional seed words are pear
additional seed words are blueberry
scoref-trainReading test data...
scoref-trainReading test data...
scoref-trainReading test data...
scoref-trainReading test data...
scoref-trainReading test data...
scoref-trainReading test data...
scoref-trainReading test data...
scoref-trainReading test data...
scoref-trainReading test data...
scoref-trainReading test data...
scoref-trainReading test data...
scoref-trainReading test data...
scoref-trainReading test data...
scoref-trainReading test data...
scoref-trainReading test data...
Usage: java com.example.MyClass
Usage: java org.apache.commons.math.linear.RealMatrix
Usage: java javax.servlet.http.HttpServlet
Usage: java java.util.ArrayList
Usage: java org.springframework.boot.SpringApplication
Usage: java org.hibernate.SessionFactory
Usage: java sun.security.ssl.SSLSocketFactory
Usage: java com.google.gson.Gson
Usage: java org.junit.jupiter.api.Test
Usage: java org.slf4j.LoggerFactory
Usage: java org.apache.logging.log4j.LogManager
Usage: java org.jsoup.Jsoup
Usage: java javax.xml.bind.JAXBContext
adding 2 for label B
adding 3 for label C
adding 4 for label D
adding 5 for label E
adding 6 for label F
adding 7 for label G
adding 8 for label H
adding 9 for label I
adding 10 for label J
adding 11 for label K
adding 12 for label L
adding 13 for label M
adding 14 for label N
scoref-trainOn train example 2/20 = 10%
scoref-trainOn train example 3/20 = 15%
scoref-trainOn train example 4/20 = 20%
scoref-trainOn train example 5/20 = 25%
scoref-trainOn train example 6/20 = 30%
scoref-trainOn train example 7/20 = 35%
scoref-trainOn train example 8/20 = 40%
scoref-trainOn train example 9/20 = 45%
scoref-trainOn train example 10/20 = 50%
scoref-trainOn train example 11/20 = 55%
scoref-trainOn train example 12/20 = 60%
scoref-trainOn train example 13/20 = 65%
scoref-trainOn train example 14/20 = 70%
scoref-trainBuilding train set...
scoref-trainBuilding train set...
scoref-trainBuilding train set...
scoref-trainBuilding train set...
scoref-trainBuilding train set...
scoref-trainBuilding train set...
scoref-trainBuilding train set...
scoref-trainBuilding train set...
scoref-trainBuilding train set...
scoref-trainBuilding train set...
scoref-trainBuilding train set...
scoref-trainBuilding train set...
scoref-trainBuilding train set...
scoref-trainBuilding train set...
scoref-trainBuilding train set...
Horrible error: ArrayIndexOutOfBoundsException
Horrible error: ClassCastException
Horrible error: FileNotFoundException
Horrible error: IOException
Horrible error: ArithmeticException
Horrible error: IllegalArgumentException
Horrible error: IllegalStateException
Horrible error: NoSuchElementException
Horrible error: UnsupportedOperationException
Horrible error: StackOverflowError
Horrible error: OutOfMemoryError
Horrible error: InterruptedException
Horrible error: SQLException
Caught IOException outputting data to file: Permission denied
Caught IOException outputting data to file: Disk space full
Caught IOException outputting data to file: Invalid file format
Caught IOException outputting data to file: Network connection lost
Caught IOException outputting data to file: Input/output error
Caught IOException outputting data to file: Access denied
Caught IOException outputting data to file: File already exists
Caught IOException outputting data to file: File corrupted
Caught IOException outputting data to file: Invalid path
Caught IOException outputting data to file: File is locked
Caught IOException outputting data to file: Insufficient privileges
Caught IOException outputting data to file: File system error
Caught IOException outputting data to file: File in use
Suggesting phrases in test
Suggesting phrases in test
Suggesting phrases in test
Suggesting phrases in test
Suggesting phrases in test
Suggesting phrases in test
Suggesting phrases in test
Suggesting phrases in test
Suggesting phrases in test
Suggesting phrases in test
Suggesting phrases in test
Suggesting phrases in test
Suggesting phrases in test
Suggesting phrases in test
Suggesting phrases in test
scoref-trainNum anaphoricity examples 89 positive, 248 total
scoref-trainNum anaphoricity examples 45 positive, 175 total
scoref-trainNum anaphoricity examples 72 positive, 212 total
scoref-trainNum anaphoricity examples 34 positive, 187 total
scoref-trainNum anaphoricity examples 56 positive, 194 total
scoref-trainNum anaphoricity examples 78 positive, 233 total
scoref-trainNum anaphoricity examples 27 positive, 162 total
scoref-trainNum anaphoricity examples 69 positive, 257 total
scoref-trainNum anaphoricity examples 43 positive, 206 total
scoref-trainNum anaphoricity examples 51 positive, 218 total
scoref-trainNum anaphoricity examples 37 positive, 185 total
scoref-trainNum anaphoricity examples 64 positive, 219 total
scoref-trainNum anaphoricity examples 53 positive, 189 total
Batch size of: 16   874,529,312,46
Batch size of: 64   356,674,54,980
Batch size of: 128   782,893,76,337
Batch size of: 256   689,213,456,745
Batch size of: 512   241,832,103,521
Batch size of: 8   936,486,567,674
Batch size of: 128   287,876,34,218
Batch size of: 64   672,543,87,127
Batch size of: 256   427,124,167,993
Batch size of: 32   543,879,821,344
Batch size of: 512   680,321,299,505
Batch size of: 16   283,425,78,990
Batch size of: 8   572,661,445,234
scoref-trainWriting models...
scoref-trainWriting models...
scoref-trainWriting models...
scoref-trainWriting models...
scoref-trainWriting models...
scoref-trainWriting models...
scoref-trainWriting models...
scoref-trainWriting models...
scoref-trainWriting models...
scoref-trainWriting models...
scoref-trainWriting models...
scoref-trainWriting models...
scoref-trainWriting models...
scoref-trainWriting models...
scoref-trainWriting models...
Bung tree caused below dump. Continuing....
Bung tree caused below dump. Continuing....
Bung tree caused below dump. Continuing....
Bung tree caused below dump. Continuing....
Bung tree caused below dump. Continuing....
Bung tree caused below dump. Continuing....
Bung tree caused below dump. Continuing....
Bung tree caused below dump. Continuing....
Bung tree caused below dump. Continuing....
Bung tree caused below dump. Continuing....
Bung tree caused below dump. Continuing....
Bung tree caused below dump. Continuing....
Bung tree caused below dump. Continuing....
Bung tree caused below dump. Continuing....
Bung tree caused below dump. Continuing....
Caught IOException outputing List to file: Access denied
Caught IOException outputing List to file: Disk full
Caught IOException outputing List to file: Connection lost
Caught IOException outputing List to file: Invalid input
Caught IOException outputing List to file: Network error
Caught IOException outputing List to file: Timeout
Caught IOException outputing List to file: File corrupted
Caught IOException outputing List to file: Insufficient permissions
Caught IOException outputing List to file: Invalid file format
Caught IOException outputing List to file: Out of memory
Caught IOException outputing List to file: Invalid path
Caught IOException outputing List to file: Disk error
Caught IOException outputing List to file: File locked
scoref-trainOn epoch: 2 / 10, document: 2 / 100
scoref-trainOn epoch: 3 / 10, document: 3 / 100
scoref-trainOn epoch: 4 / 10, document: 4 / 100
scoref-trainOn epoch: 5 / 10, document: 5 / 100
scoref-trainOn epoch: 6 / 10, document: 6 / 100
scoref-trainOn epoch: 7 / 10, document: 7 / 100
scoref-trainOn epoch: 8 / 10, document: 8 / 100
scoref-trainOn epoch: 9 / 10, document: 9 / 100
scoref-trainOn epoch: 10 / 10, document: 10 / 100
scoref-trainOn epoch: 11 / 10, document: 11 / 100
scoref-trainOn epoch: 12 / 10, document: 12 / 100
scoref-trainOn epoch: 13 / 10, document: 13 / 100
scoref-trainOn epoch: 14 / 10, document: 14 / 100
Is semi:     true
Is semi:     true
Is semi:     false
Is semi:     true
Is semi:     false
Is semi:     true
Is semi:     false
Is semi:     true
Is semi:     true
Is semi:     false
Is semi:     false
Is semi:     true
Is semi:     false
scoref-trainTraining...
scoref-trainTraining...
scoref-trainTraining...
scoref-trainTraining...
scoref-trainTraining...
scoref-trainTraining...
scoref-trainTraining...
scoref-trainTraining...
scoref-trainTraining...
scoref-trainTraining...
scoref-trainTraining...
scoref-trainTraining...
scoref-trainTraining...
scoref-trainTraining...
scoref-trainTraining...
Found table products
Found table orders
Found table customers
Found table addresses
Found table categories
Found table payments
Found table invoices
Found table reviews
Found table messages
Found table logs
Found table settings
Found table notifications
Found table transactions
scoref-trainReading train data...
scoref-trainReading train data...
scoref-trainReading train data...
scoref-trainReading train data...
scoref-trainReading train data...
scoref-trainReading train data...
scoref-trainReading train data...
scoref-trainReading train data...
scoref-trainReading train data...
scoref-trainReading train data...
scoref-trainReading train data...
scoref-trainReading train data...
scoref-trainReading train data...
scoref-trainReading train data...
scoref-trainReading train data...
Is positive: false
Is positive: true
Is positive: false
Is positive: true
Is positive: false
Is positive: true
Is positive: false
Is positive: true
Is positive: false
Is positive: true
Is positive: false
Is positive: true
Is positive: false
scoref-trainReading compression...
scoref-trainReading compression...
scoref-trainReading compression...
scoref-trainReading compression...
scoref-trainReading compression...
scoref-trainReading compression...
scoref-trainReading compression...
scoref-trainReading compression...
scoref-trainReading compression...
scoref-trainReading compression...
scoref-trainReading compression...
scoref-trainReading compression...
scoref-trainReading compression...
scoref-trainReading compression...
scoref-trainReading compression...
Is negative: false
Is negative: false
Is negative: true
Is negative: true
Is negative: false
Is negative: true
Is negative: false
Is negative: true
Is negative: false
Is negative: false
Is negative: true
Is negative: true
Is negative: false
scoref.trainB3 F1 score on training: 0.76
scoref.trainB3 F1 score on training: 0.92
scoref.trainB3 F1 score on training: 0.83
scoref.trainB3 F1 score on training: 0.78
scoref.trainB3 F1 score on validate: 0.89
scoref.trainB3 F1 score on validate: 0.94
scoref.trainB3 F1 score on validate: 0.82
scoref.trainB3 F1 score on validate: 0.91
scoref.trainB3 F1 score on validate: 0.87
scoref.trainB3 F1 score on training: 0.81
scoref.trainB3 F1 score on training: 0.77
scoref.trainB3 F1 score on training: 0.86
scoref.trainB3 F1 score on training: 0.93
Condition Number of: 5
Condition Number of: 20
Condition Number of: 2
Condition Number of: 15
Condition Number of: 8
Condition Number of: 12
Condition Number of: 6
Condition Number of: 18
Condition Number of: 3
Condition Number of: 9
Condition Number of: 4
Condition Number of: 16
Condition Number of: 7
scoref.trainTraining cost: 79
scoref.trainTraining cost: 115
scoref.trainTraining cost: 92
scoref.trainTraining cost: 137
scoref.trainTraining cost: 114
scoref.trainTraining cost: 109
scoref.trainTraining cost: 86
scoref.trainTraining cost: 101
scoref.trainTraining cost: 118
scoref.trainTraining cost: 95
scoref.trainTraining cost: 130
scoref.trainTraining cost: 77
scoref.trainTraining cost: 84
deleting table orders
deleting table products
deleting table customers
deleting table categories
deleting table transactions
deleting table inventory
deleting table reviews
deleting table payments
deleting table addresses
deleting table messages
deleting table logs
deleting table settings
deleting table notifications
scoref.trainFeatures hit rate: 0.91
scoref.trainFeatures hit rate: 0.77
scoref.trainFeatures hit rate: 0.64
scoref.trainFeatures hit rate: 0.93
scoref.trainFeatures hit rate: 0.82
scoref.trainFeatures hit rate: 0.69
scoref.trainFeatures hit rate: 0.79
scoref.trainFeatures hit rate: 0.96
scoref.trainFeatures hit rate: 0.72
scoref.trainFeatures hit rate: 0.88
scoref.trainFeatures hit rate: 0.61
scoref.trainFeatures hit rate: 0.75
scoref.trainFeatures hit rate: 0.94
Evaluating Hessian Product
Evaluating Hessian Product
Evaluating Hessian Product
Evaluating Hessian Product
Evaluating Hessian Product
Evaluating Hessian Product
Evaluating Hessian Product
Evaluating Hessian Product
Evaluating Hessian Product
Evaluating Hessian Product
Evaluating Hessian Product
Evaluating Hessian Product
Evaluating Hessian Product
Evaluating Hessian Product
Evaluating Hessian Product
scoref.trainScore hit rate: 0.76
scoref.trainScore hit rate: 0.92
scoref.trainScore hit rate: 0.65
scoref.trainScore hit rate: 0.78
scoref.trainScore hit rate: 0.81
scoref.trainScore hit rate: 0.93
scoref.trainScore hit rate: 0.72
scoref.trainScore hit rate: 0.87
scoref.trainScore hit rate: 0.93
scoref.trainScore hit rate: 0.69
scoref.trainScore hit rate: 0.78
scoref.trainScore hit rate: 0.91
scoref.trainScore hit rate: 0.82
14 percent complete
92 percent complete
33 percent complete
58 percent complete
79 percent complete
42 percent complete
85 percent complete
21 percent complete
76 percent complete
39 percent complete
63 percent complete
98 percent complete
51 percent complete
Number of datums per label: 200
Number of datums per label: 300
Number of datums per label: 400
Number of datums per label: 500
Number of datums per label: 600
Number of datums per label: 700
Number of datums per label: 800
Number of datums per label: 900
Number of datums per label: 1000
Number of datums per label: 1100
Number of datums per label: 1200
Number of datums per label: 1300
Number of datums per label: 1400
scoref.trainCost hit rate: 0.92
scoref.trainCost hit rate: 0.79
scoref.trainCost hit rate: 0.94
scoref.trainCost hit rate: 0.81
scoref.trainCost hit rate: 0.89
scoref.trainCost hit rate: 0.76
scoref.trainCost hit rate: 0.97
scoref.trainCost hit rate: 0.84
scoref.trainCost hit rate: 0.91
scoref.trainCost hit rate: 0.78
scoref.trainCost hit rate: 0.95
scoref.trainCost hit rate: 0.83
scoref.trainCost hit rate: 0.88
0 argument constructor to Main is not public.
2 argument constructor to XYZ is not public.
3 argument constructor to ABC is not public.
1 argument constructor to PQR is not public.
0 argument constructor to ClassName is not public.
2 argument constructor to ClassName is not public.
3 argument constructor to ClassName is not public.
1 argument constructor to Dependency is not public.
0 argument constructor to Dependency is not public.
2 argument constructor to Dependency is not public.
3 argument constructor to Dependency is not public.
1 argument constructor to Reader is not public.
0 argument constructor to Reader is not public.
2 argument constructor to Reader is not public.
Making sure that the stochastic derivatives are ok.
Making sure that the stochastic derivatives are ok.
Making sure that the stochastic derivatives are ok.
Making sure that the stochastic derivatives are ok.
Making sure that the stochastic derivatives are ok.
Making sure that the stochastic derivatives are ok.
Making sure that the stochastic derivatives are ok.
Making sure that the stochastic derivatives are ok.
Making sure that the stochastic derivatives are ok.
Making sure that the stochastic derivatives are ok.
Making sure that the stochastic derivatives are ok.
Making sure that the stochastic derivatives are ok.
Making sure that the stochastic derivatives are ok.
Making sure that the stochastic derivatives are ok.
Making sure that the stochastic derivatives are ok.
About to calculate the full derivative and value
About to calculate the full derivative and value
About to calculate the full derivative and value
About to calculate the full derivative and value
About to calculate the full derivative and value
About to calculate the full derivative and value
About to calculate the full derivative and value
About to calculate the full derivative and value
About to calculate the full derivative and value
About to calculate the full derivative and value
About to calculate the full derivative and value
About to calculate the full derivative and value
About to calculate the full derivative and value
About to calculate the full derivative and value
About to calculate the full derivative and value
num random chosen: 50
num random chosen: 75
num random chosen: 30
num random chosen: 62
num random chosen: 88
num random chosen: 42
num random chosen: 21
num random chosen: 55
num random chosen: 85
num random chosen: 17
num random chosen: 47
num random chosen: 93
num random chosen: 28
scoref.trainTime elapsed: 3.27 seconds
scoref.trainTime elapsed: 1.92 seconds
scoref.trainTime elapsed: 4.01 seconds
scoref.trainTime elapsed: 2.78 seconds
scoref.trainTime elapsed: 3.45 seconds
scoref.trainTime elapsed: 2.13 seconds
scoref.trainTime elapsed: 4.29 seconds
scoref.trainTime elapsed: 1.87 seconds
scoref.trainTime elapsed: 2.98 seconds
scoref.trainTime elapsed: 3.16 seconds
scoref.trainTime elapsed: 2.79 seconds
scoref.trainTime elapsed: 4.07 seconds
scoref.trainTime elapsed: 3.52 seconds
Assigning to root (0)
Assigning to root (0)
Assigning to root (0)
Assigning to root (0)
Assigning to root (0)
Assigning to root (0)
Assigning to root (0)
Assigning to root (0)
Assigning to root (0)
Assigning to root (0)
Assigning to root (0)
Assigning to root (0)
Assigning to root (0)
Assigning to root (0)
Assigning to root (0)
number of negative words from lists 5
number of negative words from lists 16
number of negative words from lists 8
number of negative words from lists 12
number of negative words from lists 3
number of negative words from lists 7
number of negative words from lists 9
number of negative words from lists 14
number of negative words from lists 6
number of negative words from lists 11
number of negative words from lists 2
number of negative words from lists 13
number of negative words from lists 4
scoref.trainBest train: 0.92
scoref.trainBest train: 0.78
scoref.trainBest train: 0.95
scoref.trainBest train: 0.88
scoref.trainBest train: 0.91
scoref.trainBest train: 0.83
scoref.trainBest train: 0.97
scoref.trainBest train: 0.76
scoref.trainBest train: 0.89
scoref.trainBest train: 0.94
scoref.trainBest train: 0.99
scoref.trainBest train: 0.82
scoref.trainBest train: 0.65
No calculate method has been specified
No calculate method has been specified
No calculate method has been specified
No calculate method has been specified
No calculate method has been specified
No calculate method has been specified
No calculate method has been specified
No calculate method has been specified
No calculate method has been specified
No calculate method has been specified
No calculate method has been specified
No calculate method has been specified
No calculate method has been specified
No calculate method has been specified
No calculate method has been specified
Making sure that the sum of stochastic gradients equals the full gradient
Making sure that the sum of stochastic gradients equals the full gradient
Making sure that the sum of stochastic gradients equals the full gradient
Making sure that the sum of stochastic gradients equals the full gradient
Making sure that the sum of stochastic gradients equals the full gradient
Making sure that the sum of stochastic gradients equals the full gradient
Making sure that the sum of stochastic gradients equals the full gradient
Making sure that the sum of stochastic gradients equals the full gradient
Making sure that the sum of stochastic gradients equals the full gradient
Making sure that the sum of stochastic gradients equals the full gradient
Making sure that the sum of stochastic gradients equals the full gradient
Making sure that the sum of stochastic gradients equals the full gradient
Making sure that the sum of stochastic gradients equals the full gradient
Making sure that the sum of stochastic gradients equals the full gradient
Making sure that the sum of stochastic gradients equals the full gradient
Attempt to test function on data of prime dimension.  This would involve a batchSize of 1 and may take a very long time.
Attempt to test function on data of prime dimension.  This would involve a batchSize of 1 and may take a very long time.
Attempt to test function on data of prime dimension.  This would involve a batchSize of 1 and may take a very long time.
Attempt to test function on data of prime dimension.  This would involve a batchSize of 1 and may take a very long time.
Attempt to test function on data of prime dimension.  This would involve a batchSize of 1 and may take a very long time.
Attempt to test function on data of prime dimension.  This would involve a batchSize of 1 and may take a very long time.
Attempt to test function on data of prime dimension.  This would involve a batchSize of 1 and may take a very long time.
Attempt to test function on data of prime dimension.  This would involve a batchSize of 1 and may take a very long time.
Attempt to test function on data of prime dimension.  This would involve a batchSize of 1 and may take a very long time.
Attempt to test function on data of prime dimension.  This would involve a batchSize of 1 and may take a very long time.
Attempt to test function on data of prime dimension.  This would involve a batchSize of 1 and may take a very long time.
Attempt to test function on data of prime dimension.  This would involve a batchSize of 1 and may take a very long time.
Attempt to test function on data of prime dimension.  This would involve a batchSize of 1 and may take a very long time.
Attempt to test function on data of prime dimension.  This would involve a batchSize of 1 and may take a very long time.
Attempt to test function on data of prime dimension.  This would involve a batchSize of 1 and may take a very long time.
scoref.trainITERATION 2
scoref.trainITERATION 3
scoref.trainITERATION 4
scoref.trainITERATION 5
scoref.trainITERATION 6
scoref.trainITERATION 7
scoref.trainITERATION 8
scoref.trainITERATION 9
scoref.trainITERATION 10
scoref.trainITERATION 11
scoref.trainITERATION 12
scoref.trainITERATION 13
scoref.trainITERATION 14
Invalid testBatchSize found, testing aborted.  Data size: 500 batchSize: 20
Invalid testBatchSize found, testing aborted.  Data size: 1000 batchSize: 5
Invalid testBatchSize found, testing aborted.  Data size: 2000 batchSize: 15
Invalid testBatchSize found, testing aborted.  Data size: 5000 batchSize: 8
Invalid testBatchSize found, testing aborted.  Data size: 10000 batchSize: 12
Invalid testBatchSize found, testing aborted.  Data size: 50000 batchSize: 3
Invalid testBatchSize found, testing aborted.  Data size: 100000 batchSize: 9
Invalid testBatchSize found, testing aborted.  Data size: 200000 batchSize: 6
Invalid testBatchSize found, testing aborted.  Data size: 500000 batchSize: 4
Invalid testBatchSize found, testing aborted.  Data size: 1000000 batchSize: 8
Invalid testBatchSize found, testing aborted.  Data size: 2000000 batchSize: 5
Invalid testBatchSize found, testing aborted.  Data size: 5000000 batchSize: 10
Invalid testBatchSize found, testing aborted.  Data size: 10000000 batchSize: 2
Invalid testBatchSize found, testing aborted.  Data size: 20000000 batchSize: 7
scoref.trainLoading training data
scoref.trainLoading training data
scoref.trainLoading training data
scoref.trainLoading training data
scoref.trainLoading training data
scoref.trainLoading training data
scoref.trainLoading training data
scoref.trainLoading training data
scoref.trainLoading training data
scoref.trainLoading training data
scoref.trainLoading training data
scoref.trainLoading training data
scoref.trainLoading training data
scoref.trainLoading training data
scoref.trainLoading training data
Attempt to test non stochastic function using StochasticDiffFunctionTester
Attempt to test non stochastic function using StochasticDiffFunctionTester
Attempt to test non stochastic function using StochasticDiffFunctionTester
Attempt to test non stochastic function using StochasticDiffFunctionTester
Attempt to test non stochastic function using StochasticDiffFunctionTester
Attempt to test non stochastic function using StochasticDiffFunctionTester
Attempt to test non stochastic function using StochasticDiffFunctionTester
Attempt to test non stochastic function using StochasticDiffFunctionTester
Attempt to test non stochastic function using StochasticDiffFunctionTester
Attempt to test non stochastic function using StochasticDiffFunctionTester
Attempt to test non stochastic function using StochasticDiffFunctionTester
Attempt to test non stochastic function using StochasticDiffFunctionTester
Attempt to test non stochastic function using StochasticDiffFunctionTester
Attempt to test non stochastic function using StochasticDiffFunctionTester
Attempt to test non stochastic function using StochasticDiffFunctionTester
No tuning set yet
No tuning set yet
No tuning set yet
No tuning set yet
No tuning set yet
No tuning set yet
No tuning set yet
No tuning set yet
No tuning set yet
No tuning set yet
No tuning set yet
No tuning set yet
No tuning set yet
No tuning set yet
No tuning set yet
Responding through SPIED through servlet context!!
Responding through SPIED through servlet context!!
Responding through SPIED through servlet context!!
Responding through SPIED through servlet context!!
Responding through SPIED through servlet context!!
Responding through SPIED through servlet context!!
Responding through SPIED through servlet context!!
Responding through SPIED through servlet context!!
Responding through SPIED through servlet context!!
Responding through SPIED through servlet context!!
Responding through SPIED through servlet context!!
Responding through SPIED through servlet context!!
Responding through SPIED through servlet context!!
Responding through SPIED through servlet context!!
Responding through SPIED through servlet context!!
WARNING: Requested batch size=5 <= 0 !!!
WARNING: Requested batch size=20 <= 0 !!!
WARNING: Requested batch size=15 <= 0 !!!
WARNING: Requested batch size=3 <= 0 !!!
WARNING: Requested batch size=8 <= 0 !!!
WARNING: Requested batch size=12 <= 0 !!!
WARNING: Requested batch size=25 <= 0 !!!
WARNING: Requested batch size=6 <= 0 !!!
WARNING: Requested batch size=18 <= 0 !!!
WARNING: Requested batch size=2 <= 0 !!!
WARNING: Requested batch size=13 <= 0 !!!
WARNING: Requested batch size=7 <= 0 !!!
WARNING: Requested batch size=11 <= 0 !!!
Long name:     CustomerInformation
Long name:     OrderDetails
Long name:     ProductCatalog
Long name:     PaymentTransaction
Long name:     ShippingAddress
Long name:     InvoiceRecord
Long name:     InventoryManagement
Long name:     EmployeeData
Long name:     SalesPerformance
Long name:     CustomerReviews
Long name:     OrderProcessing
Long name:     ProductInventory
Long name:     PaymentGateway
Responding to the request for SPIED
Responding to the request for SPIED
Responding to the request for SPIED
Responding to the request for SPIED
Responding to the request for SPIED
Responding to the request for SPIED
Responding to the request for SPIED
Responding to the request for SPIED
Responding to the request for SPIED
Responding to the request for SPIED
Responding to the request for SPIED
Responding to the request for SPIED
Responding to the request for SPIED
Responding to the request for SPIED
Responding to the request for SPIED
{"okay":false, "reason":"Something bad happened. Contact the author."}
{"okay":false, "reason":"Something bad happened. Contact the author."}
{"okay":false, "reason":"Something bad happened. Contact the author."}
{"okay":false, "reason":"Something bad happened. Contact the author."}
{"okay":false, "reason":"Something bad happened. Contact the author."}
{"okay":false, "reason":"Something bad happened. Contact the author."}
{"okay":false, "reason":"Something bad happened. Contact the author."}
{"okay":false, "reason":"Something bad happened. Contact the author."}
{"okay":false, "reason":"Something bad happened. Contact the author."}
{"okay":false, "reason":"Something bad happened. Contact the author."}
{"okay":false, "reason":"Something bad happened. Contact the author."}
{"okay":false, "reason":"Something bad happened. Contact the author."}
{"okay":false, "reason":"Something bad happened. Contact the author."}
{"okay":false, "reason":"Something bad happened. Contact the author."}
{"okay":false, "reason":"Something bad happened. Contact the author."}
WARNING: Total number of samples=50 is smaller than requested batch size=128!!!
WARNING: Total number of samples=200 is smaller than requested batch size=512!!!
WARNING: Total number of samples=80 is smaller than requested batch size=256!!!
WARNING: Total number of samples=150 is smaller than requested batch size=384!!!
WARNING: Total number of samples=120 is smaller than requested batch size=256!!!
WARNING: Total number of samples=70 is smaller than requested batch size=128!!!
WARNING: Total number of samples=250 is smaller than requested batch size=512!!!
WARNING: Total number of samples=90 is smaller than requested batch size=256!!!
WARNING: Total number of samples=180 is smaller than requested batch size=512!!!
WARNING: Total number of samples=60 is smaller than requested batch size=128!!!
WARNING: Total number of samples=220 is smaller than requested batch size=512!!!
WARNING: Total number of samples=110 is smaller than requested batch size=256!!!
WARNING: Total number of samples=140 is smaller than requested batch size=384!!!
Exiting for Debug
Exiting for Debug
Exiting for Debug
Exiting for Debug
Exiting for Debug
Exiting for Debug
Exiting for Debug
Exiting for Debug
Exiting for Debug
Exiting for Debug
Exiting for Debug
Exiting for Debug
Exiting for Debug
Exiting for Debug
Exiting for Debug
input is 1
input is 2
input is 3
input is 4
input is 5
input is 6
input is 7
input is 8
input is 9
input is A
input is B
input is C
input is D
input is E
Data for GrammaticalRelation loaded as valueOf("dobj"):
Data for GrammaticalRelation loaded as valueOf("nmod"):
Data for GrammaticalRelation loaded as valueOf("iobj"):
Data for GrammaticalRelation loaded as valueOf("obl"):
Data for GrammaticalRelation loaded as valueOf("appos"):
Data for GrammaticalRelation loaded as valueOf("advcl"):
Data for GrammaticalRelation loaded as valueOf("dep"):
Data for GrammaticalRelation loaded as valueOf("amod"):
Data for GrammaticalRelation loaded as valueOf("csubj"):
Data for GrammaticalRelation loaded as valueOf("neg"):
Data for GrammaticalRelation loaded as valueOf("aux"):
Data for GrammaticalRelation loaded as valueOf("conj"):
Data for GrammaticalRelation loaded as valueOf("ccomp"):
Data for GrammaticalRelation loaded as valueOf("mark"):
Loading pretrain wv from  word2vec.bin
Loading pretrain wv from  glove_model
Loading pretrain wv from  embedding_matrix
Loading pretrain wv from  w2v_model
Loading pretrain wv from  fasttext.bin
Loading pretrain wv from  pretrained_vectors
Loading pretrain wv from  word_embeddings.pt
Loading pretrain wv from  w2v.bin
Loading pretrain wv from  embeddings.npz
Loading pretrain wv from  model.npz
Loading pretrain wv from  vectors.pkl
Loading pretrain wv from  pretrained_word2vec.bin
Loading pretrain wv from  word_embeddings.npy
{"okay":false,"reason":"No data provided"}
{"okay":false,"reason":"No data provided"}
{"okay":false,"reason":"No data provided"}
{"okay":false,"reason":"No data provided"}
{"okay":false,"reason":"No data provided"}
{"okay":false,"reason":"No data provided"}
{"okay":false,"reason":"No data provided"}
{"okay":false,"reason":"No data provided"}
{"okay":false,"reason":"No data provided"}
{"okay":false,"reason":"No data provided"}
{"okay":false,"reason":"No data provided"}
{"okay":false,"reason":"No data provided"}
{"okay":false,"reason":"No data provided"}
{"okay":false,"reason":"No data provided"}
{"okay":false,"reason":"No data provided"}
-singleton.predictor.output [output_model_file]: was not specified
-singleton.predictor.output [output_model_file]: was not specified
-singleton.predictor.output [output_model_file]: was not specified
-singleton.predictor.output [output_model_file]: was not specified
-singleton.predictor.output [output_model_file]: was not specified
-singleton.predictor.output [output_model_file]: was not specified
-singleton.predictor.output [output_model_file]: was not specified
-singleton.predictor.output [output_model_file]: was not specified
-singleton.predictor.output [output_model_file]: was not specified
-singleton.predictor.output [output_model_file]: was not specified
-singleton.predictor.output [output_model_file]: was not specified
-singleton.predictor.output [output_model_file]: was not specified
-singleton.predictor.output [output_model_file]: was not specified
-singleton.predictor.output [output_model_file]: was not specified
-singleton.predictor.output [output_model_file]: was not specified
-dcoref.conll2011 [input_CoNLL_corpus]: was not specified
-dcoref.conll2011 [input_CoNLL_corpus]: was not specified
-dcoref.conll2011 [input_CoNLL_corpus]: was not specified
-dcoref.conll2011 [input_CoNLL_corpus]: was not specified
-dcoref.conll2011 [input_CoNLL_corpus]: was not specified
-dcoref.conll2011 [input_CoNLL_corpus]: was not specified
-dcoref.conll2011 [input_CoNLL_corpus]: was not specified
-dcoref.conll2011 [input_CoNLL_corpus]: was not specified
-dcoref.conll2011 [input_CoNLL_corpus]: was not specified
-dcoref.conll2011 [input_CoNLL_corpus]: was not specified
-dcoref.conll2011 [input_CoNLL_corpus]: was not specified
-dcoref.conll2011 [input_CoNLL_corpus]: was not specified
-dcoref.conll2011 [input_CoNLL_corpus]: was not specified
-dcoref.conll2011 [input_CoNLL_corpus]: was not specified
-dcoref.conll2011 [input_CoNLL_corpus]: was not specified
GET SPIED query from 172.16.0.1
GET SPIED query from 10.0.0.1
GET SPIED query from 192.168.1.1
GET SPIED query from 172.16.1.1
GET SPIED query from 10.0.1.1
GET SPIED query from 192.168.2.1
GET SPIED query from 172.16.2.1
GET SPIED query from 10.0.2.1
GET SPIED query from 192.168.3.1
GET SPIED query from 172.16.3.1
GET SPIED query from 10.0.3.1
GET SPIED query from 192.168.4.1
GET SPIED query from 172.16.4.1
WARNING: Total number of samples=2000 is smaller than requested tuning sample size=1000!!!
WARNING: Total number of samples=5000 is smaller than requested tuning sample size=2500!!!
WARNING: Total number of samples=10000 is smaller than requested tuning sample size=5000!!!
WARNING: Total number of samples=20000 is smaller than requested tuning sample size=10000!!!
WARNING: Total number of samples=50000 is smaller than requested tuning sample size=25000!!!
WARNING: Total number of samples=100000 is smaller than requested tuning sample size=50000!!!
WARNING: Total number of samples=200000 is smaller than requested tuning sample size=100000!!!
WARNING: Total number of samples=500000 is smaller than requested tuning sample size=250000!!!
WARNING: Total number of samples=1000000 is smaller than requested tuning sample size=500000!!!
WARNING: Total number of samples=2000000 is smaller than requested tuning sample size=1000000!!!
WARNING: Total number of samples=5000000 is smaller than requested tuning sample size=2500000!!!
WARNING: Total number of samples=10000000 is smaller than requested tuning sample size=5000000!!!
WARNING: Total number of samples=20000000 is smaller than requested tuning sample size=10000000!!!
Loading model properties from settings.ini
Loading model properties from properties.xml
Loading model properties from config.yaml
Loading model properties from model.conf
Loading model properties from preferences.properties
Loading model properties from model.properties
Loading model properties from default.properties
Loading model properties from custom.properties
Loading model properties from options.conf
Loading model properties from config.json
Loading model properties from properties.ini
Loading model properties from model.ini
Loading model properties from configuration.ini
Recall: 20 / 100 = 0.2000
Recall: 30 / 100 = 0.3000
Recall: 40 / 100 = 0.4000
Recall: 50 / 100 = 0.5000
Recall: 60 / 100 = 0.6000
Recall: 70 / 100 = 0.7000
Recall: 80 / 100 = 0.8000
Recall: 90 / 100 = 0.9000
Recall: 100 / 100 = 1.0000
Recall: 110 / 100 = 1.1000
Recall: 120 / 100 = 1.2000
Recall: 130 / 100 = 1.3000
Recall: 140 / 100 = 1.4000
Precision: 18 / 50 = 0.3600
Precision: 75 / 100 = 0.7500
Precision: 2 / 10 = 0.2000
Precision: 55 / 80 = 0.6875
Precision: 30 / 60 = 0.5000
Precision: 8 / 20 = 0.4000
Precision: 48 / 70 = 0.6857
Precision: 90 / 120 = 0.7500
Precision: 25 / 50 = 0.5000
Precision: 12 / 20 = 0.6000
Precision: 19 / 40 = 0.4750
Precision: 55 / 100 = 0.5500
Precision: 36 / 60 = 0.6000
Results:  fixedGain: 2,345.67  gain: 890.12  batch 2
Results:  fixedGain: 3,456.78  gain: 901.23  batch 3
Results:  fixedGain: 4,567.89  gain: 912.34  batch 4
Results:  fixedGain: 5,678.90  gain: 923.45  batch 5
Results:  fixedGain: 6,789.01  gain: 934.56  batch 6
Results:  fixedGain: 7,890.12  gain: 945.67  batch 7
Results:  fixedGain: 8,901.23  gain: 956.78  batch 8
Results:  fixedGain: 9,912.34  gain: 967.89  batch 9
Results:  fixedGain: 10,923.45  gain: 978.90  batch 10
Results:  fixedGain: 11,934.56  gain: 989.01  batch 11
Results:  fixedGain: 12,945.67  gain: 1,000.12  batch 12
Results:  fixedGain: 13,956.78  gain: 1,011.23  batch 13
Results:  fixedGain: 14,967.89  gain: 1,022.34  batch 14
edu.stanford.nlp.trees.GenerateTrees <input> <output> <numtrees>
edu.stanford.nlp.trees.GenerateTrees <input> <output> <numtrees>
edu.stanford.nlp.trees.GenerateTrees <input> <output> <numtrees>
edu.stanford.nlp.trees.GenerateTrees <input> <output> <numtrees>
edu.stanford.nlp.trees.GenerateTrees <input> <output> <numtrees>
edu.stanford.nlp.trees.GenerateTrees <input> <output> <numtrees>
edu.stanford.nlp.trees.GenerateTrees <input> <output> <numtrees>
edu.stanford.nlp.trees.GenerateTrees <input> <output> <numtrees>
edu.stanford.nlp.trees.GenerateTrees <input> <output> <numtrees>
edu.stanford.nlp.trees.GenerateTrees <input> <output> <numtrees>
edu.stanford.nlp.trees.GenerateTrees <input> <output> <numtrees>
edu.stanford.nlp.trees.GenerateTrees <input> <output> <numtrees>
edu.stanford.nlp.trees.GenerateTrees <input> <output> <numtrees>
edu.stanford.nlp.trees.GenerateTrees <input> <output> <numtrees>
edu.stanford.nlp.trees.GenerateTrees <input> <output> <numtrees>
loading MentionDetectionClassifier ...
loading MentionDetectionClassifier ...
loading MentionDetectionClassifier ...
loading MentionDetectionClassifier ...
loading MentionDetectionClassifier ...
loading MentionDetectionClassifier ...
loading MentionDetectionClassifier ...
loading MentionDetectionClassifier ...
loading MentionDetectionClassifier ...
loading MentionDetectionClassifier ...
loading MentionDetectionClassifier ...
loading MentionDetectionClassifier ...
loading MentionDetectionClassifier ...
loading MentionDetectionClassifier ...
loading MentionDetectionClassifier ...
Command line should be
Command line should be
Command line should be
Command line should be
Command line should be
Command line should be
Command line should be
Command line should be
Command line should be
Command line should be
Command line should be
Command line should be
Command line should be
Command line should be
Command line should be
Final value is: $9,321.20
Final value is: $25,489.75
Final value is: $6,845.90
Final value is: $18,021.15
Final value is: $12,304.80
Final value is: $32,109.40
Final value is: $7,504.30
Final value is: $13,567.60
Final value is: $38,914.95
Final value is: $11,768.40
Final value is: $21,675.70
Final value is: $45,320.10
Final value is: $10,456.90
Testing with batchsize: 16    gain:  0.7  fixedGain:  0.85
Testing with batchsize: 8    gain:  0.9  fixedGain:  1.0
Testing with batchsize: 64    gain:  0.3  fixedGain:  0.5
Testing with batchsize: 128    gain:  0.2  fixedGain:  1.25
Testing with batchsize: 256    gain:  0.1  fixedGain:  1.5
Testing with batchsize: 512    gain:  0.05  fixedGain:  0.1
Testing with batchsize: 1024    gain:  0.01  fixedGain:  1.75
Testing with batchsize: 2048    gain:  0.005  fixedGain:  0.25
Testing with batchsize: 4096    gain:  0.002  fixedGain:  1.0
Testing with batchsize: 8192    gain:  0.001  fixedGain:  1.25
Testing with batchsize: 16384    gain:  0.0005  fixedGain:  0.85
Testing with batchsize: 32768    gain:  0.0001  fixedGain:  2.0
Testing with batchsize: 65536    gain:  0.00005  fixedGain:  0.5
Applying streaming to treebank
Applying tokenization to treebank
Applying parsing to treebank
Applying machine learning to treebank
Applying feature extraction to treebank
Applying sentiment analysis to treebank
Applying named entity recognition to treebank
Applying part-of-speech tagging to treebank
Applying dependency parsing to treebank
Applying word embedding to treebank
Applying text classification to treebank
Applying information extraction to treebank
Applying topic modeling to treebank
Applying text summarization to treebank
Setting head string to entire mention
Setting head string to entire mention
Setting head string to entire mention
Setting head string to entire mention
Setting head string to entire mention
Setting head string to entire mention
Setting head string to entire mention
Setting head string to entire mention
Setting head string to entire mention
Setting head string to entire mention
Setting head string to entire mention
Setting head string to entire mention
Setting head string to entire mention
Setting head string to entire mention
Setting head string to entire mention
Before feature count threshold, dataset stats are
Before feature count threshold, dataset stats are
Before feature count threshold, dataset stats are
Before feature count threshold, dataset stats are
Before feature count threshold, dataset stats are
Before feature count threshold, dataset stats are
Before feature count threshold, dataset stats are
Before feature count threshold, dataset stats are
Before feature count threshold, dataset stats are
Before feature count threshold, dataset stats are
Before feature count threshold, dataset stats are
Before feature count threshold, dataset stats are
Before feature count threshold, dataset stats are
Before feature count threshold, dataset stats are
Before feature count threshold, dataset stats are
and adding otherdata
and adding sampletext
and adding additionalinfo
and adding customtext
and adding dataline
and adding extensivedata
and adding moreinfo
and adding detailedinfo
and adding specificdata
and adding importantinfo
and adding additionaldata
and adding specialtext
and adding uniquedata
line search failure: interval is too small
line search failure: interval is too small
line search failure: interval is too small
line search failure: interval is too small
line search failure: interval is too small
line search failure: interval is too small
line search failure: interval is too small
line search failure: interval is too small
line search failure: interval is too small
line search failure: interval is too small
line search failure: interval is too small
line search failure: interval is too small
line search failure: interval is too small
line search failure: interval is too small
line search failure: interval is too small
line search failure: minimum step length reached
line search failure: minimum step length reached
line search failure: minimum step length reached
line search failure: minimum step length reached
line search failure: minimum step length reached
line search failure: minimum step length reached
line search failure: minimum step length reached
line search failure: minimum step length reached
line search failure: minimum step length reached
line search failure: minimum step length reached
line search failure: minimum step length reached
line search failure: minimum step length reached
line search failure: minimum step length reached
line search failure: minimum step length reached
line search failure: minimum step length reached
MENTION FILTERING number: @john_doe
MENTION FILTERING number: @jane_smith
MENTION FILTERING number: @user123
MENTION FILTERING number: @test_user
MENTION FILTERING number: @random_user
MENTION FILTERING number: @example_user
MENTION FILTERING number: @guest_user
MENTION FILTERING number: @new_user
MENTION FILTERING number: @old_user
MENTION FILTERING number: @active_user
MENTION FILTERING number: @inactive_user
MENTION FILTERING number: @admin
MENTION FILTERING number: @moderator
line search failure: sufficient decrease, but gradient is more negative
line search failure: sufficient decrease, but gradient is more negative
line search failure: sufficient decrease, but gradient is more negative
line search failure: sufficient decrease, but gradient is more negative
line search failure: sufficient decrease, but gradient is more negative
line search failure: sufficient decrease, but gradient is more negative
line search failure: sufficient decrease, but gradient is more negative
line search failure: sufficient decrease, but gradient is more negative
line search failure: sufficient decrease, but gradient is more negative
line search failure: sufficient decrease, but gradient is more negative
line search failure: sufficient decrease, but gradient is more negative
line search failure: sufficient decrease, but gradient is more negative
line search failure: sufficient decrease, but gradient is more negative
line search failure: sufficient decrease, but gradient is more negative
line search failure: sufficient decrease, but gradient is more negative
line search failure: bracketed but no feasible found
line search failure: bracketed but no feasible found
line search failure: bracketed but no feasible found
line search failure: bracketed but no feasible found
line search failure: bracketed but no feasible found
line search failure: bracketed but no feasible found
line search failure: bracketed but no feasible found
line search failure: bracketed but no feasible found
line search failure: bracketed but no feasible found
line search failure: bracketed but no feasible found
line search failure: bracketed but no feasible found
line search failure: bracketed but no feasible found
line search failure: bracketed but no feasible found
line search failure: bracketed but no feasible found
line search failure: bracketed but no feasible found
{WARNING--- direction of positive gradient chosen!}
{WARNING--- direction of positive gradient chosen!}
{WARNING--- direction of positive gradient chosen!}
{WARNING--- direction of positive gradient chosen!}
{WARNING--- direction of positive gradient chosen!}
{WARNING--- direction of positive gradient chosen!}
{WARNING--- direction of positive gradient chosen!}
{WARNING--- direction of positive gradient chosen!}
{WARNING--- direction of positive gradient chosen!}
{WARNING--- direction of positive gradient chosen!}
{WARNING--- direction of positive gradient chosen!}
{WARNING--- direction of positive gradient chosen!}
{WARNING--- direction of positive gradient chosen!}
{WARNING--- direction of positive gradient chosen!}
{WARNING--- direction of positive gradient chosen!}
DEBUG: Number of unknown phrases is 10
DEBUG: Number of unknown phrases is 3
DEBUG: Number of unknown phrases is 8
DEBUG: Number of unknown phrases is 1
DEBUG: Number of unknown phrases is 6
DEBUG: Number of unknown phrases is 9
DEBUG: Number of unknown phrases is 2
DEBUG: Number of unknown phrases is 7
DEBUG: Number of unknown phrases is 4
DEBUG: Number of unknown phrases is 15
DEBUG: Number of unknown phrases is 12
DEBUG: Number of unknown phrases is 13
DEBUG: Number of unknown phrases is 11
MENTION FILTERING removeChars: Jane Smith
MENTION FILTERING removeChars: Michael Johnson
MENTION FILTERING removeChars: Emily Davis
MENTION FILTERING removeChars: Robert Wilson
MENTION FILTERING removeChars: Olivia Martinez
MENTION FILTERING removeChars: William Taylor
MENTION FILTERING removeChars: Sophia Anderson
MENTION FILTERING removeChars: Joseph Thomas
MENTION FILTERING removeChars: Ava Lee
MENTION FILTERING removeChars: Daniel Hernandez
MENTION FILTERING removeChars: Mia Lewis
MENTION FILTERING removeChars: Christopher Walker
MENTION FILTERING removeChars: Charlotte Allen
200; Total evaluations
300; Total evaluations
400; Total evaluations
500; Total evaluations
600; Total evaluations
700; Total evaluations
800; Total evaluations
900; Total evaluations
1000; Total evaluations
1100; Total evaluations
1200; Total evaluations
1300; Total evaluations
1400; Total evaluations
Total time spent in optimization: 3.5s
Total time spent in optimization: 9.8s
Total time spent in optimization: 6.4s
Total time spent in optimization: 10.1s
Total time spent in optimization: 4.2s
Total time spent in optimization: 12.7s
Total time spent in optimization: 8.9s
Total time spent in optimization: 7.6s
Total time spent in optimization: 2.9s
Total time spent in optimization: 11.3s
Total time spent in optimization: 5.7s
Total time spent in optimization: 14.6s
Total time spent in optimization: 6.8s
QNMinimizer terminated without converging
QNMinimizer terminated without converging
QNMinimizer terminated without converging
QNMinimizer terminated without converging
QNMinimizer terminated without converging
QNMinimizer terminated without converging
QNMinimizer terminated without converging
QNMinimizer terminated without converging
QNMinimizer terminated without converging
QNMinimizer terminated without converging
QNMinimizer terminated without converging
QNMinimizer terminated without converging
QNMinimizer terminated without converging
QNMinimizer terminated without converging
QNMinimizer terminated without converging
QNMinimizer terminated due to no improvement on eval
QNMinimizer terminated due to no improvement on eval
QNMinimizer terminated due to no improvement on eval
QNMinimizer terminated due to no improvement on eval
QNMinimizer terminated due to no improvement on eval
QNMinimizer terminated due to no improvement on eval
QNMinimizer terminated due to no improvement on eval
QNMinimizer terminated due to no improvement on eval
QNMinimizer terminated due to no improvement on eval
QNMinimizer terminated due to no improvement on eval
QNMinimizer terminated due to no improvement on eval
QNMinimizer terminated due to no improvement on eval
QNMinimizer terminated due to no improvement on eval
QNMinimizer terminated due to no improvement on eval
QNMinimizer terminated due to no improvement on eval
QNMinimizer terminated due to reached max iteration 200
QNMinimizer terminated due to reached max iteration 300
QNMinimizer terminated due to reached max iteration 400
QNMinimizer terminated due to reached max iteration 500
QNMinimizer terminated due to reached max iteration 600
QNMinimizer terminated due to reached max iteration 700
QNMinimizer terminated due to reached max iteration 800
QNMinimizer terminated due to reached max iteration 900
QNMinimizer terminated due to reached max iteration 1000
QNMinimizer terminated due to reached max iteration 1100
QNMinimizer terminated due to reached max iteration 1200
QNMinimizer terminated due to reached max iteration 1300
QNMinimizer terminated due to reached max iteration 1400
QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| < TOL
QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| < TOL
QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| < TOL
QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| < TOL
QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| < TOL
QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| < TOL
QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| < TOL
QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| < TOL
QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| < TOL
QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| < TOL
QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| < TOL
QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| < TOL
QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| < TOL
QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| < TOL
QNMinimizer terminated due to average improvement: | newest_val - previous_val | / |newestVal| < TOL
QNMinimizer terminated due to sufficient decrease in gradient norms: |g|/|g0| < TOL
QNMinimizer terminated due to sufficient decrease in gradient norms: |g|/|g0| < TOL
QNMinimizer terminated due to sufficient decrease in gradient norms: |g|/|g0| < TOL
QNMinimizer terminated due to sufficient decrease in gradient norms: |g|/|g0| < TOL
QNMinimizer terminated due to sufficient decrease in gradient norms: |g|/|g0| < TOL
QNMinimizer terminated due to sufficient decrease in gradient norms: |g|/|g0| < TOL
QNMinimizer terminated due to sufficient decrease in gradient norms: |g|/|g0| < TOL
QNMinimizer terminated due to sufficient decrease in gradient norms: |g|/|g0| < TOL
QNMinimizer terminated due to sufficient decrease in gradient norms: |g|/|g0| < TOL
QNMinimizer terminated due to sufficient decrease in gradient norms: |g|/|g0| < TOL
QNMinimizer terminated due to sufficient decrease in gradient norms: |g|/|g0| < TOL
QNMinimizer terminated due to sufficient decrease in gradient norms: |g|/|g0| < TOL
QNMinimizer terminated due to sufficient decrease in gradient norms: |g|/|g0| < TOL
QNMinimizer terminated due to sufficient decrease in gradient norms: |g|/|g0| < TOL
QNMinimizer terminated due to sufficient decrease in gradient norms: |g|/|g0| < TOL
QNMinimizer terminated due to numerically zero gradient: |g| < EPS  max(1,|x|)
QNMinimizer terminated due to numerically zero gradient: |g| < EPS  max(1,|x|)
QNMinimizer terminated due to numerically zero gradient: |g| < EPS  max(1,|x|)
QNMinimizer terminated due to numerically zero gradient: |g| < EPS  max(1,|x|)
QNMinimizer terminated due to numerically zero gradient: |g| < EPS  max(1,|x|)
QNMinimizer terminated due to numerically zero gradient: |g| < EPS  max(1,|x|)
QNMinimizer terminated due to numerically zero gradient: |g| < EPS  max(1,|x|)
QNMinimizer terminated due to numerically zero gradient: |g| < EPS  max(1,|x|)
QNMinimizer terminated due to numerically zero gradient: |g| < EPS  max(1,|x|)
QNMinimizer terminated due to numerically zero gradient: |g| < EPS  max(1,|x|)
QNMinimizer terminated due to numerically zero gradient: |g| < EPS  max(1,|x|)
QNMinimizer terminated due to numerically zero gradient: |g| < EPS  max(1,|x|)
QNMinimizer terminated due to numerically zero gradient: |g| < EPS  max(1,|x|)
QNMinimizer terminated due to numerically zero gradient: |g| < EPS  max(1,|x|)
QNMinimizer terminated due to numerically zero gradient: |g| < EPS  max(1,|x|)
ConjPP (different preps) adding: oranges
ConjPP (different preps) adding: bananas
ConjPP (different preps) adding: pears
ConjPP (different preps) adding: grapes
ConjPP (different preps) adding: strawberries
ConjPP (different preps) adding: watermelons
ConjPP (different preps) adding: pineapples
ConjPP (different preps) adding: mangoes
ConjPP (different preps) adding: peaches
ConjPP (different preps) adding: cherries
ConjPP (different preps) adding: plums
ConjPP (different preps) adding: lemons
ConjPP (different preps) adding: kiwis
final evalScore is: 82
final evalScore is: 95
final evalScore is: 70
final evalScore is: 88
final evalScore is: 76
final evalScore is: 93
final evalScore is: 85
final evalScore is: 79
final evalScore is: 91
final evalScore is: 84
final evalScore is: 77
final evalScore is: 89
final evalScore is: 83
(NaN dir likely due to Hessian approx - resetting)
(NaN dir likely due to Hessian approx - resetting)
(NaN dir likely due to Hessian approx - resetting)
(NaN dir likely due to Hessian approx - resetting)
(NaN dir likely due to Hessian approx - resetting)
(NaN dir likely due to Hessian approx - resetting)
(NaN dir likely due to Hessian approx - resetting)
(NaN dir likely due to Hessian approx - resetting)
(NaN dir likely due to Hessian approx - resetting)
(NaN dir likely due to Hessian approx - resetting)
(NaN dir likely due to Hessian approx - resetting)
(NaN dir likely due to Hessian approx - resetting)
(NaN dir likely due to Hessian approx - resetting)
(NaN dir likely due to Hessian approx - resetting)
(NaN dir likely due to Hessian approx - resetting)
** program by checking the QNMinimizer.wasSuccessful() method.
** program by checking the QNMinimizer.wasSuccessful() method.
** program by checking the QNMinimizer.wasSuccessful() method.
** program by checking the QNMinimizer.wasSuccessful() method.
** program by checking the QNMinimizer.wasSuccessful() method.
** program by checking the QNMinimizer.wasSuccessful() method.
** program by checking the QNMinimizer.wasSuccessful() method.
** program by checking the QNMinimizer.wasSuccessful() method.
** program by checking the QNMinimizer.wasSuccessful() method.
** program by checking the QNMinimizer.wasSuccessful() method.
** program by checking the QNMinimizer.wasSuccessful() method.
** program by checking the QNMinimizer.wasSuccessful() method.
** program by checking the QNMinimizer.wasSuccessful() method.
** program by checking the QNMinimizer.wasSuccessful() method.
** program by checking the QNMinimizer.wasSuccessful() method.
done. Merging threshold: 0.8
done. Merging threshold: 0.3
done. Merging threshold: 0.6
done. Merging threshold: 0.9
done. Merging threshold: 0.2
done. Merging threshold: 0.7
done. Merging threshold: 0.4
done. Merging threshold: 0.1
done. Merging threshold: 0.3
done. Merging threshold: 0.7
done. Merging threshold: 0.4
done. Merging threshold: 0.6
done. Merging threshold: 0.8
** increasing the max number of evaluations, or safeguarding your
** increasing the max number of evaluations, or safeguarding your
** increasing the max number of evaluations, or safeguarding your
** increasing the max number of evaluations, or safeguarding your
** increasing the max number of evaluations, or safeguarding your
** increasing the max number of evaluations, or safeguarding your
** increasing the max number of evaluations, or safeguarding your
** increasing the max number of evaluations, or safeguarding your
** increasing the max number of evaluations, or safeguarding your
** increasing the max number of evaluations, or safeguarding your
** increasing the max number of evaluations, or safeguarding your
** increasing the max number of evaluations, or safeguarding your
** increasing the max number of evaluations, or safeguarding your
** increasing the max number of evaluations, or safeguarding your
** increasing the max number of evaluations, or safeguarding your
** This is not an acceptable termination of QNMinimizer, consider
** This is not an acceptable termination of QNMinimizer, consider
** This is not an acceptable termination of QNMinimizer, consider
** This is not an acceptable termination of QNMinimizer, consider
** This is not an acceptable termination of QNMinimizer, consider
** This is not an acceptable termination of QNMinimizer, consider
** This is not an acceptable termination of QNMinimizer, consider
** This is not an acceptable termination of QNMinimizer, consider
** This is not an acceptable termination of QNMinimizer, consider
** This is not an acceptable termination of QNMinimizer, consider
** This is not an acceptable termination of QNMinimizer, consider
** This is not an acceptable termination of QNMinimizer, consider
** This is not an acceptable termination of QNMinimizer, consider
** This is not an acceptable termination of QNMinimizer, consider
** This is not an acceptable termination of QNMinimizer, consider
QNMinimizer aborted due to maximum number of function evaluations
QNMinimizer aborted due to maximum number of function evaluations
QNMinimizer aborted due to maximum number of function evaluations
QNMinimizer aborted due to maximum number of function evaluations
QNMinimizer aborted due to maximum number of function evaluations
QNMinimizer aborted due to maximum number of function evaluations
QNMinimizer aborted due to maximum number of function evaluations
QNMinimizer aborted due to maximum number of function evaluations
QNMinimizer aborted due to maximum number of function evaluations
QNMinimizer aborted due to maximum number of function evaluations
QNMinimizer aborted due to maximum number of function evaluations
QNMinimizer aborted due to maximum number of function evaluations
QNMinimizer aborted due to maximum number of function evaluations
QNMinimizer aborted due to maximum number of function evaluations
QNMinimizer aborted due to maximum number of function evaluations
QNMinimizer aborted due to surprise convergence
QNMinimizer aborted due to surprise convergence
QNMinimizer aborted due to surprise convergence
QNMinimizer aborted due to surprise convergence
QNMinimizer aborted due to surprise convergence
QNMinimizer aborted due to surprise convergence
QNMinimizer aborted due to surprise convergence
QNMinimizer aborted due to surprise convergence
QNMinimizer aborted due to surprise convergence
QNMinimizer aborted due to surprise convergence
QNMinimizer aborted due to surprise convergence
QNMinimizer aborted due to surprise convergence
QNMinimizer aborted due to surprise convergence
QNMinimizer aborted due to surprise convergence
QNMinimizer aborted due to surprise convergence
Iter ## evals ## <SCALING> [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE
Iter ## evals ## <SCALING> [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE
Iter ## evals ## <SCALING> [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE
Iter ## evals ## <SCALING> [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE
Iter ## evals ## <SCALING> [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE
Iter ## evals ## <SCALING> [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE
Iter ## evals ## <SCALING> [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE
Iter ## evals ## <SCALING> [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE
Iter ## evals ## <SCALING> [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE
Iter ## evals ## <SCALING> [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE
Iter ## evals ## <SCALING> [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE
Iter ## evals ## <SCALING> [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE
Iter ## evals ## <SCALING> [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE
Iter ## evals ## <SCALING> [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE
Iter ## evals ## <SCALING> [LINESEARCH] VALUE TIME |GNORM| {RELNORM} AVEIMPROVE EVALSCORE
apparent misparse: same P twice with only one NP object (prepOtherDep is null)
apparent misparse: same P twice with only one NP object (prepOtherDep is null)
apparent misparse: same P twice with only one NP object (prepOtherDep is null)
apparent misparse: same P twice with only one NP object (prepOtherDep is null)
apparent misparse: same P twice with only one NP object (prepOtherDep is null)
apparent misparse: same P twice with only one NP object (prepOtherDep is null)
apparent misparse: same P twice with only one NP object (prepOtherDep is null)
apparent misparse: same P twice with only one NP object (prepOtherDep is null)
apparent misparse: same P twice with only one NP object (prepOtherDep is null)
apparent misparse: same P twice with only one NP object (prepOtherDep is null)
apparent misparse: same P twice with only one NP object (prepOtherDep is null)
apparent misparse: same P twice with only one NP object (prepOtherDep is null)
apparent misparse: same P twice with only one NP object (prepOtherDep is null)
apparent misparse: same P twice with only one NP object (prepOtherDep is null)
apparent misparse: same P twice with only one NP object (prepOtherDep is null)
EVALSCORE      The last available eval score
EVALSCORE      The last available eval score
EVALSCORE      The last available eval score
EVALSCORE      The last available eval score
EVALSCORE      The last available eval score
EVALSCORE      The last available eval score
EVALSCORE      The last available eval score
EVALSCORE      The last available eval score
EVALSCORE      The last available eval score
EVALSCORE      The last available eval score
EVALSCORE      The last available eval score
EVALSCORE      The last available eval score
EVALSCORE      The last available eval score
EVALSCORE      The last available eval score
EVALSCORE      The last available eval score
AVEIMPROVE     The average improvement / current value
AVEIMPROVE     The average improvement / current value
AVEIMPROVE     The average improvement / current value
AVEIMPROVE     The average improvement / current value
AVEIMPROVE     The average improvement / current value
AVEIMPROVE     The average improvement / current value
AVEIMPROVE     The average improvement / current value
AVEIMPROVE     The average improvement / current value
AVEIMPROVE     The average improvement / current value
AVEIMPROVE     The average improvement / current value
AVEIMPROVE     The average improvement / current value
AVEIMPROVE     The average improvement / current value
AVEIMPROVE     The average improvement / current value
AVEIMPROVE     The average improvement / current value
AVEIMPROVE     The average improvement / current value
{RELNORM}      The ratio of the current to initial gradient norms
{RELNORM}      The ratio of the current to initial gradient norms
{RELNORM}      The ratio of the current to initial gradient norms
{RELNORM}      The ratio of the current to initial gradient norms
{RELNORM}      The ratio of the current to initial gradient norms
{RELNORM}      The ratio of the current to initial gradient norms
{RELNORM}      The ratio of the current to initial gradient norms
{RELNORM}      The ratio of the current to initial gradient norms
{RELNORM}      The ratio of the current to initial gradient norms
{RELNORM}      The ratio of the current to initial gradient norms
{RELNORM}      The ratio of the current to initial gradient norms
{RELNORM}      The ratio of the current to initial gradient norms
{RELNORM}      The ratio of the current to initial gradient norms
{RELNORM}      The ratio of the current to initial gradient norms
{RELNORM}      The ratio of the current to initial gradient norms
|GNORM|        The current norm of the gradient
|GNORM|        The current norm of the gradient
|GNORM|        The current norm of the gradient
|GNORM|        The current norm of the gradient
|GNORM|        The current norm of the gradient
|GNORM|        The current norm of the gradient
|GNORM|        The current norm of the gradient
|GNORM|        The current norm of the gradient
|GNORM|        The current norm of the gradient
|GNORM|        The current norm of the gradient
|GNORM|        The current norm of the gradient
|GNORM|        The current norm of the gradient
|GNORM|        The current norm of the gradient
|GNORM|        The current norm of the gradient
|GNORM|        The current norm of the gradient
adding: file1.txt
adding: image.jpg
adding: folder/
adding: script.js
adding: style.css
adding: data.json
adding: index.html
adding: document.docx
adding: picture.png
adding: code.java
adding: video.mp4
adding: audio.wav
adding: archive.zip
TIME           Total elapsed time
TIME           Total elapsed time
TIME           Total elapsed time
TIME           Total elapsed time
TIME           Total elapsed time
TIME           Total elapsed time
TIME           Total elapsed time
TIME           Total elapsed time
TIME           Total elapsed time
TIME           Total elapsed time
TIME           Total elapsed time
TIME           Total elapsed time
TIME           Total elapsed time
TIME           Total elapsed time
TIME           Total elapsed time
Elapsed time: 2.891 seconds
Elapsed time: 3.717 seconds
Elapsed time: 7.103 seconds
Elapsed time: 4.562 seconds
Elapsed time: 6.215 seconds
Elapsed time: 1.987 seconds
Elapsed time: 3.999 seconds
Elapsed time: 8.412 seconds
Elapsed time: 2.703 seconds
Elapsed time: 6.958 seconds
Elapsed time: 4.821 seconds
Elapsed time: 5.623 seconds
Elapsed time: 9.876 seconds
VALUE          The current function value
VALUE          The current function value
VALUE          The current function value
VALUE          The current function value
VALUE          The current function value
VALUE          The current function value
VALUE          The current function value
VALUE          The current function value
VALUE          The current function value
VALUE          The current function value
VALUE          The current function value
VALUE          The current function value
VALUE          The current function value
VALUE          The current function value
VALUE          The current function value
[.. B]  Backtracking
[.. B]  Backtracking
[.. B]  Backtracking
[.. B]  Backtracking
[.. B]  Backtracking
[.. B]  Backtracking
[.. B]  Backtracking
[.. B]  Backtracking
[.. B]  Backtracking
[.. B]  Backtracking
[.. B]  Backtracking
[.. B]  Backtracking
[.. B]  Backtracking
[.. B]  Backtracking
[.. B]  Backtracking
Start time: 2021-03-02T14:30:00
Start time: 2021-03-03T18:45:00
Start time: 2021-03-04T12:15:00
Start time: 2021-03-05T08:20:00
Start time: 2021-03-06T16:10:00
Start time: 2021-03-07T11:30:00
Start time: 2021-03-08T15:50:00
Start time: 2021-03-09T10:40:00
Start time: 2021-03-10T13:25:00
Start time: 2021-03-11T09:55:00
Start time: 2021-03-12T17:35:00
Start time: 2021-03-13T19:05:00
Start time: 2021-03-14T14:15:00
4-Value ok, gradient negative, negative curvature
4-Value ok, gradient negative, negative curvature
4-Value ok, gradient negative, negative curvature
4-Value ok, gradient negative, negative curvature
4-Value ok, gradient negative, negative curvature
4-Value ok, gradient negative, negative curvature
4-Value ok, gradient negative, negative curvature
4-Value ok, gradient negative, negative curvature
4-Value ok, gradient negative, negative curvature
4-Value ok, gradient negative, negative curvature
4-Value ok, gradient negative, negative curvature
4-Value ok, gradient negative, negative curvature
4-Value ok, gradient negative, negative curvature
4-Value ok, gradient negative, negative curvature
4-Value ok, gradient negative, negative curvature
3-Value ok, gradient negative, positive curvature
3-Value ok, gradient negative, positive curvature
3-Value ok, gradient negative, positive curvature
3-Value ok, gradient negative, positive curvature
3-Value ok, gradient negative, positive curvature
3-Value ok, gradient negative, positive curvature
3-Value ok, gradient negative, positive curvature
3-Value ok, gradient negative, positive curvature
3-Value ok, gradient negative, positive curvature
3-Value ok, gradient negative, positive curvature
3-Value ok, gradient negative, positive curvature
3-Value ok, gradient negative, positive curvature
3-Value ok, gradient negative, positive curvature
3-Value ok, gradient negative, positive curvature
3-Value ok, gradient negative, positive curvature
USED MEMORY (bytes): 2048
USED MEMORY (bytes): 4096
USED MEMORY (bytes): 8192
USED MEMORY (bytes): 16384
USED MEMORY (bytes): 32768
USED MEMORY (bytes): 65536
USED MEMORY (bytes): 131072
USED MEMORY (bytes): 262144
USED MEMORY (bytes): 524288
USED MEMORY (bytes): 1048576
USED MEMORY (bytes): 2097152
USED MEMORY (bytes): 4194304
USED MEMORY (bytes): 8388608
2-Value ok, gradient positive, positive curvature
2-Value ok, gradient positive, positive curvature
2-Value ok, gradient positive, positive curvature
2-Value ok, gradient positive, positive curvature
2-Value ok, gradient positive, positive curvature
2-Value ok, gradient positive, positive curvature
2-Value ok, gradient positive, positive curvature
2-Value ok, gradient positive, positive curvature
2-Value ok, gradient positive, positive curvature
2-Value ok, gradient positive, positive curvature
2-Value ok, gradient positive, positive curvature
2-Value ok, gradient positive, positive curvature
2-Value ok, gradient positive, positive curvature
2-Value ok, gradient positive, positive curvature
2-Value ok, gradient positive, positive curvature
2 document(s) processed
3 document(s) processed
4 document(s) processed
5 document(s) processed
6 document(s) processed
7 document(s) processed
8 document(s) processed
9 document(s) processed
10 document(s) processed
11 document(s) processed
12 document(s) processed
13 document(s) processed
14 document(s) processed
1-Function value was too high
1-Function value was too high
1-Function value was too high
1-Function value was too high
1-Function value was too high
1-Function value was too high
1-Function value was too high
1-Function value was too high
1-Function value was too high
1-Function value was too high
1-Function value was too high
1-Function value was too high
1-Function value was too high
1-Function value was too high
1-Function value was too high
LINESEARCH     [## M steplength]  Minpack linesearch
LINESEARCH     [## M steplength]  Minpack linesearch
LINESEARCH     [## M steplength]  Minpack linesearch
LINESEARCH     [## M steplength]  Minpack linesearch
LINESEARCH     [## M steplength]  Minpack linesearch
LINESEARCH     [## M steplength]  Minpack linesearch
LINESEARCH     [## M steplength]  Minpack linesearch
LINESEARCH     [## M steplength]  Minpack linesearch
LINESEARCH     [## M steplength]  Minpack linesearch
LINESEARCH     [## M steplength]  Minpack linesearch
LINESEARCH     [## M steplength]  Minpack linesearch
LINESEARCH     [## M steplength]  Minpack linesearch
LINESEARCH     [## M steplength]  Minpack linesearch
LINESEARCH     [## M steplength]  Minpack linesearch
LINESEARCH     [## M steplength]  Minpack linesearch
END-TO-END COREF Elapsed time: 1.987 seconds
END-TO-END COREF Elapsed time: 2.576 seconds
END-TO-END COREF Elapsed time: 4.123 seconds
END-TO-END COREF Elapsed time: 3.765 seconds
END-TO-END COREF Elapsed time: 1.956 seconds
END-TO-END COREF Elapsed time: 2.487 seconds
END-TO-END COREF Elapsed time: 3.876 seconds
END-TO-END COREF Elapsed time: 4.567 seconds
END-TO-END COREF Elapsed time: 1.345 seconds
END-TO-END COREF Elapsed time: 2.698 seconds
END-TO-END COREF Elapsed time: 3.452 seconds
END-TO-END COREF Elapsed time: 4.789 seconds
END-TO-END COREF Elapsed time: 2.345 seconds
SCALING        <D> Diagonal scaling was used; <I> Scaled Identity
SCALING        <D> Diagonal scaling was used; <I> Scaled Identity
SCALING        <D> Diagonal scaling was used; <I> Scaled Identity
SCALING        <D> Diagonal scaling was used; <I> Scaled Identity
SCALING        <D> Diagonal scaling was used; <I> Scaled Identity
SCALING        <D> Diagonal scaling was used; <I> Scaled Identity
SCALING        <D> Diagonal scaling was used; <I> Scaled Identity
SCALING        <D> Diagonal scaling was used; <I> Scaled Identity
SCALING        <D> Diagonal scaling was used; <I> Scaled Identity
SCALING        <D> Diagonal scaling was used; <I> Scaled Identity
SCALING        <D> Diagonal scaling was used; <I> Scaled Identity
SCALING        <D> Diagonal scaling was used; <I> Scaled Identity
SCALING        <D> Diagonal scaling was used; <I> Scaled Identity
SCALING        <D> Diagonal scaling was used; <I> Scaled Identity
SCALING        <D> Diagonal scaling was used; <I> Scaled Identity
evals          The number of function evaluations
evals          The number of function evaluations
evals          The number of function evaluations
evals          The number of function evaluations
evals          The number of function evaluations
evals          The number of function evaluations
evals          The number of function evaluations
evals          The number of function evaluations
evals          The number of function evaluations
evals          The number of function evaluations
evals          The number of function evaluations
evals          The number of function evaluations
evals          The number of function evaluations
evals          The number of function evaluations
evals          The number of function evaluations
Iter           The number of iterations
Iter           The number of iterations
Iter           The number of iterations
Iter           The number of iterations
Iter           The number of iterations
Iter           The number of iterations
Iter           The number of iterations
Iter           The number of iterations
Iter           The number of iterations
Iter           The number of iterations
Iter           The number of iterations
Iter           The number of iterations
Iter           The number of iterations
Iter           The number of iterations
Iter           The number of iterations
END-TO-END COREF Start time: 11:30:45
END-TO-END COREF Start time: 14:22:17
END-TO-END COREF Start time: 16:55:12
END-TO-END COREF Start time: 18:40:58
END-TO-END COREF Start time: 20:12:02
END-TO-END COREF Start time: 08:05:13
END-TO-END COREF Start time: 09:48:29
END-TO-END COREF Start time: 11:55:37
END-TO-END COREF Start time: 13:27:41
END-TO-END COREF Start time: 15:59:54
END-TO-END COREF Start time: 17:33:06
END-TO-END COREF Start time: 19:10:51
END-TO-END COREF Start time: 21:47:19
Starting coref log
Starting coref log
Starting coref log
Starting coref log
Starting coref log
Starting coref log
Starting coref log
Starting coref log
Starting coref log
Starting coref log
Starting coref log
Starting coref log
Starting coref log
Starting coref log
Starting coref log
Insufficient memory; memory
Memory leak detected; memory
Memory allocation error; memory
Memory access violation; memory
Memory corruption detected; memory
Memory utilization exceeded; memory
Low memory warning; memory
Memory usage exceeded limit; memory
Memory allocation failed; memory
Memory overload detected; memory
Memory address conflict; memory
Memory read error; memory
Memory write error; memory
Memory deallocation error; memory
Kids of td1 are: Ethan
Kids of td1 are: Olivia
Kids of td1 are: Liam
Kids of td1 are: Ava
Kids of td1 are: Noah
Kids of td1 are: Sophia
Kids of td1 are: Elijah
Kids of td1 are: Isabella
Kids of td1 are: Oliver
Kids of td1 are: Mia
Kids of td1 are: Lucas
Kids of td1 are: Amelia
Kids of td1 are: Aiden
QNMinimizer called on double function of 7 variables, using dynamic settings of M.
QNMinimizer called on double function of 8 variables, using M = 512.
QNMinimizer called on double function of 3 variables, using dynamic settings of M.
QNMinimizer called on double function of 4 variables, using M = 128.
QNMinimizer called on double function of 6 variables, using dynamic settings of M.
QNMinimizer called on double function of 2 variables, using M = 64.
QNMinimizer called on double function of 9 variables, using dynamic settings of M.
QNMinimizer called on double function of 10 variables, using M = 1024.
QNMinimizer called on double function of 1 variables, using dynamic settings of M.
QNMinimizer called on double function of 12 variables, using M = 2048.
QNMinimizer called on double function of 11 variables, using dynamic settings of M.
QNMinimizer called on double function of 15 variables, using M = 4096.
QNMinimizer called on double function of 14 variables, using dynamic settings of M.
!! Conj and prep case:
!! Conj and prep case:
!! Conj and prep case:
!! Conj and prep case:
!! Conj and prep case:
!! Conj and prep case:
!! Conj and prep case:
!! Conj and prep case:
!! Conj and prep case:
!! Conj and prep case:
!! Conj and prep case:
!! Conj and prep case:
!! Conj and prep case:
!! Conj and prep case:
!! Conj and prep case:
Running time 1min 47s
Running time 5min 23s
Running time 2min 58s
Running time 4min 40s
Running time 6min 11s
Running time 0min 25s
Running time 7min 36s
Running time 9min 59s
Running time 5min 7s
Running time 3min 30s
Running time 2min 10s
Running time 8min 45s
Running time 4min 52s
QNInfo:update() : PROBLEM WITH DIAGONAL UPDATE
QNInfo:update() : PROBLEM WITH DIAGONAL UPDATE
QNInfo:update() : PROBLEM WITH DIAGONAL UPDATE
QNInfo:update() : PROBLEM WITH DIAGONAL UPDATE
QNInfo:update() : PROBLEM WITH DIAGONAL UPDATE
QNInfo:update() : PROBLEM WITH DIAGONAL UPDATE
QNInfo:update() : PROBLEM WITH DIAGONAL UPDATE
QNInfo:update() : PROBLEM WITH DIAGONAL UPDATE
QNInfo:update() : PROBLEM WITH DIAGONAL UPDATE
QNInfo:update() : PROBLEM WITH DIAGONAL UPDATE
QNInfo:update() : PROBLEM WITH DIAGONAL UPDATE
QNInfo:update() : PROBLEM WITH DIAGONAL UPDATE
QNInfo:update() : PROBLEM WITH DIAGONAL UPDATE
QNInfo:update() : PROBLEM WITH DIAGONAL UPDATE
QNInfo:update() : PROBLEM WITH DIAGONAL UPDATE
Either convergence, or floating point errors combined with extremely linear region
Either convergence, or floating point errors combined with extremely linear region
Either convergence, or floating point errors combined with extremely linear region
Either convergence, or floating point errors combined with extremely linear region
Either convergence, or floating point errors combined with extremely linear region
Either convergence, or floating point errors combined with extremely linear region
Either convergence, or floating point errors combined with extremely linear region
Either convergence, or floating point errors combined with extremely linear region
Either convergence, or floating point errors combined with extremely linear region
Either convergence, or floating point errors combined with extremely linear region
Either convergence, or floating point errors combined with extremely linear region
Either convergence, or floating point errors combined with extremely linear region
Either convergence, or floating point errors combined with extremely linear region
Either convergence, or floating point errors combined with extremely linear region
Either convergence, or floating point errors combined with extremely linear region
Negative curvature detected, update skipped
Negative curvature detected, update skipped
Negative curvature detected, update skipped
Negative curvature detected, update skipped
Negative curvature detected, update skipped
Negative curvature detected, update skipped
Negative curvature detected, update skipped
Negative curvature detected, update skipped
Negative curvature detected, update skipped
Negative curvature detected, update skipped
Negative curvature detected, update skipped
Negative curvature detected, update skipped
Negative curvature detected, update skipped
Negative curvature detected, update skipped
Negative curvature detected, update skipped
Gradient is numerically zero, stopped on machine epsilon.
Gradient is numerically zero, stopped on machine epsilon.
Gradient is numerically zero, stopped on machine epsilon.
Gradient is numerically zero, stopped on machine epsilon.
Gradient is numerically zero, stopped on machine epsilon.
Gradient is numerically zero, stopped on machine epsilon.
Gradient is numerically zero, stopped on machine epsilon.
Gradient is numerically zero, stopped on machine epsilon.
Gradient is numerically zero, stopped on machine epsilon.
Gradient is numerically zero, stopped on machine epsilon.
Gradient is numerically zero, stopped on machine epsilon.
Gradient is numerically zero, stopped on machine epsilon.
Gradient is numerically zero, stopped on machine epsilon.
Gradient is numerically zero, stopped on machine epsilon.
Gradient is numerically zero, stopped on machine epsilon.
debug-docreaderReading document: 9835
debug-docreaderReading document: 2594
debug-docreaderReading document: 6748
debug-docreaderReading document: 1065
debug-docreaderReading document: 9843
debug-docreaderReading document: 4529
debug-docreaderReading document: 8726
debug-docreaderReading document: 1437
debug-docreaderReading document: 5235
debug-docreaderReading document: 2354
debug-docreaderReading document: 6947
debug-docreaderReading document: 9562
debug-docreaderReading document: 3126
Probed right at 15, value is 35
Probed left at 5, value is 10
Probed right at 12, value is 24
Probed left at 8, value is 16
Probed right at 18, value is 40
Probed left at 3, value is 6
Probed right at 14, value is 28
Probed left at 7, value is 14
Probed right at 21, value is 50
Probed left at 1, value is 2
Probed right at 13, value is 26
Probed left at 6, value is 12
Probed right at 19, value is 44
Total token count: 200
Total token count: 300
Total token count: 400
Total token count: 500
Total token count: 600
Total token count: 700
Total token count: 800
Total token count: 900
Total token count: 1000
Total token count: 1100
Total token count: 1200
Total token count: 1300
Total token count: 1400
Current low, mid, high: 12,500 25,000 37,500
Current low, mid, high: 31,800 63,600 95,400
Current low, mid, high: 18,750 37,500 56,250
Current low, mid, high: 42,300 84,600 126,900
Current low, mid, high: 8,900 17,800 26,700
Current low, mid, high: 55,000 110,000 165,000
Current low, mid, high: 27,400 54,800 82,200
Current low, mid, high: 37,200 74,400 111,600
Current low, mid, high: 15,600 31,200 46,800
Current low, mid, high: 49,750 99,500 149,250
Current low, mid, high: 22,100 44,200 66,300
Current low, mid, high: 39,800 79,600 119,400
Current low, mid, high: 10,250 20,500 30,750
Total sentence count: 45
Total sentence count: 22
Total sentence count: 60
Total sentence count: 12
Total sentence count: 37
Total sentence count: 50
Total sentence count: 18
Total sentence count: 27
Total sentence count: 42
Total sentence count: 35
Total sentence count: 52
Total sentence count: 20
Total sentence count: 8
Warning: GoldenSectionLineSearch init didn't find slope!!
Warning: GoldenSectionLineSearch init didn't find slope!!
Warning: GoldenSectionLineSearch init didn't find slope!!
Warning: GoldenSectionLineSearch init didn't find slope!!
Warning: GoldenSectionLineSearch init didn't find slope!!
Warning: GoldenSectionLineSearch init didn't find slope!!
Warning: GoldenSectionLineSearch init didn't find slope!!
Warning: GoldenSectionLineSearch init didn't find slope!!
Warning: GoldenSectionLineSearch init didn't find slope!!
Warning: GoldenSectionLineSearch init didn't find slope!!
Warning: GoldenSectionLineSearch init didn't find slope!!
Warning: GoldenSectionLineSearch init didn't find slope!!
Warning: GoldenSectionLineSearch init didn't find slope!!
Warning: GoldenSectionLineSearch init didn't find slope!!
Warning: GoldenSectionLineSearch init didn't find slope!!
Forest.CPU Features of selected phrases
Oak.TLS Features of selected phrases
Pine.Memory Features of selected phrases
Birch.IO Features of selected phrases
Cedar.Network Features of selected phrases
Maple.Security Features of selected phrases
Willow.Logging Features of selected phrases
Spruce.Performance Features of selected phrases
Aspen.Debugging Features of selected phrases
Juniper.ErrorHandling Features of selected phrases
Fir.Testing Features of selected phrases
Hickory.Optimization Features of selected phrases
Alder.Concurrency Features of selected phrases
Total document count: 500
Total document count: 1000
Total document count: 200
Total document count: 10000
Total document count: 50
Total document count: 100000
Total document count: 20
Total document count: 1000000
Total document count: 10
Total document count: 5000
Total document count: 10000000
Total document count: 2000
Total document count: 100000000
Initially probed at 11:30 AM, value is 120
Initially probed at 1:45 PM, value is 80
Initially probed at 3:15 PM, value is 90
Initially probed at 4:30 PM, value is 110
Initially probed at 5:45 PM, value is 95
Initially probed at 7:00 PM, value is 105
Initially probed at 8:30 PM, value is 115
Initially probed at 9:45 PM, value is 85
Initially probed at 10:50 PM, value is 75
Initially probed at 11:55 PM, value is 125
Initially probed at 12:05 AM, value is 135
Initially probed at 1:10 AM, value is 65
Initially probed at 2:25 AM, value is 115
size of learned scores is 15
size of learned scores is 20
size of learned scores is 5
size of learned scores is 12
size of learned scores is 8
size of learned scores is 25
size of learned scores is 17
size of learned scores is 13
size of learned scores is 3
size of learned scores is 7
size of learned scores is 9
size of learned scores is 19
size of learned scores is 14
Found a good mid point at (5, 15)
Found a good mid point at (8, 20)
Found a good mid point at (11, 25)
Found a good mid point at (14, 30)
Found a good mid point at (17, 35)
Found a good mid point at (20, 40)
Found a good mid point at (23, 45)
Found a good mid point at (26, 50)
Found a good mid point at (29, 55)
Found a good mid point at (32, 60)
Found a good mid point at (35, 65)
Found a good mid point at (38, 70)
Found a good mid point at (41, 75)
NER Chunk: location
NER Chunk: organization
NER Chunk: date
NER Chunk: time
NER Chunk: money
NER Chunk: percentage
NER Chunk: duration
NER Chunk: event
NER Chunk: product
NER Chunk: language
NER Chunk: document
NER Chunk: software
NER Chunk: hardware
Warning: GoldenSectionLineSearch grid search couldn't find slope!!
Warning: GoldenSectionLineSearch grid search couldn't find slope!!
Warning: GoldenSectionLineSearch grid search couldn't find slope!!
Warning: GoldenSectionLineSearch grid search couldn't find slope!!
Warning: GoldenSectionLineSearch grid search couldn't find slope!!
Warning: GoldenSectionLineSearch grid search couldn't find slope!!
Warning: GoldenSectionLineSearch grid search couldn't find slope!!
Warning: GoldenSectionLineSearch grid search couldn't find slope!!
Warning: GoldenSectionLineSearch grid search couldn't find slope!!
Warning: GoldenSectionLineSearch grid search couldn't find slope!!
Warning: GoldenSectionLineSearch grid search couldn't find slope!!
Warning: GoldenSectionLineSearch grid search couldn't find slope!!
Warning: GoldenSectionLineSearch grid search couldn't find slope!!
Warning: GoldenSectionLineSearch grid search couldn't find slope!!
Warning: GoldenSectionLineSearch grid search couldn't find slope!!
Sentence Tokens: I, am, a, helpful, assistant
Sentence Tokens: How, can, I, assist, you
Sentence Tokens: The, weather, today, is, sunny
Sentence Tokens: Have, a, great, day
Sentence Tokens: What, is, your, name
Sentence Tokens: I, don't, understand, that, question
Sentence Tokens: Let, me, find, the, answer
Sentence Tokens: Sorry, I, can't, do, that
Sentence Tokens: Can, you, please, repeat, that
Sentence Tokens: I, apologize, for, the, inconvenience
Sentence Tokens: Here, are, some, suggestions
Sentence Tokens: Thank, you, for, your, feedback
Sentence Tokens: I, appreciate, your, patience
Sentence Tokens: I, will, do, my, best
Probed at 1:45 AM, value is 86
Probed at 3:15 PM, value is 92
Probed at 7:00 AM, value is 70
Probed at 11:20 AM, value is 115
Probed at 4:45 PM, value is 98
Probed at 8:10 AM, value is 68
Probed at 2:35 PM, value is 88
Probed at 6:50 AM, value is 71
Probed at 10:15 PM, value is 105
Probed at 5:30 AM, value is 65
Probed at 9:55 AM, value is 120
Probed at 11:40 PM, value is 82
Probed at 2:05 PM, value is 91
Reading word vectors: example.txt
Reading word vectors: data.bin
Reading word vectors: model.pt
Reading word vectors: embeddings.npy
Reading word vectors: vectors.csv
Reading word vectors: word2vec.bin
Reading word vectors: glove.6B.100d.txt
Reading word vectors: fasttext.vec
Reading word vectors: w2v.txt
Reading word vectors: vocab.pickle
Reading word vectors: word2vec.model
Reading word vectors: embeddings.mat
Reading word vectors: vectors.pkl
20 point gridsearch for good mid point....
20 point gridsearch for good mid point....
20 point gridsearch for good mid point....
20 point gridsearch for good mid point....
20 point gridsearch for good mid point....
20 point gridsearch for good mid point....
20 point gridsearch for good mid point....
20 point gridsearch for good mid point....
20 point gridsearch for good mid point....
20 point gridsearch for good mid point....
20 point gridsearch for good mid point....
20 point gridsearch for good mid point....
20 point gridsearch for good mid point....
20 point gridsearch for good mid point....
20 point gridsearch for good mid point....
Warning: exiting minimize because ITER exceeded!
Warning: exiting minimize because ITER exceeded!
Warning: exiting minimize because ITER exceeded!
Warning: exiting minimize because ITER exceeded!
Warning: exiting minimize because ITER exceeded!
Warning: exiting minimize because ITER exceeded!
Warning: exiting minimize because ITER exceeded!
Warning: exiting minimize because ITER exceeded!
Warning: exiting minimize because ITER exceeded!
Warning: exiting minimize because ITER exceeded!
Warning: exiting minimize because ITER exceeded!
Warning: exiting minimize because ITER exceeded!
Warning: exiting minimize because ITER exceeded!
Warning: exiting minimize because ITER exceeded!
Warning: exiting minimize because ITER exceeded!
Nested inside NER Mention parent node: phrase
Nested inside NER Mention parent node: paragraph
Nested inside NER Mention parent node: section
Nested inside NER Mention parent node: title
Nested inside NER Mention parent node: heading
Nested inside NER Mention parent node: header
Nested inside NER Mention parent node: content
Nested inside NER Mention parent node: element
Nested inside NER Mention parent node: block
Nested inside NER Mention parent node: body
Nested inside NER Mention parent node: document
Nested inside NER Mention parent node: group
Nested inside NER Mention parent node: container
Nested inside NER Mention: organization
Nested inside NER Mention: location
Nested inside NER Mention: date
Nested inside NER Mention: event
Nested inside NER Mention: product
Nested inside NER Mention: time
Nested inside NER Mention: money
Nested inside NER Mention: percent
Nested inside NER Mention: language
Nested inside NER Mention: ordinal
Nested inside NER Mention: cardinal
Nested inside NER Mention: quantity
Nested inside NER Mention: facility
Result is pQ86bc3 after 5000
Result is d3V94H6 after 7500
Result is sZjOpB2 after 2500
Result is rNvGmP5 after 8000
Result is fXlYqC9 after 6000
Result is aBwKsJ4 after 9000
Result is jHmUtN8 after 3500
Result is gYvXqD7 after 7000
Result is mZyRwH1 after 4500
Result is eLuSnT6 after 5500
Result is tKoXmB3 after 4000
Result is nYpXkH2 after 6500
Result is bQvRaE5 after 8500
NER Mention: organization
NER Mention: location
NER Mention: date
NER Mention: event
NER Mention: product
NER Mention: money
NER Mention: percentage
NER Mention: time
NER Mention: email
NER Mention: website
NER Mention: document
NER Mention: project
NER Mention: software
NPT: Mention span is 5, 6, mention is Mary
NPT: Mention span is 20, 21, mention is Peter
NPT: Mention span is 15, 16, mention is David
NPT: Mention span is 8, 9, mention is Lisa
NPT: Mention span is 25, 26, mention is Sarah
NPT: Mention span is 12, 13, mention is Mike
NPT: Mention span is 18, 19, mention is Emily
NPT: Mention span is 3, 4, mention is Alex
NPT: Mention span is 7, 8, mention is Jessica
NPT: Mention span is 14, 15, mention is Daniel
NPT: Mention span is 6, 7, mention is Laura
NPT: Mention span is 17, 18, mention is Olivia
NPT: Mention span is 11, 12, mention is Benjamin
Initial deriv: calculateDerivative(f, xi, epsilon)
Initial deriv: getInitialDerivative(xi, numToPrint)
Initial deriv: solveEquation(f, a, b, n)
Initial deriv: computeDerivative(f, xi, h)
Initial deriv: evaluateFunction(f, xi)
Initial deriv: findRoot(f, a, b, epsilon)
Initial deriv: calculateSlope(f, xi, h)
Initial deriv: getDerivative(f, xi)
Initial deriv: integrateFunction(f, a, b, n)
Initial deriv: approximateIntegral(f, a, b, n)
Initial deriv: calculateGradient(f, xi, epsilon)
Initial deriv: computeJacobian(f, xi, h)
Initial deriv: optimizeFunction(f, xi, n)
NPT: Tree span is 15, tree node is B
NPT: Tree span is 8, tree node is C
NPT: Tree span is 12, tree node is D
NPT: Tree span is 6, tree node is E
NPT: Tree span is 9, tree node is F
NPT: Tree span is 14, tree node is G
NPT: Tree span is 7, tree node is H
NPT: Tree span is 11, tree node is I
NPT: Tree span is 13, tree node is J
NPT: Tree span is 5, tree node is K
NPT: Tree span is 17, tree node is L
NPT: Tree span is 20, tree node is M
NPT: Tree span is 16, tree node is N
Mention span is 23 31, mention is dog
Mention span is 7 15, mention is bird
Mention span is 2 9, mention is rabbit
Mention span is 12 21, mention is horse
Mention span is 5 13, mention is cow
Mention span is 17 24, mention is elephant
Mention span is 8 17, mention is lion
Mention span is 13 19, mention is tiger
Mention span is 22 30, mention is monkey
Mention span is 4 11, mention is giraffe
Mention span is 14 23, mention is zebra
Mention span is 6 12, mention is kangaroo
Mention span is 9 17, mention is panda
Initial at: [4, 5, 6]
Initial at: [7, 8, 9]
Initial at: [10, 11, 12]
Initial at: [13, 14, 15]
Initial at: [16, 17, 18]
Initial at: [19, 20, 21]
Initial at: [22, 23, 24]
Initial at: [25, 26, 27]
Initial at: [28, 29, 30]
Initial at: [31, 32, 33]
Initial at: [34, 35, 36]
Initial at: [37, 38, 39]
Initial at: [40, 41, 42]
Initial: image.jpg
Initial: socket.jpg
Initial: data.txt
Initial: video.mp4
Initial: log.txt
Initial: config.xml
Initial: temp.jpg
Initial: report.doc
Initial: code.java
Initial: backup.zip
Initial: design.ppt
Initial: project.xlsx
Initial: index.html
Adding new collaboration dependency from Sarah to Michael (subj/obj case)
Adding new partnership dependency from Lisa to David (subj/obj case)
Adding new connection dependency from Robert to Jennifer (subj/obj case)
Adding new communication dependency from Emily to Christopher (subj/obj case)
Adding new alliance dependency from Jessica to William (subj/obj case)
Adding new bond dependency from Amanda to Daniel (subj/obj case)
Adding new relation dependency from Olivia to Joseph (subj/obj case)
Adding new affiliation dependency from Sophia to Ethan (subj/obj case)
Adding new association dependency from Mia to Benjamin (subj/obj case)
Adding new union dependency from Ava to Alexander (subj/obj case)
Adding new link dependency from Harper to James (subj/obj case)
Adding new connection dependency from Evelyn to Samuel (subj/obj case)
Adding new attachment dependency from Grace to Henry (subj/obj case)
Adding new partnership dependency from Victoria to Charles (subj/obj case)
Bad bracket order!
Bad bracket order!
Bad bracket order!
Bad bracket order!
Bad bracket order!
Bad bracket order!
Bad bracket order!
Bad bracket order!
Bad bracket order!
Bad bracket order!
Bad bracket order!
Bad bracket order!
Bad bracket order!
Bad bracket order!
Bad bracket order!
java edu.stanford.nlp.dcoref.CoNLL2011DocumentReader [-ext <extension to match>] -i <inputpath> -o <outputfile>
java edu.stanford.nlp.dcoref.CoNLL2011DocumentReader [-ext <extension to match>] -i <inputpath> -o <outputfile>
java edu.stanford.nlp.dcoref.CoNLL2011DocumentReader [-ext <extension to match>] -i <inputpath> -o <outputfile>
java edu.stanford.nlp.dcoref.CoNLL2011DocumentReader [-ext <extension to match>] -i <inputpath> -o <outputfile>
java edu.stanford.nlp.dcoref.CoNLL2011DocumentReader [-ext <extension to match>] -i <inputpath> -o <outputfile>
java edu.stanford.nlp.dcoref.CoNLL2011DocumentReader [-ext <extension to match>] -i <inputpath> -o <outputfile>
java edu.stanford.nlp.dcoref.CoNLL2011DocumentReader [-ext <extension to match>] -i <inputpath> -o <outputfile>
java edu.stanford.nlp.dcoref.CoNLL2011DocumentReader [-ext <extension to match>] -i <inputpath> -o <outputfile>
java edu.stanford.nlp.dcoref.CoNLL2011DocumentReader [-ext <extension to match>] -i <inputpath> -o <outputfile>
java edu.stanford.nlp.dcoref.CoNLL2011DocumentReader [-ext <extension to match>] -i <inputpath> -o <outputfile>
java edu.stanford.nlp.dcoref.CoNLL2011DocumentReader [-ext <extension to match>] -i <inputpath> -o <outputfile>
java edu.stanford.nlp.dcoref.CoNLL2011DocumentReader [-ext <extension to match>] -i <inputpath> -o <outputfile>
java edu.stanford.nlp.dcoref.CoNLL2011DocumentReader [-ext <extension to match>] -i <inputpath> -o <outputfile>
java edu.stanford.nlp.dcoref.CoNLL2011DocumentReader [-ext <extension to match>] -i <inputpath> -o <outputfile>
java edu.stanford.nlp.dcoref.CoNLL2011DocumentReader [-ext <extension to match>] -i <inputpath> -o <outputfile>
Unexpected begin document at line (" + filename + "," + lineCnt + ")
Unexpected begin document at line (" + filename + "," + lineCnt + ")
Unexpected begin document at line (" + filename + "," + lineCnt + ")
Unexpected begin document at line (" + filename + "," + lineCnt + ")
Unexpected begin document at line (" + filename + "," + lineCnt + ")
Unexpected begin document at line (" + filename + "," + lineCnt + ")
Unexpected begin document at line (" + filename + "," + lineCnt + ")
Unexpected begin document at line (" + filename + "," + lineCnt + ")
Unexpected begin document at line (" + filename + "," + lineCnt + ")
Unexpected begin document at line (" + filename + "," + lineCnt + ")
Unexpected begin document at line (" + filename + "," + lineCnt + ")
Unexpected begin document at line (" + filename + "," + lineCnt + ")
Unexpected begin document at line (" + filename + "," + lineCnt + ")
Unexpected begin document at line (" + filename + "," + lineCnt + ")
Unexpected begin document at line (" + filename + "," + lineCnt + ")
Warning: exiting dbrent because ITMAX exceeded!
Warning: exiting dbrent because ITMAX exceeded!
Warning: exiting dbrent because ITMAX exceeded!
Warning: exiting dbrent because ITMAX exceeded!
Warning: exiting dbrent because ITMAX exceeded!
Warning: exiting dbrent because ITMAX exceeded!
Warning: exiting dbrent because ITMAX exceeded!
Warning: exiting dbrent because ITMAX exceeded!
Warning: exiting dbrent because ITMAX exceeded!
Warning: exiting dbrent because ITMAX exceeded!
Warning: exiting dbrent because ITMAX exceeded!
Warning: exiting dbrent because ITMAX exceeded!
Warning: exiting dbrent because ITMAX exceeded!
Warning: exiting dbrent because ITMAX exceeded!
Warning: exiting dbrent because ITMAX exceeded!
ignoreWordsAll contains word U.S. is false
ignoreWordsAll contains word U.S. is true
ignoreWordsAll contains word U.S. is false
ignoreWordsAll contains word U.S. is true
ignoreWordsAll contains word U.S. is false
ignoreWordsAll contains word U.S. is true
ignoreWordsAll contains word U.S. is false
ignoreWordsAll contains word U.S. is true
ignoreWordsAll contains word U.S. is false
ignoreWordsAll contains word U.S. is true
ignoreWordsAll contains word U.S. is false
ignoreWordsAll contains word U.S. is true
ignoreWordsAll contains word U.S. is false
ignoreWordsAll contains word U.S. is true
dbrent returning because derivative is broken
dbrent returning because derivative is broken
dbrent returning because derivative is broken
dbrent returning because derivative is broken
dbrent returning because derivative is broken
dbrent returning because derivative is broken
dbrent returning because derivative is broken
dbrent returning because derivative is broken
dbrent returning because derivative is broken
dbrent returning because derivative is broken
dbrent returning because derivative is broken
dbrent returning because derivative is broken
dbrent returning because derivative is broken
dbrent returning because derivative is broken
dbrent returning because derivative is broken
count for word U.S. is 187
count for word U.S. is 339
count for word U.S. is 402
count for word U.S. is 555
count for word U.S. is 136
count for word U.S. is 298
count for word U.S. is 412
count for word U.S. is 623
count for word U.S. is 504
count for word U.S. is 241
count for word U.S. is 372
count for word U.S. is 545
count for word U.S. is 496
debug-docreaderReading document: document2 part: 2
debug-docreaderReading document: document3 part: 1
debug-docreaderReading document: document4 part: 2
debug-docreaderReading document: document5 part: 1
debug-docreaderReading document: document6 part: 2
debug-docreaderReading document: document7 part: 1
debug-docreaderReading document: document8 part: 2
debug-docreaderReading document: document9 part: 1
debug-docreaderReading document: document10 part: 2
debug-docreaderReading document: document11 part: 1
debug-docreaderReading document: document12 part: 2
debug-docreaderReading document: document13 part: 1
debug-docreaderReading document: document14 part: 2
dbrent returning because min is cornered 0.5 (0.5) ~ 1.5 (1.5) 2.5 (2.5)
dbrent returning because min is cornered 1.0 (1.0) ~ 2.0 (2.0) 3.0 (3.0)
dbrent returning because min is cornered 1.5 (1.5) ~ 2.5 (2.5) 3.5 (3.5)
dbrent returning because min is cornered 2.0 (2.0) ~ 3.0 (3.0) 4.0 (4.0)
dbrent returning because min is cornered 2.5 (2.5) ~ 3.5 (3.5) 4.5 (4.5)
dbrent returning because min is cornered 3.0 (3.0) ~ 4.0 (4.0) 5.0 (5.0)
dbrent returning because min is cornered 3.5 (3.5) ~ 4.5 (4.5) 5.5 (5.5)
dbrent returning because min is cornered 4.0 (4.0) ~ 5.0 (5.0) 6.0 (6.0)
dbrent returning because min is cornered 4.5 (4.5) ~ 5.5 (5.5) 6.5 (6.5)
dbrent returning because min is cornered 5.0 (5.0) ~ 6.0 (6.0) 7.0 (7.0)
dbrent returning because min is cornered 5.5 (5.5) ~ 6.5 (6.5) 7.5 (7.5)
dbrent returning because min is cornered 6.0 (6.0) ~ 7.0 (7.0) 8.0 (8.0)
dbrent returning because min is cornered 6.5 (6.5) ~ 7.5 (7.5) 8.5 (8.5)
Attempt to use ExternalFiniteDifference without passing currentDerivative
Attempt to use ExternalFiniteDifference without passing currentDerivative
Attempt to use ExternalFiniteDifference without passing currentDerivative
Attempt to use ExternalFiniteDifference without passing currentDerivative
Attempt to use ExternalFiniteDifference without passing currentDerivative
Attempt to use ExternalFiniteDifference without passing currentDerivative
Attempt to use ExternalFiniteDifference without passing currentDerivative
Attempt to use ExternalFiniteDifference without passing currentDerivative
Attempt to use ExternalFiniteDifference without passing currentDerivative
Attempt to use ExternalFiniteDifference without passing currentDerivative
Attempt to use ExternalFiniteDifference without passing currentDerivative
Attempt to use ExternalFiniteDifference without passing currentDerivative
Attempt to use ExternalFiniteDifference without passing currentDerivative
Attempt to use ExternalFiniteDifference without passing currentDerivative
Attempt to use ExternalFiniteDifference without passing currentDerivative
ERROR: File/path /var/www/html/index.html does not exist. Skipping.
INFO: File/path /usr/local/bin/script.sh does not exist. Skipping.
WARNING: File/path /home/user/downloads/image.jpg does not exist. Skipping.
ERROR: File/path /etc/config/config.yml does not exist. Skipping.
INFO: File/path /usr/share/fonts/arial.ttf does not exist. Skipping.
WARNING: File/path /home/user/music/song.mp3 does not exist. Skipping.
ERROR: File/path /var/log/system.log does not exist. Skipping.
INFO: File/path /usr/bin/program does not exist. Skipping.
WARNING: File/path /home/user/pictures/photo.png does not exist. Skipping.
ERROR: File/path /var/www/html/index.php does not exist. Skipping.
INFO: File/path /usr/local/lib/library.so does not exist. Skipping.
WARNING: File/path /home/user/videos/video.mp4 does not exist. Skipping.
ERROR: File/path /etc/config/settings.ini does not exist. Skipping.
Setting H dot V.
Setting H dot V.
Setting H dot V.
Setting H dot V.
Setting H dot V.
Setting H dot V.
Setting H dot V.
Setting H dot V.
Setting H dot V.
Setting H dot V.
Setting H dot V.
Setting H dot V.
Setting H dot V.
Setting H dot V.
Setting H dot V.
Options: 	-v verbose output
Options: 	-v verbose output
Options: 	-v verbose output
Options: 	-v verbose output
Options: 	-v verbose output
Options: 	-v verbose output
Options: 	-v verbose output
Options: 	-v verbose output
Options: 	-v verbose output
Options: 	-v verbose output
Options: 	-v verbose output
Options: 	-v verbose output
Options: 	-v verbose output
Options: 	-v verbose output
Options: 	-v verbose output
Setting perturbed.
Setting perturbed.
Setting perturbed.
Setting perturbed.
Setting perturbed.
Setting perturbed.
Setting perturbed.
Setting perturbed.
Setting perturbed.
Setting perturbed.
Setting perturbed.
Setting perturbed.
Setting perturbed.
Setting perturbed.
Setting perturbed.
Usage: 	java ...DependencyScoring [-v True/False] [-conllx True/False] [-jsonOutput True/False] [-ignorePunc True/False] -g goldFile -s systemFile
Usage: 	java ...DependencyScoring [-v True/False] [-conllx True/False] [-jsonOutput True/False] [-ignorePunc True/False] -g goldFile -s systemFile
Usage: 	java ...DependencyScoring [-v True/False] [-conllx True/False] [-jsonOutput True/False] [-ignorePunc True/False] -g goldFile -s systemFile
Usage: 	java ...DependencyScoring [-v True/False] [-conllx True/False] [-jsonOutput True/False] [-ignorePunc True/False] -g goldFile -s systemFile
Usage: 	java ...DependencyScoring [-v True/False] [-conllx True/False] [-jsonOutput True/False] [-ignorePunc True/False] -g goldFile -s systemFile
Usage: 	java ...DependencyScoring [-v True/False] [-conllx True/False] [-jsonOutput True/False] [-ignorePunc True/False] -g goldFile -s systemFile
Usage: 	java ...DependencyScoring [-v True/False] [-conllx True/False] [-jsonOutput True/False] [-ignorePunc True/False] -g goldFile -s systemFile
Usage: 	java ...DependencyScoring [-v True/False] [-conllx True/False] [-jsonOutput True/False] [-ignorePunc True/False] -g goldFile -s systemFile
Usage: 	java ...DependencyScoring [-v True/False] [-conllx True/False] [-jsonOutput True/False] [-ignorePunc True/False] -g goldFile -s systemFile
Usage: 	java ...DependencyScoring [-v True/False] [-conllx True/False] [-jsonOutput True/False] [-ignorePunc True/False] -g goldFile -s systemFile
Usage: 	java ...DependencyScoring [-v True/False] [-conllx True/False] [-jsonOutput True/False] [-ignorePunc True/False] -g goldFile -s systemFile
Usage: 	java ...DependencyScoring [-v True/False] [-conllx True/False] [-jsonOutput True/False] [-ignorePunc True/False] -g goldFile -s systemFile
Usage: 	java ...DependencyScoring [-v True/False] [-conllx True/False] [-jsonOutput True/False] [-ignorePunc True/False] -g goldFile -s systemFile
Usage: 	java ...DependencyScoring [-v True/False] [-conllx True/False] [-jsonOutput True/False] [-ignorePunc True/False] -g goldFile -s systemFile
Usage: 	java ...DependencyScoring [-v True/False] [-conllx True/False] [-jsonOutput True/False] [-ignorePunc True/False] -g goldFile -s systemFile
Setting approximate gradient.
Setting approximate gradient.
Setting approximate gradient.
Setting approximate gradient.
Setting approximate gradient.
Setting approximate gradient.
Setting approximate gradient.
Setting approximate gradient.
Setting approximate gradient.
Setting approximate gradient.
Setting approximate gradient.
Setting approximate gradient.
Setting approximate gradient.
Setting approximate gradient.
Setting approximate gradient.
Setting last batch
Setting last batch
Setting last batch
Setting last batch
Setting last batch
Setting last batch
Setting last batch
Setting last batch
Setting last batch
Setting last batch
Setting last batch
Setting last batch
Setting last batch
Setting last batch
Setting last batch
Normalized numbers in token: 456 => 0.456
Normalized numbers in token: 789 => 0.789
Normalized numbers in token: 987 => 0.987
Normalized numbers in token: 654 => 0.654
Normalized numbers in token: 321 => 0.321
Normalized numbers in token: 246 => 0.246
Normalized numbers in token: 135 => 0.135
Normalized numbers in token: 864 => 0.864
Normalized numbers in token: 975 => 0.975
Normalized numbers in token: 531 => 0.531
Normalized numbers in token: 642 => 0.642
Normalized numbers in token: 879 => 0.879
Normalized numbers in token: 294 => 0.294
debug-preprocessorEmpty paragraph; skipping findParagraphSpeaker
debug-preprocessorEmpty paragraph; skipping findParagraphSpeaker
debug-preprocessorEmpty paragraph; skipping findParagraphSpeaker
debug-preprocessorEmpty paragraph; skipping findParagraphSpeaker
debug-preprocessorEmpty paragraph; skipping findParagraphSpeaker
debug-preprocessorEmpty paragraph; skipping findParagraphSpeaker
debug-preprocessorEmpty paragraph; skipping findParagraphSpeaker
debug-preprocessorEmpty paragraph; skipping findParagraphSpeaker
debug-preprocessorEmpty paragraph; skipping findParagraphSpeaker
debug-preprocessorEmpty paragraph; skipping findParagraphSpeaker
debug-preprocessorEmpty paragraph; skipping findParagraphSpeaker
debug-preprocessorEmpty paragraph; skipping findParagraphSpeaker
debug-preprocessorEmpty paragraph; skipping findParagraphSpeaker
debug-preprocessorEmpty paragraph; skipping findParagraphSpeaker
debug-preprocessorEmpty paragraph; skipping findParagraphSpeaker
Dropping Punctuation Dependency: period
Dropping Punctuation Dependency: question mark
Dropping Punctuation Dependency: exclamation mark
Dropping Punctuation Dependency: semicolon
Dropping Punctuation Dependency: colon
Dropping Punctuation Dependency: hyphen
Dropping Punctuation Dependency: parentheses
Dropping Punctuation Dependency: quotation marks
Dropping Punctuation Dependency: apostrophe
Dropping Punctuation Dependency: ellipsis
Dropping Punctuation Dependency: asterisk
Dropping Punctuation Dependency: dollar sign
Dropping Punctuation Dependency: percent sign
Setting Derivative.
Setting Derivative.
Setting Derivative.
Setting Derivative.
Setting Derivative.
Setting Derivative.
Setting Derivative.
Setting Derivative.
Setting Derivative.
Setting Derivative.
Setting Derivative.
Setting Derivative.
Setting Derivative.
Setting Derivative.
Setting Derivative.
Tree transformed
Tree transformed
Tree transformed
Tree transformed
Tree transformed
Tree transformed
Tree transformed
Tree transformed
Tree transformed
Tree transformed
Tree transformed
Tree transformed
Tree transformed
Tree transformed
Tree transformed
Setting previous gain (v)
Setting previous gain (v)
Setting previous gain (v)
Setting previous gain (v)
Setting previous gain (v)
Setting previous gain (v)
Setting previous gain (v)
Setting previous gain (v)
Setting previous gain (v)
Setting previous gain (v)
Setting previous gain (v)
Setting previous gain (v)
Setting previous gain (v)
Setting previous gain (v)
Setting previous gain (v)
Original tree
Original tree
Original tree
Original tree
Original tree
Original tree
Original tree
Original tree
Original tree
Original tree
Original tree
Original tree
Original tree
Original tree
Original tree
Setting previous position (x).
Setting previous position (x).
Setting previous position (x).
Setting previous position (x).
Setting previous position (x).
Setting previous position (x).
Setting previous position (x).
Setting previous position (x).
Setting previous position (x).
Setting previous position (x).
Setting previous position (x).
Setting previous position (x).
Setting previous position (x).
Setting previous position (x).
Setting previous position (x).
DD: Bob
DD: Carol
DD: Dave
DD: Eve
DD: Frank
DD: Grace
DD: Henry
DD: Ivy
DD: Jack
DD: Kelly
DD: Lucy
DD: Mike
DD: Nancy
After transformCC:             branch
After transformCC:             leaf
After transformCC:             trunk
After transformCC:             stem
After transformCC:             twig
After transformCC:             shoot
After transformCC:             bud
After transformCC:             flower
After transformCC:             fruit
After transformCC:             seed
After transformCC:             petal
After transformCC:             thorn
After transformCC:             stamen
incrementing random 2 times.
incrementing random 3 times.
incrementing random 4 times.
incrementing random 5 times.
incrementing random 6 times.
incrementing random 7 times.
incrementing random 8 times.
incrementing random 9 times.
incrementing random 10 times.
incrementing random 11 times.
incrementing random 12 times.
incrementing random 13 times.
incrementing random 14 times.
ALL gradients passed. Yay!
ALL gradients passed. Yay!
ALL gradients passed. Yay!
ALL gradients passed. Yay!
ALL gradients passed. Yay!
ALL gradients passed. Yay!
ALL gradients passed. Yay!
ALL gradients passed. Yay!
ALL gradients passed. Yay!
ALL gradients passed. Yay!
ALL gradients passed. Yay!
ALL gradients passed. Yay!
ALL gradients passed. Yay!
ALL gradients passed. Yay!
ALL gradients passed. Yay!
print tree
print tree
print tree
print tree
print tree
print tree
print tree
print tree
print tree
print tree
print tree
print tree
print tree
print tree
print tree
Bad indices:
Bad indices:
Bad indices:
Bad indices:
Bad indices:
Bad indices:
Bad indices:
Bad indices:
Bad indices:
Bad indices:
Bad indices:
Bad indices:
Bad indices:
Bad indices:
Bad indices:
more CC index 2
more CC index 3
more CC index 4
more CC index 5
more CC index 6
more CC index 7
more CC index 8
more CC index 9
more CC index 10
more CC index 11
more CC index 12
more CC index 13
more CC index 14
extremePatDebugadding word orange
extremePatDebugadding word banana
extremePatDebugadding word mango
extremePatDebugadding word grapefruit
extremePatDebugadding word pineapple
extremePatDebugadding word watermelon
extremePatDebugadding word kiwi
extremePatDebugadding word strawberry
extremePatDebugadding word lemon
extremePatDebugadding word peach
extremePatDebugadding word pear
extremePatDebugadding word cherry
extremePatDebugadding word blueberry
Youch! No t children
Youch! No t children
Youch! No t children
Youch! No t children
Youch! No t children
Youch! No t children
Youch! No t children
Youch! No t children
Youch! No t children
Youch! No t children
Youch! No t children
Youch! No t children
Youch! No t children
Youch! No t children
Youch! No t children
Grad fail at 1, appGrad=0.8, calcGrad=0.75, diff=0.05, pct=6%
Grad fail at 2, appGrad=1.2, calcGrad=1.18, diff=0.02, pct=2%
Grad fail at 3, appGrad=-0.2, calcGrad=-0.25, diff=-0.05, pct=20%
Grad fail at 4, appGrad=0.5, calcGrad=0.52, diff=-0.02, pct=4%
Grad fail at 5, appGrad=-1.0, calcGrad=-1.02, diff=-0.02, pct=2%
Grad fail at 6, appGrad=1.5, calcGrad=1.48, diff=0.02, pct=2%
Grad fail at 7, appGrad=-0.7, calcGrad=-0.72, diff=0.02, pct=3%
Grad fail at 8, appGrad=0.3, calcGrad=0.32, diff=-0.02, pct=6%
Grad fail at 9, appGrad=1.8, calcGrad=1.85, diff=-0.05, pct=3%
Grad fail at 10, appGrad=-0.1, calcGrad=-0.05, diff=0.05, pct=50%
Grad fail at 11, appGrad=0.4, calcGrad=0.38, diff=0.02, pct=5%
Grad fail at 12, appGrad=1.6, calcGrad=1.62, diff=-0.02, pct=1%
Grad fail at 13, appGrad=-0.6, calcGrad=-0.62, diff=0.02, pct=3%
print left tree
print left tree
print left tree
print left tree
print left tree
print left tree
print left tree
print left tree
print left tree
print left tree
print left tree
print left tree
print left tree
print left tree
print left tree
extremePatDebugnot adding banana because it matched false in common English word
extremePatDebugnot adding cat because it matched true in common English word
extremePatDebugnot adding dog because it matched false in common English word
extremePatDebugnot adding elephant because it matched true in common English word
extremePatDebugnot adding fish because it matched false in common English word
extremePatDebugnot adding grape because it matched true in common English word
extremePatDebugnot adding hat because it matched false in common English word
extremePatDebugnot adding ice cream because it matched true in common English word
extremePatDebugnot adding jellyfish because it matched false in common English word
extremePatDebugnot adding koala because it matched true in common English word
extremePatDebugnot adding lion because it matched false in common English word
extremePatDebugnot adding mango because it matched true in common English word
extremePatDebugnot adding necklace because it matched false in common English word
Youch! No child children
Youch! No child children
Youch! No child children
Youch! No child children
Youch! No child children
Youch! No child children
Youch! No child children
Youch! No child children
Youch! No child children
Youch! No child children
Youch! No child children
Youch! No child children
Youch! No child children
Youch! No child children
Youch! No child children
Youch! No right children
Youch! No right children
Youch! No right children
Youch! No right children
Youch! No right children
Youch! No right children
Youch! No right children
Youch! No right children
Youch! No right children
Youch! No right children
Youch! No right children
Youch! No right children
Youch! No right children
Youch! No right children
Youch! No right children
speakers value: 456
speakers value: 789
speakers value: 987
speakers value: 654
speakers value: 321
speakers value: 246
speakers value: 135
speakers value: 864
speakers value: 975
speakers value: 357
speakers value: 159
speakers value: 753
speakers value: 582
Youch! No left children
Youch! No left children
Youch! No left children
Youch! No left children
Youch! No left children
Youch! No left children
Youch! No left children
Youch! No left children
Youch! No left children
Youch! No left children
Youch! No left children
Youch! No left children
Youch! No left children
Youch! No left children
Youch! No left children
extremePatDebugNot adding banana because the number of non redundant patterns are below threshold of 10:{key4, key5, key6}
extremePatDebugNot adding orange because the number of non redundant patterns are below threshold of 10:{key7, key8, key9}
extremePatDebugNot adding cherry because the number of non redundant patterns are below threshold of 10:{key10, key11, key12}
extremePatDebugNot adding pear because the number of non redundant patterns are below threshold of 10:{key13, key14, key15}
extremePatDebugNot adding watermelon because the number of non redundant patterns are below threshold of 10:{key16, key17, key18}
extremePatDebugNot adding pineapple because the number of non redundant patterns are below threshold of 10:{key19, key20, key21}
extremePatDebugNot adding strawberry because the number of non redundant patterns are below threshold of 10:{key22, key23, key24}
extremePatDebugNot adding grape because the number of non redundant patterns are below threshold of 10:{key25, key26, key27}
extremePatDebugNot adding mango because the number of non redundant patterns are below threshold of 10:{key28, key29, key30}
extremePatDebugNot adding kiwi because the number of non redundant patterns are below threshold of 10:{key31, key32, key33}
extremePatDebugNot adding lemon because the number of non redundant patterns are below threshold of 10:{key34, key35, key36}
extremePatDebugNot adding pomegranate because the number of non redundant patterns are below threshold of 10:{key37, key38, key39}
extremePatDebugNot adding peach because the number of non redundant patterns are below threshold of 10:{key40, key41, key42}
usage: XMLBeginEndIterator file element keepInternalBoolean
usage: XMLBeginEndIterator file element keepInternalBoolean
usage: XMLBeginEndIterator file element keepInternalBoolean
usage: XMLBeginEndIterator file element keepInternalBoolean
usage: XMLBeginEndIterator file element keepInternalBoolean
usage: XMLBeginEndIterator file element keepInternalBoolean
usage: XMLBeginEndIterator file element keepInternalBoolean
usage: XMLBeginEndIterator file element keepInternalBoolean
usage: XMLBeginEndIterator file element keepInternalBoolean
usage: XMLBeginEndIterator file element keepInternalBoolean
usage: XMLBeginEndIterator file element keepInternalBoolean
usage: XMLBeginEndIterator file element keepInternalBoolean
usage: XMLBeginEndIterator file element keepInternalBoolean
usage: XMLBeginEndIterator file element keepInternalBoolean
usage: XMLBeginEndIterator file element keepInternalBoolean
utterance: How can I help you?
utterance: What's on your mind?
utterance: Is there anything specific you'd like to talk about?
utterance: Tell me more about it
utterance: I'm here to assist you
utterance: Let's get started
utterance: How may I assist you today?
utterance: What brings you here today?
utterance: Feel free to ask any questions
utterance: Let's dive into the details
utterance: I'm all ears
utterance: What would you like to discuss?
utterance: How can I make your day better?
After rearrangeNowThat:           20
After rearrangeNowThat:           30
After rearrangeNowThat:           40
After rearrangeNowThat:           50
After rearrangeNowThat:           60
After rearrangeNowThat:           70
After rearrangeNowThat:           80
After rearrangeNowThat:           90
After rearrangeNowThat:           100
After rearrangeNowThat:           110
After rearrangeNowThat:           120
After rearrangeNowThat:           130
After rearrangeNowThat:           140
extremePatDebugcomputing rlogfposneg
extremePatDebugcomputing rlogfposneg
extremePatDebugcomputing rlogfposneg
extremePatDebugcomputing rlogfposneg
extremePatDebugcomputing rlogfposneg
extremePatDebugcomputing rlogfposneg
extremePatDebugcomputing rlogfposneg
extremePatDebugcomputing rlogfposneg
extremePatDebugcomputing rlogfposneg
extremePatDebugcomputing rlogfposneg
extremePatDebugcomputing rlogfposneg
extremePatDebugcomputing rlogfposneg
extremePatDebugcomputing rlogfposneg
extremePatDebugcomputing rlogfposneg
extremePatDebugcomputing rlogfposneg
extremePatDebugcomputing rlogfunlabeg
extremePatDebugcomputing rlogfunlabeg
extremePatDebugcomputing rlogfunlabeg
extremePatDebugcomputing rlogfunlabeg
extremePatDebugcomputing rlogfunlabeg
extremePatDebugcomputing rlogfunlabeg
extremePatDebugcomputing rlogfunlabeg
extremePatDebugcomputing rlogfunlabeg
extremePatDebugcomputing rlogfunlabeg
extremePatDebugcomputing rlogfunlabeg
extremePatDebugcomputing rlogfunlabeg
extremePatDebugcomputing rlogfunlabeg
extremePatDebugcomputing rlogfunlabeg
extremePatDebugcomputing rlogfunlabeg
extremePatDebugcomputing rlogfunlabeg
WARNING: Dimensionality of numHid parameter and word vectors do not match, deleting word vector dimensions to fit!
WARNING: Dimensionality of numHid parameter and word vectors do not match, deleting word vector dimensions to fit!
WARNING: Dimensionality of numHid parameter and word vectors do not match, deleting word vector dimensions to fit!
WARNING: Dimensionality of numHid parameter and word vectors do not match, deleting word vector dimensions to fit!
WARNING: Dimensionality of numHid parameter and word vectors do not match, deleting word vector dimensions to fit!
WARNING: Dimensionality of numHid parameter and word vectors do not match, deleting word vector dimensions to fit!
WARNING: Dimensionality of numHid parameter and word vectors do not match, deleting word vector dimensions to fit!
WARNING: Dimensionality of numHid parameter and word vectors do not match, deleting word vector dimensions to fit!
WARNING: Dimensionality of numHid parameter and word vectors do not match, deleting word vector dimensions to fit!
WARNING: Dimensionality of numHid parameter and word vectors do not match, deleting word vector dimensions to fit!
WARNING: Dimensionality of numHid parameter and word vectors do not match, deleting word vector dimensions to fit!
WARNING: Dimensionality of numHid parameter and word vectors do not match, deleting word vector dimensions to fit!
WARNING: Dimensionality of numHid parameter and word vectors do not match, deleting word vector dimensions to fit!
WARNING: Dimensionality of numHid parameter and word vectors do not match, deleting word vector dimensions to fit!
WARNING: Dimensionality of numHid parameter and word vectors do not match, deleting word vector dimensions to fit!
After moveRB:                     false
After moveRB:                     5
After moveRB:                     null
After moveRB:                     [1, 2, 3]
After moveRB:                     'text'
After moveRB:                     {name: 'John', age: 30}
After moveRB:                     [12, 34, 56]
After moveRB:                     {id: 123, name: 'Product A'}
After moveRB:                     3.14
After moveRB:                     (endMillis - startMillis)
After moveRB:                     function() { console.log('Hello') }
After moveRB:                     ['a', 'b', 'c']
After moveRB:                     new Date()
extremePatDebugcomputing rlogfneg
extremePatDebugcomputing rlogfneg
extremePatDebugcomputing rlogfneg
extremePatDebugcomputing rlogfneg
extremePatDebugcomputing rlogfneg
extremePatDebugcomputing rlogfneg
extremePatDebugcomputing rlogfneg
extremePatDebugcomputing rlogfneg
extremePatDebugcomputing rlogfneg
extremePatDebugcomputing rlogfneg
extremePatDebugcomputing rlogfneg
extremePatDebugcomputing rlogfneg
extremePatDebugcomputing rlogfneg
extremePatDebugcomputing rlogfneg
extremePatDebugcomputing rlogfneg
debug-md# of found gold mentions: 5 / # of gold mentions: 15
debug-md# of found gold mentions: 8 / # of gold mentions: 18
debug-md# of found gold mentions: 12 / # of gold mentions: 22
debug-md# of found gold mentions: 6 / # of gold mentions: 16
debug-md# of found gold mentions: 9 / # of gold mentions: 19
debug-md# of found gold mentions: 7 / # of gold mentions: 17
debug-md# of found gold mentions: 15 / # of gold mentions: 25
debug-md# of found gold mentions: 11 / # of gold mentions: 21
debug-md# of found gold mentions: 3 / # of gold mentions: 13
debug-md# of found gold mentions: 14 / # of gold mentions: 24
debug-md# of found gold mentions: 4 / # of gold mentions: 14
debug-md# of found gold mentions: 13 / # of gold mentions: 23
debug-md# of found gold mentions: 2 / # of gold mentions: 12
LOAD: WordVectors
LOAD: WordVectors
LOAD: WordVectors
LOAD: WordVectors
LOAD: WordVectors
LOAD: WordVectors
LOAD: WordVectors
LOAD: WordVectors
LOAD: WordVectors
LOAD: WordVectors
LOAD: WordVectors
LOAD: WordVectors
LOAD: WordVectors
LOAD: WordVectors
LOAD: WordVectors
LOADING SEMANTICS
LOADING SEMANTICS
LOADING SEMANTICS
LOADING SEMANTICS
LOADING SEMANTICS
LOADING SEMANTICS
LOADING SEMANTICS
LOADING SEMANTICS
LOADING SEMANTICS
LOADING SEMANTICS
LOADING SEMANTICS
LOADING SEMANTICS
LOADING SEMANTICS
LOADING SEMANTICS
LOADING SEMANTICS
Nothing to do for image.jpg
Nothing to do for data.csv
Nothing to do for script.js
Nothing to do for config.properties
Nothing to do for index.html
Nothing to do for style.css
Nothing to do for report.pdf
Nothing to do for log.txt
Nothing to do for backup.zip
Nothing to do for README.md
Nothing to do for test.py
Nothing to do for presentation.pptx
Nothing to do for database.sql
After DateTreeTransformer:        2022-02-02
After DateTreeTransformer:        2023-03-03
After DateTreeTransformer:        2024-04-04
After DateTreeTransformer:        2025-05-05
After DateTreeTransformer:        2026-06-06
After DateTreeTransformer:        2027-07-07
After DateTreeTransformer:        2028-08-08
After DateTreeTransformer:        2029-09-09
After DateTreeTransformer:        2030-10-10
After DateTreeTransformer:        2031-11-11
After DateTreeTransformer:        2032-12-12
After DateTreeTransformer:        2033-01-01
After DateTreeTransformer:        2034-02-02
After QPTreeTransformer:          optimized tree2
After QPTreeTransformer:          simplified tree3
After QPTreeTransformer:          modified tree4
After QPTreeTransformer:          optimized tree5
After QPTreeTransformer:          simplified tree6
After QPTreeTransformer:          transformed tree7
After QPTreeTransformer:          modified tree8
After QPTreeTransformer:          optimized tree9
After QPTreeTransformer:          simplified tree10
After QPTreeTransformer:          transformed tree11
After QPTreeTransformer:          optimized tree12
After QPTreeTransformer:          modified tree13
After QPTreeTransformer:          simplified tree14
r  (interm):  0.456
r  (interm):  0.789
r  (interm):  1.234
r  (interm):  2.345
r  (interm):  3.456
r  (interm):  4.567
r  (interm):  5.678
r  (interm):  6.789
r  (interm):  7.890
r  (interm):  8.901
r  (interm):  9.012
r  (interm):  10.123
r  (interm):  11.234
Number of documents added are 250
Number of documents added are 500
Number of documents added are 1000
Number of documents added are 2000
Number of documents added are 5000
Number of documents added are 10000
Number of documents added are 15000
Number of documents added are 20000
Number of documents added are 25000
Number of documents added are 50000
Number of documents added are 75000
Number of documents added are 100000
Number of documents added are 150000
p  (interm):  4.56
p  (interm):  7.89
p  (interm):  0.12
p  (interm):  3.45
p  (interm):  6.78
p  (interm):  9.01
p  (interm):  2.34
p  (interm):  5.67
p  (interm):  8.90
p  (interm):  1.23
p  (interm):  4.56
p  (interm):  7.89
p  (interm):  0.12
r  (split):   34561
r  (split):   91286
r  (split):   62984
r  (split):   12345
r  (split):   67890
r  (split):   24680
r  (split):   13579
r  (split):   54321
r  (split):   98765
r  (split):   11111
r  (split):   22222
r  (split):   33333
r  (split):   44444
p  (split):   2022-06-02
p  (split):   2022-06-03
p  (split):   2022-06-04
p  (split):   2022-06-05
p  (split):   2022-06-06
p  (split):   2022-06-07
p  (split):   2022-06-08
p  (split):   2022-06-09
p  (split):   2022-06-10
p  (split):   2022-06-11
p  (split):   2022-06-12
p  (split):   2022-06-13
p  (split):   2022-06-14
interm count: 15
interm count: 10
interm count: 17
interm count: 5
interm count: 9
interm count: 30
interm count: 12
interm count: 28
interm count: 20
interm count: 7
interm count: 14
interm count: 26
interm count: 19
split count:  2
split count:  5
split count:  8
split count:  3
split count:  9
split count:  6
split count:  1
split count:  7
split count:  4
split count:  15
split count:  10
split count:  12
split count:  11
split count:  13
size:         500
size:         1000
size:         2000
size:         5000
size:         10000
size:         20000
size:         50000
size:         100000
size:         200000
size:         500000
size:         1000000
size:         2000000
size:         5000000
Input to CoordinationTransformer: 200
Input to CoordinationTransformer: 300
Input to CoordinationTransformer: 400
Input to CoordinationTransformer: 500
Input to CoordinationTransformer: 600
Input to CoordinationTransformer: 700
Input to CoordinationTransformer: 800
Input to CoordinationTransformer: 900
Input to CoordinationTransformer: 1000
Input to CoordinationTransformer: 1100
Input to CoordinationTransformer: 1200
Input to CoordinationTransformer: 1300
Input to CoordinationTransformer: 1400
CollocationFinder: Not collapsing the/a word: banana
CollocationFinder: Not collapsing the/a word: cherry
CollocationFinder: Not collapsing the/a word: dog
CollocationFinder: Not collapsing the/a word: elephant
CollocationFinder: Not collapsing the/a word: fish
CollocationFinder: Not collapsing the/a word: grape
CollocationFinder: Not collapsing the/a word: hat
CollocationFinder: Not collapsing the/a word: ice cream
CollocationFinder: Not collapsing the/a word: jacket
CollocationFinder: Not collapsing the/a word: kiwi
CollocationFinder: Not collapsing the/a word: lemon
CollocationFinder: Not collapsing the/a word: mango
CollocationFinder: Not collapsing the/a word: necklace
debug-clusterNE: LOCATION	first Mention's ID: 456	Heads: X Y Z	words: elephant fox goat
debug-clusterNE: ORGANIZATION	first Mention's ID: 789	Heads: P Q R	words: dog cat rat
debug-clusterNE: DATE	first Mention's ID: 987	Heads: S T U	words: orange lemon lime
debug-clusterNE: MONEY	first Mention's ID: 654	Heads: L M N	words: peach plum pear
debug-clusterNE: PERSON	first Mention's ID: 321	Heads: D E F	words: watermelon mango pineapple
debug-clusterNE: LOCATION	first Mention's ID: 654	Heads: X Y Z	words: kiwi strawberry blueberry
debug-clusterNE: ORGANIZATION	first Mention's ID: 987	Heads: P Q R	words: grapefruit pomegranate coconut
debug-clusterNE: DATE	first Mention's ID: 321	Heads: S T U	words: raspberry blackberry cherry
debug-clusterNE: MONEY	first Mention's ID: 123	Heads: L M N	words: banana orange coconut
debug-clusterNE: PERSON	first Mention's ID: 456	Heads: A B C	words: apple watermelon lime
debug-clusterNE: LOCATION	first Mention's ID: 789	Heads: X Y Z	words: mango strawberry pear
debug-clusterNE: ORGANIZATION	first Mention's ID: 321	Heads: P Q R	words: kiwi lemon pineapple
debug-clusterNE: DATE	first Mention's ID: 654	Heads: S T U	words: grapefruit blackberry blueberry
Sentence #2: Here comes the second sentence.
Sentence #3: A third sentence appears.
Sentence #4: The fourth sentence is now present.
Sentence #5: Welcome to sentence number five.
Sentence #6: Moving on to the sixth sentence.
Sentence #7: Lucky number seven is here.
Sentence #8: Sentence eight is ready.
Sentence #9: The ninth sentence is now available.
Sentence #10: We have reached sentence number ten.
Sentence #11: The eleventh sentence has arrived.
Sentence #12: Twelve is the number for this sentence.
Sentence #13: Thirteen is an unlucky sentence.
Sentence #14: Sentence fourteen is in place.
Found collocation in wordnet: set fire
Found collocation in wordnet: break down
Found collocation in wordnet: cut off
Found collocation in wordnet: give up
Found collocation in wordnet: pick up
Found collocation in wordnet: run out
Found collocation in wordnet: turn on
Found collocation in wordnet: set up
Found collocation in wordnet: take off
Found collocation in wordnet: look up
Found collocation in wordnet: make up
Found collocation in wordnet: break up
Found collocation in wordnet: go ahead
DONE processing files. 1 exceptions encountered.
DONE processing files. 2 exceptions encountered.
DONE processing files. 3 exceptions encountered.
DONE processing files. 4 exceptions encountered.
DONE processing files. 5 exceptions encountered.
DONE processing files. 6 exceptions encountered.
DONE processing files. 7 exceptions encountered.
DONE processing files. 8 exceptions encountered.
DONE processing files. 9 exceptions encountered.
DONE processing files. 10 exceptions encountered.
DONE processing files. 11 exceptions encountered.
DONE processing files. 12 exceptions encountered.
DONE processing files. 13 exceptions encountered.
DONE processing files. 14 exceptions encountered.
All files have been queued; awaiting termination...
All files have been queued; awaiting termination...
All files have been queued; awaiting termination...
All files have been queued; awaiting termination...
All files have been queued; awaiting termination...
All files have been queued; awaiting termination...
All files have been queued; awaiting termination...
All files have been queued; awaiting termination...
All files have been queued; awaiting termination...
All files have been queued; awaiting termination...
All files have been queued; awaiting termination...
All files have been queued; awaiting termination...
All files have been queued; awaiting termination...
All files have been queued; awaiting termination...
All files have been queued; awaiting termination...
Restructured tree is:
Restructured tree is:
Restructured tree is:
Restructured tree is:
Restructured tree is:
Restructured tree is:
Restructured tree is:
Restructured tree is:
Restructured tree is:
Restructured tree is:
Restructured tree is:
Restructured tree is:
Restructured tree is:
Restructured tree is:
Restructured tree is:
No lines found on standard in
No lines found on standard in
No lines found on standard in
No lines found on standard in
No lines found on standard in
No lines found on standard in
No lines found on standard in
No lines found on standard in
No lines found on standard in
No lines found on standard in
No lines found on standard in
No lines found on standard in
No lines found on standard in
No lines found on standard in
No lines found on standard in
Processing from stdin. Enter one sentence per line.
Processing from stdin. Enter one sentence per line.
Processing from stdin. Enter one sentence per line.
Processing from stdin. Enter one sentence per line.
Processing from stdin. Enter one sentence per line.
Processing from stdin. Enter one sentence per line.
Processing from stdin. Enter one sentence per line.
Processing from stdin. Enter one sentence per line.
Processing from stdin. Enter one sentence per line.
Processing from stdin. Enter one sentence per line.
Processing from stdin. Enter one sentence per line.
Processing from stdin. Enter one sentence per line.
Processing from stdin. Enter one sentence per line.
Processing from stdin. Enter one sentence per line.
Processing from stdin. Enter one sentence per line.
Cannot read file or file does not exist: 'image.jpg'
Cannot read file or file does not exist: 'document.doc'
Cannot read file or file does not exist: 'data.csv'
Cannot read file or file does not exist: 'config.json'
Cannot read file or file does not exist: 'backup.zip'
Cannot read file or file does not exist: 'script.js'
Cannot read file or file does not exist: 'style.css'
Cannot read file or file does not exist: 'audio.mp3'
Cannot read file or file does not exist: 'video.mp4'
Cannot read file or file does not exist: 'presentation.ppt'
Cannot read file or file does not exist: 'spreadsheet.xlsx'
Cannot read file or file does not exist: 'database.db'
Cannot read file or file does not exist: 'log.txt'
If you specify custom annotators, you must at least include 'openie'
If you specify custom annotators, you must at least include 'openie'
If you specify custom annotators, you must at least include 'openie'
If you specify custom annotators, you must at least include 'openie'
If you specify custom annotators, you must at least include 'openie'
If you specify custom annotators, you must at least include 'openie'
If you specify custom annotators, you must at least include 'openie'
If you specify custom annotators, you must at least include 'openie'
If you specify custom annotators, you must at least include 'openie'
If you specify custom annotators, you must at least include 'openie'
If you specify custom annotators, you must at least include 'openie'
If you specify custom annotators, you must at least include 'openie'
If you specify custom annotators, you must at least include 'openie'
If you specify custom annotators, you must at least include 'openie'
If you specify custom annotators, you must at least include 'openie'
Not loading a splitter model
Not loading a splitter model
Not loading a splitter model
Not loading a splitter model
Not loading a splitter model
Not loading a splitter model
Not loading a splitter model
Not loading a splitter model
Not loading a splitter model
Not loading a splitter model
Not loading a splitter model
Not loading a splitter model
Not loading a splitter model
Not loading a splitter model
Not loading a splitter model
Failed to annotate: I like to eat apples.
Failed to annotate: The sun is shining brightly.
Failed to annotate: Please close the door.
Failed to annotate: He is a talented musician.
Failed to annotate: They went for a long walk.
Failed to annotate: It's raining outside.
Failed to annotate: The dog is barking loudly.
Failed to annotate: I can't find my keys.
Failed to annotate: She is wearing a blue dress.
Failed to annotate: We need to hurry.
Failed to annotate: The book is on the table.
Failed to annotate: He's going to the park.
Failed to annotate: They are playing soccer.
{ok:false, entailments:[], triples=[], msg=""}
{ok:false, entailments:[], triples=[], msg=""}
{ok:false, entailments:[], triples=[], msg=""}
{ok:false, entailments:[], triples=[], msg=""}
{ok:false, entailments:[], triples=[], msg=""}
{ok:false, entailments:[], triples=[], msg=""}
{ok:false, entailments:[], triples=[], msg=""}
{ok:false, entailments:[], triples=[], msg=""}
{ok:false, entailments:[], triples=[], msg=""}
{ok:false, entailments:[], triples=[], msg=""}
{ok:false, entailments:[], triples=[], msg=""}
{ok:false, entailments:[], triples=[], msg=""}
{ok:false, entailments:[], triples=[], msg=""}
{ok:false, entailments:[], triples=[], msg=""}
{ok:false, entailments:[], triples=[], msg=""}
Could not load servlet context. Are you on the command line?
Could not load servlet context. Are you on the command line?
Could not load servlet context. Are you on the command line?
Could not load servlet context. Are you on the command line?
Could not load servlet context. Are you on the command line?
Could not load servlet context. Are you on the command line?
Could not load servlet context. Are you on the command line?
Could not load servlet context. Are you on the command line?
Could not load servlet context. Are you on the command line?
Could not load servlet context. Are you on the command line?
Could not load servlet context. Are you on the command line?
Could not load servlet context. Are you on the command line?
Could not load servlet context. Are you on the command line?
Could not load servlet context. Are you on the command line?
Could not load servlet context. Are you on the command line?
dataset size: 150
dataset size: 200
dataset size: 250
dataset size: 300
dataset size: 350
dataset size: 400
dataset size: 450
dataset size: 500
dataset size: 550
dataset size: 600
dataset size: 650
dataset size: 700
dataset size: 750
250 trees processed yielding 1000 known extractions
50 trees processed yielding 250 known extractions
300 trees processed yielding 1500 known extractions
75 trees processed yielding 375 known extractions
200 trees processed yielding 1000 known extractions
150 trees processed yielding 750 known extractions
80 trees processed yielding 400 known extractions
125 trees processed yielding 625 known extractions
400 trees processed yielding 2000 known extractions
90 trees processed yielding 450 known extractions
175 trees processed yielding 875 known extractions
60 trees processed yielding 300 known extractions
225 trees processed yielding 1125 known extractions
350 trees processed yielding 1750 known extractions
Created index with size 25. Don't worry if it's zero and you are using batch process sents.
Created index with size 50. Don't worry if it's zero and you are using batch process sents.
Created index with size 100. Don't worry if it's zero and you are using batch process sents.
Created index with size 200. Don't worry if it's zero and you are using batch process sents.
Created index with size 500. Don't worry if it's zero and you are using batch process sents.
Created index with size 1000. Don't worry if it's zero and you are using batch process sents.
Created index with size 2000. Don't worry if it's zero and you are using batch process sents.
Created index with size 5000. Don't worry if it's zero and you are using batch process sents.
Created index with size 10000. Don't worry if it's zero and you are using batch process sents.
Created index with size 20000. Don't worry if it's zero and you are using batch process sents.
Created index with size 50000. Don't worry if it's zero and you are using batch process sents.
Created index with size 100000. Don't worry if it's zero and you are using batch process sents.
Created index with size 200000. Don't worry if it's zero and you are using batch process sents.
allChildren is: [6, 7, 8, 9, 10]
allChildren is: [11, 12, 13, 14, 15]
allChildren is: [16, 17, 18, 19, 20]
allChildren is: [21, 22, 23, 24, 25]
allChildren is: [26, 27, 28, 29, 30]
allChildren is: [31, 32, 33, 34, 35]
allChildren is: [36, 37, 38, 39, 40]
allChildren is: [41, 42, 43, 44, 45]
allChildren is: [46, 47, 48, 49, 50]
allChildren is: [51, 52, 53, 54, 55]
allChildren is: [56, 57, 58, 59, 60]
allChildren is: [61, 62, 63, 64, 65]
allChildren is: [66, 67, 68, 69, 70]
Error: failed to get coref score from directory
Error: failed to get coref score from directory
Error: failed to get coref score from directory
Error: failed to get coref score from directory
Error: failed to get coref score from directory
Error: failed to get coref score from directory
Error: failed to get coref score from directory
Error: failed to get coref score from directory
Error: failed to get coref score from directory
Error: failed to get coref score from directory
Error: failed to get coref score from directory
Error: failed to get coref score from directory
Error: failed to get coref score from directory
Error: failed to get coref score from directory
Error: failed to get coref score from directory
Out of memory! Either change the memory allotted by running as java -mx20g ... for example if you want to allocate 20G. Or consider using batchProcessSents and numMaxSentencesPerBatchFile flags
Out of memory! Either change the memory allotted by running as java -mx20g ... for example if you want to allocate 20G. Or consider using batchProcessSents and numMaxSentencesPerBatchFile flags
Out of memory! Either change the memory allotted by running as java -mx20g ... for example if you want to allocate 20G. Or consider using batchProcessSents and numMaxSentencesPerBatchFile flags
Out of memory! Either change the memory allotted by running as java -mx20g ... for example if you want to allocate 20G. Or consider using batchProcessSents and numMaxSentencesPerBatchFile flags
Out of memory! Either change the memory allotted by running as java -mx20g ... for example if you want to allocate 20G. Or consider using batchProcessSents and numMaxSentencesPerBatchFile flags
Out of memory! Either change the memory allotted by running as java -mx20g ... for example if you want to allocate 20G. Or consider using batchProcessSents and numMaxSentencesPerBatchFile flags
Out of memory! Either change the memory allotted by running as java -mx20g ... for example if you want to allocate 20G. Or consider using batchProcessSents and numMaxSentencesPerBatchFile flags
Out of memory! Either change the memory allotted by running as java -mx20g ... for example if you want to allocate 20G. Or consider using batchProcessSents and numMaxSentencesPerBatchFile flags
Out of memory! Either change the memory allotted by running as java -mx20g ... for example if you want to allocate 20G. Or consider using batchProcessSents and numMaxSentencesPerBatchFile flags
Out of memory! Either change the memory allotted by running as java -mx20g ... for example if you want to allocate 20G. Or consider using batchProcessSents and numMaxSentencesPerBatchFile flags
Out of memory! Either change the memory allotted by running as java -mx20g ... for example if you want to allocate 20G. Or consider using batchProcessSents and numMaxSentencesPerBatchFile flags
Out of memory! Either change the memory allotted by running as java -mx20g ... for example if you want to allocate 20G. Or consider using batchProcessSents and numMaxSentencesPerBatchFile flags
Out of memory! Either change the memory allotted by running as java -mx20g ... for example if you want to allocate 20G. Or consider using batchProcessSents and numMaxSentencesPerBatchFile flags
Out of memory! Either change the memory allotted by running as java -mx20g ... for example if you want to allocate 20G. Or consider using batchProcessSents and numMaxSentencesPerBatchFile flags
Out of memory! Either change the memory allotted by running as java -mx20g ... for example if you want to allocate 20G. Or consider using batchProcessSents and numMaxSentencesPerBatchFile flags
All Extracted phrases are {word4, word5, word6}
All Extracted phrases are {word7, word8, word9}
All Extracted phrases are {word10, word11, word12}
All Extracted phrases are {word13, word14, word15}
All Extracted phrases are {word16, word17, word18}
All Extracted phrases are {word19, word20, word21}
All Extracted phrases are {word22, word23, word24}
All Extracted phrases are {word25, word26, word27}
All Extracted phrases are {word28, word29, word30}
All Extracted phrases are {word31, word32, word33}
All Extracted phrases are {word34, word35, word36}
All Extracted phrases are {word37, word38, word39}
All Extracted phrases are {word40, word41, word42}
fitting sigmoid...
fitting sigmoid...
fitting sigmoid...
fitting sigmoid...
fitting sigmoid...
fitting sigmoid...
fitting sigmoid...
fitting sigmoid...
fitting sigmoid...
fitting sigmoid...
fitting sigmoid...
fitting sigmoid...
fitting sigmoid...
fitting sigmoid...
fitting sigmoid...
-> average Score: 85.2
-> average Score: 93.8
-> average Score: 79.6
-> average Score: 88.3
-> average Score: 92.7
-> average Score: 78.9
-> average Score: 81.4
-> average Score: 89.1
-> average Score: 84.5
-> average Score: 90.6
-> average Score: 82.9
-> average Score: 87.2
-> average Score: 91.8
-> average Score: 88.9
Constituents: [1, 2, 3]
Constituents: [2, 3, 4]
Constituents: [3, 4, 5, 6]
Constituents: [0, 1, 2]
Constituents: [1, 2]
Constituents: [2, 3, 4, 5]
Constituents: [3, 4, 5, 6, 7, 8]
Constituents: [0, 1, 2, 3, 4]
Constituents: [1, 2, 3, 4, 5, 6]
Constituents: [2]
Constituents: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
Constituents: [1, 2, 3, 4, 5]
Constituents: [2, 3, 4, 5, 6, 7]
collocation string: banana
collocation string: orange
collocation string: mango
collocation string: strawberry
collocation string: pineapple
collocation string: watermelon
collocation string: grape
collocation string: lemon
collocation string: cherry
collocation string: kiwi
collocation string: plum
collocation string: peach
collocation string: pear
ERROR: failed to save model to path: /usr/local/models/model2
ERROR: failed to save model to path: /data/models/model3
ERROR: failed to save model to path: /opt/models/model4
ERROR: failed to save model to path: /var/lib/models/model5
ERROR: failed to save model to path: /mnt/models/model6
ERROR: failed to save model to path: /tmp/models/model7
ERROR: failed to save model to path: /var/log/models/model8
ERROR: failed to save model to path: /home/user/models/model9
ERROR: failed to save model to path: /usr/local/models/model10
ERROR: failed to save model to path: /data/models/model11
ERROR: failed to save model to path: /opt/models/model12
ERROR: failed to save model to path: /var/lib/models/model13
ERROR: failed to save model to path: /mnt/models/model14
in Cross Validate
in Cross Validate
in Cross Validate
in Cross Validate
in Cross Validate
in Cross Validate
in Cross Validate
in Cross Validate
in Cross Validate
in Cross Validate
in Cross Validate
in Cross Validate
in Cross Validate
in Cross Validate
in Cross Validate
head label: Login
head label: Home
head label: Profile
head label: Settings
head label: Dashboard
head label: Search
head label: Notifications
head label: Messages
head label: Favorites
head label: Orders
head label: Cart
head label: Wishlist
head label: Payments
NUM NONZERO PARAMETERS: 1
NUM NONZERO PARAMETERS: 2
NUM NONZERO PARAMETERS: 3
NUM NONZERO PARAMETERS: 4
NUM NONZERO PARAMETERS: 5
NUM NONZERO PARAMETERS: 6
NUM NONZERO PARAMETERS: 7
NUM NONZERO PARAMETERS: 8
NUM NONZERO PARAMETERS: 9
NUM NONZERO PARAMETERS: 10
NUM NONZERO PARAMETERS: 11
NUM NONZERO PARAMETERS: 12
NUM NONZERO PARAMETERS: 13
NUM NONZERO PARAMETERS: 14
processed 250 training sentences: 10000 datums
processed 400 training sentences: 15000 datums
processed 550 training sentences: 20000 datums
processed 700 training sentences: 25000 datums
processed 850 training sentences: 30000 datums
processed 1000 training sentences: 35000 datums
processed 1150 training sentences: 40000 datums
processed 1300 training sentences: 45000 datums
processed 1450 training sentences: 50000 datums
processed 1600 training sentences: 55000 datums
processed 1750 training sentences: 60000 datums
processed 1900 training sentences: 65000 datums
processed 2050 training sentences: 70000 datums
Found matching collocation for tree:
Found matching collocation for tree:
Found matching collocation for tree:
Found matching collocation for tree:
Found matching collocation for tree:
Found matching collocation for tree:
Found matching collocation for tree:
Found matching collocation for tree:
Found matching collocation for tree:
Found matching collocation for tree:
Found matching collocation for tree:
Found matching collocation for tree:
Found matching collocation for tree:
Found matching collocation for tree:
Found matching collocation for tree:
Removing patterns from iteration 2
Removing patterns from iteration 3
Removing patterns from iteration 4
Removing patterns from iteration 5
Removing patterns from iteration 6
Removing patterns from iteration 7
Removing patterns from iteration 8
Removing patterns from iteration 9
Removing patterns from iteration 10
Removing patterns from iteration 11
Removing patterns from iteration 12
Removing patterns from iteration 13
Removing patterns from iteration 14
Number of active feature types: 10
Number of active feature types: 5
Number of active feature types: 12
Number of active feature types: 3
Number of active feature types: 8
Number of active feature types: 6
Number of active feature types: 9
Number of active feature types: 4
Number of active feature types: 11
Number of active feature types: 2
Number of active feature types: 13
Number of active feature types: 1
Number of active feature types: 14
Collected collocations: verb phrases
Collected collocations: adjective phrases
Collected collocations: prepositional phrases
Collected collocations: adverb phrases
Collected collocations: conjunction phrases
Collected collocations: interjection phrases
Collected collocations: pronoun phrases
Collected collocations: determiner phrases
Collected collocations: numeral phrases
Collected collocations: article phrases
Collected collocations: possessive phrases
Collected collocations: infinitive phrases
Collected collocations: gerund phrases
Collected collocations: participle phrases
WARNING: Number of extracted dependencies (5) does not match yield (10):
WARNING: Number of extracted dependencies (7) does not match yield (3):
WARNING: Number of extracted dependencies (9) does not match yield (6):
WARNING: Number of extracted dependencies (15) does not match yield (4):
WARNING: Number of extracted dependencies (2) does not match yield (11):
WARNING: Number of extracted dependencies (8) does not match yield (7):
WARNING: Number of extracted dependencies (6) does not match yield (9):
WARNING: Number of extracted dependencies (11) does not match yield (2):
WARNING: Number of extracted dependencies (4) does not match yield (15):
WARNING: Number of extracted dependencies (10) does not match yield (5):
WARNING: Number of extracted dependencies (3) does not match yield (7):
WARNING: Number of extracted dependencies (13) does not match yield (14):
WARNING: Number of extracted dependencies (1) does not match yield (3):
Number of features (Phi(X) types): 15
Number of features (Phi(X) types): 20
Number of features (Phi(X) types): 25
Number of features (Phi(X) types): 30
Number of features (Phi(X) types): 35
Number of features (Phi(X) types): 40
Number of features (Phi(X) types): 45
Number of features (Phi(X) types): 50
Number of features (Phi(X) types): 55
Number of features (Phi(X) types): 60
Number of features (Phi(X) types): 65
Number of features (Phi(X) types): 70
Number of features (Phi(X) types): 75
Number of labels: 15
Number of labels: 7
Number of labels: 22
Number of labels: 5
Number of labels: 13
Number of labels: 9
Number of labels: 18
Number of labels: 12
Number of labels: 20
Number of labels: 11
Number of labels: 16
Number of labels: 8
Number of labels: 14
No eval sentences or list of gold entities provided to evaluate! Make sure evalFileWithGoldLabels or goldEntitiesEvalFiles is set, or turn off the evaluate flag
No eval sentences or list of gold entities provided to evaluate! Make sure evalFileWithGoldLabels or goldEntitiesEvalFiles is set, or turn off the evaluate flag
No eval sentences or list of gold entities provided to evaluate! Make sure evalFileWithGoldLabels or goldEntitiesEvalFiles is set, or turn off the evaluate flag
No eval sentences or list of gold entities provided to evaluate! Make sure evalFileWithGoldLabels or goldEntitiesEvalFiles is set, or turn off the evaluate flag
No eval sentences or list of gold entities provided to evaluate! Make sure evalFileWithGoldLabels or goldEntitiesEvalFiles is set, or turn off the evaluate flag
No eval sentences or list of gold entities provided to evaluate! Make sure evalFileWithGoldLabels or goldEntitiesEvalFiles is set, or turn off the evaluate flag
No eval sentences or list of gold entities provided to evaluate! Make sure evalFileWithGoldLabels or goldEntitiesEvalFiles is set, or turn off the evaluate flag
No eval sentences or list of gold entities provided to evaluate! Make sure evalFileWithGoldLabels or goldEntitiesEvalFiles is set, or turn off the evaluate flag
No eval sentences or list of gold entities provided to evaluate! Make sure evalFileWithGoldLabels or goldEntitiesEvalFiles is set, or turn off the evaluate flag
No eval sentences or list of gold entities provided to evaluate! Make sure evalFileWithGoldLabels or goldEntitiesEvalFiles is set, or turn off the evaluate flag
No eval sentences or list of gold entities provided to evaluate! Make sure evalFileWithGoldLabels or goldEntitiesEvalFiles is set, or turn off the evaluate flag
No eval sentences or list of gold entities provided to evaluate! Make sure evalFileWithGoldLabels or goldEntitiesEvalFiles is set, or turn off the evaluate flag
No eval sentences or list of gold entities provided to evaluate! Make sure evalFileWithGoldLabels or goldEntitiesEvalFiles is set, or turn off the evaluate flag
No eval sentences or list of gold entities provided to evaluate! Make sure evalFileWithGoldLabels or goldEntitiesEvalFiles is set, or turn off the evaluate flag
No eval sentences or list of gold entities provided to evaluate! Make sure evalFileWithGoldLabels or goldEntitiesEvalFiles is set, or turn off the evaluate flag
Known nonterms are: [NonTerminal4, NonTerminal5, NonTerminal6]
Known nonterms are: [NonTerminal7, NonTerminal8, NonTerminal9]
Known nonterms are: [NonTerminal10, NonTerminal11, NonTerminal12]
Known nonterms are: [NonTerminal13, NonTerminal14, NonTerminal15]
Known nonterms are: [NonTerminal16, NonTerminal17, NonTerminal18]
Known nonterms are: [NonTerminal19, NonTerminal20, NonTerminal21]
Known nonterms are: [NonTerminal22, NonTerminal23, NonTerminal24]
Known nonterms are: [NonTerminal25, NonTerminal26, NonTerminal27]
Known nonterms are: [NonTerminal28, NonTerminal29, NonTerminal30]
Known nonterms are: [NonTerminal31, NonTerminal32, NonTerminal33]
Known nonterms are: [NonTerminal34, NonTerminal35, NonTerminal36]
Known nonterms are: [NonTerminal37, NonTerminal38, NonTerminal39]
Known nonterms are: [NonTerminal40, NonTerminal41, NonTerminal42]
numFeatures (Phi(X) types): 200
numFeatures (Phi(X) types): 300
numFeatures (Phi(X) types): 400
numFeatures (Phi(X) types): 500
numFeatures (Phi(X) types): 600
numFeatures (Phi(X) types): 700
numFeatures (Phi(X) types): 800
numFeatures (Phi(X) types): 900
numFeatures (Phi(X) types): 1000
numFeatures (Phi(X) types): 1100
numFeatures (Phi(X) types): 1200
numFeatures (Phi(X) types): 1300
numFeatures (Phi(X) types): 1400
numDatums: 500
numDatums: 1000
numDatums: 2000
numDatums: 5000
numDatums: 10000
numDatums: 20000
numDatums: 50000
numDatums: 100000
numDatums: 200000
numDatums: 500000
numDatums: 1000000
numDatums: 2000000
numDatums: 5000000
Only one child determines ADJP as head of phrase
Only one child determines VP as head of clause
Only one child determines PP as head of prepositional phrase
Only one child determines NN as head of noun phrase
Only one child determines RB as head of adverb phrase
Only one child determines DT as head of determiner phrase
Only one child determines JJ as head of adjective phrase
Only one child determines CC as head of conjunction phrase
Only one child determines VBD as head of past verb phrase
Only one child determines IN as head of preposition phrase
Only one child determines NP as head of noun phrase
Only one child determines VBZ as head of present verb phrase
Only one child determines CD as head of cardinal phrase
Only one child determines PRP as head of pronoun phrase
model precision at recall 0.7 0.86
model precision at recall 0.9 0.79
model precision at recall 0.3 0.96
model precision at recall 0.6 0.89
model precision at recall 0.8 0.82
model precision at recall 0.2 0.98
model precision at recall 0.4 0.91
model precision at recall 0.1 1.00
model precision at recall 0.75 0.84
model precision at recall 0.85 0.80
model precision at recall 0.15 0.99
model precision at recall 0.35 0.94
model precision at recall 0.65 0.87
optimal precision at recall 0.2 0.71
optimal precision at recall 0.3 0.62
optimal precision at recall 0.4 0.56
optimal precision at recall 0.5 0.51
optimal precision at recall 0.6 0.47
optimal precision at recall 0.7 0.44
optimal precision at recall 0.8 0.41
optimal precision at recall 0.9 0.38
optimal precision at recall 1.0 0.35
optimal precision at recall 1.1 0.32
optimal precision at recall 1.2 0.30
optimal precision at recall 1.3 0.28
optimal precision at recall 1.4 0.26
Entering read_lambdas
Entering read_lambdas
Entering read_lambdas
Entering read_lambdas
Entering read_lambdas
Entering read_lambdas
Entering read_lambdas
Entering read_lambdas
Entering read_lambdas
Entering read_lambdas
Entering read_lambdas
Entering read_lambdas
Entering read_lambdas
Entering read_lambdas
Entering read_lambdas
P(a | b) = 0.32
P(x | y) = 0.67
P(k | m) = 0.91
P(p | q) = 0.76
P(u | v) = 0.42
P(c | d) = 0.55
P(e | f) = 0.28
P(g | h) = 0.69
P(w | z) = 0.81
P(r | s) = 0.63
P(n | o) = 0.49
P(t | x) = 0.72
P(l | n) = 0.58
size of q 20
size of q 5
size of q 15
size of q 8
size of q 12
size of q 3
size of q 18
size of q 7
size of q 14
size of q 6
size of q 9
size of q 17
size of q 4
The sum of all empirical expectations is 25
The sum of all empirical expectations is 37
The sum of all empirical expectations is 50
The sum of all empirical expectations is 63
The sum of all empirical expectations is 78
The sum of all empirical expectations is 92
The sum of all empirical expectations is 105
The sum of all empirical expectations is 118
The sum of all empirical expectations is 133
The sum of all empirical expectations is 146
The sum of all empirical expectations is 158
The sum of all empirical expectations is 170
The sum of all empirical expectations is 185
Error resolving temporal_FormatException
Error resolving temporal_NullPointerException
Error resolving temporal_ArrayIndexOutOfBoundsException
Error resolving temporal_IllegalArgumentException
Error resolving temporal_ClassCastException
Error resolving temporal_ParseException
Error resolving temporal_IOException
Error resolving temporal_StackOverflowError
Error resolving temporal_NoSuchElementException
Error resolving temporal_StringIndexOutOfBoundsException
Error resolving temporal_NumberFormatException
Error resolving temporal_ArithmeticException
Error resolving temporal_OutOfMemoryError
Gold entity list does not contain words Blue for label Color. Assuming them as negative.
Gold entity list does not contain words Car for label Object. Assuming them as negative.
Gold entity list does not contain words Dog for label Animal. Assuming them as negative.
Gold entity list does not contain words Elephant for label Animal. Assuming them as negative.
Gold entity list does not contain words Flower for label Plant. Assuming them as negative.
Gold entity list does not contain words Green for label Color. Assuming them as negative.
Gold entity list does not contain words House for label Object. Assuming them as negative.
Gold entity list does not contain words Ice Cream for label Food. Assuming them as negative.
Gold entity list does not contain words Jacket for label Clothing. Assuming them as negative.
Gold entity list does not contain words Kangaroo for label Animal. Assuming them as negative.
Gold entity list does not contain words Laptop for label Device. Assuming them as negative.
Gold entity list does not contain words Mango for label Fruit. Assuming them as negative.
Gold entity list does not contain words Notebook for label Object. Assuming them as negative.
x size 15  ysize 25
x size 8  ysize 16
x size 12  ysize 18
x size 6  ysize 10
x size 11  ysize 19
x size 9  ysize 14
x size 7  ysize 12
x size 16  ysize 32
x size 14  ysize 28
x size 13  ysize 26
x size 5  ysize 9
x size 19  ysize 38
x size 17  ysize 34
incoming size 200 resulting 100
incoming size 150 resulting 75
incoming size 80 resulting 40
incoming size 120 resulting 60
incoming size 250 resulting 125
incoming size 300 resulting 150
incoming size 180 resulting 90
incoming size 220 resulting 110
incoming size 90 resulting 45
incoming size 240 resulting 120
incoming size 160 resulting 80
incoming size 130 resulting 65
incoming size 190 resulting 95
Failed to process timex '14:30' with attributes 'type=time, value=14:30:00'
Failed to process timex 'today' with attributes 'type=date, value=2022-03-01'
Failed to process timex '2 hours ago' with attributes 'type=duration, value=-PT2H'
Failed to process timex 'next Monday' with attributes 'type=recursive, value=2022-W10'
Failed to process timex 'this year' with attributes 'type=recursive, value=2022'
Failed to process timex 'now' with attributes 'type=other, value='
Failed to process timex '3rd week of February' with attributes 'type=recursive, value=2022-W07'
Failed to process timex 'tomorrow' with attributes 'type=date, value=2022-03-02'
Failed to process timex '10 minutes' with attributes 'type=duration, value=PT10M'
Failed to process timex 'Thursday' with attributes 'type=date, value=2022-03-03'
Failed to process timex '3 days ago' with attributes 'type=duration, value=-P3D'
Failed to process timex '5th month of the year' with attributes 'type=recursive, value=YYYY-M05'
Failed to process timex 'next week' with attributes 'type=recursive, value=2022-W11'
Failed to get attributes from test.txt, timeIndex 1
Failed to get attributes from file.txt, timeIndex 2
Failed to get attributes from data.csv, timeIndex 3
Failed to get attributes from image.jpg, timeIndex 4
Failed to get attributes from log.txt, timeIndex 5
Failed to get attributes from report.docx, timeIndex 6
Failed to get attributes from index.html, timeIndex 7
Failed to get attributes from config.xml, timeIndex 8
Failed to get attributes from script.js, timeIndex 9
Failed to get attributes from style.css, timeIndex 10
Failed to get attributes from app.py, timeIndex 11
Failed to get attributes from form.html, timeIndex 12
Failed to get attributes from database.sql, timeIndex 13
Failed to get attributes from template.html, timeIndex 14
WARNING: No document date specified
WARNING: No document date specified
WARNING: No document date specified
WARNING: No document date specified
WARNING: No document date specified
WARNING: No document date specified
WARNING: No document date specified
WARNING: No document date specified
WARNING: No document date specified
WARNING: No document date specified
WARNING: No document date specified
WARNING: No document date specified
WARNING: No document date specified
WARNING: No document date specified
WARNING: No document date specified
Using following SUTime rules: french.sutime.txt
Using following SUTime rules: german.sutime.txt
Using following SUTime rules: spanish.sutime.txt
Using following SUTime rules: italian.sutime.txt
Using following SUTime rules: chinese.sutime.txt
Using following SUTime rules: japanese.sutime.txt
Using following SUTime rules: korean.sutime.txt
Using following SUTime rules: russian.sutime.txt
Using following SUTime rules: arabic.sutime.txt
Using following SUTime rules: portuguese.sutime.txt
Using following SUTime rules: dutch.sutime.txt
Using following SUTime rules: swedish.sutime.txt
Using following SUTime rules: danish.sutime.txt
Sigma used: 3.5
Sigma used: 1.8
Sigma used: 4.2
Sigma used: 2.7
Sigma used: 1.5
Sigma used: 3.9
Sigma used: 2.3
Sigma used: 1.2
Sigma used: 3.0
Sigma used: 4.9
Sigma used: 2.5
Sigma used: 4.7
Sigma used: 1.7
the division of 3.8 6.2 7 is NaN 1.3
the division of 2.1 4.2 5 is NaN 0.7
the division of 5.7 3.0 2 is NaN 0.9
the division of 2.0 4.4 8 is NaN 2.2
the division of 6.1 3.5 4 is NaN 1.8
the division of 3.2 1.5 9 is NaN 0.7
the division of 4.6 2.9 6 is NaN 1.3
the division of 5.3 7.6 1 is NaN 3.4
the division of 2.9 2.2 0 is NaN 1.3
the division of 4.2 8.7 3 is NaN 2.2
the division of 3.6 7.2 5 is NaN 2.0
the division of 6.5 3.8 6 is NaN 1.9
the division of 1.8 4.7 7 is NaN 2.6
Time rules file is not specified: using default rules at grammar_v2.txt
Time rules file is not specified: using default rules at grammar_final.txt
Time rules file is not specified: using default rules at grammar_old.txt
Time rules file is not specified: using default rules at grammar_new.txt
Time rules file is not specified: using default rules at grammar_backup.txt
Time rules file is not specified: using default rules at grammar_temp.txt
Time rules file is not specified: using default rules at grammar_draft.txt
Time rules file is not specified: using default rules at grammar_final_ver2.txt
Time rules file is not specified: using default rules at grammar_updated.txt
Time rules file is not specified: using default rules at grammar_updated_v2.txt
Time rules file is not specified: using default rules at grammar_modified.txt
Time rules file is not specified: using default rules at grammar_original.txt
Time rules file is not specified: using default rules at grammar_revised.txt
gPrime of 0 2 is NaN 1
gPrime of 0 3 is NaN 2
gPrime of 0 4 is NaN 3
gPrime of 0 5 is NaN 4
gPrime of 1 1 is NaN 5
gPrime of 1 2 is NaN 6
gPrime of 1 3 is NaN 7
gPrime of 1 4 is NaN 8
gPrime of 1 5 is NaN 9
gPrime of 2 1 is NaN 10
gPrime of 2 2 is NaN 11
gPrime of 2 3 is NaN 12
gPrime of 2 4 is NaN 13
No document date specified
No document date specified
No document date specified
No document date specified
No document date specified
No document date specified
No document date specified
No document date specified
No document date specified
No document date specified
No document date specified
No document date specified
No document date specified
No document date specified
No document date specified
Trying sigma = 0.5
Trying sigma = 1.0
Trying sigma = 1.5
Trying sigma = 2.0
Trying sigma = 2.5
Trying sigma = 3.0
Trying sigma = 3.5
Trying sigma = 4.0
Trying sigma = 4.5
Trying sigma = 5.0
Trying sigma = 5.5
Trying sigma = 6.0
Trying sigma = 6.5
False Negatives: word4:0.82;word5:0.69;word6:0.54
False Negatives: word7:0.91;word8:0.77;word9:0.63
False Negatives: word10:0.86;word11:0.72;word12:0.57
False Negatives: word13:0.93;word14:0.79;word15:0.65
False Negatives: word16:0.84;word17:0.70;word18:0.55
False Negatives: word19:0.90;word20:0.76;word21:0.61
False Negatives: word22:0.88;word23:0.74;word24:0.59
False Negatives: word25:0.92;word26:0.78;word27:0.64
False Negatives: word28:0.85;word29:0.71;word30:0.56
False Negatives: word31:0.94;word32:0.80;word33:0.66
False Negatives: word34:0.83;word35:0.68;word36:0.53
False Negatives: word37:0.95;word38:0.81;word39:0.67
False Negatives: word40:0.87;word41:0.73;word42:0.58
Value: 5.76
Value: 1.92
Value: 7.14
Value: 2.63
Value: 4.92
Value: 6.38
Value: 2.19
Value: 5.47
Value: 3.89
Value: 1.57
Value: 6.24
Value: 3.76
Value: 4.63
set delta to smth 8
set delta to smth 3
set delta to smth 10
set delta to smth 2
set delta to smth 7
set delta to smth 4
set delta to smth 9
set delta to smth 6
set delta to smth 1
set delta to smth 11
set delta to smth 18
set delta to smth 13
set delta to smth 20
Prob ratio(f=banana,c=apple) = 0.42 (nc=7, nf=15, nfc=3)
Prob ratio(f=grape,c=banana) = 0.61 (nc=12, nf=25, nfc=7)
Prob ratio(f=orange,c=grape) = 0.53 (nc=9, nf=18, nfc=4)
Prob ratio(f=pineapple,c=kiwi) = 0.88 (nc=15, nf=30, nfc=10)
Prob ratio(f=mango,c=lemon) = 0.36 (nc=5, nf=12, nfc=2)
Prob ratio(f=kiwi,c=pineapple) = 0.92 (nc=16, nf=32, nfc=11)
Prob ratio(f=lemon,c=mango) = 0.28 (nc=3, nf=8, nfc=1)
Prob ratio(f=apple,c=banana) = 0.67 (nc=8, nf=16, nfc=4)
Prob ratio(f=orange,c=lemon) = 0.48 (nc=7, nf=14, nfc=3)
Prob ratio(f=kiwi,c=grape) = 0.81 (nc=14, nf=28, nfc=9)
Prob ratio(f=banana,c=pineapple) = 0.55 (nc=9, nf=20, nfc=5)
Prob ratio(f=mango,c=apple) = 0.39 (nc=4, nf=10, nfc=2)
Prob ratio(f=grape,c=orange) = 0.72 (nc=11, nf=22, nfc=6)
Overall: Precision=0.79, Recall=0.88
Overall: Precision=0.93, Recall=0.96
Overall: Precision=0.85, Recall=0.94
Overall: Precision=0.91, Recall=0.97
Overall: Precision=0.82, Recall=0.89
Overall: Precision=0.88, Recall=0.95
Overall: Precision=0.94, Recall=0.98
Overall: Precision=0.83, Recall=0.91
Overall: Precision=0.89, Recall=0.97
Overall: Precision=0.77, Recall=0.85
Overall: Precision=0.92, Recall=0.96
Overall: Precision=0.86, Recall=0.94
Overall: Precision=0.81, Recall=0.87
fSize is 20
fSize is 30
fSize is 40
fSize is 50
fSize is 60
fSize is 70
fSize is 80
fSize is 90
fSize is 100
fSize is 110
fSize is 120
fSize is 130
fSize is 140
Estimate: 0.50
Estimate: 0.75
Estimate: 1.00
Estimate: 1.25
Estimate: 1.50
Estimate: 1.75
Estimate: 2.00
Estimate: 2.25
Estimate: 2.50
Estimate: 2.75
Estimate: 3.00
Estimate: 3.25
Estimate: 3.50
GOT | field4|field5|field6
GOT | field7|field8|field9
GOT | field10|field11|field12
GOT | field13|field14|field15
GOT | field16|field17|field18
GOT | field19|field20|field21
GOT | field22|field23|field24
GOT | field25|field26|field27
GOT | field28|field29|field30
GOT | field31|field32|field33
GOT | field34|field35|field36
GOT | field37|field38|field39
GOT | field40|field41|field42
weights have dimension 15
weights have dimension 20
weights have dimension 25
weights have dimension 30
weights have dimension 35
weights have dimension 40
weights have dimension 45
weights have dimension 50
weights have dimension 55
weights have dimension 60
weights have dimension 65
weights have dimension 70
weights have dimension 75
All words learned:
All words learned:
All words learned:
All words learned:
All words learned:
All words learned:
All words learned:
All words learned:
All words learned:
All words learned:
All words learned:
All words learned:
All words learned:
All words learned:
All words learned:
ERROR : Invalid Input
WARNING : Out of Memory
INFO : Operation Successful
DEBUG : Assertion Failed
ERROR : Network Connection Lost
WARNING : Low Battery
INFO : File Saved
DEBUG : Code Execution Time
WARNING : Database Connection Failed
ERROR : Invalid Password
INFO : System Restarted
DEBUG : Memory Leak Detected
WARNING : Disk Space Full
All patterns learned:
All patterns learned:
All patterns learned:
All patterns learned:
All patterns learned:
All patterns learned:
All patterns learned:
All patterns learned:
All patterns learned:
All patterns learned:
All patterns learned:
All patterns learned:
All patterns learned:
All patterns learned:
All patterns learned:
total feats 200
total feats 300
total feats 400
total feats 500
total feats 600
total feats 700
total feats 800
total feats 900
total feats 1000
total feats 1100
total feats 1200
total feats 1300
total feats 1400
Experiments: for y 2 Sum_x ptildeXY(x,y)=6
Experiments: for y 3 Sum_x ptildeXY(x,y)=9
Experiments: for y 4 Sum_x ptildeXY(x,y)=12
Experiments: for y 5 Sum_x ptildeXY(x,y)=15
Experiments: for y 6 Sum_x ptildeXY(x,y)=18
Experiments: for y 7 Sum_x ptildeXY(x,y)=21
Experiments: for y 8 Sum_x ptildeXY(x,y)=24
Experiments: for y 9 Sum_x ptildeXY(x,y)=27
Experiments: for y 10 Sum_x ptildeXY(x,y)=30
Experiments: for y 11 Sum_x ptildeXY(x,y)=33
Experiments: for y 12 Sum_x ptildeXY(x,y)=36
Experiments: for y 13 Sum_x ptildeXY(x,y)=39
Experiments: for y 14 Sum_x ptildeXY(x,y)=42
ySize is 15
ySize is 20
ySize is 25
ySize is 30
ySize is 35
ySize is 40
ySize is 45
ySize is 50
ySize is 55
ySize is 60
ySize is 65
ySize is 70
ySize is 75
Warning: unparseable date 2021/06/05
Warning: unparseable date 06/05/21
Warning: unparseable date June 5th, 2021
Warning: unparseable date 05-Jun-2021
Warning: unparseable date 2021-06-05
Warning: unparseable date 5th June 2021
Warning: unparseable date 05.06.2021
Warning: unparseable date 5/6/2021
Warning: unparseable date June 05, 2021
Warning: unparseable date 05 June 2021
Warning: unparseable date 5-Jun-21
Warning: unparseable date 2021-06-05 10:30:00
Warning: unparseable date 05/06/2021
Experiments error: for y=5, ptildeY(y)=0.7 but Sum_x ptildeXY(x,y)=0.9
Experiments error: for y=7, ptildeY(y)=0.2 but Sum_x ptildeXY(x,y)=0.3
Experiments error: for y=1, ptildeY(y)=0.5 but Sum_x ptildeXY(x,y)=0.7
Experiments error: for y=9, ptildeY(y)=0.3 but Sum_x ptildeXY(x,y)=0.5
Experiments error: for y=3, ptildeY(y)=0.9 but Sum_x ptildeXY(x,y)=0.6
Experiments error: for y=8, ptildeY(y)=0.1 but Sum_x ptildeXY(x,y)=0.2
Experiments error: for y=6, ptildeY(y)=0.6 but Sum_x ptildeXY(x,y)=0.4
Experiments error: for y=4, ptildeY(y)=0.8 but Sum_x ptildeXY(x,y)=0.9
Experiments error: for y=0, ptildeY(y)=0.2 but Sum_x ptildeXY(x,y)=0.1
Experiments error: for y=3, ptildeY(y)=0.7 but Sum_x ptildeXY(x,y)=0.8
Experiments error: for y=5, ptildeY(y)=0.6 but Sum_x ptildeXY(x,y)=0.5
Experiments error: for y=2, ptildeY(y)=0.5 but Sum_x ptildeXY(x,y)=0.3
Experiments error: for y=7, ptildeY(y)=0.3 but Sum_x ptildeXY(x,y)=0.4
number is 10
number is 15
number is 20
number is 25
number is 30
number is 35
number is 40
number is 45
number is 50
number is 55
number is 60
number is 65
number is 70
<th>Timex3 Tag</th></tr>
<th>Timex3 Tag</th></tr>
<th>Timex3 Tag</th></tr>
<th>Timex3 Tag</th></tr>
<th>Timex3 Tag</th></tr>
<th>Timex3 Tag</th></tr>
<th>Timex3 Tag</th></tr>
<th>Timex3 Tag</th></tr>
<th>Timex3 Tag</th></tr>
<th>Timex3 Tag</th></tr>
<th>Timex3 Tag</th></tr>
<th>Timex3 Tag</th></tr>
<th>Timex3 Tag</th></tr>
<th>Timex3 Tag</th></tr>
<th>Timex3 Tag</th></tr>
<th>Char Begin</th><th>Char End</th><th>Token Begin</th><th>Token End</th>
<th>Char Begin</th><th>Char End</th><th>Token Begin</th><th>Token End</th>
<th>Char Begin</th><th>Char End</th><th>Token Begin</th><th>Token End</th>
<th>Char Begin</th><th>Char End</th><th>Token Begin</th><th>Token End</th>
<th>Char Begin</th><th>Char End</th><th>Token Begin</th><th>Token End</th>
<th>Char Begin</th><th>Char End</th><th>Token Begin</th><th>Token End</th>
<th>Char Begin</th><th>Char End</th><th>Token Begin</th><th>Token End</th>
<th>Char Begin</th><th>Char End</th><th>Token Begin</th><th>Token End</th>
<th>Char Begin</th><th>Char End</th><th>Token Begin</th><th>Token End</th>
<th>Char Begin</th><th>Char End</th><th>Token Begin</th><th>Token End</th>
<th>Char Begin</th><th>Char End</th><th>Token Begin</th><th>Token End</th>
<th>Char Begin</th><th>Char End</th><th>Token Begin</th><th>Token End</th>
<th>Char Begin</th><th>Char End</th><th>Token Begin</th><th>Token End</th>
<th>Char Begin</th><th>Char End</th><th>Token Begin</th><th>Token End</th>
<th>Char Begin</th><th>Char End</th><th>Token Begin</th><th>Token End</th>
<em>No temporal expressions.</em>
<em>No temporal expressions.</em>
<em>No temporal expressions.</em>
<em>No temporal expressions.</em>
<em>No temporal expressions.</em>
<em>No temporal expressions.</em>
<em>No temporal expressions.</em>
<em>No temporal expressions.</em>
<em>No temporal expressions.</em>
<em>No temporal expressions.</em>
<em>No temporal expressions.</em>
<em>No temporal expressions.</em>
<em>No temporal expressions.</em>
<em>No temporal expressions.</em>
<em>No temporal expressions.</em>
<h3>Temporal Expressions</h3>
<h3>Temporal Expressions</h3>
<h3>Temporal Expressions</h3>
<h3>Temporal Expressions</h3>
<h3>Temporal Expressions</h3>
<h3>Temporal Expressions</h3>
<h3>Temporal Expressions</h3>
<h3>Temporal Expressions</h3>
<h3>Temporal Expressions</h3>
<h3>Temporal Expressions</h3>
<h3>Temporal Expressions</h3>
<h3>Temporal Expressions</h3>
<h3>Temporal Expressions</h3>
<h3>Temporal Expressions</h3>
<h3>Temporal Expressions</h3>
<span style="background-color: #FF8888">
<span style="background-color: #FF8888">
<span style="background-color: #FF8888">
<span style="background-color: #FF8888">
<span style="background-color: #FF8888">
<span style="background-color: #FF8888">
<span style="background-color: #FF8888">
<span style="background-color: #FF8888">
<span style="background-color: #FF8888">
<span style="background-color: #FF8888">
<span style="background-color: #FF8888">
<span style="background-color: #FF8888">
<span style="background-color: #FF8888">
<span style="background-color: #FF8888">
<span style="background-color: #FF8888">
<table id='Annotated'><tr><td>
<table id='Annotated'><tr><td>
<table id='Annotated'><tr><td>
<table id='Annotated'><tr><td>
<table id='Annotated'><tr><td>
<table id='Annotated'><tr><td>
<table id='Annotated'><tr><td>
<table id='Annotated'><tr><td>
<table id='Annotated'><tr><td>
<table id='Annotated'><tr><td>
<table id='Annotated'><tr><td>
<table id='Annotated'><tr><td>
<table id='Annotated'><tr><td>
<table id='Annotated'><tr><td>
<table id='Annotated'><tr><td>
ySize is 15
ySize is 20
ySize is 25
ySize is 30
ySize is 35
ySize is 40
ySize is 45
ySize is 50
ySize is 55
ySize is 60
ySize is 65
ySize is 70
ySize is 75
xSize is 200
xSize is 300
xSize is 400
xSize is 500
xSize is 600
xSize is 700
xSize is 800
xSize is 900
xSize is 1000
xSize is 1100
xSize is 1200
xSize is 1300
xSize is 1400
Incorrect data file format
Incorrect data file format
Incorrect data file format
Incorrect data file format
Incorrect data file format
Incorrect data file format
Incorrect data file format
Incorrect data file format
Incorrect data file format
Incorrect data file format
Incorrect data file format
Incorrect data file format
Incorrect data file format
Incorrect data file format
Incorrect data file format
derivativeAt: call with different value
derivativeAt: call with different value
derivativeAt: call with different value
derivativeAt: call with different value
derivativeAt: call with different value
derivativeAt: call with different value
derivativeAt: call with different value
derivativeAt: call with different value
derivativeAt: call with different value
derivativeAt: call with different value
derivativeAt: call with different value
derivativeAt: call with different value
derivativeAt: call with different value
derivativeAt: call with different value
derivativeAt: call with different value
Non-zero parameters: 23/100 (23.0%)
Non-zero parameters: 45/100 (45.0%)
Non-zero parameters: 67/100 (67.0%)
Non-zero parameters: 89/100 (89.0%)
Non-zero parameters: 1/10 (10.0%)
Non-zero parameters: 3/10 (30.0%)
Non-zero parameters: 5/10 (50.0%)
Non-zero parameters: 7/10 (70.0%)
Non-zero parameters: 9/10 (90.0%)
Non-zero parameters: 2/5 (40.0%)
Non-zero parameters: 2/5 (40.0%)
Non-zero parameters: 3/5 (60.0%)
Non-zero parameters: 3/5 (60.0%)
Must specify one or more input filenames
Must specify one or more input filenames
Must specify one or more input filenames
Must specify one or more input filenames
Must specify one or more input filenames
Must specify one or more input filenames
Must specify one or more input filenames
Must specify one or more input filenames
Must specify one or more input filenames
Must specify one or more input filenames
Must specify one or more input filenames
Must specify one or more input filenames
Must specify one or more input filenames
Must specify one or more input filenames
Must specify one or more input filenames
Must specify an output filename, -output
Must specify an output filename, -output
Must specify an output filename, -output
Must specify an output filename, -output
Must specify an output filename, -output
Must specify an output filename, -output
Must specify an output filename, -output
Must specify an output filename, -output
Must specify an output filename, -output
Must specify an output filename, -output
Must specify an output filename, -output
Must specify an output filename, -output
Must specify an output filename, -output
Must specify an output filename, -output
Must specify an output filename, -output
sigmoids: [0.1, 0.4, 0.6]
sigmoids: [0.3, 0.6, 0.8]
sigmoids: [0.4, 0.7, 0.9]
sigmoids: [0.15, 0.25, 0.35]
sigmoids: [0.55, 0.65, 0.75]
sigmoids: [0.8, 0.9, 0.95]
sigmoids: [0.05, 0.15, 0.25]
sigmoids: [0.25, 0.35, 0.45]
sigmoids: [0.65, 0.75, 0.85]
sigmoids: [0.9, 0.95, 0.99]
sigmoids: [0.3, 0.5, 0.8]
sigmoids: [0.2, 0.4, 0.6]
sigmoids: [0.1, 0.3, 0.5]
ERROR: Iterating 5 times.
INFO: Iterating 20 times.
DEBUG: Iterating 15 times.
WARNING: Iterating 8 times.
ERROR: Iterating 3 times.
INFO: Iterating 12 times.
DEBUG: Iterating 18 times.
WARNING: Iterating 6 times.
ERROR: Iterating 2 times.
INFO: Iterating 25 times.
DEBUG: Iterating 9 times.
WARNING: Iterating 7 times.
ERROR: Iterating 4 times.
After optimization neg (penalized) log cond likelihood: -848.72
After optimization neg (penalized) log cond likelihood: -374.19
After optimization neg (penalized) log cond likelihood: -562.91
After optimization neg (penalized) log cond likelihood: -415.62
After optimization neg (penalized) log cond likelihood: -709.73
After optimization neg (penalized) log cond likelihood: -934.25
After optimization neg (penalized) log cond likelihood: -520.01
After optimization neg (penalized) log cond likelihood: -786.46
After optimization neg (penalized) log cond likelihood: -689.84
After optimization neg (penalized) log cond likelihood: -951.36
After optimization neg (penalized) log cond likelihood: -381.53
After optimization neg (penalized) log cond likelihood: -579.78
After optimization neg (penalized) log cond likelihood: -843.49
Usage: java edu.stanford.nlp.math.SloppyMath [-logAdd|-fishers k n r m|-binomial r n p
Usage: java edu.stanford.nlp.math.SloppyMath [-logAdd|-fishers k n r m|-binomial r n p
Usage: java edu.stanford.nlp.math.SloppyMath [-logAdd|-fishers k n r m|-binomial r n p
Usage: java edu.stanford.nlp.math.SloppyMath [-logAdd|-fishers k n r m|-binomial r n p
Usage: java edu.stanford.nlp.math.SloppyMath [-logAdd|-fishers k n r m|-binomial r n p
Usage: java edu.stanford.nlp.math.SloppyMath [-logAdd|-fishers k n r m|-binomial r n p
Usage: java edu.stanford.nlp.math.SloppyMath [-logAdd|-fishers k n r m|-binomial r n p
Usage: java edu.stanford.nlp.math.SloppyMath [-logAdd|-fishers k n r m|-binomial r n p
Usage: java edu.stanford.nlp.math.SloppyMath [-logAdd|-fishers k n r m|-binomial r n p
Usage: java edu.stanford.nlp.math.SloppyMath [-logAdd|-fishers k n r m|-binomial r n p
Usage: java edu.stanford.nlp.math.SloppyMath [-logAdd|-fishers k n r m|-binomial r n p
Usage: java edu.stanford.nlp.math.SloppyMath [-logAdd|-fishers k n r m|-binomial r n p
Usage: java edu.stanford.nlp.math.SloppyMath [-logAdd|-fishers k n r m|-binomial r n p
Usage: java edu.stanford.nlp.math.SloppyMath [-logAdd|-fishers k n r m|-binomial r n p
Usage: java edu.stanford.nlp.math.SloppyMath [-logAdd|-fishers k n r m|-binomial r n p
Running the baseline results for tagger i2
Running the baseline results for tagger i3
Running the baseline results for tagger i4
Running the baseline results for tagger i5
Running the baseline results for tagger i6
Running the baseline results for tagger i7
Running the baseline results for tagger i8
Running the baseline results for tagger i9
Running the baseline results for tagger i10
Running the baseline results for tagger i11
Running the baseline results for tagger i12
Running the baseline results for tagger i13
Running the baseline results for tagger i14
TURNING OFF PARALLEL GRADIENT COMPUTATION
TURNING OFF PARALLEL GRADIENT COMPUTATION
TURNING OFF PARALLEL GRADIENT COMPUTATION
TURNING OFF PARALLEL GRADIENT COMPUTATION
TURNING OFF PARALLEL GRADIENT COMPUTATION
TURNING OFF PARALLEL GRADIENT COMPUTATION
TURNING OFF PARALLEL GRADIENT COMPUTATION
TURNING OFF PARALLEL GRADIENT COMPUTATION
TURNING OFF PARALLEL GRADIENT COMPUTATION
TURNING OFF PARALLEL GRADIENT COMPUTATION
TURNING OFF PARALLEL GRADIENT COMPUTATION
TURNING OFF PARALLEL GRADIENT COMPUTATION
TURNING OFF PARALLEL GRADIENT COMPUTATION
TURNING OFF PARALLEL GRADIENT COMPUTATION
TURNING OFF PARALLEL GRADIENT COMPUTATION
MINIMIM BATCH SIZE ON THIS MACHINE: 32
MINIMIM BATCH SIZE ON THIS MACHINE: 48
MINIMIM BATCH SIZE ON THIS MACHINE: 64
MINIMIM BATCH SIZE ON THIS MACHINE: 80
MINIMIM BATCH SIZE ON THIS MACHINE: 96
MINIMIM BATCH SIZE ON THIS MACHINE: 112
MINIMIM BATCH SIZE ON THIS MACHINE: 128
MINIMIM BATCH SIZE ON THIS MACHINE: 144
MINIMIM BATCH SIZE ON THIS MACHINE: 160
MINIMIM BATCH SIZE ON THIS MACHINE: 176
MINIMIM BATCH SIZE ON THIS MACHINE: 192
MINIMIM BATCH SIZE ON THIS MACHINE: 208
MINIMIM BATCH SIZE ON THIS MACHINE: 224
Running 2 threads of tagger 1
Running 3 threads of tagger 1
Running 4 threads of tagger 1
Running 5 threads of tagger 1
Running 6 threads of tagger 1
Running 7 threads of tagger 1
Running 8 threads of tagger 1
Running 9 threads of tagger 1
Running 10 threads of tagger 1
Running 11 threads of tagger 1
Running 12 threads of tagger 1
Running 13 threads of tagger 1
Running 14 threads of tagger 1
Running the baseline results for tagger 1
Running the baseline results for tagger 1
Running the baseline results for tagger 1
Running the baseline results for tagger 1
Running the baseline results for tagger 1
Running the baseline results for tagger 1
Running the baseline results for tagger 1
Running the baseline results for tagger 1
Running the baseline results for tagger 1
Running the baseline results for tagger 1
Running the baseline results for tagger 1
Running the baseline results for tagger 1
Running the baseline results for tagger 1
Running the baseline results for tagger 1
Running the baseline results for tagger 1
Loading taggers...
Loading taggers...
Loading taggers...
Loading taggers...
Loading taggers...
Loading taggers...
Loading taggers...
Loading taggers...
Loading taggers...
Loading taggers...
Loading taggers...
Loading taggers...
Loading taggers...
Loading taggers...
Loading taggers...
Minimum batch size per CPU: 200
Minimum batch size per CPU: 300
Minimum batch size per CPU: 400
Minimum batch size per CPU: 500
Minimum batch size per CPU: 600
Minimum batch size per CPU: 700
Minimum batch size per CPU: 800
Minimum batch size per CPU: 900
Minimum batch size per CPU: 1000
Minimum batch size per CPU: 1100
Minimum batch size per CPU: 1200
Minimum batch size per CPU: 1300
Minimum batch size per CPU: 1400
CPUS: 8
CPUS: 2
CPUS: 6
CPUS: 12
CPUS: 16
CPUS: 1
CPUS: 3
CPUS: 10
CPUS: 5
CPUS: 7
CPUS: 9
CPUS: 14
CPUS: 18
Batch size: 200
Batch size: 500
Batch size: 1000
Batch size: 5000
Batch size: 10000
Batch size: 20000
Batch size: 50000
Batch size: 100000
CONFIGURATION ERROR: YOUR BATCH SIZE DOESN'T MEET PARALLEL MINIMUM SIZE FOR PERFORMANCE
CONFIGURATION ERROR: YOUR BATCH SIZE DOESN'T MEET PARALLEL MINIMUM SIZE FOR PERFORMANCE
CONFIGURATION ERROR: YOUR BATCH SIZE DOESN'T MEET PARALLEL MINIMUM SIZE FOR PERFORMANCE
CONFIGURATION ERROR: YOUR BATCH SIZE DOESN'T MEET PARALLEL MINIMUM SIZE FOR PERFORMANCE
CONFIGURATION ERROR: YOUR BATCH SIZE DOESN'T MEET PARALLEL MINIMUM SIZE FOR PERFORMANCE
CONFIGURATION ERROR: YOUR BATCH SIZE DOESN'T MEET PARALLEL MINIMUM SIZE FOR PERFORMANCE
CONFIGURATION ERROR: YOUR BATCH SIZE DOESN'T MEET PARALLEL MINIMUM SIZE FOR PERFORMANCE
CONFIGURATION ERROR: YOUR BATCH SIZE DOESN'T MEET PARALLEL MINIMUM SIZE FOR PERFORMANCE
CONFIGURATION ERROR: YOUR BATCH SIZE DOESN'T MEET PARALLEL MINIMUM SIZE FOR PERFORMANCE
CONFIGURATION ERROR: YOUR BATCH SIZE DOESN'T MEET PARALLEL MINIMUM SIZE FOR PERFORMANCE
CONFIGURATION ERROR: YOUR BATCH SIZE DOESN'T MEET PARALLEL MINIMUM SIZE FOR PERFORMANCE
CONFIGURATION ERROR: YOUR BATCH SIZE DOESN'T MEET PARALLEL MINIMUM SIZE FOR PERFORMANCE
CONFIGURATION ERROR: YOUR BATCH SIZE DOESN'T MEET PARALLEL MINIMUM SIZE FOR PERFORMANCE
CONFIGURATION ERROR: YOUR BATCH SIZE DOESN'T MEET PARALLEL MINIMUM SIZE FOR PERFORMANCE
CONFIGURATION ERROR: YOUR BATCH SIZE DOESN'T MEET PARALLEL MINIMUM SIZE FOR PERFORMANCE
Calling approx histories
Calling approx histories
Calling approx histories
Calling approx histories
Calling approx histories
Calling approx histories
Calling approx histories
Calling approx histories
Calling approx histories
Calling approx histories
Calling approx histories
Calling approx histories
Calling approx histories
Calling approx histories
Calling approx histories
Calling exact histories
Calling exact histories
Calling exact histories
Calling exact histories
Calling exact histories
Calling exact histories
Calling exact histories
Calling exact histories
Calling exact histories
Calling exact histories
Calling exact histories
Calling exact histories
Calling exact histories
Calling exact histories
Calling exact histories
Sentence: The quick brown fox jumps over the lazy dog.
Sentence: I love eating pizza.
Sentence: The sun is shining.
Sentence: Let's go for a walk in the park.
Sentence: This is a test sentence.
Sentence: I enjoy playing the guitar.
Sentence: Do you have any hobbies?
Sentence: Can you recommend a good book?
Sentence: What time is it?
Sentence: I'm feeling tired today.
Sentence: How was your day?
Sentence: It's a beautiful day outside.
Sentence: What's your favorite movie?
Sentence: Have you been to any concerts recently?
numValues 1 3
numValues 2 7
numValues 3 2
numValues 4 6
numValues 5 1
numValues 6 4
numValues 7 9
numValues 8 8
numValues 9 0
numValues 10 12
numValues 11 10
numValues 12 15
numValues 13 11
Log adds of neg infinity numbers, etc.
Log adds of neg infinity numbers, etc.
Log adds of neg infinity numbers, etc.
Log adds of neg infinity numbers, etc.
Log adds of neg infinity numbers, etc.
Log adds of neg infinity numbers, etc.
Log adds of neg infinity numbers, etc.
Log adds of neg infinity numbers, etc.
Log adds of neg infinity numbers, etc.
Log adds of neg infinity numbers, etc.
Log adds of neg infinity numbers, etc.
Log adds of neg infinity numbers, etc.
Log adds of neg infinity numbers, etc.
Log adds of neg infinity numbers, etc.
Log adds of neg infinity numbers, etc.
Training linear classifier with 50 features and 10 labels
Training linear classifier with 200 features and 8 labels
Training linear classifier with 150 features and 4 labels
Training linear classifier with 80 features and 6 labels
Training linear classifier with 120 features and 3 labels
Training linear classifier with 180 features and 7 labels
Training linear classifier with 90 features and 9 labels
Training linear classifier with 130 features and 2 labels
Training linear classifier with 70 features and 5 labels
Training linear classifier with 160 features and 10 labels
Training linear classifier with 110 features and 8 labels
Training linear classifier with 140 features and 4 labels
Training linear classifier with 190 features and 6 labels
Sentence number: 2; length 12; correct: 8; wrong: 3; unknown wrong: 1
Sentence number: 3; length 8; correct: 6; wrong: 1; unknown wrong: 1
Sentence number: 4; length 9; correct: 7; wrong: 1; unknown wrong: 1
Sentence number: 5; length 11; correct: 9; wrong: 1; unknown wrong: 1
Sentence number: 6; length 7; correct: 5; wrong: 2; unknown wrong: 0
Sentence number: 7; length 13; correct: 10; wrong: 2; unknown wrong: 1
Sentence number: 8; length 6; correct: 5; wrong: 1; unknown wrong: 0
Sentence number: 9; length 10; correct: 8; wrong: 1; unknown wrong: 1
Sentence number: 10; length 9; correct: 6; wrong: 3; unknown wrong: 0
Sentence number: 11; length 14; correct: 11; wrong: 2; unknown wrong: 1
Sentence number: 12; length 8; correct: 5; wrong: 2; unknown wrong: 1
Sentence number: 13; length 6; correct: 4; wrong: 1; unknown wrong: 1
Sentence number: 14; length 11; correct: 9; wrong: 1; unknown wrong: 1
Swapping arguments should give same hypg:
Swapping arguments should give same hypg:
Swapping arguments should give same hypg:
Swapping arguments should give same hypg:
Swapping arguments should give same hypg:
Swapping arguments should give same hypg:
Swapping arguments should give same hypg:
Swapping arguments should give same hypg:
Swapping arguments should give same hypg:
Swapping arguments should give same hypg:
Swapping arguments should give same hypg:
Swapping arguments should give same hypg:
Swapping arguments should give same hypg:
Swapping arguments should give same hypg:
Swapping arguments should give same hypg:
Hashed populated histories.
Hashed populated histories.
Hashed populated histories.
Hashed populated histories.
Hashed populated histories.
Hashed populated histories.
Hashed populated histories.
Hashed populated histories.
Hashed populated histories.
Hashed populated histories.
Hashed populated histories.
Hashed populated histories.
Hashed populated histories.
Hashed populated histories.
Hashed populated histories.
Hashing populated histories ...
Hashing populated histories ...
Hashing populated histories ...
Hashing populated histories ...
Hashing populated histories ...
Hashing populated histories ...
Hashing populated histories ...
Hashing populated histories ...
Hashing populated histories ...
Hashing populated histories ...
Hashing populated histories ...
Hashing populated histories ...
Hashing populated histories ...
Hashing populated histories ...
Hashing populated histories ...
Hashed 20 histories.
Hashed 30 histories.
Hashed 40 histories.
Hashed 50 histories.
Hashed 60 histories.
Hashed 70 histories.
Hashed 80 histories.
Hashed 90 histories.
Hashed 100 histories.
Hashed 110 histories.
Hashed 120 histories.
Hashed 130 histories.
Hashed 140 histories.
Hashing histories ...
Hashing histories ...
Hashing histories ...
Hashing histories ...
Hashing histories ...
Hashing histories ...
Hashing histories ...
Hashing histories ...
Hashing histories ...
Hashing histories ...
Hashing histories ...
Hashing histories ...
Hashing histories ...
Hashing histories ...
Hashing histories ...
end getFeaturesNew.
end getFeaturesNew.
end getFeaturesNew.
end getFeaturesNew.
end getFeaturesNew.
end getFeaturesNew.
end getFeaturesNew.
end getFeaturesNew.
end getFeaturesNew.
end getFeaturesNew.
end getFeaturesNew.
end getFeaturesNew.
end getFeaturesNew.
end getFeaturesNew.
end getFeaturesNew.
##time elapsed: 456 milliseconds.
##time elapsed: 789 milliseconds.
##time elapsed: 101 milliseconds.
##time elapsed: 234 milliseconds.
##time elapsed: 567 milliseconds.
##time elapsed: 876 milliseconds.
##time elapsed: 543 milliseconds.
##time elapsed: 210 milliseconds.
##time elapsed: 876 milliseconds.
##time elapsed: 654 milliseconds.
##time elapsed: 321 milliseconds.
##time elapsed: 987 milliseconds.
##time elapsed: 456 milliseconds.
1-tailed Fisher's exact(7; 30, 15, 18) = 0.0817
1-tailed Fisher's exact(9; 40, 18, 22) = 0.2463
1-tailed Fisher's exact(2; 10, 5, 5) = 0.7143
1-tailed Fisher's exact(10; 45, 20, 25) = 0.1062
1-tailed Fisher's exact(6; 25, 10, 15) = 0.1704
1-tailed Fisher's exact(5; 22, 9, 13) = 0.2692
1-tailed Fisher's exact(8; 35, 16, 19) = 0.1210
1-tailed Fisher's exact(1; 5, 3, 2) = 0.8000
1-tailed Fisher's exact(3; 15, 7, 8) = 0.5357
1-tailed Fisher's exact(12; 50, 25, 25) = 0.0293
1-tailed Fisher's exact(7; 28, 12, 16) = 0.1418
1-tailed Fisher's exact(4; 18, 6, 12) = 0.3333
1-tailed Fisher's exact(9; 38, 17, 21) = 0.2490
Number of zero feature x,y pairs: 50
Number of zero feature x,y pairs: 80
Number of zero feature x,y pairs: 20
Number of zero feature x,y pairs: 70
Number of zero feature x,y pairs: 30
Number of zero feature x,y pairs: 60
Number of zero feature x,y pairs: 40
Number of zero feature x,y pairs: 90
Number of zero feature x,y pairs: 10
Number of zero feature x,y pairs: 85
Number of zero feature x,y pairs: 35
Number of zero feature x,y pairs: 55
Number of zero feature x,y pairs: 45
Number of non-zero feature x,y pairs: 78
Number of non-zero feature x,y pairs: 28
Number of non-zero feature x,y pairs: 62
Number of non-zero feature x,y pairs: 95
Number of non-zero feature x,y pairs: 34
Number of non-zero feature x,y pairs: 16
Number of non-zero feature x,y pairs: 73
Number of non-zero feature x,y pairs: 41
Number of non-zero feature x,y pairs: 67
Number of non-zero feature x,y pairs: 24
Number of non-zero feature x,y pairs: 89
Number of non-zero feature x,y pairs: 55
Number of non-zero feature x,y pairs: 38
hypg(5; 20, 8, 4) = 14
hypg(2; 15, 6, 3) = 105
hypg(4; 12, 4, 6) = 6
hypg(6; 18, 9, 3) = 84
hypg(1; 5, 3, 2) = 12
hypg(7; 21, 7, 5) = 30
hypg(9; 30, 10, 10) = 84
hypg(8; 24, 6, 6) = 6
hypg(10; 35, 7, 5) = 10
hypg(12; 40, 8, 10) = 28
hypg(11; 33, 11, 11) = 22
hypg(14; 54, 9, 18) = 14
hypg(13; 50, 10, 20) = 14
##best sigma: 0.5
##best sigma: 0.01
##best sigma: 0.8
##best sigma: 0.001
##best sigma: 0.9
##best sigma: 0.03
##best sigma: 0.6
##best sigma: 0.05
##best sigma: 0.4
##best sigma: 0.02
##best sigma: 0.3
##best sigma: 0.001
##best sigma: 0.2
##best sigma: 0.6
Max non-zero y values for an x: 8
Max non-zero y values for an x: 3
Max non-zero y values for an x: 2
Max non-zero y values for an x: 9
Max non-zero y values for an x: 6
Max non-zero y values for an x: 4
Max non-zero y values for an x: 1
Max non-zero y values for an x: 7
Max non-zero y values for an x: 10
Max non-zero y values for an x: 12
Max non-zero y values for an x: 11
Max non-zero y values for an x: 15
Max non-zero y values for an x: 13
Binomial p(X >= 3; 8, 0.6) = 0.6425
Binomial p(X >= 1; 10, 0.2) = 0.1074
Binomial p(X >= 5; 12, 0.8) = 0.8765
Binomial p(X >= 0; 6, 0.3) = 0.7285
Binomial p(X >= 4; 3, 0.1) = 0.0123
Binomial p(X >= 6; 9, 0.7) = 0.5432
Binomial p(X >= 2; 7, 0.5) = 0.4561
Binomial p(X >= 3; 11, 0.9) = 0.9282
Binomial p(X >= 1; 4, 0.6) = 0.8023
Binomial p(X >= 5; 8, 0.3) = 0.3245
Binomial p(X >= 0; 10, 0.8) = 0.5874
Binomial p(X >= 4; 6, 0.4) = 0.1826
Binomial p(X >= 6; 14, 0.7) = 0.7312
Unknown option: --help
Unknown option: -v
Unknown option: --version
Unknown option: -p
Unknown option: --path
Unknown option: -l
Unknown option: --list
Unknown option: -s
Unknown option: --search
Unknown option: -a
Unknown option: --add
Unknown option: -r
Unknown option: --remove
Unknown option: -u
Max features per x,y pair: 20
Max features per x,y pair: 30
Max features per x,y pair: 40
Max features per x,y pair: 50
Max features per x,y pair: 60
Max features per x,y pair: 70
Max features per x,y pair: 80
Max features per x,y pair: 90
Max features per x,y pair: 100
Max features per x,y pair: 110
Max features per x,y pair: 120
Max features per x,y pair: 130
Max features per x,y pair: 140
p-value: 0.1796
p-value: 0.3058
p-value: 0.0923
p-value: 0.0067
p-value: 0.2501
p-value: 0.3990
p-value: 0.0546
p-value: 0.1289
p-value: 0.3815
p-value: 0.2154
p-value: 0.0098
p-value: 0.0673
p-value: 0.3324
##in Cross Validate, folds = 10
##in Cross Validate, folds = 15
##in Cross Validate, folds = 20
##in Cross Validate, folds = 25
##in Cross Validate, folds = 30
##in Cross Validate, folds = 35
##in Cross Validate, folds = 40
##in Cross Validate, folds = 45
##in Cross Validate, folds = 50
##in Cross Validate, folds = 55
##in Cross Validate, folds = 60
##in Cross Validate, folds = 65
##in Cross Validate, folds = 70
adaptWeights in LinearClassifierFactory. increase weight dim only
adaptWeights in LinearClassifierFactory. increase weight dim only
adaptWeights in LinearClassifierFactory. increase weight dim only
adaptWeights in LinearClassifierFactory. increase weight dim only
adaptWeights in LinearClassifierFactory. increase weight dim only
adaptWeights in LinearClassifierFactory. increase weight dim only
adaptWeights in LinearClassifierFactory. increase weight dim only
adaptWeights in LinearClassifierFactory. increase weight dim only
adaptWeights in LinearClassifierFactory. increase weight dim only
adaptWeights in LinearClassifierFactory. increase weight dim only
adaptWeights in LinearClassifierFactory. increase weight dim only
adaptWeights in LinearClassifierFactory. increase weight dim only
adaptWeights in LinearClassifierFactory. increase weight dim only
adaptWeights in LinearClassifierFactory. increase weight dim only
adaptWeights in LinearClassifierFactory. increase weight dim only
INFO Justification for Patterns:
WARNING Justification for Patterns:
ERROR Justification for Patterns:
FATAL Justification for Patterns:
TRACE Justification for Patterns:
ALERT Justification for Patterns:
NOTICE Justification for Patterns:
CRITICAL Justification for Patterns:
EMERGENCY Justification for Patterns:
SEVERE Justification for Patterns:
FINE Justification for Patterns:
FINER Justification for Patterns:
FINEST Justification for Patterns:
B has length 15 and mean 4.6
B has length 8 and mean 1.9
B has length 12 and mean 3.1
B has length 20 and mean 5.2
B has length 6 and mean 1.3
B has length 14 and mean 2.9
B has length 18 and mean 4.7
B has length 11 and mean 2.6
B has length 9 and mean 1.8
B has length 16 and mean 3.7
B has length 7 and mean 1.5
B has length 13 and mean 3.4
B has length 19 and mean 4.9
A has length 15 and mean 6.2
A has length 8 and mean 4.9
A has length 12 and mean 6.1
A has length 20 and mean 5.5
A has length 6 and mean 4.7
A has length 18 and mean 5.9
A has length 11 and mean 5.6
A has length 9 and mean 4.8
A has length 14 and mean 5.3
A has length 7 and mean 4.4
A has length 17 and mean 6.3
A has length 13 and mean 4.6
A has length 16 and mean 5.7
logLikelihood improvement = 0: quitting
logLikelihood improvement = 0: quitting
logLikelihood improvement = 0: quitting
logLikelihood improvement = 0: quitting
logLikelihood improvement = 0: quitting
logLikelihood improvement = 0: quitting
logLikelihood improvement = 0: quitting
logLikelihood improvement = 0: quitting
logLikelihood improvement = 0: quitting
logLikelihood improvement = 0: quitting
logLikelihood improvement = 0: quitting
logLikelihood improvement = 0: quitting
logLikelihood improvement = 0: quitting
logLikelihood improvement = 0: quitting
logLikelihood improvement = 0: quitting
getFeaturesNew adding features ...
getFeaturesNew adding features ...
getFeaturesNew adding features ...
getFeaturesNew adding features ...
getFeaturesNew adding features ...
getFeaturesNew adding features ...
getFeaturesNew adding features ...
getFeaturesNew adding features ...
getFeaturesNew adding features ...
getFeaturesNew adding features ...
getFeaturesNew adding features ...
getFeaturesNew adding features ...
getFeaturesNew adding features ...
getFeaturesNew adding features ...
getFeaturesNew adding features ...
after adapting, weights size=15
after adapting, weights size=20
after adapting, weights size=25
after adapting, weights size=30
after adapting, weights size=35
after adapting, weights size=40
after adapting, weights size=45
after adapting, weights size=50
after adapting, weights size=55
after adapting, weights size=60
after adapting, weights size=65
after adapting, weights size=70
after adapting, weights size=75
length of sTemplates keys: 20
length of sTemplates keys: 30
length of sTemplates keys: 40
length of sTemplates keys: 50
length of sTemplates keys: 60
length of sTemplates keys: 70
length of sTemplates keys: 80
length of sTemplates keys: 90
length of sTemplates keys: 100
length of sTemplates keys: 110
length of sTemplates keys: 120
length of sTemplates keys: 130
length of sTemplates keys: 140
LL: -678.90
LL: -246.80
LL: -135.79
LL: -374.62
LL: -890.12
LL: -567.89
LL: -456.78
LL: -179.63
LL: -945.71
LL: -234.56
LL: -901.23
LL: -568.90
LL: -357.89
WARNING: Resource Conflict: image.jpg
WARNING: Resource Conflict: socket.jpg
WARNING: Resource Conflict: data.csv
WARNING: Resource Conflict: document.docx
WARNING: Resource Conflict: picture.png
WARNING: Resource Conflict: code.cpp
WARNING: Resource Conflict: audio.mp3
WARNING: Resource Conflict: video.mp4
WARNING: Resource Conflict: config.json
WARNING: Resource Conflict: log.txt
WARNING: Resource Conflict: backup.zip
WARNING: Resource Conflict: exec.sh
WARNING: Resource Conflict: archive.rar
TaggerExperiments.getFeaturesNew: initializing fnumArr.
TaggerExperiments.getFeaturesNew: initializing fnumArr.
TaggerExperiments.getFeaturesNew: initializing fnumArr.
TaggerExperiments.getFeaturesNew: initializing fnumArr.
TaggerExperiments.getFeaturesNew: initializing fnumArr.
TaggerExperiments.getFeaturesNew: initializing fnumArr.
TaggerExperiments.getFeaturesNew: initializing fnumArr.
TaggerExperiments.getFeaturesNew: initializing fnumArr.
TaggerExperiments.getFeaturesNew: initializing fnumArr.
TaggerExperiments.getFeaturesNew: initializing fnumArr.
TaggerExperiments.getFeaturesNew: initializing fnumArr.
TaggerExperiments.getFeaturesNew: initializing fnumArr.
TaggerExperiments.getFeaturesNew: initializing fnumArr.
TaggerExperiments.getFeaturesNew: initializing fnumArr.
TaggerExperiments.getFeaturesNew: initializing fnumArr.
before adapting, weights size=15
before adapting, weights size=5
before adapting, weights size=20
before adapting, weights size=8
before adapting, weights size=12
before adapting, weights size=3
before adapting, weights size=7
before adapting, weights size=17
before adapting, weights size=6
before adapting, weights size=9
before adapting, weights size=13
before adapting, weights size=4
before adapting, weights size=11
Linear classifier with the following weights
Linear classifier with the following weights
Linear classifier with the following weights
Linear classifier with the following weights
Linear classifier with the following weights
Linear classifier with the following weights
Linear classifier with the following weights
Linear classifier with the following weights
Linear classifier with the following weights
Linear classifier with the following weights
Linear classifier with the following weights
Linear classifier with the following weights
Linear classifier with the following weights
Linear classifier with the following weights
Linear classifier with the following weights
xSize [num Phi templates] = 5; ySize [num classes] = 2
xSize [num Phi templates] = 8; ySize [num classes] = 7
xSize [num Phi templates] = 12; ySize [num classes] = 6
xSize [num Phi templates] = 2; ySize [num classes] = 9
xSize [num Phi templates] = 4; ySize [num classes] = 1
xSize [num Phi templates] = 6; ySize [num classes] = 11
xSize [num Phi templates] = 9; ySize [num classes] = 13
xSize [num Phi templates] = 7; ySize [num classes] = 15
xSize [num Phi templates] = 1; ySize [num classes] = 14
xSize [num Phi templates] = 11; ySize [num classes] = 5
xSize [num Phi templates] = 3; ySize [num classes] = 10
xSize [num Phi templates] = 13; ySize [num classes] = 4
xSize [num Phi templates] = 15; ySize [num classes] = 8
[206 ms, threads waiting 39 ms]
[334 ms, threads waiting 42 ms]
[84 ms, threads waiting 16 ms]
[291 ms, threads waiting 51 ms]
[225 ms, threads waiting 37 ms]
[178 ms, threads waiting 29 ms]
[273 ms, threads waiting 48 ms]
[113 ms, threads waiting 25 ms]
[345 ms, threads waiting 47 ms]
[207 ms, threads waiting 43 ms]
[162 ms, threads waiting 32 ms]
[238 ms, threads waiting 41 ms]
[125 ms, threads waiting 20 ms]
Featurizing tagged data tokens...
Featurizing tagged data tokens...
Featurizing tagged data tokens...
Featurizing tagged data tokens...
Featurizing tagged data tokens...
Featurizing tagged data tokens...
Featurizing tagged data tokens...
Featurizing tagged data tokens...
Featurizing tagged data tokens...
Featurizing tagged data tokens...
Featurizing tagged data tokens...
Featurizing tagged data tokens...
Featurizing tagged data tokens...
Featurizing tagged data tokens...
Featurizing tagged data tokens...
TaggerExperiments: adding word/tags
TaggerExperiments: adding word/tags
TaggerExperiments: adding word/tags
TaggerExperiments: adding word/tags
TaggerExperiments: adding word/tags
TaggerExperiments: adding word/tags
TaggerExperiments: adding word/tags
TaggerExperiments: adding word/tags
TaggerExperiments: adding word/tags
TaggerExperiments: adding word/tags
TaggerExperiments: adding word/tags
TaggerExperiments: adding word/tags
TaggerExperiments: adding word/tags
TaggerExperiments: adding word/tags
TaggerExperiments: adding word/tags
Derivative norm 0.113 < 0.05: quitting
Derivative norm 0.185 < 0.05: quitting
Derivative norm 0.301 < 0.05: quitting
Derivative norm 0.082 < 0.05: quitting
Derivative norm 0.175 < 0.05: quitting
Derivative norm 0.142 < 0.05: quitting
Derivative norm 0.199 < 0.05: quitting
Derivative norm 0.275 < 0.05: quitting
Derivative norm 0.123 < 0.05: quitting
Derivative norm 0.156 < 0.05: quitting
Derivative norm 0.211 < 0.05: quitting
Derivative norm 0.331 < 0.05: quitting
Derivative norm 0.094 < 0.05: quitting
# if the word occurred at least this many times in the training data.
# if the word occurred at least this many times in the training data.
# if the word occurred at least this many times in the training data.
# if the word occurred at least this many times in the training data.
# if the word occurred at least this many times in the training data.
# if the word occurred at least this many times in the training data.
# if the word occurred at least this many times in the training data.
# if the word occurred at least this many times in the training data.
# if the word occurred at least this many times in the training data.
# if the word occurred at least this many times in the training data.
# if the word occurred at least this many times in the training data.
# if the word occurred at least this many times in the training data.
# if the word occurred at least this many times in the training data.
# if the word occurred at least this many times in the training data.
# if the word occurred at least this many times in the training data.
training completed by interruption
training completed by interruption
training completed by interruption
training completed by interruption
training completed by interruption
training completed by interruption
training completed by interruption
training completed by interruption
training completed by interruption
training completed by interruption
training completed by interruption
training completed by interruption
training completed by interruption
training completed by interruption
training completed by interruption
# The tagger will only use tags it has seen for a particular word
# The tagger will only use tags it has seen for a particular word
# The tagger will only use tags it has seen for a particular word
# The tagger will only use tags it has seen for a particular word
# The tagger will only use tags it has seen for a particular word
# The tagger will only use tags it has seen for a particular word
# The tagger will only use tags it has seen for a particular word
# The tagger will only use tags it has seen for a particular word
# The tagger will only use tags it has seen for a particular word
# The tagger will only use tags it has seen for a particular word
# The tagger will only use tags it has seen for a particular word
# The tagger will only use tags it has seen for a particular word
# The tagger will only use tags it has seen for a particular word
# The tagger will only use tags it has seen for a particular word
# The tagger will only use tags it has seen for a particular word
received quit command: quitting
received quit command: quitting
received quit command: quitting
received quit command: quitting
received quit command: quitting
received quit command: quitting
received quit command: quitting
received quit command: quitting
received quit command: quitting
received quit command: quitting
received quit command: quitting
received quit command: quitting
received quit command: quitting
received quit command: quitting
received quit command: quitting
training completed without interruption
training completed without interruption
training completed without interruption
training completed without interruption
training completed without interruption
training completed without interruption
training completed without interruption
training completed without interruption
training completed without interruption
training completed without interruption
training completed without interruption
training completed without interruption
training completed without interruption
training completed without interruption
training completed without interruption
if left to their own devices.
if left to their own devices.
if left to their own devices.
if left to their own devices.
if left to their own devices.
if left to their own devices.
if left to their own devices.
if left to their own devices.
if left to their own devices.
if left to their own devices.
if left to their own devices.
if left to their own devices.
if left to their own devices.
if left to their own devices.
if left to their own devices.
The convergence criteria are quite aggressive if left uninterrupted, and will run for a while
The convergence criteria are quite aggressive if left uninterrupted, and will run for a while
The convergence criteria are quite aggressive if left uninterrupted, and will run for a while
The convergence criteria are quite aggressive if left uninterrupted, and will run for a while
The convergence criteria are quite aggressive if left uninterrupted, and will run for a while
The convergence criteria are quite aggressive if left uninterrupted, and will run for a while
The convergence criteria are quite aggressive if left uninterrupted, and will run for a while
The convergence criteria are quite aggressive if left uninterrupted, and will run for a while
The convergence criteria are quite aggressive if left uninterrupted, and will run for a while
The convergence criteria are quite aggressive if left uninterrupted, and will run for a while
The convergence criteria are quite aggressive if left uninterrupted, and will run for a while
The convergence criteria are quite aggressive if left uninterrupted, and will run for a while
The convergence criteria are quite aggressive if left uninterrupted, and will run for a while
The convergence criteria are quite aggressive if left uninterrupted, and will run for a while
The convergence criteria are quite aggressive if left uninterrupted, and will run for a while
NOTE: you can press any key (and maybe ENTER afterwards to jog stdin) to terminate learning early.
NOTE: you can press any key (and maybe ENTER afterwards to jog stdin) to terminate learning early.
NOTE: you can press any key (and maybe ENTER afterwards to jog stdin) to terminate learning early.
NOTE: you can press any key (and maybe ENTER afterwards to jog stdin) to terminate learning early.
NOTE: you can press any key (and maybe ENTER afterwards to jog stdin) to terminate learning early.
NOTE: you can press any key (and maybe ENTER afterwards to jog stdin) to terminate learning early.
NOTE: you can press any key (and maybe ENTER afterwards to jog stdin) to terminate learning early.
NOTE: you can press any key (and maybe ENTER afterwards to jog stdin) to terminate learning early.
NOTE: you can press any key (and maybe ENTER afterwards to jog stdin) to terminate learning early.
NOTE: you can press any key (and maybe ENTER afterwards to jog stdin) to terminate learning early.
NOTE: you can press any key (and maybe ENTER afterwards to jog stdin) to terminate learning early.
NOTE: you can press any key (and maybe ENTER afterwards to jog stdin) to terminate learning early.
NOTE: you can press any key (and maybe ENTER afterwards to jog stdin) to terminate learning early.
NOTE: you can press any key (and maybe ENTER afterwards to jog stdin) to terminate learning early.
NOTE: you can press any key (and maybe ENTER afterwards to jog stdin) to terminate learning early.
[Quiet training complete]
[Quiet training complete]
[Quiet training complete]
[Quiet training complete]
[Quiet training complete]
[Quiet training complete]
[Quiet training complete]
[Quiet training complete]
[Quiet training complete]
[Quiet training complete]
[Quiet training complete]
[Quiet training complete]
[Quiet training complete]
[Quiet training complete]
[Quiet training complete]
[Beginning quiet training]
[Beginning quiet training]
[Beginning quiet training]
[Beginning quiet training]
[Beginning quiet training]
[Beginning quiet training]
[Beginning quiet training]
[Beginning quiet training]
[Beginning quiet training]
[Beginning quiet training]
[Beginning quiet training]
[Beginning quiet training]
[Beginning quiet training]
[Beginning quiet training]
[Beginning quiet training]
This clique for tree: 25
This clique for tree: 30
This clique for tree: 15
This clique for tree: 5
This clique for tree: 20
This clique for tree: 12
This clique for tree: 18
This clique for tree: 9
This clique for tree: 22
This clique for tree: 8
This clique for tree: 16
This clique for tree: 14
This clique for tree: 7
# nthreads = 5
# nthreads = 25
# nthreads = 15
# nthreads = 8
# nthreads = 12
# nthreads = 3
# nthreads = 20
# nthreads = 7
# nthreads = 18
# nthreads = 2
# nthreads = 9
# nthreads = 13
# nthreads = 6
# testFile and textFile can use multiple threads to process text.
# testFile and textFile can use multiple threads to process text.
# testFile and textFile can use multiple threads to process text.
# testFile and textFile can use multiple threads to process text.
# testFile and textFile can use multiple threads to process text.
# testFile and textFile can use multiple threads to process text.
# testFile and textFile can use multiple threads to process text.
# testFile and textFile can use multiple threads to process text.
# testFile and textFile can use multiple threads to process text.
# testFile and textFile can use multiple threads to process text.
# testFile and textFile can use multiple threads to process text.
# testFile and textFile can use multiple threads to process text.
# testFile and textFile can use multiple threads to process text.
# testFile and textFile can use multiple threads to process text.
# testFile and textFile can use multiple threads to process text.
Pre-existing for tree: treePartitionFunctions[trees[1]]
Pre-existing for tree: treePartitionFunctions[trees[2]]
Pre-existing for tree: treePartitionFunctions[trees[3]]
Pre-existing for tree: treePartitionFunctions[trees[4]]
Pre-existing for tree: treePartitionFunctions[trees[5]]
Pre-existing for tree: treePartitionFunctions[trees[6]]
Pre-existing for tree: treePartitionFunctions[trees[7]]
Pre-existing for tree: treePartitionFunctions[trees[8]]
Pre-existing for tree: treePartitionFunctions[trees[9]]
Pre-existing for tree: treePartitionFunctions[trees[10]]
Pre-existing for tree: treePartitionFunctions[trees[11]]
Pre-existing for tree: treePartitionFunctions[trees[12]]
Pre-existing for tree: treePartitionFunctions[trees[13]]
# veryCommonWordThresh = 75
# veryCommonWordThresh = 100
# veryCommonWordThresh = 125
# veryCommonWordThresh = 150
# veryCommonWordThresh = 175
# veryCommonWordThresh = 200
# veryCommonWordThresh = 225
# veryCommonWordThresh = 250
# veryCommonWordThresh = 275
# veryCommonWordThresh = 300
# veryCommonWordThresh = 325
# veryCommonWordThresh = 350
# veryCommonWordThresh = 375
Max weight: 70 min weight: 20
Max weight: 90 min weight: 30
Max weight: 30 min weight: 5
Max weight: 60 min weight: 15
Max weight: 80 min weight: 25
Max weight: 40 min weight: 8
Max weight: 55 min weight: 12
Max weight: 75 min weight: 18
Max weight: 95 min weight: 28
Max weight: 35 min weight: 6
Max weight: 65 min weight: 16
Max weight: 85 min weight: 22
Max weight: 45 min weight: 9
# you are using equivalence classes.
# you are using equivalence classes.
# you are using equivalence classes.
# you are using equivalence classes.
# you are using equivalence classes.
# you are using equivalence classes.
# you are using equivalence classes.
# you are using equivalence classes.
# you are using equivalence classes.
# you are using equivalence classes.
# you are using equivalence classes.
# you are using equivalence classes.
# you are using equivalence classes.
# you are using equivalence classes.
# you are using equivalence classes.
# times will form an equivalence class by themselves. ignored unless
# times will form an equivalence class by themselves. ignored unless
# times will form an equivalence class by themselves. ignored unless
# times will form an equivalence class by themselves. ignored unless
# times will form an equivalence class by themselves. ignored unless
# times will form an equivalence class by themselves. ignored unless
# times will form an equivalence class by themselves. ignored unless
# times will form an equivalence class by themselves. ignored unless
# times will form an equivalence class by themselves. ignored unless
# times will form an equivalence class by themselves. ignored unless
# times will form an equivalence class by themselves. ignored unless
# times will form an equivalence class by themselves. ignored unless
# times will form an equivalence class by themselves. ignored unless
# times will form an equivalence class by themselves. ignored unless
# times will form an equivalence class by themselves. ignored unless
# very common word threshold. words that occur more than this number of
# very common word threshold. words that occur more than this number of
# very common word threshold. words that occur more than this number of
# very common word threshold. words that occur more than this number of
# very common word threshold. words that occur more than this number of
# very common word threshold. words that occur more than this number of
# very common word threshold. words that occur more than this number of
# very common word threshold. words that occur more than this number of
# very common word threshold. words that occur more than this number of
# very common word threshold. words that occur more than this number of
# very common word threshold. words that occur more than this number of
# very common word threshold. words that occur more than this number of
# very common word threshold. words that occur more than this number of
# very common word threshold. words that occur more than this number of
# very common word threshold. words that occur more than this number of
# rareWordMinFeatureThresh = 10
# rareWordMinFeatureThresh = 15
# rareWordMinFeatureThresh = 20
# rareWordMinFeatureThresh = 25
# rareWordMinFeatureThresh = 30
# rareWordMinFeatureThresh = 35
# rareWordMinFeatureThresh = 40
# rareWordMinFeatureThresh = 45
# rareWordMinFeatureThresh = 50
# rareWordMinFeatureThresh = 55
# rareWordMinFeatureThresh = 60
# rareWordMinFeatureThresh = 65
# rareWordMinFeatureThresh = 70
Different partition functions for tree Oak:
Different partition functions for tree Maple:
Different partition functions for tree Pine:
Different partition functions for tree Willow:
Different partition functions for tree Cedar:
Different partition functions for tree Palm:
Different partition functions for tree Fir:
Different partition functions for tree Elm:
Different partition functions for tree Ash:
Different partition functions for tree Poplar:
Different partition functions for tree Cypress:
Different partition functions for tree Beech:
Different partition functions for tree Spruce:
Average weight: 57.2; std dev: 3.1
Average weight: 66.8; std dev: 1.8
Average weight: 60.6; std dev: 2.9
Average weight: 68.1; std dev: 1.5
Average weight: 59.4; std dev: 3.4
Average weight: 64.9; std dev: 2.6
Average weight: 55.8; std dev: 3.7
Average weight: 61.3; std dev: 2.1
Average weight: 69.7; std dev: 1.2
Average weight: 58.1; std dev: 3.6
Average weight: 63.7; std dev: 2.4
Average weight: 70.3; std dev: 1.1
Average weight: 56.5; std dev: 3.8
# appear less than this times will be ignored.
# appear less than this times will be ignored.
# appear less than this times will be ignored.
# appear less than this times will be ignored.
# appear less than this times will be ignored.
# appear less than this times will be ignored.
# appear less than this times will be ignored.
# appear less than this times will be ignored.
# appear less than this times will be ignored.
# appear less than this times will be ignored.
# appear less than this times will be ignored.
# appear less than this times will be ignored.
# appear less than this times will be ignored.
# appear less than this times will be ignored.
# appear less than this times will be ignored.
# rare word minimum feature threshold. features of rare words whose histories
# rare word minimum feature threshold. features of rare words whose histories
# rare word minimum feature threshold. features of rare words whose histories
# rare word minimum feature threshold. features of rare words whose histories
# rare word minimum feature threshold. features of rare words whose histories
# rare word minimum feature threshold. features of rare words whose histories
# rare word minimum feature threshold. features of rare words whose histories
# rare word minimum feature threshold. features of rare words whose histories
# rare word minimum feature threshold. features of rare words whose histories
# rare word minimum feature threshold. features of rare words whose histories
# rare word minimum feature threshold. features of rare words whose histories
# rare word minimum feature threshold. features of rare words whose histories
# rare word minimum feature threshold. features of rare words whose histories
# rare word minimum feature threshold. features of rare words whose histories
# rare word minimum feature threshold. features of rare words whose histories
# curWordMinFeatureThresh = 5
# curWordMinFeatureThresh = 8
# curWordMinFeatureThresh = 11
# curWordMinFeatureThresh = 14
# curWordMinFeatureThresh = 17
# curWordMinFeatureThresh = 20
# curWordMinFeatureThresh = 23
# curWordMinFeatureThresh = 26
# curWordMinFeatureThresh = 29
# curWordMinFeatureThresh = 32
# curWordMinFeatureThresh = 35
# curWordMinFeatureThresh = 38
# curWordMinFeatureThresh = 41
Linear classifier with 5 f(x,y) features
Linear classifier with 15 f(x,y) features
Linear classifier with 8 f(x,y) features
Linear classifier with 12 f(x,y) features
Linear classifier with 3 f(x,y) features
Linear classifier with 7 f(x,y) features
Linear classifier with 20 f(x,y) features
Linear classifier with 9 f(x,y) features
Linear classifier with 6 f(x,y) features
Linear classifier with 14 f(x,y) features
Linear classifier with 18 f(x,y) features
Linear classifier with 4 f(x,y) features
Linear classifier with 11 f(x,y) features
# number of times will generate features with all of their occurring
# number of times will generate features with all of their occurring
# number of times will generate features with all of their occurring
# number of times will generate features with all of their occurring
# number of times will generate features with all of their occurring
# number of times will generate features with all of their occurring
# number of times will generate features with all of their occurring
# number of times will generate features with all of their occurring
# number of times will generate features with all of their occurring
# number of times will generate features with all of their occurring
# number of times will generate features with all of their occurring
# number of times will generate features with all of their occurring
# number of times will generate features with all of their occurring
# number of times will generate features with all of their occurring
# number of times will generate features with all of their occurring
# current word feature threshold. words that occur more than this
# current word feature threshold. words that occur more than this
# current word feature threshold. words that occur more than this
# current word feature threshold. words that occur more than this
# current word feature threshold. words that occur more than this
# current word feature threshold. words that occur more than this
# current word feature threshold. words that occur more than this
# current word feature threshold. words that occur more than this
# current word feature threshold. words that occur more than this
# current word feature threshold. words that occur more than this
# current word feature threshold. words that occur more than this
# current word feature threshold. words that occur more than this
# current word feature threshold. words that occur more than this
# current word feature threshold. words that occur more than this
# current word feature threshold. words that occur more than this
# than this number of times are ignored.
# than this number of times are ignored.
# than this number of times are ignored.
# than this number of times are ignored.
# than this number of times are ignored.
# than this number of times are ignored.
# than this number of times are ignored.
# than this number of times are ignored.
# than this number of times are ignored.
# than this number of times are ignored.
# than this number of times are ignored.
# than this number of times are ignored.
# than this number of times are ignored.
# than this number of times are ignored.
# than this number of times are ignored.
# minimum feature threshold. features whose history appears less
# minimum feature threshold. features whose history appears less
# minimum feature threshold. features whose history appears less
# minimum feature threshold. features whose history appears less
# minimum feature threshold. features whose history appears less
# minimum feature threshold. features whose history appears less
# minimum feature threshold. features whose history appears less
# minimum feature threshold. features whose history appears less
# minimum feature threshold. features whose history appears less
# minimum feature threshold. features whose history appears less
# minimum feature threshold. features whose history appears less
# minimum feature threshold. features whose history appears less
# minimum feature threshold. features whose history appears less
# minimum feature threshold. features whose history appears less
# minimum feature threshold. features whose history appears less
node A: 0.34
node B: 0.56
node C: 0.67
node D: 0.45
node E: 0.52
node F: 0.61
node G: 0.49
node H: 0.58
node I: 0.44
node J: 0.51
node K: 0.69
node L: 0.47
node M: 0.54
node N: 0.63
node O: 0.48
Labeling 12 sentences with 5 phrases for label NOUN
Labeling 8 sentences with 3 phrases for label VERB
Labeling 10 sentences with 4 phrases for label ADJ
Labeling 9 sentences with 6 phrases for label ADV
Labeling 11 sentences with 7 phrases for label PRON
Labeling 7 sentences with 2 phrases for label DET
Labeling 13 sentences with 8 phrases for label CONJ
Labeling 14 sentences with 9 phrases for label NUM
Index 1 is element 0 of binaryTensor 42 , 12
Index 0 is element -1 of binaryTensor 13 , 34
Index 4 is element 3 of binaryTensor 5 , 6
Index index is element index - curIndex of binaryTensor entry.getFirstKey() , entry.getSecondKey()
Index 2 is element 1 of binaryTensor 23 , 56
Index 3 is element 2 of binaryTensor 77 , 88
Index 1 is element 0 of binaryTensor 99 , 11
Index index - curIndex is element entry.getFirstKey() of binaryTensor entry.getSecondKey() , entry.getFirstKey()
Index 0 is element -1 of binaryTensor 0 , 1
Index 4 is element 3 of binaryTensor 42 , 13
Index index - curIndex is element entry.getFirstKey() of binaryTensor entry.getSecondKey() , entry.getFirstKey()
Index 3 is element 2 of binaryTensor 55 , 11
Index 2 is element 1 of binaryTensor 13 , 22
Index 1 is element 0 of binaryTensor 32 , 44
Index 0 is element -1 of binaryTensor 18 , 7
[142 ms, threads waiting 37 ms]
[548 ms, threads waiting 73 ms]
[319 ms, threads waiting 51 ms]
[925 ms, threads waiting 42 ms]
[211 ms, threads waiting 88 ms]
[635 ms, threads waiting 29 ms]
[467 ms, threads waiting 64 ms]
[752 ms, threads waiting 56 ms]
[632 ms, threads waiting 93 ms]
[189 ms, threads waiting 76 ms]
[843 ms, threads waiting 67 ms]
[726 ms, threads waiting 84 ms]
[511 ms, threads waiting 45 ms]
[376 ms, threads waiting 62 ms]
[982 ms, threads waiting 71 ms]
Parseable: false
Parseable: true
Parseable: false
Parseable: true
Parseable: false
Parseable: true
Parseable: false
Parseable: true
Parseable: false
Parseable: true
Parseable: false
Parseable: true
Parseable: false
Input:     20
Input:     30
Input:     40
Input:     50
Input:     60
Input:     70
Input:     80
Input:     90
Input:     100
Input:     110
Input:     120
Input:     130
Input:     140
(sent ave 91.23, evalb 85.67)   Exact: 92.10
(sent ave 75.89, evalb 83.12)   Exact: 85.43
(sent ave 82.45, evalb 79.34)   Exact: 91.56
(sent ave 87.12, evalb 86.23)   Exact: 88.76
(sent ave 79.45, evalb 80.56)   Exact: 82.34
(sent ave 89.67, evalb 77.89)   Exact: 94.78
(sent ave 84.32, evalb 87.91)   Exact: 79.56
(sent ave 76.78, evalb 82.12)   Exact: 87.90
(sent ave 83.56, evalb 89.45)   Exact: 83.61
(sent ave 81.23, evalb 84.67)   Exact: 88.34
(sent ave 88.90, evalb 79.45)   Exact: 91.23
(sent ave 75.34, evalb 86.43)   Exact: 83.09
(sent ave 86.12, evalb 88.90)   Exact: 77.23
Training treebank does not exist! /path/to/treebank2
Training treebank does not exist! /path/to/treebank3
Training treebank does not exist! /path/to/treebank4
Training treebank does not exist! /path/to/treebank5
Training treebank does not exist! /path/to/treebank6
Training treebank does not exist! /path/to/treebank7
Training treebank does not exist! /path/to/treebank8
Training treebank does not exist! /path/to/treebank9
Training treebank does not exist! /path/to/treebank10
Training treebank does not exist! /path/to/treebank11
Training treebank does not exist! /path/to/treebank12
Training treebank does not exist! /path/to/treebank13
Training treebank does not exist! /path/to/treebank14
R: 94.76
R: 66.42
R: 78.21
R: 92.84
R: 71.36
R: 85.91
R: 62.81
R: 96.07
R: 74.59
R: 81.12
R: 69.29
R: 88.94
R: 76.45
Test treebank does not exist! /path/to/anotherTestTreebank
Test treebank does not exist! /path/to/yetAnotherTestTreebank
Test treebank does not exist! /path/to/fourthTestTreebank
Test treebank does not exist! /path/to/fifthTestTreebank
Test treebank does not exist! /path/to/sixthTestTreebank
Test treebank does not exist! /path/to/seventhTestTreebank
Test treebank does not exist! /path/to/eighthTestTreebank
Test treebank does not exist! /path/to/ninthTestTreebank
Test treebank does not exist! /path/to/tenthTestTreebank
Test treebank does not exist! /path/to/eleventhTestTreebank
Test treebank does not exist! /path/to/twelfthTestTreebank
Test treebank does not exist! /path/to/thirteenthTestTreebank
Test treebank does not exist! /path/to/fourteenthTestTreebank
(sent ave 85.23) (evalb 76.54)
(sent ave 76.89) (evalb 68.97)
(sent ave 91.36) (evalb 81.98)
(sent ave 78.56) (evalb 70.45)
(sent ave 83.24) (evalb 74.59)
(sent ave 77.89) (evalb 69.34)
(sent ave 89.07) (evalb 80.15)
(sent ave 82.14) (evalb 73.27)
(sent ave 87.65) (evalb 78.56)
(sent ave 75.68) (evalb 67.89)
(sent ave 84.56) (evalb 75.21)
(sent ave 79.45) (evalb 71.89)
(sent ave 88.43) (evalb 79.67)
Using chromatic sampling with 7 threads
Using chromatic sampling with 2 threads
Using chromatic sampling with 9 threads
Using chromatic sampling with 5 threads
Using chromatic sampling with 3 threads
Using chromatic sampling with 6 threads
Using chromatic sampling with 8 threads
Using chromatic sampling with 10 threads
Using chromatic sampling with 11 threads
Using chromatic sampling with 12 threads
Using chromatic sampling with 13 threads
Using chromatic sampling with 14 threads
P: 76.89
P: 85.42
P: 92.16
P: 64.75
P: 81.23
P: 97.88
P: 73.45
P: 89.92
P: 67.29
P: 93.78
P: 79.56
P: 84.57
P: 71.34
itr 2: 130
itr 3: 115
itr 4: 135
itr 5: 105
itr 6: 125
itr 7: 140
itr 8: 110
itr 9: 130
itr 10: 115
itr 11: 125
itr 12: 105
itr 13: 135
itr 14: 120
Unknown option:--version
Unknown option:-f
Unknown option:--verbose
Unknown option:--o
Unknown option:--debug
Unknown option:-l
Unknown option:--output
Unknown option:-h
Unknown option:--input
Unknown option:--config
Unknown option:-t
Unknown option:--log
Unknown option:-d
Unknown option:--force
DateRange: previous: yesterday
DateRange: previous: one month ago
DateRange: previous: three days ago
DateRange: previous: two weeks ago
DateRange: previous: last month
DateRange: previous: four days ago
DateRange: previous: three weeks ago
DateRange: previous: two months ago
DateRange: previous: five days ago
DateRange: previous: four weeks ago
DateRange: previous: three months ago
DateRange: previous: six days ago
DateRange: previous: five weeks ago
Index 1 is element -1 of unaryClassification \"B\"
Index 2 is element 5 of unaryClassification \"C\"
Index 3 is element 0 of unaryClassification \"D\"
Index 4 is element 3 of unaryClassification \"E\"
Index 5 is element -2 of unaryClassification \"F\"
Index 6 is element 2 of unaryClassification \"G\"
Index 7 is element 1 of unaryClassification \"H\"
Index 8 is element 4 of unaryClassification \"I\"
Index 9 is element -3 of unaryClassification \"J\"
Index 10 is element 0 of unaryClassification \"K\"
Index 11 is element 6 of unaryClassification \"L\"
Index 12 is element 2 of unaryClassification \"M\"
Index 13 is element -1 of unaryClassification \"N\"
Node 2 support is false
Node 3 support is true
Node 4 support is false
Node 5 support is true
Node 6 support is false
Node 7 support is true
Node 8 support is false
Node 9 support is true
Node 10 support is false
Node 11 support is true
Node 12 support is false
Node 13 support is true
Node 14 support is false
WARNING: relation 'orders' not tested
WARNING: relation 'products' not tested
WARNING: relation 'employees' not tested
WARNING: relation 'customers' not tested
WARNING: relation 'inventory' not tested
WARNING: relation 'categories' not tested
WARNING: relation 'suppliers' not tested
WARNING: relation 'payments' not tested
WARNING: relation 'reviews' not tested
WARNING: relation 'transactions' not tested
WARNING: relation 'logs' not tested
WARNING: relation 'settings' not tested
WARNING: relation 'messages' not tested
WARNING: relation 'assets' not tested
Got stems:s3 : s4
Got stems:s5 : s6
Got stems:s7 : s8
Got stems:s9 : s10
Got stems:s11 : s12
Got stems:s13 : s14
Got stems:s15 : s16
Got stems:s17 : s18
Got stems:s19 : s20
Got stems:s21 : s22
Got stems:s23 : s24
Got stems:s25 : s26
Got stems:s27 : s28
Testing match:s3 : s4
Testing match:s5 : s6
Testing match:s7 : s8
Testing match:s9 : s10
Testing match:s11 : s12
Testing match:s13 : s14
Testing match:s15 : s16
Testing match:s17 : s18
Testing match:s19 : s20
Testing match:s21 : s22
Testing match:s23 : s24
Testing match:s25 : s26
Testing match:s27 : s28
child basic cats: Maine Coon
child basic cats: Bengal
child basic cats: Persian
child basic cats: Ragdoll
child basic cats: Abyssinian
child basic cats: Sphynx
child basic cats: British Shorthair
child basic cats: Scottish Fold
child basic cats: Siberian
child basic cats: Norwegian Forest
child basic cats: Birman
child basic cats: Oriental Shorthair
child basic cats: Balinese
Matching pattern pattern2 to graph2 :doesn't match
Matching pattern pattern3 to graph3 :matches
Matching pattern pattern4 to graph4 :matches
Matching pattern pattern5 to graph5 :doesn't match
Matching pattern pattern6 to graph6 :matches
Matching pattern pattern7 to graph7 :doesn't match
Matching pattern pattern8 to graph8 :matches
Matching pattern pattern9 to graph9 :doesn't match
Matching pattern pattern10 to graph10 :matches
Matching pattern pattern11 to graph11 :matches
Matching pattern pattern12 to graph12 :doesn't match
Matching pattern pattern13 to graph13 :matches
Matching pattern pattern14 to graph14 :doesn't match
month extracted: February
month extracted: March
month extracted: April
month extracted: May
month extracted: June
month extracted: July
month extracted: August
month extracted: September
month extracted: October
month extracted: November
month extracted: December
month extracted: Spring
month extracted: Summer
month extracted: Autumn
ISODateInstance: Couldn't parse probable century: 1990-05-15
ISODateInstance: Couldn't parse probable century: 2008-09-30
ISODateInstance: Couldn't parse probable century: 1985-07-12
ISODateInstance: Couldn't parse probable century: 1999-12-31
ISODateInstance: Couldn't parse probable century: 2005-06-20
ISODateInstance: Couldn't parse probable century: 1980-04-08
ISODateInstance: Couldn't parse probable century: 1975-11-23
ISODateInstance: Couldn't parse probable century: 1992-03-17
ISODateInstance: Couldn't parse probable century: 2002-08-05
ISODateInstance: Couldn't parse probable century: 1988-02-11
ISODateInstance: Couldn't parse probable century: 1983-10-01
ISODateInstance: Couldn't parse probable century: 1994-09-14
ISODateInstance: Couldn't parse probable century: 2007-06-29
ISODateInstance: Couldn't parse probable century: 1996-07-22
Classifier uppercase
Classifier lowercase
Classifier camelCase
Classifier snake_case
Classifier PascalCase
Classifier Title Case
Classifier kebab-case
Classifier 123
Classifier !@#$
Classifier word123
Classifier WOrdShAPe
Classifier UPPER_CASE
Classifier Random
Extracting year from: |2019|
Extracting year from: |1998|
Extracting year from: |2005|
Extracting year from: |2014|
Extracting year from: |2000|
Extracting year from: |1995|
Extracting year from: |2018|
Extracting year from: |2007|
Extracting year from: |2012|
Extracting year from: |1999|
Extracting year from: |2004|
Extracting year from: |2009|
Extracting year from: |2017|
Extracting year from: |2003|
First annotator: b2
First annotator: c3
First annotator: d4
First annotator: e5
First annotator: f6
First annotator: g7
First annotator: h8
First annotator: i9
First annotator: j10
First annotator: k11
First annotator: l12
First annotator: m13
First annotator: n14
NO NUMBER: banana
NO NUMBER: orange
NO NUMBER: cherry
NO NUMBER: pineapple
NO NUMBER: mango
NO NUMBER: grape
NO NUMBER: watermelon
NO NUMBER: kiwi
NO NUMBER: strawberry
NO NUMBER: lemon
NO NUMBER: peach
NO NUMBER: pear
NO NUMBER: plum
Graph = sg2
Graph = sg3
Graph = sg4
Graph = sg5
Graph = sg6
Graph = sg7
Graph = sg8
Graph = sg9
Graph = sg10
Graph = sg11
Graph = sg12
Graph = sg13
Graph = sg14
Reading Ssurgeon patterns from directory = /path/to/directory2
Reading Ssurgeon patterns from directory = /path/to/directory3
Reading Ssurgeon patterns from directory = /path/to/directory4
Reading Ssurgeon patterns from directory = /path/to/directory5
Reading Ssurgeon patterns from directory = /path/to/directory6
Reading Ssurgeon patterns from directory = /path/to/directory7
Reading Ssurgeon patterns from directory = /path/to/directory8
Reading Ssurgeon patterns from directory = /path/to/directory9
Reading Ssurgeon patterns from directory = /path/to/directory10
Reading Ssurgeon patterns from directory = /path/to/directory11
Reading Ssurgeon patterns from directory = /path/to/directory12
Reading Ssurgeon patterns from directory = /path/to/directory13
Reading Ssurgeon patterns from directory = /path/to/directory14
Could not read Regex mapping: regex.txt
Could not read Regex mapping: patterns.yaml
Could not read Regex mapping: rules.xml
Could not read Regex mapping: mappings.json
Could not read Regex mapping: definitions.xml
Could not read Regex mapping: rules.txt
Could not read Regex mapping: patterns.ini
Could not read Regex mapping: mappings.yaml
Could not read Regex mapping: config.json
Could not read Regex mapping: patterns.xml
Could not read Regex mapping: rules.yaml
Could not read Regex mapping: definitions.txt
Could not read Regex mapping: config.yaml
Tags are: 2
Tags are: 3
Tags are: 4
Tags are: 5
Tags are: 6
Tags are: 7
Tags are: 8
Tags are: 9
Tags are: 10
Tags are: 11
Tags are: 12
Tags are: 13
Tags are: 14
No test treebank path specified.  Using train path: \"path2\"
No test treebank path specified.  Using train path: \"path3\"
No test treebank path specified.  Using train path: \"path4\"
No test treebank path specified.  Using train path: \"path5\"
No test treebank path specified.  Using train path: \"path6\"
No test treebank path specified.  Using train path: \"path7\"
No test treebank path specified.  Using train path: \"path8\"
No test treebank path specified.  Using train path: \"path9\"
No test treebank path specified.  Using train path: \"path10\"
No test treebank path specified.  Using train path: \"path11\"
No test treebank path specified.  Using train path: \"path12\"
No test treebank path specified.  Using train path: \"path13\"
No test treebank path specified.  Using train path: \"path14\"
No tune treebank path specified. Using train path: \"/data/train_path_2\"
No tune treebank path specified. Using train path: \"/data/train_path_3\"
No tune treebank path specified. Using train path: \"/data/train_path_4\"
No tune treebank path specified. Using train path: \"/data/train_path_5\"
No tune treebank path specified. Using train path: \"/data/train_path_6\"
No tune treebank path specified. Using train path: \"/data/train_path_7\"
No tune treebank path specified. Using train path: \"/data/train_path_8\"
No tune treebank path specified. Using train path: \"/data/train_path_9\"
No tune treebank path specified. Using train path: \"/data/train_path_10\"
No tune treebank path specified. Using train path: \"/data/train_path_11\"
No tune treebank path specified. Using train path: \"/data/train_path_12\"
No tune treebank path specified. Using train path: \"/data/train_path_13\"
No tune treebank path specified. Using train path: \"/data/train_path_14\"
Writing out binary trees to file2...
Writing out binary trees to file3...
Writing out binary trees to file4...
Writing out binary trees to file5...
Writing out binary trees to file6...
Writing out binary trees to file7...
Writing out binary trees to file8...
Writing out binary trees to file9...
Writing out binary trees to file10...
Writing out binary trees to file11...
Writing out binary trees to file12...
Writing out binary trees to file13...
Writing out binary trees to file14...
Gold: [5, 8, 2, 7, 3]
Gold: [1, 4, 6, 9, 0]
Gold: [2, 3, 1, 5, 8]
Gold: [9, 0, 4, 7, 6]
Gold: [5, 1, 2, 4, 3]
Gold: [6, 8, 0, 7, 9]
Gold: [3, 4, 9, 6, 1]
Gold: [8, 5, 7, 3, 2]
Gold: [0, 2, 6, 4, 9]
Gold: [1, 7, 5, 3, 8]
Gold: [4, 9, 0, 2, 6]
Gold: [7, 1, 6, 3, 5]
Gold: [2, 0, 8, 9, 4]
undirected nodes btw 1 and 3 is B
undirected nodes btw 1 and 3 is C
undirected nodes btw 1 and 3 is D
undirected nodes btw 1 and 3 is E
undirected nodes btw 1 and 3 is F
undirected nodes btw 1 and 3 is G
undirected nodes btw 1 and 3 is H
undirected nodes btw 1 and 3 is I
undirected nodes btw 1 and 3 is J
undirected nodes btw 1 and 3 is K
undirected nodes btw 1 and 3 is L
undirected nodes btw 1 and 3 is M
undirected nodes btw 1 and 3 is N
directed path nodes btw 2 and 4 is 7
directed path nodes btw 2 and 4 is 6
directed path nodes btw 2 and 4 is 9
directed path nodes btw 2 and 4 is 5
directed path nodes btw 2 and 4 is 10
directed path nodes btw 2 and 4 is 8
directed path nodes btw 2 and 4 is 1
directed path nodes btw 2 and 4 is 12
directed path nodes btw 2 and 4 is 11
directed path nodes btw 2 and 4 is 15
directed path nodes btw 2 and 4 is 13
directed path nodes btw 2 and 4 is 14
directed path nodes btw 2 and 4 is 18
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 2
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 3
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 4
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 5
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 6
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 7
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 8
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 9
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 10
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 11
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 12
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 13
WHOA! SemanticGraph.toCompactStringHelper() ran into problems at node 14
Failed to open file in writeToDOTfile: output.txt
Failed to open file in writeToDOTfile: log.txt
Failed to open file in writeToDOTfile: data.csv
Failed to open file in writeToDOTfile: config.ini
Failed to open file in writeToDOTfile: report.docx
Failed to open file in writeToDOTfile: script.js
Failed to open file in writeToDOTfile: style.css
Failed to open file in writeToDOTfile: image.jpg
Failed to open file in writeToDOTfile: video.mp4
Failed to open file in writeToDOTfile: audio.wav
Failed to open file in writeToDOTfile: template.html
Failed to open file in writeToDOTfile: database.sql
Failed to open file in writeToDOTfile: backup.zip
Empty top speakers list for: quote2.toShorterString() [no candidate top speakers found – just ignore!
Empty top speakers list for: quote3.toShorterString() [no candidate top speakers found – just ignore!
Empty top speakers list for: quote4.toShorterString() [no candidate top speakers found – just ignore!
Empty top speakers list for: quote5.toShorterString() [no candidate top speakers found – just ignore!
Empty top speakers list for: quote6.toShorterString() [no candidate top speakers found – just ignore!
Empty top speakers list for: quote7.toShorterString() [no candidate top speakers found – just ignore!
Empty top speakers list for: quote8.toShorterString() [no candidate top speakers found – just ignore!
Empty top speakers list for: quote9.toShorterString() [no candidate top speakers found – just ignore!
Empty top speakers list for: quote10.toShorterString() [no candidate top speakers found – just ignore!
Empty top speakers list for: quote11.toShorterString() [no candidate top speakers found – just ignore!
Empty top speakers list for: quote12.toShorterString() [no candidate top speakers found – just ignore!
Empty top speakers list for: quote13.toShorterString() [no candidate top speakers found – just ignore!
Empty top speakers list for: quote14.toShorterString() [no candidate top speakers found – just ignore!
creating graph for products
creating graph for orders
creating graph for categories
creating graph for reviews
creating graph for transactions
creating graph for posts
creating graph for comments
creating graph for likes
creating graph for messages
creating graph for notifications
creating graph for friends
creating graph for settings
creating graph for preferences
Serializing relation extraction model to model2 ...
Serializing relation extraction model to model3 ...
Serializing relation extraction model to model4 ...
Serializing relation extraction model to model5 ...
Serializing relation extraction model to model6 ...
Serializing relation extraction model to model7 ...
Serializing relation extraction model to model8 ...
Serializing relation extraction model to model9 ...
Serializing relation extraction model to model10 ...
Serializing relation extraction model to model11 ...
Serializing relation extraction model to model12 ...
Serializing relation extraction model to model13 ...
Serializing relation extraction model to model14 ...
Gold Mention: social media post
Gold Mention: blog post
Gold Mention: forum comment
Gold Mention: customer review
Gold Mention: email
Gold Mention: press release
Gold Mention: research paper
Gold Mention: survey response
Gold Mention: interview transcript
Gold Mention: webinar recording
Gold Mention: podcast episode
Gold Mention: video testimonial
Gold Mention: product description
annotating partition 2
annotating partition 3
annotating partition 4
annotating partition 5
annotating partition 6
annotating partition 7
annotating partition 8
annotating partition 9
annotating partition 10
annotating partition 11
annotating partition 12
annotating partition 13
annotating partition 14
Loading affix dictionary from affixes.dat [done].
Loading affix dictionary from prefix.bin [done].
Loading affix dictionary from suffix.bin [done].
Loading affix dictionary from en_US.dic [done].
Loading affix dictionary from custom.dat [done].
Loading affix dictionary from thesaurus.txt [done].
Loading affix dictionary from abbreviations.dat [done].
Loading affix dictionary from spellcheck.dic [done].
Loading affix dictionary from grammar.xml [done].
Loading affix dictionary from exceptions.dat [done].
Loading affix dictionary from synonyms.txt [done].
Loading affix dictionary from acronyms.dat [done].
Loading affix dictionary from definitions.xml [done].
Loading affix dictionary from hyphenations.dat [done].
found match at: node2
found match at: node3
found match at: node4
found match at: node5
found match at: node6
found match at: node7
found match at: node8
found match at: node9
found match at: node10
found match at: node11
found match at: node12
found match at: node13
found match at: node14
checking for tokensregex pattern: {sentenceBoundaryMultiTokenPattern2}
checking for tokensregex pattern: {sentenceBoundaryMultiTokenPattern3}
checking for tokensregex pattern: {sentenceBoundaryMultiTokenPattern4}
checking for tokensregex pattern: {sentenceBoundaryMultiTokenPattern5}
checking for tokensregex pattern: {sentenceBoundaryMultiTokenPattern6}
checking for tokensregex pattern: {sentenceBoundaryMultiTokenPattern7}
checking for tokensregex pattern: {sentenceBoundaryMultiTokenPattern8}
checking for tokensregex pattern: {sentenceBoundaryMultiTokenPattern9}
checking for tokensregex pattern: {sentenceBoundaryMultiTokenPattern10}
checking for tokensregex pattern: {sentenceBoundaryMultiTokenPattern11}
checking for tokensregex pattern: {sentenceBoundaryMultiTokenPattern12}
checking for tokensregex pattern: {sentenceBoundaryMultiTokenPattern13}
checking for tokensregex pattern: {sentenceBoundaryMultiTokenPattern14}
Features: registration
Features: search
Features: payment
Features: messaging
Features: profile
Features: settings
Features: notifications
Features: analytics
Features: reporting
Features: integration
Features: customization
Features: authentication
Features: file management
CONVERTED EVENT MENTION: Presentation
CONVERTED EVENT MENTION: Interview
CONVERTED EVENT MENTION: Workshop
CONVERTED EVENT MENTION: Seminar
CONVERTED EVENT MENTION: Training
CONVERTED EVENT MENTION: Conference
CONVERTED EVENT MENTION: Webinar
CONVERTED EVENT MENTION: Hackathon
CONVERTED EVENT MENTION: Symposium
CONVERTED EVENT MENTION: Keynote
CONVERTED EVENT MENTION: Exhibition
CONVERTED EVENT MENTION: Lecture
CONVERTED EVENT MENTION: Panel Discussion
CONVERTED RELATION MENTION: Collaboration
CONVERTED RELATION MENTION: Integration
CONVERTED RELATION MENTION: Cooperation
CONVERTED RELATION MENTION: Partnership
CONVERTED RELATION MENTION: Alliance
CONVERTED RELATION MENTION: Synergy
CONVERTED RELATION MENTION: Engagement
CONVERTED RELATION MENTION: Interaction
CONVERTED RELATION MENTION: Association
CONVERTED RELATION MENTION: Connection
CONVERTED RELATION MENTION: Bond
CONVERTED RELATION MENTION: Relationship
CONVERTED RELATION MENTION: Link
[<ERROR>] Processed 200 sentences {20 sentences / second}...
[<INFO>] Processed 300 sentences {30 sentences / second}...
[<DEBUG>] Processed 400 sentences {40 sentences / second}...
[<FATAL>] Processed 500 sentences {50 sentences / second}...
[<ALERT>] Processed 600 sentences {60 sentences / second}...
[<CRITICAL>] Processed 700 sentences {70 sentences / second}...
[<EMERGENCY>] Processed 800 sentences {80 sentences / second}...
[<NOTICE>] Processed 900 sentences {90 sentences / second}...
[<TRACE>] Processed 1000 sentences {100 sentences / second}...
[<IMPORTANT>] Processed 1100 sentences {110 sentences / second}...
[<WARNING>] Processed 1200 sentences {120 sentences / second}...
[<ERROR>] Processed 1300 sentences {130 sentences / second}...
[<INFO>] Processed 1400 sentences {140 sentences / second}...
Reading sentence: \"I love to eat pizza.\"
Reading sentence: \"Today is a beautiful day.\"
Reading sentence: \"The cat is sleeping.\"
Reading sentence: \"I enjoy reading books.\"
Reading sentence: \"The movie was amazing.\"
Reading sentence: \"I am going on vacation.\"
Reading sentence: \"The flowers are blooming.\"
Reading sentence: \"I like to go hiking.\"
Reading sentence: \"The music is so soothing.\"
Reading sentence: \"I need a cup of coffee.\"
Reading sentence: \"The rain is pouring down.\"
Reading sentence: \"I am learning a new language.\"
Reading sentence: \"The traffic is very busy.\"
Created PCFG parser arrays of size 100
Created PCFG parser arrays of size 500
Created PCFG parser arrays of size 1000
Created PCFG parser arrays of size 2000
Created PCFG parser arrays of size 5000
Created PCFG parser arrays of size 10000
Created PCFG parser arrays of size 20000
Created PCFG parser arrays of size 50000
Created PCFG parser arrays of size 100000
Created PCFG parser arrays of size 200000
Created PCFG parser arrays of size 500000
Created PCFG parser arrays of size 1000000
Created PCFG parser arrays of size 2000000
At end of text, prevWord is |oranges|, its after set to |.|
At end of text, prevWord is |bananas|, its after set to |for|
At end of text, prevWord is |grapes|, its after set to |in|
At end of text, prevWord is |pears|, its after set to |the|
At end of text, prevWord is |berries|, its after set to |,|
At end of text, prevWord is |pineapples|, its after set to |on|
At end of text, prevWord is |kiwis|, its after set to |with|
At end of text, prevWord is |mangoes|, its after set to |whereas|
At end of text, prevWord is |watermelons|, its after set to |but|
At end of text, prevWord is |cherries|, its after set to |also|
At end of text, prevWord is |peaches|, its after set to |while|
At end of text, prevWord is |plums|, its after set to |because|
At end of text, prevWord is |lemons|, its after set to |when|
At end of text, prevWord is |apricots|, its after set to |with|
Parent: Node1
Parent: Node2
Parent: Node3
Parent: Node4
Parent: Node5
Parent: Node6
Parent: Node7
Parent: Node8
Parent: Node9
Parent: Node10
Parent: Node11
Parent: Node12
Parent: Node13
False Negative:negative
False Negative:true
False Negative:false
False Negative:zero
False Negative:one
False Negative:high
False Negative:low
False Negative:success
False Negative:failure
False Negative:valid
False Negative:invalid
False Negative:complete
False Negative:incomplete
False Negative:empty
Bound: 20
Bound: 30
Bound: 40
Bound: 50
Bound: 60
Bound: 70
Bound: 80
Bound: 90
Bound: 100
Bound: 110
Bound: 120
Bound: 130
Bound: 140
Used {ALEX} to recognize token2
Used {ALEX} to recognize token3
Used {ALEX} to recognize token4
Used {ALEX} to recognize token5
Used {ALEX} to recognize token6
Used {ALEX} to recognize token7
Used {ALEX} to recognize token8
Used {ALEX} to recognize token9
Used {ALEX} to recognize token10
Used {ALEX} to recognize token11
Used {ALEX} to recognize token12
Used {ALEX} to recognize token13
Used {ALEX} to recognize token14
Used {LDOTS3} to recognize word as normal
Used {LDOTS3} to recognize character as standard
Used {LDOTS3} to recognize code as normal
Used {LDOTS3} to recognize string as standard
Used {LDOTS3} to recognize symbol as normal
Used {LDOTS3} to recognize segment as standard
Used {LDOTS3} to recognize value as normal
Used {LDOTS3} to recognize number as standard
Used {LDOTS3} to recognize data as normal
Used {LDOTS3} to recognize element as standard
Used {LDOTS3} to recognize form as normal
Used {LDOTS3} to recognize block as standard
Used {LDOTS3} to recognize input as normal
Used {HTHINGEXCEPTIONSUFFIXED} to recognize word as normalized
Used {HTHINGEXCEPTIONSUFFIXED} to recognize input as standardized
Used {HTHINGEXCEPTIONSUFFIXED} to recognize character as processed
Used {HTHINGEXCEPTIONSUFFIXED} to recognize string as formatted
Used {HTHINGEXCEPTIONSUFFIXED} to recognize value as transformed
Used {HTHINGEXCEPTIONSUFFIXED} to recognize expression as evaluated
Used {HTHINGEXCEPTIONSUFFIXED} to recognize path as resolved
Used {HTHINGEXCEPTIONSUFFIXED} to recognize parameter as validated
Used {HTHINGEXCEPTIONSUFFIXED} to recognize request as handled
Used {HTHINGEXCEPTIONSUFFIXED} to recognize response as generated
Used {HTHINGEXCEPTIONSUFFIXED} to recognize result as calculated
Used {HTHINGEXCEPTIONSUFFIXED} to recognize condition as checked
Used {HTHINGEXCEPTIONSUFFIXED} to recognize exception as caught
Used SPAMP to recognize Lorem Ipsum as Text
Used SPAMP to recognize 12345 as Number
Used SPAMP to recognize example@example.com as Email
Used SPAMP to recognize www.example.com as URL
Used SPAMP to recognize (123) 456-7890 as Phone Number
Used SPAMP to recognize 10.0.0.1 as IP Address
Used SPAMP to recognize Monday as Day
Used SPAMP to recognize January as Month
Used SPAMP to recognize $100 as Currency
Used SPAMP to recognize 2022 as Year
Used SPAMP to recognize True as Boolean
Used SPAMP to recognize Apple as Fruit
Used SPAMP to recognize Eiffel Tower as Landmark
Used SPAMP to recognize Python as Programming Language
WARNING: Hello again from the class logger
ERROR: Hello again from the class logger
FATAL: Hello again from the class logger
DEBUG: Hello again from the class logger
TRACE: Hello again from the class logger
INFO: Hello again from the class logger
WARNING: Hello again from the class logger
ERROR: Hello again from the class logger
FATAL: Hello again from the class logger
DEBUG: Hello again from the class logger
TRACE: Hello again from the class logger
INFO: Hello again from the class logger
WARNING: Hello again from the class logger
content type: text/html
content type: application/json
content type: application/xml
content type: text/css
content type: image/jpeg
content type: audio/mpeg
content type: video/mp4
content type: application/pdf
content type: text/markdown
content type: application/javascript
content type: application/octet-stream
content type: text/csv
content type: application/vnd.ms-excel
Unknown Action: IndexOutOfBoundsException
Unknown Action: FileNotFoundException
Unknown Action: IllegalArgumentException
Unknown Action: ArrayIndexOutOfBoundsException
Unknown Action: UnsupportedOperationException
Unknown Action: ClassCastException
Unknown Action: NumberFormatException
Unknown Action: NoSuchMethodException
Unknown Action: StackOverflowError
Unknown Action: InterruptedException
Unknown Action: NullPointerException
Unknown Action: IllegalAccessException
Unknown Action: IllegalStateException
#observations: 218
#observations: 175
#observations: 309
#observations: 83
#observations: 256
#observations: 142
#observations: 197
#observations: 301
#observations: 134
#observations: 224
#observations: 186
#observations: 278
#observations: 156
hs contains b? false
hs contains b? true
hs contains b? false
hs contains b? true
hs contains b? false
hs contains b? true
hs contains b? false
hs contains b? true
hs contains b? false
hs contains b? true
hs contains b? false
hs contains b? true
hs contains b? false
Generated with pb_go_hTWds: 2,345,678 pb_aTW_hTWd: 4.32 p_aTW_aT: 654.32 pb_aT_hTWd: 0.987
Generated with pb_go_hTWds: 3,456,789 pb_aTW_hTWd: 1.23 p_aTW_aT: 321.23 pb_aT_hTWd: 0.456
Generated with pb_go_hTWds: 4,567,890 pb_aTW_hTWd: 7.65 p_aTW_aT: 678.90 pb_aT_hTWd: 0.789
Generated with pb_go_hTWds: 5,678,901 pb_aTW_hTWd: 3.21 p_aTW_aT: 8765.43 pb_aT_hTWd: 0.234
Generated with pb_go_hTWds: 6,789,012 pb_aTW_hTWd: 9.87 p_aTW_aT: 543.21 pb_aT_hTWd: 0.567
Generated with pb_go_hTWds: 7,890,123 pb_aTW_hTWd: 6.54 p_aTW_aT: 901.23 pb_aT_hTWd: 0.890
Generated with pb_go_hTWds: 8,901,234 pb_aTW_hTWd: 2.10 p_aTW_aT: 234.56 pb_aT_hTWd: 0.012
Generated with pb_go_hTWds: 9,012,345 pb_aTW_hTWd: 5.43 p_aTW_aT: 567.89 pb_aT_hTWd: 0.345
Generated with pb_go_hTWds: 1,123,456 pb_aTW_hTWd: 9.87 p_aTW_aT: 890.12 pb_aT_hTWd: 0.678
Generated with pb_go_hTWds: 2,234,567 pb_aTW_hTWd: 6.54 p_aTW_aT: 123.45 pb_aT_hTWd: 0.901
Generated with pb_go_hTWds: 3,345,678 pb_aTW_hTWd: 2.10 p_aTW_aT: 456.78 pb_aT_hTWd: 0.234
Generated with pb_go_hTWds: 4,456,789 pb_aTW_hTWd: 5.43 p_aTW_aT: 789.01 pb_aT_hTWd: 0.567
Generated with pb_go_hTWds: 5,567,890 pb_aTW_hTWd: 9.87 p_aTW_aT: 012.34 pb_aT_hTWd: 0.890
Generated with pb_go_hTWds: 6,678,901 pb_aTW_hTWd: 6.54 p_aTW_aT: 345.67 pb_aT_hTWd: 0.123
Used HTHING to recognize image.jpg as normalized
Used HTHING to recognize audio.mp3 as token
Used HTHING to recognize video.mp4 as normalized
Used HTHING to recognize data.csv as token
Used HTHING to recognize document.docx as normalized
Used HTHING to recognize code.py as token
Used HTHING to recognize result.txt as normalized
Used HTHING to recognize input.pdf as token
Used HTHING to recognize output.xlsx as normalized
Used HTHING to recognize file1.txt as token
Used HTHING to recognize file2.jpg as normalized
Used HTHING to recognize file3.png as token
Used HTHING to recognize file4.docx as normalized
No test treebank path specified. Using train path: /different/train/path
No test treebank path specified. Using train path: /another/train/path
No test treebank path specified. Using train path: /some/train/folder
No test treebank path specified. Using train path: /new/train/directory
No test treebank path specified. Using train path: /different/dataset/folder
No test treebank path specified. Using train path: /custom/train/path
No test treebank path specified. Using train path: /data/train
No test treebank path specified. Using train path: /samples/train
No test treebank path specified. Using train path: /trn/train
No test treebank path specified. Using train path: /set/train/folder
No test treebank path specified. Using train path: /resource/train
No test treebank path specified. Using train path: /data/train
No test treebank path specified. Using train path: /source/train
Error reading cache file cache2.  Making a new cache and NOT backing to file.
Error reading cache file cache3.  Making a new cache and NOT backing to file.
Error reading cache file cache4.  Making a new cache and NOT backing to file.
Error reading cache file cache5.  Making a new cache and NOT backing to file.
Error reading cache file cache6.  Making a new cache and NOT backing to file.
Error reading cache file cache7.  Making a new cache and NOT backing to file.
Error reading cache file cache8.  Making a new cache and NOT backing to file.
Error reading cache file cache9.  Making a new cache and NOT backing to file.
Error reading cache file cache10.  Making a new cache and NOT backing to file.
Error reading cache file cache11.  Making a new cache and NOT backing to file.
Error reading cache file cache12.  Making a new cache and NOT backing to file.
Error reading cache file cache13.  Making a new cache and NOT backing to file.
Error reading cache file cache14.  Making a new cache and NOT backing to file.
Cache file cache2.jpg has not been created yet. Making new one.
Cache file cache3.png has not been created yet. Making new one.
Cache file cache4.doc has not been created yet. Making new one.
Cache file cache5.mp3 has not been created yet. Making new one.
Cache file cache6.xlsx has not been created yet. Making new one.
Cache file cache7.html has not been created yet. Making new one.
Cache file cache8.zip has not been created yet. Making new one.
Cache file cache9.csv has not been created yet. Making new one.
Cache file cache10.json has not been created yet. Making new one.
Cache file cache11.log has not been created yet. Making new one.
Cache file cache12.xml has not been created yet. Making new one.
Cache file cache13.dat has not been created yet. Making new one.
Cache file cache14.sql has not been created yet. Making new one.
Writing segmenter in serialized format to file data.bin
Writing segmenter in serialized format to file weights.bin
Writing segmenter in serialized format to file output.bin
Writing segmenter in serialized format to file results.bin
Writing segmenter in serialized format to file temp.bin
Writing segmenter in serialized format to file cache.bin
Writing segmenter in serialized format to file config.bin
Writing segmenter in serialized format to file settings.bin
Writing segmenter in serialized format to file index.bin
Writing segmenter in serialized format to file params.bin
Writing segmenter in serialized format to file log.bin
Writing segmenter in serialized format to file metadata.bin
Writing segmenter in serialized format to file buffer.bin
Writing segmenter in serialized format to file archive.bin
exp took: 142 ms
exp took: 92 ms
exp took: 278 ms
exp took: 205 ms
exp took: 67 ms
exp took: 312 ms
exp took: 184 ms
exp took: 99 ms
exp took: 260 ms
exp took: 122 ms
exp took: 243 ms
exp took: 176 ms
exp took: 81 ms
grad took: 187 ms
grad took: 312 ms
grad took: 420 ms
grad took: 160 ms
grad took: 275 ms
grad took: 202 ms
grad took: 390 ms
grad took: 150 ms
grad took: 285 ms
grad took: 330 ms
grad took: 420 ms
grad took: 195 ms
grad took: 378 ms
Writing lexicon in serialized format to file lexicon2.dat
Writing lexicon in serialized format to file lexicon3.dat
Writing lexicon in serialized format to file lexicon4.dat
Writing lexicon in serialized format to file lexicon5.dat
Writing lexicon in serialized format to file lexicon6.dat
Writing lexicon in serialized format to file lexicon7.dat
Writing lexicon in serialized format to file lexicon8.dat
Writing lexicon in serialized format to file lexicon9.dat
Writing lexicon in serialized format to file lexicon10.dat
Writing lexicon in serialized format to file lexicon11.dat
Writing lexicon in serialized format to file lexicon12.dat
Writing lexicon in serialized format to file lexicon13.dat
Writing lexicon in serialized format to file lexicon14.dat
Writing parser in serialized format to file data.dat
Writing parser in serialized format to file output.json
Writing parser in serialized format to file config.ini
Writing parser in serialized format to file log.txt
Writing parser in serialized format to file result.xml
Writing parser in serialized format to file backup.bak
Writing parser in serialized format to file settings.cfg
Writing parser in serialized format to file archive.zip
Writing parser in serialized format to file debug.log
Writing parser in serialized format to file temp.tmp
Writing parser in serialized format to file backup.zip
Writing parser in serialized format to file report.docx
Writing parser in serialized format to file data.csv
beta took: 456 ms
beta took: 789 ms
beta took: 321 ms
beta took: 654 ms
beta took: 987 ms
beta took: 234 ms
beta took: 567 ms
beta took: 890 ms
beta took: 432 ms
beta took: 765 ms
beta took: 098 ms
beta took: 345 ms
beta took: 678 ms
alpha took: 1000 ms
alpha took: 250 ms
alpha took: 750 ms
alpha took: 1500 ms
alpha took: 300 ms
alpha took: 600 ms
alpha took: 1200 ms
alpha took: 400 ms
alpha took: 800 ms
alpha took: 1600 ms
alpha took: 200 ms
alpha took: 450 ms
alpha took: 900 ms
cond prob took: 187 ms
cond prob took: 312 ms
cond prob took: 491 ms
cond prob took: 589 ms
cond prob took: 792 ms
cond prob took: 678 ms
cond prob took: 354 ms
cond prob took: 429 ms
cond prob took: 215 ms
cond prob took: 657 ms
cond prob took: 786 ms
cond prob took: 521 ms
cond prob took: 236 ms
cond prob took: 376 ms
prevGivenCurr[1]:
prevGivenCurr[2]:
prevGivenCurr[3]:
prevGivenCurr[4]:
prevGivenCurr[5]:
prevGivenCurr[6]:
prevGivenCurr[7]:
prevGivenCurr[8]:
prevGivenCurr[9]:
prevGivenCurr[10]:
prevGivenCurr[11]:
prevGivenCurr[12]:
prevGivenCurr[13]:
Morphology.stem() had error on word orange
Morphology.stem() had error on word banana
Morphology.stem() had error on word cherry
Morphology.stem() had error on word grape
Morphology.stem() had error on word lemon
Morphology.stem() had error on word strawberry
Morphology.stem() had error on word pineapple
Morphology.stem() had error on word watermelon
Morphology.stem() had error on word mango
Morphology.stem() had error on word peach
Morphology.stem() had error on word pear
Morphology.stem() had error on word plum
Morphology.stem() had error on word kiwi
inputLayerWeights.length= 20
inputLayerWeights.length= 30
inputLayerWeights.length= 40
inputLayerWeights.length= 50
inputLayerWeights.length= 60
inputLayerWeights.length= 70
inputLayerWeights.length= 80
inputLayerWeights.length= 90
inputLayerWeights.length= 100
inputLayerWeights.length= 110
inputLayerWeights.length= 120
inputLayerWeights.length= 130
inputLayerWeights.length= 140
Could not parse URL: ftp://ftp.example.com
Could not parse URL: file:///path/to/file
Could not parse URL: http://localhost
Could not parse URL: https://www.example.org
Could not parse URL: ftp://example.com
Could not parse URL: file:///path/to/another-file
Could not parse URL: http://192.168.0.1
Could not parse URL: https://api.example.com
Could not parse URL: ftp://192.168.1.1
Could not parse URL: file:///var/www/html
Could not parse URL: http://example.com
Could not parse URL: https://test.example.org
Could not parse URL: ftp://ftp.example.org
Serializing class index to data.ser... FAILED.IOException
Serializing class index to archive.dat... FAILED.FileNotFoundException
Serializing class index to cache.txt... FAILED.ParseError
Serializing class index to backup.bak... FAILED.SecurityException
Serializing class index to log.log... FAILED.NetworkError
Serializing class index to temp.tmp... FAILED.DatabaseError
Serializing class index to config.cfg... FAILED.NullPointerException
Serializing class index to file.txt... FAILED.OutofMemoryError
Serializing class index to image.jpg... FAILED.FormatError
Serializing class index to script.js... FAILED.IndexOutOfBoundsError
Serializing class index to style.css... FAILED.StackOverflowError
Serializing class index to settings.ini... FAILED.ClassNotFoundError
Serializing class index to app.apk... FAILED.MethodNotFoundError
Serializing class index to document.doc... FAILED.TypeMismatchError
featureIndex.size()=	200
featureIndex.size()=	300
featureIndex.size()=	400
featureIndex.size()=	500
featureIndex.size()=	600
featureIndex.size()=	700
featureIndex.size()=	800
featureIndex.size()=	900
featureIndex.size()=	1000
featureIndex.size()=	1100
featureIndex.size()=	1200
featureIndex.size()=	1300
featureIndex.size()=	1400
Warning!! Stanford Parser does not work with tags containing '@' like: dog
Warning!! Stanford Parser does not work with tags containing '@' like: mouse
Warning!! Stanford Parser does not work with tags containing '@' like: bird
Warning!! Stanford Parser does not work with tags containing '@' like: elephant
Warning!! Stanford Parser does not work with tags containing '@' like: lion
Warning!! Stanford Parser does not work with tags containing '@' like: giraffe
Warning!! Stanford Parser does not work with tags containing '@' like: monkey
Warning!! Stanford Parser does not work with tags containing '@' like: tiger
Warning!! Stanford Parser does not work with tags containing '@' like: horse
Warning!! Stanford Parser does not work with tags containing '@' like: cow
Warning!! Stanford Parser does not work with tags containing '@' like: sheep
Warning!! Stanford Parser does not work with tags containing '@' like: rabbit
Warning!! Stanford Parser does not work with tags containing '@' like: snake
Warning!! Stanford Parser does not work with categories containing '@' like: fruit
Warning!! Stanford Parser does not work with categories containing '@' like: vehicle
Warning!! Stanford Parser does not work with categories containing '@' like: food
Warning!! Stanford Parser does not work with categories containing '@' like: person
Warning!! Stanford Parser does not work with categories containing '@' like: location
Warning!! Stanford Parser does not work with categories containing '@' like: device
Warning!! Stanford Parser does not work with categories containing '@' like: book
Warning!! Stanford Parser does not work with categories containing '@' like: movie
Warning!! Stanford Parser does not work with categories containing '@' like: music
Warning!! Stanford Parser does not work with categories containing '@' like: sport
Warning!! Stanford Parser does not work with categories containing '@' like: job
Warning!! Stanford Parser does not work with categories containing '@' like: game
Warning!! Stanford Parser does not work with categories containing '@' like: hobby
Warning! 15 items are tags and categories: nature
Warning! 5 items are tags and categories: food
Warning! 8 items are tags and categories: sports
Warning! 12 items are tags and categories: music
Warning! 9 items are tags and categories: travel
Warning! 7 items are tags and categories: fashion
Warning! 11 items are tags and categories: art
Warning! 6 items are tags and categories: technology
Warning! 13 items are tags and categories: science
Warning! 4 items are tags and categories: movies
Warning! 14 items are tags and categories: books
Warning! 16 items are tags and categories: history
Warning! 3 items are tags and categories: photography
Caution! 3 word types are known empty elements: ['NOUN', 'VERB', 'ADJ']
Caution! 5 word types are known empty elements: ['NOUN', 'VERB', 'ADJ', 'ADV', 'PREP']
Caution! 2 word types are known empty elements: ['ADJ', 'ADV']
Caution! 1 word types are known empty elements: ['PREP']
Caution! 4 word types are known empty elements: ['NOUN', 'VERB', 'ADJ', 'ADV']
Caution! 3 word types are known empty elements: ['NOUN', 'ADJ', 'PREP']
Caution! 2 word types are known empty elements: ['ADJ', 'ADV']
Caution! 6 word types are known empty elements: ['NOUN', 'VERB', 'ADJ', 'ADV', 'PREP', 'CONJ']
Caution! 2 word types are known empty elements: ['NOUN', 'VERB']
Caution! 4 word types are known empty elements: ['NOUN', 'ADJ', 'ADV', 'PREP']
Caution! 1 word types are known empty elements: ['ADJ']
Caution! 5 word types are known empty elements: ['NOUN', 'VERB', 'ADJ', 'ADV', 'CONJ']
Caution! 3 word types are known empty elements: ['NOUN', 'ADJ', 'PREP']
Encountered a non-leaf/phrasal/pre-terminal node s2
Encountered a non-leaf/phrasal/pre-terminal node s3
Encountered a non-leaf/phrasal/pre-terminal node s4
Encountered a non-leaf/phrasal/pre-terminal node s5
Encountered a non-leaf/phrasal/pre-terminal node s6
Encountered a non-leaf/phrasal/pre-terminal node s7
Encountered a non-leaf/phrasal/pre-terminal node s8
Encountered a non-leaf/phrasal/pre-terminal node s9
Encountered a non-leaf/phrasal/pre-terminal node s10
Encountered a non-leaf/phrasal/pre-terminal node s11
Encountered a non-leaf/phrasal/pre-terminal node s12
Encountered a non-leaf/phrasal/pre-terminal node s13
Encountered a non-leaf/phrasal/pre-terminal node s14
Sentences range from 5 to 15 words, with an average length of 6.0 words.
Sentences range from 2 to 9 words, with an average length of 3.5 words.
Sentences range from 7 to 17 words, with an average length of 8.0 words.
Sentences range from 4 to 13 words, with an average length of 5.5 words.
Sentences range from 5 to 15 words, with an average length of 6.0 words.
Sentences range from 3 to 12 words, with an average length of 4.5 words.
Sentences range from 6 to 16 words, with an average length of 7.0 words.
Sentences range from 2 to 9 words, with an average length of 3.5 words.
Sentences range from 7 to 17 words, with an average length of 8.0 words.
Sentences range from 3 to 12 words, with an average length of 4.5 words.
Sentences range from 5 to 15 words, with an average length of 6.0 words.
Sentences range from 4 to 13 words, with an average length of 5.5 words.
Sentences range from 6 to 16 words, with an average length of 7.0 words.
Warning! 6 preterminal nodes with multiple children.
Warning! 1 preterminal nodes with multiple children.
Warning! 9 preterminal nodes with multiple children.
Warning! 4 preterminal nodes with multiple children.
Warning! 7 preterminal nodes with multiple children.
Warning! 2 preterminal nodes with multiple children.
Warning! 8 preterminal nodes with multiple children.
Warning! 5 preterminal nodes with multiple children.
Warning! 10 preterminal nodes with multiple children.
Warning! 13 preterminal nodes with multiple children.
Warning! 11 preterminal nodes with multiple children.
Warning! 14 preterminal nodes with multiple children.
Warning! 12 preterminal nodes with multiple children.
Warning! 5 tree nodes with null or empty string labels, e.g.:
Warning! 7 tree nodes with null or empty string labels, e.g.:
Warning! 12 tree nodes with null or empty string labels, e.g.:
Warning! 3 tree nodes with null or empty string labels, e.g.:
Warning! 9 tree nodes with null or empty string labels, e.g.:
Warning! 8 tree nodes with null or empty string labels, e.g.:
Warning! 6 tree nodes with null or empty string labels, e.g.:
Warning! 2 tree nodes with null or empty string labels, e.g.:
Warning! 4 tree nodes with null or empty string labels, e.g.:
Warning! 11 tree nodes with null or empty string labels, e.g.:
Warning! 15 tree nodes with null or empty string labels, e.g.:
Warning! 1 tree nodes with null or empty string labels, e.g.:
Warning! 14 tree nodes with null or empty string labels, e.g.:
Warning! Non-phrasal trees: 7 bare leaves; 12 root rewrites as leaf; and 5 root rewrites as tagged word
Warning! Non-phrasal trees: 20 bare leaves; 32 root rewrites as leaf; and 15 root rewrites as tagged word
Warning! Non-phrasal trees: 9 bare leaves; 18 root rewrites as leaf; and 7 root rewrites as tagged word
Warning! Non-phrasal trees: 12 bare leaves; 20 root rewrites as leaf; and 9 root rewrites as tagged word
Warning! Non-phrasal trees: 5 bare leaves; 10 root rewrites as leaf; and 4 root rewrites as tagged word
Warning! Non-phrasal trees: 17 bare leaves; 28 root rewrites as leaf; and 13 root rewrites as tagged word
Warning! Non-phrasal trees: 8 bare leaves; 15 root rewrites as leaf; and 6 root rewrites as tagged word
Warning! Non-phrasal trees: 11 bare leaves; 22 root rewrites as leaf; and 8 root rewrites as tagged word
Warning! Non-phrasal trees: 14 bare leaves; 25 root rewrites as leaf; and 11 root rewrites as tagged word
Warning! Non-phrasal trees: 6 bare leaves; 11 root rewrites as leaf; and 5 root rewrites as tagged word
Warning! Non-phrasal trees: 13 bare leaves; 23 root rewrites as leaf; and 9 root rewrites as tagged word
Warning! Non-phrasal trees: 16 bare leaves; 27 root rewrites as leaf; and 12 root rewrites as tagged word
Warning! Non-phrasal trees: 10 bare leaves; 19 root rewrites as leaf; and 7 root rewrites as tagged word
Warning! 3 trees without unary initial rewrite.
Warning! 8 trees without unary initial rewrite.
Warning! 5 trees without unary initial rewrite.
Warning! 1 trees without unary initial rewrite.
Warning! 10 trees without unary initial rewrite.
Warning! 7 trees without unary initial rewrite.
Warning! 4 trees without unary initial rewrite.
Warning! 2 trees without unary initial rewrite.
Warning! 6 trees without unary initial rewrite.
Warning! 9 trees without unary initial rewrite.
Warning! 12 trees without unary initial rewrite.
Warning! 11 trees without unary initial rewrite.
Warning! 15 trees without unary initial rewrite.
Warning! 13 trees without unary initial rewrite.
Warning! 3 different roots in treebank: [rootA, rootB, rootC]
Warning! 7 different roots in treebank: [rootX, rootY, rootZ, rootW, rootV, rootU, rootT]
Warning! 2 different roots in treebank: [rootAlpha, rootBeta]
Warning! 4 different roots in treebank: [rootOne, rootTwo, rootThree, rootFour]
Warning! 6 different roots in treebank: [rootA, rootB, rootC, rootD, rootE, rootF]
Warning! 8 different roots in treebank: [root001, root002, root003, root004, root005, root006, root007, root008]
Warning! 9 different roots in treebank: [rootI, rootII, rootIII, rootIV, rootV, rootVI, rootVII, rootVIII, rootIX]
Warning! 1 different roots in treebank: [rootOne]
Warning! 10 different roots in treebank: [rootStart, rootEnd, rootMiddle, rootLeft, rootRight, rootTop, rootBottom, rootFront, rootBack, rootCenter]
Warning! 12 different roots in treebank: [rootA, rootB, rootC, rootD, rootE, rootF, rootG, rootH, rootI, rootJ, rootK, rootL]
Warning! 11 different roots in treebank: [root111, root222, root333, root444, root555, root666, root777, root888, root999, rootAAA, rootBBB]
Warning! 15 different roots in treebank: [root001, root002, root003, root004, root005, root006, root007, root008, root009, root010, root011, root012, root013, root014, root015]
Warning! 13 different roots in treebank: [rootA, rootB, rootC, rootD, rootE, rootF, rootG, rootH, rootI, rootJ, rootK, rootL, rootM]
Time to convert docs to data/labels: 8.21 seconds
Time to convert docs to data/labels: 2.17 seconds
Time to convert docs to data/labels: 3.98 seconds
Time to convert docs to data/labels: 6.95 seconds
Time to convert docs to data/labels: 1.64 seconds
Time to convert docs to data/labels: 4.76 seconds
Time to convert docs to data/labels: 9.02 seconds
Time to convert docs to data/labels: 2.40 seconds
Time to convert docs to data/labels: 7.13 seconds
Time to convert docs to data/labels: 1.92 seconds
Time to convert docs to data/labels: 5.85 seconds
Time to convert docs to data/labels: 3.71 seconds
Time to convert docs to data/labels: 8.36 seconds
Time to convert docs to feature indices: 0.754 seconds
Time to convert docs to feature indices: 2.065 seconds
Time to convert docs to feature indices: 1.892 seconds
Time to convert docs to feature indices: 0.943 seconds
Time to convert docs to feature indices: 1.512 seconds
Time to convert docs to feature indices: 0.976 seconds
Time to convert docs to feature indices: 1.787 seconds
Time to convert docs to feature indices: 2.314 seconds
Time to convert docs to feature indices: 0.638 seconds
Time to convert docs to feature indices: 1.109 seconds
Time to convert docs to feature indices: 2.504 seconds
Time to convert docs to feature indices: 1.674 seconds
Time to convert docs to feature indices: 0.807 seconds
Time to convert docs to feature indices: 1.453 seconds
Unable to instantiate dataset type json
Unable to instantiate dataset type parquet
Unable to instantiate dataset type avro
Unable to instantiate dataset type tfrecord
Unable to instantiate dataset type hdf5
Unable to instantiate dataset type bigquery
Unable to instantiate dataset type mysql
Unable to instantiate dataset type sqlite
Unable to instantiate dataset type postgresql
Unable to instantiate dataset type mongodb
Unable to instantiate dataset type redis
Unable to instantiate dataset type kafka
Unable to instantiate dataset type rabbitmq
phrasal types:	12
phrasal types:	3
phrasal types:	9
phrasal types:	5
phrasal types:	10
phrasal types:	6
phrasal types:	15
phrasal types:	8
phrasal types:	2
phrasal types:	13
phrasal types:	11
phrasal types:	4
phrasal types:	1
tokens:    187
tokens:    312
tokens:    430
tokens:    582
tokens:    690
tokens:    831
tokens:    976
tokens:    1040
tokens:    1293
tokens:    1420
tokens:    1668
tokens:    1843
tokens:    1967
words:         15
words:         5
words:         20
words:         8
words:         12
words:         3
words:         6
words:         18
words:         7
words:         13
words:         9
words:         11
words:         17
trees:		20
trees:		30
trees:		40
trees:		50
trees:		60
trees:		70
trees:		80
trees:		90
trees:		100
trees:		110
trees:		120
trees:		130
trees:		140
#richtags:	5
#richtags:	7
#richtags:	8
#richtags:	3
#richtags:	2
#richtags:	6
#richtags:	4
#richtags:	9
#richtags:	1
#richtags:	5
#richtags:	7
#richtags:	3
#richtags:	2
#lemmaTagPairs:	5
#lemmaTagPairs:	15
#lemmaTagPairs:	3
#lemmaTagPairs:	7
#lemmaTagPairs:	12
#lemmaTagPairs:	8
#lemmaTagPairs:	6
#lemmaTagPairs:	9
#lemmaTagPairs:	2
#lemmaTagPairs:	11
#lemmaTagPairs:	4
#lemmaTagPairs:	14
#lemmaTagPairs:	13
#lemmas: 15
#lemmas: 7
#lemmas: 18
#lemmas: 12
#lemmas: 5
#lemmas: 21
#lemmas: 9
#lemmas: 14
#lemmas: 6
#lemmas: 11
#lemmas: 8
#lemmas: 17
#lemmas: 13
Before feature count thresholding, numFeatures = 200
Before feature count thresholding, numFeatures = 300
Before feature count thresholding, numFeatures = 400
Before feature count thresholding, numFeatures = 500
Before feature count thresholding, numFeatures = 600
Before feature count thresholding, numFeatures = 700
Before feature count thresholding, numFeatures = 800
Before feature count thresholding, numFeatures = 900
Before feature count thresholding, numFeatures = 1000
Before feature count thresholding, numFeatures = 1100
Before feature count thresholding, numFeatures = 1200
Before feature count thresholding, numFeatures = 1300
Before feature count thresholding, numFeatures = 1400
#wordTagPairs:	15
#wordTagPairs:	7
#wordTagPairs:	20
#wordTagPairs:	12
#wordTagPairs:	5
#wordTagPairs:	9
#wordTagPairs:	18
#wordTagPairs:	6
#wordTagPairs:	14
#wordTagPairs:	11
#wordTagPairs:	8
#wordTagPairs:	13
#wordTagPairs:	17
#words:	15
#words:	5
#words:	12
#words:	8
#words:	20
#words:	7
#words:	16
#words:	9
#words:	18
#words:	6
#words:	11
#words:	13
#words:	4
Time to combine CRFClassifier: 1.87 seconds
Time to combine CRFClassifier: 3.12 seconds
Time to combine CRFClassifier: 0.95 seconds
Time to combine CRFClassifier: 2.01 seconds
Time to combine CRFClassifier: 1.45 seconds
Time to combine CRFClassifier: 3.78 seconds
Time to combine CRFClassifier: 4.23 seconds
Time to combine CRFClassifier: 2.17 seconds
Time to combine CRFClassifier: 1.63 seconds
Time to combine CRFClassifier: 3.94 seconds
Time to combine CRFClassifier: 2.89 seconds
Time to combine CRFClassifier: 1.29 seconds
Time to combine CRFClassifier: 4.56 seconds
Received bad output format in scenegraph 'format2'
Received bad output format in scenegraph 'format3'
Received bad output format in scenegraph 'format4'
Received bad output format in scenegraph 'format5'
Received bad output format in scenegraph 'format6'
Received bad output format in scenegraph 'format7'
Received bad output format in scenegraph 'format8'
Received bad output format in scenegraph 'format9'
Received bad output format in scenegraph 'format10'
Received bad output format in scenegraph 'format11'
Received bad output format in scenegraph 'format12'
Received bad output format in scenegraph 'format13'
Received bad output format in scenegraph 'format14'
Unknown word in position pos2: word2
Unknown word in position pos3: word3
Unknown word in position pos4: word4
Unknown word in position pos5: word5
Unknown word in position pos6: word6
Unknown word in position pos7: word7
Unknown word in position pos8: word8
Unknown word in position pos9: word9
Unknown word in position pos10: word10
Unknown word in position pos11: word11
Unknown word in position pos12: word12
Unknown word in position pos13: word13
Unknown word in position pos14: word14
Processed 20 documents
Processed 30 documents
Processed 40 documents
Processed 50 documents
Processed 60 documents
Processed 70 documents
Processed 80 documents
Processed 90 documents
Processed 100 documents
Processed 110 documents
Processed 120 documents
Processed 130 documents
Processed 140 documents
TfTbApply transforming table2
TfTbApply transforming table3
TfTbApply transforming table4
TfTbApply transforming table5
TfTbApply transforming table6
TfTbApply transforming table7
TfTbApply transforming table8
TfTbApply transforming table9
TfTbApply transforming table10
TfTbApply transforming table11
TfTbApply transforming table12
TfTbApply transforming table13
TfTbApply transforming table14
Total time for StanfordCoreNLP pipeline: 3.5 sec.
Total time for StanfordCoreNLP pipeline: 1.8 sec.
Total time for StanfordCoreNLP pipeline: 2.2 sec.
Total time for StanfordCoreNLP pipeline: 4.7 sec.
Total time for StanfordCoreNLP pipeline: 3.2 sec.
Total time for StanfordCoreNLP pipeline: 1.5 sec.
Total time for StanfordCoreNLP pipeline: 2.8 sec.
Total time for StanfordCoreNLP pipeline: 3.4 sec.
Total time for StanfordCoreNLP pipeline: 1.9 sec.
Total time for StanfordCoreNLP pipeline: 2.5 sec.
Total time for StanfordCoreNLP pipeline: 1.2 sec.
Total time for StanfordCoreNLP pipeline: 3.9 sec.
Total time for StanfordCoreNLP pipeline: 2.6 sec.
vpContainsParticiple examining kid2
vpContainsParticiple examining kid3
vpContainsParticiple examining kid4
vpContainsParticiple examining kid5
vpContainsParticiple examining kid6
vpContainsParticiple examining kid7
vpContainsParticiple examining kid8
vpContainsParticiple examining kid9
vpContainsParticiple examining kid10
vpContainsParticiple examining kid11
vpContainsParticiple examining kid12
vpContainsParticiple examining kid13
vpContainsParticiple examining kid14
Done filtering rules; 15 binary matrices, 7 unary matrices, 300 word vectors
Done filtering rules; 8 binary matrices, 3 unary matrices, 150 word vectors
Done filtering rules; 12 binary matrices, 6 unary matrices, 250 word vectors
Done filtering rules; 20 binary matrices, 8 unary matrices, 400 word vectors
Done filtering rules; 16 binary matrices, 9 unary matrices, 350 word vectors
Done filtering rules; 14 binary matrices, 4 unary matrices, 180 word vectors
Done filtering rules; 9 binary matrices, 2 unary matrices, 120 word vectors
Done filtering rules; 11 binary matrices, 7 unary matrices, 290 word vectors
Done filtering rules; 13 binary matrices, 6 unary matrices, 220 word vectors
Done filtering rules; 18 binary matrices, 9 unary matrices, 420 word vectors
Done filtering rules; 7 binary matrices, 4 unary matrices, 170 word vectors
Done filtering rules; 17 binary matrices, 8 unary matrices, 380 word vectors
Done filtering rules; 6 binary matrices, 2 unary matrices, 100 word vectors
Optimal so far is: batch size: 64
Optimal so far is: batch size: 128
Optimal so far is: batch size: 256
Optimal so far is: batch size: 512
Optimal so far is: batch size: 1024
Optimal so far is: batch size: 2048
Optimal so far is: batch size: 4096
Optimal so far is: batch size: 8192
Optimal so far is: batch size: 16384
Optimal so far is: batch size: 32768
Optimal so far is: batch size: 65536
Optimal so far is: batch size: 131072
Optimal so far is: batch size: 262144
EstimationAndCorrection.basicconnoisseur: Grand parent parse tag null
CommonMethods.favoritesponsor: Grand parent parse tag null
MathematicalModeling.debugsmash: Grand parent parse tag null
DataExtraction.reportschedule: Grand parent parse tag null
PerformanceOptimization.optimizeanimation: Grand parent parse tag null
ErrorHandling.handlerouting: Grand parent parse tag null
DataStructures.alggenetic: Grand parent parse tag null
FileManagement.externalcache: Grand parent parse tag null
Networking.communicationprotocol: Grand parent parse tag null
SecurityEncryption.encryptcredentials: Grand parent parse tag null
DatabaseManagement.queryoptimization: Grand parent parse tag null
UserInterface.designprinciples: Grand parent parse tag null
SystemArchitecture.scalabilitystrategy: Grand parent parse tag null
Optimal so far is: fixedgain: 0.45
Optimal so far is: fixedgain: 0.25
Optimal so far is: fixedgain: 0.55
Optimal so far is: fixedgain: 0.15
Optimal so far is: fixedgain: 0.65
Optimal so far is: fixedgain: 0.05
Optimal so far is: fixedgain: 0.75
Optimal so far is: fixedgain: 0.95
Optimal so far is: fixedgain: 0.85
Optimal so far is: fixedgain: 0.25
Optimal so far is: fixedgain: 0.15
Optimal so far is: fixedgain: 0.45
Optimal so far is: fixedgain: 0.65
AFTER feature count threshold of 100, dataset stats are
AFTER feature count threshold of 150, dataset stats are
AFTER feature count threshold of 200, dataset stats are
AFTER feature count threshold of 250, dataset stats are
AFTER feature count threshold of 300, dataset stats are
AFTER feature count threshold of 350, dataset stats are
AFTER feature count threshold of 400, dataset stats are
AFTER feature count threshold of 450, dataset stats are
AFTER feature count threshold of 500, dataset stats are
AFTER feature count threshold of 550, dataset stats are
AFTER feature count threshold of 600, dataset stats are
AFTER feature count threshold of 650, dataset stats are
AFTER feature count threshold of 700, dataset stats are
AFTER feature count threshold of 750, dataset stats are
all positive phrases of size 5 are wonderful
all positive phrases of size 8 are amazing
all positive phrases of size 3 are excellent
all positive phrases of size 6 are fantastic
all positive phrases of size 9 are incredible
all positive phrases of size 7 are awesome
all positive phrases of size 4 are superb
all positive phrases of size 2 are terrific
all positive phrases of size 1 are fantastic
all positive phrases of size 15 are marvelous
all positive phrases of size 12 are splendid
all positive phrases of size 11 are outstanding
all positive phrases of size 14 are exceptional
deleting: test.txt  file1.txt  file2.txt
deleting: image.jpg  photo1.png  photo2.png
deleting: data.csv  results.xls  report.doc
deleting: folder1  folder2  folder3
deleting: backup.tar.gz  archive.zip  backup.bak
deleting: index.html  style.css  script.js
deleting: user1  user2  user3
deleting: doc.pdf  manual.docx  readme.txt
deleting: log.txt  error.log  debug.log
deleting: config.yml  settings.ini  properties.xml
deleting: image1.jpg  image2.png  image3.jpeg
deleting: index.php  login.html  register.jsp
deleting: file1.txt  file2.txt  file3.txt
deleting: data.json  api.yaml  schema.sql
removing: temporary files
removing: unused dependencies
removing: duplicate entries
removing: obsolete code
removing: deprecated functions
removing: expired tokens
removing: invalid configurations
removing: broken links
removing: expired cache
removing: unused variables
removing: unused imports
removing: unused classes
removing: unused methods
8; DomainDimension
15; DomainDimension
5; DomainDimension
10; DomainDimension
7; DomainDimension
9; DomainDimension
14; DomainDimension
6; DomainDimension
11; DomainDimension
13; DomainDimension
4; DomainDimension
3; DomainDimension
1; DomainDimension
Parse bit with multiple *: 123
Parse bit with multiple *: xyz
Parse bit with multiple *: def
Parse bit with multiple *: 456
Parse bit with multiple *: mno
Parse bit with multiple *: ghi
Parse bit with multiple *: 789
Parse bit with multiple *: pqr
Parse bit with multiple *: lmn
Parse bit with multiple *: 321
Parse bit with multiple *: uvw
Parse bit with multiple *: jkl
Parse bit with multiple *: 987
debug-preprocessor Cannot find node in dependency for word banana
debug-preprocessor Cannot find node in dependency for word cherry
debug-preprocessor Cannot find node in dependency for word dog
debug-preprocessor Cannot find node in dependency for word elephant
debug-preprocessor Cannot find node in dependency for word fruit
debug-preprocessor Cannot find node in dependency for word grape
debug-preprocessor Cannot find node in dependency for word hat
debug-preprocessor Cannot find node in dependency for word ice cream
debug-preprocessor Cannot find node in dependency for word jacket
debug-preprocessor Cannot find node in dependency for word kangaroo
debug-preprocessor Cannot find node in dependency for word lemon
debug-preprocessor Cannot find node in dependency for word mango
debug-preprocessor Cannot find node in dependency for word nectarine
BB SPEAKER INFO MAP: {speaker1=David, speaker2=Emily, speaker3=Sarah}
BB SPEAKER INFO MAP: {speaker1=Michael, speaker2=Jessica, speaker3=Christopher}
BB SPEAKER INFO MAP: {speaker1=Andrew, speaker2=Olivia, speaker3=Daniel}
BB SPEAKER INFO MAP: {speaker1=Matthew, speaker2=Sophia, speaker3=Ethan}
BB SPEAKER INFO MAP: {speaker1=James, speaker2=Isabella, speaker3=Alexander}
BB SPEAKER INFO MAP: {speaker1=Benjamin, speaker2=Abigail, speaker3=William}
BB SPEAKER INFO MAP: {speaker1=Daniel, speaker2=Emma, speaker3=Joseph}
BB SPEAKER INFO MAP: {speaker1=Henry, speaker2=Charlotte, speaker3=Samuel}
BB SPEAKER INFO MAP: {speaker1=Samuel, speaker2=Oliver, speaker3=Elizabeth}
BB SPEAKER INFO MAP: {speaker1=Joshua, speaker2=Sophia, speaker3=David}
BB SPEAKER INFO MAP: {speaker1=Daniel, speaker2=Emily, speaker3=Joseph}
BB SPEAKER INFO MAP: {speaker1=Ethan, speaker2=Isabella, speaker3=Daniel}
BB SPEAKER INFO MAP: {speaker1=Benjamin, speaker2=Ava, speaker3=William}
num words = 1000
num words = 1500
num words = 2000
num words = 2500
num words = 3000
num words = 3500
num words = 4000
num words = 4500
num words = 5000
num words = 5500
num words = 6000
num words = 6500
num words = 7000
detected embedding size = 200
detected embedding size = 150
detected embedding size = 300
detected embedding size = 250
detected embedding size = 175
detected embedding size = 225
detected embedding size = 120
detected embedding size = 180
detected embedding size = 280
detected embedding size = 160
detected embedding size = 230
detected embedding size = 140
detected embedding size = 190
changed to: car
changed to: tree
changed to: book
changed to: dog
changed to: cat
changed to: chair
changed to: table
changed to: lamp
changed to: laptop
changed to: flower
changed to: phone
changed to: pen
changed to: bicycle
for x 2 number graphs 0.8
for x 3 number graphs 0.2
for x 4 number graphs 0.9
for x 5 number graphs 0.3
for x 6 number graphs 0.7
for x 7 number graphs 0.6
for x 8 number graphs 0.1
for x 9 number graphs 0.4
for x 10 number graphs 0.6
for x 11 number graphs 0.3
for x 12 number graphs 0.2
for x 13 number graphs 0.9
for x 14 number graphs 0.5
Looking for head of VP; value is |Verb|, baseCat is |Verb_pres|
Looking for head of PP; value is |Prep|, baseCat is |Prep_in|
Looking for head of ADJ; value is |Adjective|, baseCat is |Adjective_comp|
Looking for head of ADV; value is |Adverb|, baseCat is |Adverb_ly|
Looking for head of DET; value is |Determiner|, baseCat is |Determiner_indef|
Looking for head of CONJ; value is |Conjunction|, baseCat is |Conjunction_coordinating|
Looking for head of NUM; value is |Number|, baseCat is |Number_card|
Looking for head of PRON; value is |Pronoun|, baseCat is |Pronoun_personal|
Looking for head of AUX; value is |Auxiliary|, baseCat is |Auxiliary_modal|
Looking for head of PART; value is |Particle|, baseCat is |Particle_neg|
Looking for head of INF; value is |Infinitive|, baseCat is |Infinitive_to|
Looking for head of GER; value is |Gerund|, baseCat is |Gerund_ing|
Looking for head of PUNCT; value is |Punctuation|, baseCat is |Punctuation_period|
Looking for head of S; value is |Sentence|, baseCat is |Sentence_decl|
NB CF: 2000 data items
NB CF: 3000 data items
NB CF: 4000 data items
NB CF: 5000 data items
NB CF: 6000 data items
NB CF: 7000 data items
NB CF: 8000 data items
NB CF: 9000 data items
NB CF: 10000 data items
NB CF: 11000 data items
NB CF: 12000 data items
NB CF: 13000 data items
NB CF: 14000 data items
total feats: 15, populated: 8
total feats: 20, populated: 12
total feats: 25, populated: 15
total feats: 30, populated: 18
total feats: 35, populated: 20
total feats: 40, populated: 23
total feats: 45, populated: 27
total feats: 50, populated: 30
total feats: 55, populated: 33
total feats: 60, populated: 36
total feats: 65, populated: 39
total feats: 70, populated: 42
total feats: 75, populated: 45
Backtracking derivative norm 0.012345 < 1.0e-9: quitting
Backtracking derivative norm 0.078901 < 1.0e-9: quitting
Backtracking derivative norm 0.024689 < 1.0e-9: quitting
Backtracking derivative norm 0.099876 < 1.0e-9: quitting
Backtracking derivative norm 0.034567 < 1.0e-9: quitting
Backtracking derivative norm 0.087654 < 1.0e-9: quitting
Backtracking derivative norm 0.043210 < 1.0e-9: quitting
Backtracking derivative norm 0.066789 < 1.0e-9: quitting
Backtracking derivative norm 0.055321 < 1.0e-9: quitting
Backtracking derivative norm 0.010987 < 1.0e-9: quitting
Backtracking derivative norm 0.045987 < 1.0e-9: quitting
Backtracking derivative norm 0.023456 < 1.0e-9: quitting
Backtracking derivative norm 0.079876 < 1.0e-9: quitting
<div> </node>
<p> </node>
<li> </node>
<span> </node>
<h1> </node>
<img> </node>
<a> </node>
<ul> </node>
<table> </node>
<tr> </node>
<td> </node>
<br> </node>
<script> </node>
<style> </node>
chtbl.flex tokenization error: "unexpected end of input"
chtbl.flex tokenization error: "invalid character '!'"
chtbl.flex tokenization error: "unmatched parentheses"
chtbl.flex tokenization error: "missing separator ','"
chtbl.flex tokenization error: "unknown keyword 'foo'"
chtbl.flex tokenization error: "empty string"
chtbl.flex tokenization error: "invalid escape sequence '\q'"
chtbl.flex tokenization error: "string too long"
chtbl.flex tokenization error: "invalid number '3.14.15'"
chtbl.flex tokenization error: "unterminated string"
chtbl.flex tokenization error: "invalid operator '++'"
chtbl.flex tokenization error: "missing value"
chtbl.flex tokenization error: "invalid character '\n'"
chtbl.flex tokenization error: "unknown symbol 'bar'"
chtbl.flex tokenization error: "invalid expression 'a + b *'"
Correcting error: NP preterminal w/o NP parent, changing preterminal to NN: (NP (NNP China)) GB18030
Correcting error: NP preterminal w/o NP parent, changing preterminal to NN: (NP (NNP Beijing)) GB2312
Correcting error: NP preterminal w/o NP parent, changing preterminal to NN: (NP (NNP Shanghai)) UTF-8
Correcting error: NP preterminal w/o NP parent, changing preterminal to NN: (NP (NNP Taiwan)) Big5
Correcting error: NP preterminal w/o NP parent, changing preterminal to NN: (NP (NNP Hong Kong)) ISO-8859-1
Correcting error: NP preterminal w/o NP parent, changing preterminal to NN: (NP (NNP Macau)) ISO-2022-CN
Correcting error: NP preterminal w/o NP parent, changing preterminal to NN: (NP (NNP Singapore)) ASCII
Correcting error: NP preterminal w/o NP parent, changing preterminal to NN: (NP (NNP Malaysia)) UTF-16
Correcting error: NP preterminal w/o NP parent, changing preterminal to NN: (NP (NNP Indonesia)) UTF-32
Correcting error: NP preterminal w/o NP parent, changing preterminal to NN: (NP (NNP Thailand)) TIS-620
Correcting error: NP preterminal w/o NP parent, changing preterminal to NN: (NP (NNP Vietnam)) VISCII
Correcting error: NP preterminal w/o NP parent, changing preterminal to NN: (NP (NNP Japan)) Shift-JIS
Correcting error: NP preterminal w/o NP parent, changing preterminal to NN: (NP (NNP Korea)) EUC-KR
Correcting error: NP preterminal w/o NP parent, changing preterminal to NN: (NP (NNP India)) ISCII
Correcting error: NP preterminal w/o NP parent, changing preterminal to NN: (NP (NNP Nepal)) Devanagari
Correcting error: "PU" under PU tag; tag changed to NN: (PU (, ,)) GB2312
ConstantsAndVariables.extremedebug Removing already identified patterns of size 12. New patterns size 8
ConstantsAndVariables.extremedebug Removing already identified patterns of size 9. New patterns size 6
ConstantsAndVariables.extremedebug Removing already identified patterns of size 15. New patterns size 10
ConstantsAndVariables.extremedebug Removing already identified patterns of size 7. New patterns size 5
ConstantsAndVariables.extremedebug Removing already identified patterns of size 10. New patterns size 7
ConstantsAndVariables.extremedebug Removing already identified patterns of size 13. New patterns size 9
ConstantsAndVariables.extremedebug Removing already identified patterns of size 11. New patterns size 8
ConstantsAndVariables.extremedebug Removing already identified patterns of size 14. New patterns size 10
ConstantsAndVariables.extremedebug Removing already identified patterns of size 8. New patterns size 6
ConstantsAndVariables.extremedebug Removing already identified patterns of size 16. New patterns size 11
ConstantsAndVariables.extremedebug Removing already identified patterns of size 6. New patterns size 4
ConstantsAndVariables.extremedebug Removing already identified patterns of size 17. New patterns size 12
ConstantsAndVariables.extremedebug Removing already identified patterns of size 18. New patterns size 13
ConstantsAndVariables.extremedebug Removing already identified patterns of size 19. New patterns size 14
Printing dependencies around "the" index 3
Printing dependencies around "a" index 7
Printing dependencies around "of" index 12
Printing dependencies around "and" index 15
Printing dependencies around "to" index 18
Printing dependencies around "in" index 21
Printing dependencies around "for" index 24
Printing dependencies around "on" index 27
Printing dependencies around "with" index 30
Printing dependencies around "from" index 33
Printing dependencies around "by" index 36
Printing dependencies around "as" index 39
Printing dependencies around "at" index 42
Printing dependencies around "an" index 45
<br><br>Error creating annotator for html
<br><br>Error creating annotator for xml
<br><br>Error creating annotator for json
<br><br>Error creating annotator for csv
<br><br>Error creating annotator for txt
<br><br>Error creating annotator for pdf
<br><br>Error creating annotator for docx
<br><br>Error creating annotator for xlsx
<br><br>Error creating annotator for pptx
<br><br>Error creating annotator for md
<br><br>Error creating annotator for java
<br><br>Error creating annotator for py
<br><br>Error creating annotator for csharp
<br><br>Error creating annotator for sql
<br><br>Error creating annotator for regex
Experiments: for y = 0 Sum_x ptildeXY(x,y) = 0.5
Experiments: for y = 1 Sum_x ptildeXY(x,y) = 0.4
Experiments: for y = 2 Sum_x ptildeXY(x,y) = 0.3
Experiments: for y = 3 Sum_x ptildeXY(x,y) = 0.2
Experiments: for y = 4 Sum_x ptildeXY(x,y) = 0.1
Experiments: for y = 5 Sum_x ptildeXY(x,y) = 0.05
Experiments: for y = 6 Sum_x ptildeXY(x,y) = 0.04
Experiments: for y = 7 Sum_x ptildeXY(x,y) = 0.03
Experiments: for y = 8 Sum_x ptildeXY(x,y) = 0.02
Experiments: for y = 9 Sum_x ptildeXY(x,y) = 0.01
Experiments: for y = 10 Sum_x ptildeXY(x,y) = 0.005
Experiments: for y = 11 Sum_x ptildeXY(x,y) = 0.004
Experiments: for y = 12 Sum_x ptildeXY(x,y) = 0.003
Experiments: for y = 13 Sum_x ptildeXY(x,y) = 0.002
Experiments: for y = 14 Sum_x ptildeXY(x,y) = 0.001
Correcting error: "FRAG" under PU tag; tag changed to VC: (PU (FRAG *pro*)) GB18030
Correcting error: "CP" under PU tag; tag changed to VC: (PU (CP *pro*)) GB18030
Correcting error: "DNP" under PU tag; tag changed to VC: (PU (DNP *pro*)) GB18030
Correcting error: "ADVP" under PU tag; tag changed to VC: (PU (ADVP *pro*)) GB18030
Correcting error: "DP" under PU tag; tag changed to VC: (PU (DP *pro*)) GB18030
Correcting error: "PRN" under PU tag; tag changed to VC: (PU *pro*) GB18030
Correcting error: "UCP" under PU tag; tag changed to VC: *pro* GB18030
Correcting error: NP under PU tag; tag changed to LC: (PU (PU ,)) GB18030
Correcting error: VP under PU tag; tag changed to LC: (PU (PU .)) GB18030
Correcting error: ADJP under PU tag; tag changed to LC: (PU (PU ,)) GB18030
Correcting error: QP under PU tag; tag changed to LC: (PU (PU ;)) GB18030
Correcting error: LCP under PU tag; tag changed to LC: (PU (PU :)) GB18030
Correcting error: IP under PU tag; tag changed to LC: (PU (PU !)) GB18030
Correcting error: CP under PU tag; tag changed to LC: (PU (PU ?)) GB18030
Correcting error: DNP under PU tag; tag changed to LC: (PU (PU ——)) GB18030
Correcting error: ADVP under PU tag; tag changed to LC: (PU (PU ")) GB18030
Correcting error: DP under PU tag; tag changed to LC: (PU (PU ")) GB18030
Correcting error: FRAG under PU tag; tag changed to LC: (PU (PU ')) GB18030
Correcting error: PRN under PU tag; tag changed to LC: (PU (PU ')) GB18030
Correcting error: UCP under PU tag; tag changed to LC: (PU (PU ()) GB18030
Error resolving temporal, using docDate= 2021-10-27
Error resolving temporal, using docDate= 2020-12-31
Error resolving temporal, using docDate= 2023-01-01
Error resolving temporal, using docDate= 2019-04-15
Error resolving temporal, using docDate= 2022-02-29
Error resolving temporal, using docDate= 2018-07-30
Error resolving temporal, using docDate= 2024-05-05
Error resolving temporal, using docDate= 2017-09-09
Error resolving temporal, using docDate= 2025-03-14
Error resolving temporal, using docDate= 2016-11-11
Error resolving temporal, using docDate= 2021-08-08
Error resolving temporal, using docDate= 2020-06-06
Error resolving temporal, using docDate= 2023-10-10
Error resolving temporal, using docDate= 2019-12-12
Correcting error: MSP phrasal tag changed to VP: (FRAG ...) HZ
Correcting error: MSP phrasal tag changed to VP: (PRN ...) Unicode
Correcting error: MSP phrasal tag changed to VP: (...) UTF-7
Correcting error: MSP phrasal tag changed to VP: (...) UTF-EBCDIC
Correcting error: NP preterminal w/ NP parent; preterminal changed to NN: (NP (NNP John) (NNP Smith)) UTF-8
Correcting error: NP preterminal w/ NP parent; preterminal changed to NN: (NP (DT the) (NN book)) GB2312
Correcting error: NP preterminal w/ NP parent; preterminal changed to NN: (NP (PRP$ his) (NN dog)) BIG5
Correcting error: NP preterminal w/ NP parent; preterminal changed to NN: (NP (JJ red) (NN apple)) GBK
Correcting error: NP preterminal w/ NP parent; preterminal changed to NN: (NP (CD one) (NN hundred)) UTF-16
Correcting error: NP preterminal w/ NP parent; preterminal changed to NN: (NP (NNP Beijing) (NNP University)) ISO-8859-1
Correcting error: NP preterminal w/ NP parent; preterminal changed to NN: (NP (RB very) (NN good)) ASCII
Correcting error: NP preterminal w/ NP parent; preterminal changed to NN: (NP (JJR bigger) (NN house)) UTF-32
Correcting error: NP preterminal w/ NP parent; preterminal changed to NN: (NP (NNS books) (NN shelf)) EUC-CN
Correcting error: NP preterminal w/ NP parent; preterminal changed to NN: (NP (VBN broken) (NN window)) EUC-JP
Correcting error: NP preterminal w/ NP parent; preterminal changed to NN: (NP (DT a) (NN car)) EUC-KR
Correcting error: NP preterminal w/ NP parent; preterminal changed to NN: (NP (PRP she) (NN friend)) Shift_JIS
Correcting error: NP preterminal w/ NP parent; preterminal changed to NN: (NP (JJS best) (NN teacher)) ISO-2022-JP
Correcting error: NP preterminal w/ NP parent; preterminal changed to NN: (NP (CD two) (NN cats)) ISO-2022-CN
Correcting error: NP preterminal w/ NP parent; preterminal changed to NN: (NP (JJ blue) (NN sky)) ISO-2022-KR
scoresOf(): length of tags is 5; position is 1; endSizePairs = [(3, 2), (4, 1)]; size is 2; leftWindow is [0, 1]
scoresOf(): length of tags is 4; position is 2; endSizePairs = [(2, 3), (3, 2)]; size is 3; leftWindow is [1, 2]
scoresOf(): length of tags is 6; position is 3; endSizePairs = [(4, 2), (5, 1)]; size is 2; leftWindow is [2, 3]
scoresOf(): length of tags is 7; position is 4; endSizePairs = [(5, 3), (6, 2)]; size is 3; leftWindow is [3, 4]
scoresOf(): length of tags is 8; position is 5; endSizePairs = [(6, 4), (7, 3)]; size is 4; leftWindow is [4, 5]
scoresOf(): length of tags is 9; position is 6; endSizePairs = [(7, 5), (8, 4)]; size is 5; leftWindow is [5, 6]
scoresOf(): length of tags is 10; position is 7; endSizePairs = [(8, 6), (9, 5)]; size is 6; leftWindow is [6, 7]
scoresOf(): length of tags is 11; position is 8; endSizePairs = [(9, 7), (10, 6)]; size is 7; leftWindow is [7, 8]
scoresOf(): length of tags is 12; position is 9; endSizePairs = [(10, 8), (11, 7)]; size is 8; leftWindow is [8, 9]
scoresOf(): length of tags is 13; position is 10; endSizePairs = [(11, 9), (12, 8)]; size is 9; leftWindow is [9, 10]
scoresOf(): length of tags is 14; position is 11; endSizePairs = [(12, 10), (13, 9)]; size is 10; leftWindow is [10, 11]
scoresOf(): length of tags is 15; position is 12; endSizePairs = [(13, 11), (14, 10)]; size is 11; leftWindow is [11, 12]
scoresOf(): length of tags is 16; position is 13; endSizePairs = [(14, 12), (15, 11)]; size is 12; leftWindow is [12, 13]
scoresOf(): length of tags is 17; position is 14; endSizePairs = [(15, 13), (16, 12)]; size is 13; leftWindow is [13, 14]
Correcting error: Updated tree using tregex NP < (NP=main $.. (VP < VV=target)) and tsurgeon move target >-1 main GB18030
Correcting error: Updated tree using tregex S < (NP=np $ VP=vp) and tsurgeon relabel np NP-SBJ relabel vp VP-ROOT GB18030
Correcting error: Updated tree using tregex VP < (PP=pp $.. NP) and tsurgeon delete pp GB18030
Correcting error: Updated tree using tregex NP=np <, DT and tsurgeon delete np GB18030
Correcting error: Updated tree using tregex NP < (QP=qp $.. CP) and tsurgeon move qp $-1 CP GB18030
Correcting error: Updated tree using tregex S=s < (VP=vp $.. PU) and tsurgeon move vp $-1 s GB18030
Correcting error: Updated tree using tregex VP < (ADVP=advp $.. VV) and tsurgeon move advp >0 VV GB18030
Correcting error: "NP" under PU tag; tag changed to VC: (PU (NP (NNP John))) GB18030
Correcting error: "VP" under PU tag; tag changed to VC: (PU (VP (VBZ likes) (NP (NN apples)))) GB18030
Correcting error: "ADJP" under PU tag; tag changed to VC: (PU (ADJP (JJ very) (JJ good))) GB18030
Correcting error: "PP" under PU tag; tag changed to VC: (PU (PP (IN on) (NP (DT the) (NN table)))) GB18030
Correcting error: "QP" under PU tag; tag changed to VC: (PU (QP (CD three) (CD hundred))) GB18030
Correcting error: "ADVP" under PU tag; tag changed to VC: (PU (ADVP (RB quickly))) GB18030
Correcting error: "PRN" under PU tag; tag changed to VC: (PU (PRN (-LRB- -LRB-) (NP (NNP Mary)) (-RRB- -RRB-))) GB18030
Correcting error: "INTJ" under PU tag; tag changed to VC: (PU (INTJ (UH oh))) GB18030
Correcting error: "CONJP" under PU tag; tag changed to VC: (PU (CONJP (IN as) (RB well) (IN as))) GB18030
Correcting error: "FRAG" under PU tag; tag changed to VC: (PU (FRAG (NP (DT a) (NN book)))) GB18030
Correcting error: "LST" under PU tag; tag changed to VC: (PU (LST (-LSB- -LSB-) 1 (-RSB- -RSB-))) GB18030
Correcting error: "NAC" under PU tag; tag changed to VC: (PU (NAC (NP-SBJ (-NONE- *)) (-NONE- *T*))) GB18030
Correcting error: "NX" under PU tag; tag changed to VC: (PU (NX (-NONE- *PRO*))) GB18030
Correcting error: "RRC" under PU tag; tag changed to VC: (PU (RRC (-NONE- 0) (-NONE- *T*))) GB18030
Correcting error: "SINV" under PU tag; tag changed to VC: (PU (SINV (-NONE- *T*) (-NONE- *T*))) GB18030
FOUND NUMBER 456 with offsets 20 25 and value 200.75 and type Float
FOUND NUMBER 789 with offsets 30 35 and value 300.25 and type Double
FOUND NUMBER 234 with offsets 40 45 and value 400.5 and type Integer
FOUND NUMBER 567 with offsets 50 55 and value 500.75 and type Float
FOUND NUMBER 890 with offsets 60 65 and value 600.25 and type Double
FOUND NUMBER 345 with offsets 70 75 and value 700.5 and type Integer
FOUND NUMBER 678 with offsets 80 85 and value 800.75 and type Float
FOUND NUMBER 901 with offsets 90 95 and value 900.25 and type Double
FOUND NUMBER 456 with offsets 100 105 and value 1000.5 and type Integer
FOUND NUMBER 789 with offsets 110 115 and value 1100.75 and type Float
FOUND NUMBER 012 with offsets 120 125 and value 1200.25 and type Double
FOUND NUMBER 567 with offsets 130 135 and value 1300.5 and type Integer
FOUND NUMBER 890 with offsets 140 145 and value 1400.75 and type Float
For pos 1 scores.length is 17; tagNum[pos] = 8; windowScore[pos].length = 9
For pos 2 scores.length is 8; tagNum[pos] = 5; windowScore[pos].length = 7
For pos 3 scores.length is 15; tagNum[pos] = 6; windowScore[pos].length = 11
For pos 4 scores.length is 12; tagNum[pos] = 7; windowScore[pos].length = 6
For pos 5 scores.length is 9; tagNum[pos] = 2; windowScore[pos].length = 3
For pos 6 scores.length is 6; tagNum[pos] = 3; windowScore[pos].length = 8
For pos 7 scores.length is 14; tagNum[pos] = 9; windowScore[pos].length = 10
For pos 8 scores.length is 11; tagNum[pos] = 1; windowScore[pos].length = 4
For pos 9 scores.length is 20; tagNum[pos] = 10; windowScore[pos].length = 12
For pos 10 scores.length is 13; tagNum[pos] = 6; windowScore[pos].length = 2
For pos 11 scores.length is 7; tagNum[pos] = 8; windowScore[pos].length = 14
For pos 12 scores.length is 18; tagNum[pos] = 3; windowScore[pos].length = 9
For pos 13 scores.length is 5; tagNum[pos] = 4; windowScore[pos].length = 13
#1: Changing normalized NER from '2021-02-01' to '2021-02-28' at index 1
#1: Changing normalized NER from '2021-03-01' to '2021-03-31' at index 2
#1: Changing normalized NER from '2021-04-01' to '2021-04-30' at index 3
#1: Changing normalized NER from '2021-05-01' to '2021-05-31' at index 4
#1: Changing normalized NER from '2021-06-01' to '2021-06-30' at index 5
#1: Changing normalized NER from '2021-07-01' to '2021-07-31' at index 6
#1: Changing normalized NER from '2021-08-01' to '2021-08-31' at index 7
#1: Changing normalized NER from '2021-09-01' to '2021-09-30' at index 8
#1: Changing normalized NER from '2021-10-01' to '2021-10-31' at index 9
#1: Changing normalized NER from '2021-11-01' to '2021-11-30' at index 10
#1: Changing normalized NER from '2021-12-01' to '2021-12-31' at index 11
#1: Changing normalized NER from '2022-01-01' to '2022-01-31' at index 12
#1: Changing normalized NER from '2022-02-01' to '2022-02-28' at index 13
Index 1 is element -1 of binaryTensor "C, D"
Index 2 is element 2 of binaryTensor "E, F"
Index 3 is element 1 of binaryTensor "G, H"
Index 4 is element -2 of binaryTensor "I, J"
Index 5 is element 3 of binaryTensor "K, L"
Index 6 is element -3 of binaryTensor "M, N"
Index 7 is element 4 of binaryTensor "O, P"
Index 8 is element -4 of binaryTensor "Q, R"
Index 9 is element 5 of binaryTensor "S, T"
Index 10 is element -5 of binaryTensor "U, V"
Index 11 is element 6 of binaryTensor "W, X"
Index 12 is element -6 of binaryTensor "Y, Z"
Index 13 is element 7 of binaryTensor "AA, BB"
Parsing sent. 2 len. 8: Hello world
Parsing sent. 3 len. 13: How are you today
Parsing sent. 4 len. 9: I love pizza
Parsing sent. 5 len. 7: Good morning
Parsing sent. 6 len. 12: Where is the nearest coffee shop
Parsing sent. 7 len. 6: I am tired
Parsing sent. 8 len. 11: Can you help me with this
Parsing sent. 9 len. 14: What time is it now
Parsing sent. 10 len. 5: I'm sorry
Parsing sent. 11 len. 10: It's raining outside
Parsing sent. 12 len. 9: I need a break
Parsing sent. 13 len. 13: The quick brown fox
Parsing sent. 14 len. 8: I forgot my password
Index 1 is element -1 of binaryTransform <key3>,<key4>
Index 2 is element -2 of binaryTransform <key5>,<key6>
Index 3 is element -3 of binaryTransform <key7>,<key8>
Index 4 is element -4 of binaryTransform <key9>,<key10>
Index 5 is element -5 of binaryTransform <key11>,<key12>
Index 6 is element -6 of binaryTransform <key13>,<key14>
Index 7 is element -7 of binaryTransform <key15>,<key16>
Index 8 is element -8 of binaryTransform <key17>,<key18>
Index 9 is element -9 of binaryTransform <key19>,<key20>
Index 10 is element -10 of binaryTransform <key21>,<key22>
Index 11 is element -11 of binaryTransform <key23>,<key24>
Index 12 is element -12 of binaryTransform <key25>,<key26>
Index 13 is element -13 of binaryTransform <key27>,<key28>
Index 1 is element 1 of binaryClassification <A,B>
Index 2 is element 2 of binaryClassification <P,Q>
Index 3 is element 3 of binaryClassification <M,N>
Index 4 is element 4 of binaryClassification <K,L>
Index 5 is element 5 of binaryClassification <R,S>
Index 6 is element 6 of binaryClassification <E,F>
Index 7 is element 7 of binaryClassification <G,H>
Index 8 is element 8 of binaryClassification <I,J>
Index 9 is element 9 of binaryClassification <U,V>
Index 10 is element 10 of binaryClassification <C,D>
Index 11 is element 11 of binaryClassification <W,Z>
Index 12 is element 12 of binaryClassification <O,N>
Index 13 is element 13 of binaryClassification <S,T>
W tensor size: 8x8x2
W tensor size: 15x2x7
W tensor size: 6x4x1
W tensor size: 9x6x4
W tensor size: 12x3x2
W tensor size: 7x7x3
W tensor size: 11x4x2
W tensor size: 13x6x1
W tensor size: 9x7x5
W tensor size: 14x5x4
W tensor size: 10x3x2
W tensor size: 8x6x3
W tensor size: 11x7x1
Yikes!!! Expected true got 'False'
Yikes!!! Expected 'hello' got 'world'
Yikes!!! Expected 5.3 got 5.4
Yikes!!! Expected null got 'Undefined'
Yikes!!! Expected [1, 2, 3] got [4, 5, 6]
Yikes!!! Expected 'SUCCESS' got 'FAILED'
Yikes!!! Expected 25 got 20
Yikes!!! Expected false got 'True'
Yikes!!! Expected 'abc' got 'ABC'
Yikes!!! Expected [7, 8, 9] got [9, 10, 11]
Yikes!!! Expected 'OK' got 'ERROR'
Yikes!!! Expected 1000 got 2000
Yikes!!! Expected 'right' got 'wrong'
Yikes!!! Expected 'YES' got 'NO'
INFO 1 best sequence: FGHIJ; score: 200
INFO 1 best sequence: KLMNO; score: 300
INFO 1 best sequence: PQRST; score: 400
INFO 1 best sequence: UVWXY; score: 500
INFO 1 best sequence: ZABCD; score: 600
INFO 1 best sequence: EFGHI; score: 700
INFO 1 best sequence: JKLMN; score: 800
INFO 1 best sequence: OPQRS; score: 900
INFO 1 best sequence: TUVWX; score: 1000
INFO 1 best sequence: YZABC; score: 1100
INFO 1 best sequence: DEFGH; score: 1200
INFO 1 best sequence: IJKLM; score: 1300
INFO 1 best sequence: NOPQR; score: 1400
Combined approximate label accuracy: 0.912
Combined approximate label accuracy: 0.788
Combined approximate label accuracy: 0.934
Combined approximate label accuracy: 0.821
Combined approximate label accuracy: 0.905
Combined approximate label accuracy: 0.799
Combined approximate label accuracy: 0.876
Combined approximate label accuracy: 0.932
Combined approximate label accuracy: 0.904
Combined approximate label accuracy: 0.822
Combined approximate label accuracy: 0.799
Combined approximate label accuracy: 0.892
Combined approximate label accuracy: 0.945
insertNPinPP=7
insertNPinPP=10
insertNPinPP=5
insertNPinPP=2
insertNPinPP=8
insertNPinPP=9
insertNPinPP=4
insertNPinPP=1
insertNPinPP=6
insertNPinPP=11
insertNPinPP=15
insertNPinPP=13
insertNPinPP=12
markZuVP=abc
markZuVP=true
markZuVP=456
markZuVP=def
markZuVP=false
markZuVP=789
markZuVP=ghi
markZuVP=null
markZuVP=234
markZuVP=jkl
markZuVP=undefined
markZuVP=567
markZuVP=mno
Src text = Lorem Ipsum
Src text = Random Text
Src text = Example Content
Src text = Simulated Data
Src text = Testing 123
Src text = Placeholder Text
Src text = Generated Text
Src text = Dummy Content
Src text = Sample Text
Src text = Mock Data
Src text = Text Example
Src text = Random Words
Src text = Placeholder Phrase
AFTER remove dupe, size=50
AFTER remove dupe, size=75
AFTER remove dupe, size=80
AFTER remove dupe, size=65
AFTER remove dupe, size=90
AFTER remove dupe, size=120
AFTER remove dupe, size=110
AFTER remove dupe, size=70
AFTER remove dupe, size=95
AFTER remove dupe, size=60
AFTER remove dupe, size=85
AFTER remove dupe, size=105
AFTER remove dupe, size=115
Starting Ssurgeon log, date=Wednesday, April 21, 2022
Starting Ssurgeon log, date=Thursday, April 22, 2022
Starting Ssurgeon log, date=Friday, April 23, 2022
Starting Ssurgeon log, date=Saturday, April 24, 2022
Starting Ssurgeon log, date=Sunday, April 25, 2022
Starting Ssurgeon log, date=Monday, April 26, 2022
Starting Ssurgeon log, date=Tuesday, April 27, 2022
Starting Ssurgeon log, date=Wednesday, April 28, 2022
Starting Ssurgeon log, date=Thursday, April 29, 2022
Starting Ssurgeon log, date=Friday, April 30, 2022
Starting Ssurgeon log, date=Saturday, May 1, 2022
Starting Ssurgeon log, date=Sunday, May 2, 2022
Starting Ssurgeon log, date=Monday, May 3, 2022
Starting Ssurgeon log, at /logs/error.log date=2022-03-15
Starting Ssurgeon log, at /logs/access.log date=2022-03-15
Starting Ssurgeon log, at /logs/debug.log date=2022-03-15
Starting Ssurgeon log, at /logs/info.log date=2022-03-15
Starting Ssurgeon log, at /logs/application.log date=2022-03-15
Starting Ssurgeon log, at /logs/system.log date=2022-03-15
Starting Ssurgeon log, at /logs/warning.log date=2022-03-15
Starting Ssurgeon log, at /logs/security.log date=2022-03-15
Starting Ssurgeon log, at /logs/audit.log date=2022-03-15
Starting Ssurgeon log, at /logs/runtime.log date=2022-03-15
Starting Ssurgeon log, at /logs/api.log date=2022-03-15
Starting Ssurgeon log, at /logs/debugger.log date=2022-03-15
Starting Ssurgeon log, at /logs/cronjob.log date=2022-03-15
Match 1 at: Smith
Match 2 at: Emily
Match 3 at: David
Match 4 at: Olivia
Match 5 at: James
Match 6 at: Sophia
Match 7 at: Benjamin
Match 8 at: Ava
Match 9 at: William
Match 10 at: Isabella
Match 11 at: Michael
Match 12 at: Emma
Match 13 at: Alexander
undirected nodes btw 1 and 5 is banana
undirected nodes btw 1 and 5 is cherry
undirected nodes btw 1 and 5 is durian
undirected nodes btw 1 and 5 is elderberry
undirected nodes btw 1 and 5 is fig
undirected nodes btw 1 and 5 is grape
undirected nodes btw 1 and 5 is honeydew
undirected nodes btw 1 and 5 is ice cream
undirected nodes btw 1 and 5 is jackfruit
undirected nodes btw 1 and 5 is kiwi
undirected nodes btw 1 and 5 is lemon
undirected nodes btw 1 and 5 is mango
undirected nodes btw 1 and 5 is nectarine
Processing sentence file sentences_02.txt
Processing sentence file sentences_03.txt
Processing sentence file sentences_04.txt
Processing sentence file sentences_05.txt
Processing sentence file sentences_06.txt
Processing sentence file sentences_07.txt
Processing sentence file sentences_08.txt
Processing sentence file sentences_09.txt
Processing sentence file sentences_10.txt
Processing sentence file sentences_11.txt
Processing sentence file sentences_12.txt
Processing sentence file sentences_13.txt
Processing sentence file sentences_14.txt
Using fraction of train: 0.6
Using fraction of train: 0.4
Using fraction of train: 0.2
Using fraction of train: 0.9
Using fraction of train: 0.7
Using fraction of train: 0.5
Using fraction of train: 0.3
Using fraction of train: 0.1
Using fraction of train: 0.85
Using fraction of train: 0.65
Using fraction of train: 0.45
Using fraction of train: 0.25
Using fraction of train: 0.95
TagAffixDetector: useChPos=true | useCTBChar2=true | usePKChar2=true
TagAffixDetector: useChPos=false | useCTBChar2=true | usePKChar2=false
TagAffixDetector: useChPos=false | useCTBChar2=false | usePKChar2=true
TagAffixDetector: useChPos=true | useCTBChar2=false | usePKChar2=false
TagAffixDetector: useChPos=false | useCTBChar2=false | usePKChar2=false
TagAffixDetector: useChPos=true | useCTBChar2=false | usePKChar2=true
TagAffixDetector: useChPos=false | useCTBChar2=true | usePKChar2=true
TagAffixDetector: useChPos=true | useCTBChar2=true | usePKChar2=false
TagAffixDetector: useChPos=false | useCTBChar2=true | usePKChar2=true
TagAffixDetector: useChPos=true | useCTBChar2=false | usePKChar2=true
TagAffixDetector: useChPos=false | useCTBChar2=false | usePKChar2=false
TagAffixDetector: useChPos=true | useCTBChar2=false | usePKChar2=false
TagAffixDetector: useChPos=true | useCTBChar2=true | usePKChar2=true
Sighan2005DocRandW: using normalization file table2_UTF-8
Sighan2005DocRandW: using normalization file table3_UTF-8
Sighan2005DocRandW: using normalization file table4_UTF-8
Sighan2005DocRandW: using normalization file table5_UTF-8
Sighan2005DocRandW: using normalization file table6_UTF-8
Sighan2005DocRandW: using normalization file table7_UTF-8
Sighan2005DocRandW: using normalization file table8_UTF-8
Sighan2005DocRandW: using normalization file table9_UTF-8
Sighan2005DocRandW: using normalization file table10_UTF-8
Sighan2005DocRandW: using normalization file table11_UTF-8
Sighan2005DocRandW: using normalization file table12_UTF-8
Sighan2005DocRandW: using normalization file table13_UTF-8
Sighan2005DocRandW: using normalization file table14_UTF-8
Class not found: OtherClass
Class not found: ThirdClass
Class not found: AnotherClass
Class not found: TestClass
Class not found: SampleClass
Class not found: DemoClass
Class not found: UtilityClass
Class not found: HelperClass
Class not found: MyClass
Class not found: OtherClass
Class not found: ThirdClass
Class not found: AnotherClass
Class not found: TestClass
CLASSIFIER: training_dataUTF-8
CLASSIFIER: test_dataUTF-8
CLASSIFIER: modelUTF-8
CLASSIFIER: predictionsUTF-8
CLASSIFIER: resultsUTF-8
CLASSIFIER: accuracyUTF-8
CLASSIFIER: precisionUTF-8
CLASSIFIER: recallUTF-8
CLASSIFIER: F1_scoreUTF-8
CLASSIFIER: lossUTF-8
CLASSIFIER: epochUTF-8
CLASSIFIER: iterationUTF-8
CLASSIFIER: batch_sizeUTF-8
CLASSIFIER: learning_rateUTF-8
Unknown words: apples oranges
Unknown words: trees flowers
Unknown words: mountains rivers
Unknown words: cars bikes
Unknown words: beaches islands
Unknown words: books pens
Unknown words: computers laptops
Unknown words: movies music
Unknown words: shoes socks
Unknown words: chairs tables
Unknown words: clouds sun
Unknown words: tea coffee
Unknown words: hats gloves
F1        (macro average): 0.923%
F1        (macro average): 0.748%
F1        (macro average): 0.912%
F1        (macro average): 0.665%
F1        (macro average): 0.888%
F1        (macro average): 0.743%
F1        (macro average): 0.799%
F1        (macro average): 0.915%
F1        (macro average): 0.805%
F1        (macro average): 0.882%
F1        (macro average): 0.776%
F1        (macro average): 0.795%
F1        (macro average): 0.904%
hs contains c? false
hs contains c? true
hs contains c? true
hs contains c? false
hs contains c? true
hs contains c? false
hs contains c? true
hs contains c? false
hs contains c? true
hs contains c? false
hs contains c? false
hs contains c? true
hs contains c? false
Writing cache (size: 80) to cache2.txt
Writing cache (size: 120) to cache3.txt
Writing cache (size: 95) to cache4.txt
Writing cache (size: 110) to cache5.txt
Writing cache (size: 90) to cache6.txt
Writing cache (size: 115) to cache7.txt
Writing cache (size: 105) to cache8.txt
Writing cache (size: 75) to cache9.txt
Writing cache (size: 130) to cache10.txt
Writing cache (size: 85) to cache11.txt
Writing cache (size: 125) to cache12.txt
Writing cache (size: 70) to cache13.txt
Writing cache (size: 140) to cache14.txt
Could not load class in jar: com.example.Bar at path: /path/to/bar.jar
Could not load class in jar: com.example.Baz at path: /path/to/baz.jar
Could not load class in jar: com.example.Qux at path: /path/to/qux.jar
Could not load class in jar: com.example.FooBar at path: /path/to/foobar.jar
Could not load class in jar: com.example.BarBaz at path: /path/to/barbaz.jar
Could not load class in jar: com.example.BazQux at path: /path/to/bazqux.jar
Could not load class in jar: com.example.QuxFoo at path: /path/to/quxfoo.jar
Could not load class in jar: com.example.FooBarBaz at path: /path/to/foobarbaz.jar
Could not load class in jar: com.example.BarBazQux at path: /path/to/barbazqux.jar
Could not load class in jar: com.example.BazQuxFoo at path: /path/to/bazquxfoo.jar
Could not load class in jar: com.example.QuxFooBar at path: /path/to/quxfoobar.jar
Could not load class in jar: com.example.FooBarBazQux at path: /path/to/foobarbazqux.jar
Could not load class in jar: com.example.BarBazQuxFoo at path: /path/to/barbazquxfoo.jar
Removing from agenda: 2 score i 0.2 + o 0.4 = 0.6
Removing from agenda: 3 score i 0.7 + o 0.8 = 1.5
Removing from agenda: 4 score i 0.1 + o 0.2 = 0.3
Removing from agenda: 5 score i 0.3 + o 0.1 = 0.4
Removing from agenda: 6 score i 0.6 + o 0.7 = 1.3
Removing from agenda: 7 score i 0.4 + o 0.5 = 0.9
Removing from agenda: 8 score i 0.9 + o 0.6 = 1.5
Removing from agenda: 9 score i 0.8 + o 0.9 = 1.7
Removing from agenda: 10 score i 0.5 + o 0.4 = 0.9
Removing from agenda: 11 score i 0.2 + o 0.8 = 1.0
Removing from agenda: 12 score i 0.7 + o 0.1 = 0.8
Removing from agenda: 13 score i 0.1 + o 0.7 = 0.8
Removing from agenda: 14 score i 0.3 + o 0.6 = 0.9
SENTENCE_SKIPPED_OR_UNPARSABLE Parse #2 with score 0.6
SENTENCE_SKIPPED_OR_UNPARSABLE Parse #3 with score 0.9
SENTENCE_SKIPPED_OR_UNPARSABLE Parse #4 with score 0.7
SENTENCE_SKIPPED_OR_UNPARSABLE Parse #5 with score 0.5
SENTENCE_SKIPPED_OR_UNPARSABLE Parse #6 with score 0.4
SENTENCE_SKIPPED_OR_UNPARSABLE Parse #7 with score 0.2
SENTENCE_SKIPPED_OR_UNPARSABLE Parse #8 with score 0.3
SENTENCE_SKIPPED_OR_UNPARSABLE Parse #9 with score 0.1
SENTENCE_SKIPPED_OR_UNPARSABLE Parse #10 with score 0.2
SENTENCE_SKIPPED_OR_UNPARSABLE Parse #11 with score 0.6
SENTENCE_SKIPPED_OR_UNPARSABLE Parse #12 with score 0.7
SENTENCE_SKIPPED_OR_UNPARSABLE Parse #13 with score 0.9
SENTENCE_SKIPPED_OR_UNPARSABLE Parse #14 with score 0.3
Tuning selected smoothUnseen 0.1 smoothSeen 0.7 at 99.3
Tuning selected smoothUnseen 0.3 smoothSeen 0.4 at 102.1
Tuning selected smoothUnseen 0.4 smoothSeen 0.8 at 98.7
Tuning selected smoothUnseen 0.5 smoothSeen 0.6 at 100.8
Tuning selected smoothUnseen 0.6 smoothSeen 0.3 at 103.2
Tuning selected smoothUnseen 0.7 smoothSeen 0.9 at 97.9
Tuning selected smoothUnseen 0.8 smoothSeen 0.2 at 101.6
Tuning selected smoothUnseen 0.9 smoothSeen 0.7 at 99.1
Tuning selected smoothUnseen 0.3 smoothSeen 0.5 at 100.0
Tuning selected smoothUnseen 0.6 smoothSeen 0.4 at 101.2
Tuning selected smoothUnseen 0.8 smoothSeen 0.6 at 98.3
Tuning selected smoothUnseen 0.2 smoothSeen 0.9 at 97.1
Tuning selected smoothUnseen 0.4 smoothSeen 0.8 at 102.7
Taking 2 out of 10 slices of node features for training
Taking 8 out of 10 slices of node features for training
Taking 3 out of 10 slices of node features for training
Taking 9 out of 10 slices of node features for training
Taking 1 out of 10 slices of node features for training
Taking 6 out of 10 slices of node features for training
Taking 4 out of 10 slices of node features for training
Taking 7 out of 10 slices of node features for training
Taking 10 out of 10 slices of node features for training
Taking 3 out of 10 slices of node features for training
Taking 1 out of 10 slices of node features for training
Taking 9 out of 10 slices of node features for training
Taking 2 out of 10 slices of node features for training
baseClassifiers index 1 : /path/to/classifier1
baseClassifiers index 2 : /path/to/classifier2
baseClassifiers index 3 : /path/to/classifier3
baseClassifiers index 4 : /path/to/classifier4
baseClassifiers index 5 : /path/to/classifier5
baseClassifiers index 6 : /path/to/classifier6
baseClassifiers index 7 : /path/to/classifier7
baseClassifiers index 8 : /path/to/classifier8
baseClassifiers index 9 : /path/to/classifier9
baseClassifiers index 10 : /path/to/classifier10
baseClassifiers index 11 : /path/to/classifier11
baseClassifiers index 12 : /path/to/classifier12
baseClassifiers index 13 : /path/to/classifier13
Finished loading pipeline.  Current memory usage: 2048mb
Finished loading pipeline.  Current memory usage: 3072mb
Finished loading pipeline.  Current memory usage: 4096mb
Finished loading pipeline.  Current memory usage: 5120mb
Finished loading pipeline.  Current memory usage: 6144mb
Finished loading pipeline.  Current memory usage: 7168mb
Finished loading pipeline.  Current memory usage: 8192mb
Finished loading pipeline.  Current memory usage: 9216mb
Finished loading pipeline.  Current memory usage: 10240mb
Finished loading pipeline.  Current memory usage: 11264mb
Finished loading pipeline.  Current memory usage: 12288mb
Finished loading pipeline.  Current memory usage: 13312mb
Finished loading pipeline.  Current memory usage: 14336mb
Processing file /path/to/file2.txt ... writing to output2.txt
Processing file /path/to/file3.txt ... writing to output3.txt
Processing file /path/to/file4.txt ... writing to output4.txt
Processing file /path/to/file5.txt ... writing to output5.txt
Processing file /path/to/file6.txt ... writing to output6.txt
Processing file /path/to/file7.txt ... writing to output7.txt
Processing file /path/to/file8.txt ... writing to output8.txt
Processing file /path/to/file9.txt ... writing to output9.txt
Processing file /path/to/file10.txt ... writing to output10.txt
Processing file /path/to/file11.txt ... writing to output11.txt
Processing file /path/to/file12.txt ... writing to output12.txt
Processing file /path/to/file13.txt ... writing to output13.txt
Processing file /path/to/file14.txt ... writing to output14.txt
@@ Position is: 2:
@@ Position is: 3:
@@ Position is: 4:
@@ Position is: 5:
@@ Position is: 6:
@@ Position is: 7:
@@ Position is: 8:
@@ Position is: 9:
@@ Position is: 10:
@@ Position is: 11:
@@ Position is: 12:
@@ Position is: 13:
@@ Position is: 14:
Replacing relation argument: cat with predicted mention dog
Replacing relation argument: apple with predicted mention orange
Replacing relation argument: book with predicted mention notebook
Replacing relation argument: car with predicted mention bicycle
Replacing relation argument: chair with predicted mention stool
Replacing relation argument: elephant with predicted mention lion
Replacing relation argument: flower with predicted mention tree
Replacing relation argument: guitar with predicted mention piano
Replacing relation argument: hat with predicted mention cap
Replacing relation argument: ice cream with predicted mention popsicle
Replacing relation argument: jacket with predicted mention coat
Replacing relation argument: key with predicted mention lock
Replacing relation argument: lemon with predicted mention lime
Replacing relation argument: monkey with predicted mention gorilla
Creating partition #2 using offsets [100,200) out of 5000
Creating partition #3 using offsets [200,300) out of 5000
Creating partition #4 using offsets [300,400) out of 5000
Creating partition #5 using offsets [400,500) out of 5000
Creating partition #6 using offsets [500,600) out of 5000
Creating partition #7 using offsets [600,700) out of 5000
Creating partition #8 using offsets [700,800) out of 5000
Creating partition #9 using offsets [800,900) out of 5000
Creating partition #10 using offsets [900,1000) out of 5000
Creating partition #11 using offsets [1000,1100) out of 5000
Creating partition #12 using offsets [1100,1200) out of 5000
Creating partition #13 using offsets [1200,1300) out of 5000
Creating partition #14 using offsets [1300,1400) out of 5000
after deleting 2, and then zero deg nodes [Graph2]
after deleting 2, and then zero deg nodes [Graph3]
after deleting 2, and then zero deg nodes [Graph4]
after deleting 2, and then zero deg nodes [Graph5]
after deleting 2, and then zero deg nodes [Graph6]
after deleting 2, and then zero deg nodes [Graph7]
after deleting 2, and then zero deg nodes [Graph8]
after deleting 2, and then zero deg nodes [Graph9]
after deleting 2, and then zero deg nodes [Graph10]
after deleting 2, and then zero deg nodes [Graph11]
after deleting 2, and then zero deg nodes [Graph12]
after deleting 2, and then zero deg nodes [Graph13]
after deleting 2, and then zero deg nodes [Graph14]
WARNING: Unseen word counter is empty!
WARNING: Unseen word counter is empty!
WARNING: Unseen word counter is empty!
WARNING: Unseen word counter is empty!
WARNING: Unseen word counter is empty!
WARNING: Unseen word counter is empty!
WARNING: Unseen word counter is empty!
WARNING: Unseen word counter is empty!
WARNING: Unseen word counter is empty!
WARNING: Unseen word counter is empty!
WARNING: Unseen word counter is empty!
WARNING: Unseen word counter is empty!
WARNING: Unseen word counter is empty!
ANS (before comma norm): FirefoxUTF-8
ANS (before comma norm): SafariUTF-8
ANS (before comma norm): EdgeUTF-8
ANS (before comma norm): OperaUTF-8
ANS (before comma norm): Internet ExplorerUTF-8
ANS (before comma norm): BraveUTF-8
ANS (before comma norm): VivaldiUTF-8
ANS (before comma norm): MozillaUTF-8
ANS (before comma norm): ChromiumUTF-8
ANS (before comma norm): TorUTF-8
ANS (before comma norm): DolphinUTF-8
ANS (before comma norm): UC BrowserUTF-8
ANS (before comma norm): MaxthonUTF-8
POSTPROCESSED: NoUTF-8
POSTPROCESSED: MaybeUTF-8
POSTPROCESSED: TrueUTF-8
POSTPROCESSED: FalseUTF-8
POSTPROCESSED: SuccessUTF-8
POSTPROCESSED: FailureUTF-8
POSTPROCESSED: CompletedUTF-8
POSTPROCESSED: PendingUTF-8
POSTPROCESSED: ApprovedUTF-8
POSTPROCESSED: RejectedUTF-8
POSTPROCESSED: StartedUTF-8
POSTPROCESSED: StoppedUTF-8
POSTPROCESSED: FinishedUTF-8
POSTPROCESSED: UpdatedUTF-8
boundariesToDiscard=20
boundariesToDiscard=30
boundariesToDiscard=40
boundariesToDiscard=50
boundariesToDiscard=60
boundariesToDiscard=70
boundariesToDiscard=80
boundariesToDiscard=90
boundariesToDiscard=100
boundariesToDiscard=110
boundariesToDiscard=120
boundariesToDiscard=130
boundariesToDiscard=140
Loading Chinese dictionaries from 1 files
Loading Chinese dictionaries from 2 files
Loading Chinese dictionaries from 3 files
Loading Chinese dictionaries from 4 files
Loading Chinese dictionaries from 5 files
Loading Chinese dictionaries from 6 files
Loading Chinese dictionaries from 7 files
Loading Chinese dictionaries from 8 files
Loading Chinese dictionaries from 9 files
Loading Chinese dictionaries from 10 files
Loading Chinese dictionaries from 11 files
Loading Chinese dictionaries from 12 files
Loading Chinese dictionaries from 13 files
Loading Chinese dictionaries from 14 files
Serializing dictionaries to D:/backup/dictionary2.dat ...
Serializing dictionaries to E:/temp/dictionary3.dat ...
Serializing dictionaries to F:/archive/dictionary4.dat ...
Serializing dictionaries to G:/repository/dictionary5.dat ...
Serializing dictionaries to H:/storage/dictionary6.dat ...
Serializing dictionaries to I:/cache/dictionary7.dat ...
Serializing dictionaries to J:/logs/dictionary8.dat ...
Serializing dictionaries to K:/data/dictionary9.dat ...
Serializing dictionaries to L:/backup/dictionary10.dat ...
Serializing dictionaries to M:/temp/dictionary11.dat ...
Serializing dictionaries to N:/archive/dictionary12.dat ...
Serializing dictionaries to O:/repository/dictionary13.dat ...
Serializing dictionaries to P:/storage/dictionary14.dat ...
Usage: java FactoredLexicon language features train_file dev_file
Usage: java FactoredLexicon language features train_file dev_file
Usage: java FactoredLexicon language features train_file dev_file
Usage: java FactoredLexicon language features train_file dev_file
Usage: java FactoredLexicon language features train_file dev_file
Usage: java FactoredLexicon language features train_file dev_file
Usage: java FactoredLexicon language features train_file dev_file
Usage: java FactoredLexicon language features train_file dev_file
Usage: java FactoredLexicon language features train_file dev_file
Usage: java FactoredLexicon language features train_file dev_file
Usage: java FactoredLexicon language features train_file dev_file
Usage: java FactoredLexicon language features train_file dev_file
Usage: java FactoredLexicon language features train_file dev_file
Saved 50/200 entries for dictionary spanish.
Saved 20/300 entries for dictionary french.
Saved 5/50 entries for dictionary german.
Saved 30/150 entries for dictionary italian.
Saved 15/250 entries for dictionary portuguese.
Saved 8/80 entries for dictionary chinese.
Saved 12/120 entries for dictionary japanese.
Saved 25/250 entries for dictionary korean.
Saved 18/180 entries for dictionary russian.
Saved 40/400 entries for dictionary arabic.
Saved 7/70 entries for dictionary turkish.
Saved 35/350 entries for dictionary dutch.
Saved 22/220 entries for dictionary swedish.
Flooding tags for 2 UTF-8
Flooding tags for 3 UTF-8
Flooding tags for 4 UTF-8
Flooding tags for 5 UTF-8
Flooding tags for 6 UTF-8
Flooding tags for 7 UTF-8
Flooding tags for 8 UTF-8
Flooding tags for 9 UTF-8
Flooding tags for 10 UTF-8
Flooding tags for 11 UTF-8
Flooding tags for 12 UTF-8
Flooding tags for 13 UTF-8
Flooding tags for 14 UTF-8
Forced FlexiTagging index1 UTF-8
Forced FlexiTagging index2 UTF-8
Forced FlexiTagging index3 UTF-8
Forced FlexiTagging index4 UTF-8
Forced FlexiTagging index5 UTF-8
Forced FlexiTagging index6 UTF-8
Forced FlexiTagging index7 UTF-8
Forced FlexiTagging index8 UTF-8
Forced FlexiTagging index9 UTF-8
Forced FlexiTagging index10 UTF-8
Forced FlexiTagging index11 UTF-8
Forced FlexiTagging index12 UTF-8
Forced FlexiTagging index13 UTF-8
Rule 2 over [1,2) has log score 0.75 from L[state=1] = B R[state=2] = C
Rule 3 over [2,3) has log score 0.6 from L[state=2] = C R[state=3] = D
Rule 4 over [3,4) has log score 0.8 from L[state=3] = D R[state=4] = E
Rule 5 over [4,5) has log score 0.9 from L[state=4] = E R[state=5] = F
Rule 6 over [5,6) has log score 0.4 from L[state=5] = F R[state=6] = G
Rule 7 over [6,7) has log score 0.7 from L[state=6] = G R[state=7] = H
Rule 8 over [7,8) has log score 0.65 from L[state=7] = H R[state=8] = I
Rule 9 over [8,9) has log score 0.55 from L[state=8] = I R[state=9] = J
Rule 10 over [9,10) has log score 0.95 from L[state=9] = J R[state=10] = K
Rule 11 over [10,11) has log score 0.85 from L[state=10] = K R[state=11] = L
Rule 12 over [11,12) has log score 0.3 from L[state=11] = L R[state=12] = M
Rule 13 over [12,13) has log score 0.6 from L[state=12] = M R[state=13] = N
Rule 14 over [13,14) has log score 0.45 from L[state=13] = N R[state=14] = O
Used {BANGMAGAZINES} to recognize example2.txt
Used {BANGMAGAZINES} to recognize example3.txt
Used {BANGMAGAZINES} to recognize example4.txt
Used {BANGMAGAZINES} to recognize example5.txt
Used {BANGMAGAZINES} to recognize example6.txt
Used {BANGMAGAZINES} to recognize example7.txt
Used {BANGMAGAZINES} to recognize example8.txt
Used {BANGMAGAZINES} to recognize example9.txt
Used {BANGMAGAZINES} to recognize example10.txt
Used {BANGMAGAZINES} to recognize example11.txt
Used {BANGMAGAZINES} to recognize example12.txt
Used {BANGMAGAZINES} to recognize example13.txt
Used {BANGMAGAZINES} to recognize example14.txt
Score: 75
Score: 85
Score: 98
Score: 82
Score: 93
Score: 88
Score: 79
Score: 91
Score: 87
Score: 95
Score: 83
Score: 76
Score: 84
Used {HTHINGEXCEPTIONWHOLE} (2) to recognize string as normalized_string
Used {HTHINGEXCEPTIONWHOLE} (2) to recognize value as normalized_value
Used {HTHINGEXCEPTIONWHOLE} (2) to recognize word as normalized_word
Used {HTHINGEXCEPTIONWHOLE} (2) to recognize character as normalized_character
Used {HTHINGEXCEPTIONWHOLE} (2) to recognize number as normalized_number
Used {HTHINGEXCEPTIONWHOLE} (2) to recognize symbol as normalized_symbol
Used {HTHINGEXCEPTIONWHOLE} (2) to recognize expression as normalized_expression
Used {HTHINGEXCEPTIONWHOLE} (2) to recognize identifier as normalized_identifier
Used {HTHINGEXCEPTIONWHOLE} (2) to recognize code as normalized_code
Used {HTHINGEXCEPTIONWHOLE} (2) to recognize path as normalized_path
Used {HTHINGEXCEPTIONWHOLE} (2) to recognize route as normalized_route
Used {HTHINGEXCEPTIONWHOLE} (2) to recognize routeID as normalized_routeID
Used {HTHINGEXCEPTIONWHOLE} (2) to recognize username as normalized_username
Used pty ltd to recognize script.txt
Used pty ltd to recognize document.txt
Used pty ltd to recognize notes.txt
Used pty ltd to recognize data.txt
Used pty ltd to recognize info.txt
Used pty ltd to recognize log.txt
Used pty ltd to recognize config.txt
Used pty ltd to recognize report.txt
Used pty ltd to recognize myfile.txt
Used pty ltd to recognize sample.txt
Used pty ltd to recognize test.txt
Used pty ltd to recognize file.txt
Used pty ltd to recognize output.txt
Used {ABBREV1 pl} (2) to recognize image2.txt
Used {ABBREV1 pl} (2) to recognize image3.txt
Used {ABBREV1 pl} (2) to recognize image4.txt
Used {ABBREV1 pl} (2) to recognize image5.txt
Used {ABBREV1 pl} (2) to recognize image6.txt
Used {ABBREV1 pl} (2) to recognize image7.txt
Used {ABBREV1 pl} (2) to recognize image8.txt
Used {ABBREV1 pl} (2) to recognize image9.txt
Used {ABBREV1 pl} (2) to recognize image10.txt
Used {ABBREV1 pl} (2) to recognize image11.txt
Used {ABBREV1 pl} (2) to recognize image12.txt
Used {ABBREV1 pl} (2) to recognize image13.txt
Used {ABBREV1 pl} (2) to recognize image14.txt
Used {HTHINGEXCEPTIONWHOLE} to recognize input as normalized input
Used {HTHINGEXCEPTIONWHOLE} to recognize event as normalized event
Used {HTHINGEXCEPTIONWHOLE} to recognize value as normalized value
Used {HTHINGEXCEPTIONWHOLE} to recognize parameter as normalized parameter
Used {HTHINGEXCEPTIONWHOLE} to recognize string as normalized string
Used {HTHINGEXCEPTIONWHOLE} to recognize number as normalized number
Used {HTHINGEXCEPTIONWHOLE} to recognize output as normalized output
Used {HTHINGEXCEPTIONWHOLE} to recognize error as normalized error
Used {HTHINGEXCEPTIONWHOLE} to recognize document as normalized document
Used {HTHINGEXCEPTIONWHOLE} to recognize request as normalized request
Used {HTHINGEXCEPTIONWHOLE} to recognize response as normalized response
Used {HTHINGEXCEPTIONWHOLE} to recognize code as normalized code
Used {HTHINGEXCEPTIONWHOLE} to recognize status as normalized status
Used {HTHINGEXCEPTIONPREFIXED} (2) to recognize name as normalized
Used {HTHINGEXCEPTIONPREFIXED} (2) to recognize number as normalized
Used {HTHINGEXCEPTIONPREFIXED} (2) to recognize string as normalized
Used {HTHINGEXCEPTIONPREFIXED} (2) to recognize symbol as normalized
Used {HTHINGEXCEPTIONPREFIXED} (2) to recognize identifier as normalized
Used {HTHINGEXCEPTIONPREFIXED} (2) to recognize expression as normalized
Used {HTHINGEXCEPTIONPREFIXED} (2) to recognize statement as normalized
Used {HTHINGEXCEPTIONPREFIXED} (2) to recognize function as normalized
Used {HTHINGEXCEPTIONPREFIXED} (2) to recognize variable as normalized
Used {HTHINGEXCEPTIONPREFIXED} (2) to recognize parameter as normalized
Used {HTHINGEXCEPTIONPREFIXED} (2) to recognize operator as normalized
Used {HTHINGEXCEPTIONPREFIXED} (2) to recognize argument as normalized
Used {HTHINGEXCEPTIONPREFIXED} (2) to recognize condition as normalized
position:2, entities[position-1] = Entity2
position:3, entities[position-1] = Entity3
position:4, entities[position-1] = Entity4
position:5, entities[position-1] = Entity5
position:6, entities[position-1] = Entity6
position:7, entities[position-1] = Entity7
position:8, entities[position-1] = Entity8
position:9, entities[position-1] = Entity9
position:10, entities[position-1] = Entity10
position:11, entities[position-1] = Entity11
position:12, entities[position-1] = Entity12
position:13, entities[position-1] = Entity13
position:14, entities[position-1] = Entity14
Used {tok} (2) to recognize data as normalized
Used {tok} (2) to recognize input as normalized
Used {tok} (2) to recognize output as normalized
Used {tok} (2) to recognize file as normalized
Used {tok} (2) to recognize image as normalized
Used {tok} (2) to recognize video as normalized
Used {tok} (2) to recognize audio as normalized
Used {tok} (2) to recognize code as normalized
Used {tok} (2) to recognize variable as normalized
Used {tok} (2) to recognize function as normalized
Used {tok} (2) to recognize class as normalized
Used {tok} (2) to recognize object as normalized
Used {tok} (2) to recognize error as normalized
Used {ABBREV3} (2) to recognize file2.txt
Used {ABBREV3} (2) to recognize file3.txt
Used {ABBREV3} (2) to recognize file4.txt
Used {ABBREV3} (2) to recognize file5.txt
Used {ABBREV3} (2) to recognize file6.txt
Used {ABBREV3} (2) to recognize file7.txt
Used {ABBREV3} (2) to recognize file8.txt
Used {ABBREV3} (2) to recognize file9.txt
Used {ABBREV3} (2) to recognize file10.txt
Used {ABBREV3} (2) to recognize file11.txt
Used {ABBREV3} (2) to recognize file12.txt
Used {ABBREV3} (2) to recognize file13.txt
Used {ABBREV3} (2) to recognize file14.txt
Warning: no unknown word model in place! Giving the combination cat noun zero probability.
Warning: no unknown word model in place! Giving the combination house noun zero probability.
Warning: no unknown word model in place! Giving the combination car noun zero probability.
Warning: no unknown word model in place! Giving the combination book noun zero probability.
Warning: no unknown word model in place! Giving the combination computer noun zero probability.
Warning: no unknown word model in place! Giving the combination apple noun zero probability.
Warning: no unknown word model in place! Giving the combination orange noun zero probability.
Warning: no unknown word model in place! Giving the combination shoe noun zero probability.
Warning: no unknown word model in place! Giving the combination chair noun zero probability.
Warning: no unknown word model in place! Giving the combination tree noun zero probability.
Warning: no unknown word model in place! Giving the combination table noun zero probability.
Warning: no unknown word model in place! Giving the combination pen noun zero probability.
Warning: no unknown word model in place! Giving the combination paper noun zero probability.
Used {THINGA} (2) to recognize content as token
Used {THINGA} (2) to recognize input as token
Used {THINGA} (2) to recognize text as token
Used {THINGA} (2) to recognize data as token
Used {THINGA} (2) to recognize message as token
Used {THINGA} (2) to recognize information as token
Used {THINGA} (2) to recognize sentence as token
Used {THINGA} (2) to recognize value as token
Used {THINGA} (2) to recognize parameter as token
Used {THINGA} (2) to recognize request as token
Used {THINGA} (2) to recognize output as token
Used {THINGA} (2) to recognize result as token
Used {THINGA} (2) to recognize format as token
Read cache from file2.txt, contains 50 entries. Backing file is file2.dat
Read cache from file3.txt, contains 200 entries. Backing file is file3.dat
Read cache from file4.txt, contains 75 entries. Backing file is file4.dat
Read cache from file5.txt, contains 150 entries. Backing file is file5.dat
Read cache from file6.txt, contains 120 entries. Backing file is file6.dat
Read cache from file7.txt, contains 80 entries. Backing file is file7.dat
Read cache from file8.txt, contains 90 entries. Backing file is file8.dat
Read cache from file9.txt, contains 180 entries. Backing file is file9.dat
Read cache from file10.txt, contains 250 entries. Backing file is file10.dat
Read cache from file11.txt, contains 60 entries. Backing file is file11.dat
Read cache from file12.txt, contains 30 entries. Backing file is file12.dat
Read cache from file13.txt, contains 220 entries. Backing file is file13.dat
Read cache from file14.txt, contains 140 entries. Backing file is file14.dat
Built items:      edge2 hook2
Built items:      edge3 hook3
Built items:      edge4 hook4
Built items:      edge5 hook5
Built items:      edge6 hook6
Built items:      edge7 hook7
Built items:      edge8 hook8
Built items:      edge9 hook9
Built items:      edge10 hook10
Built items:      edge11 hook11
Built items:      edge12 hook12
Built items:      edge13 hook13
Built items:      edge14 hook14
outputLayerWeights.length=    35
outputLayerWeights.length=    12
outputLayerWeights.length=    18
outputLayerWeights.length=    25
outputLayerWeights.length=    30
outputLayerWeights.length=    42
outputLayerWeights.length=    15
outputLayerWeights.length=    28
outputLayerWeights.length=    37
outputLayerWeights.length=    23
outputLayerWeights.length=    10
outputLayerWeights.length=    19
outputLayerWeights.length=    33
<link rel="stylesheet" type="text/css" href="brat2/style-vis.css"/>
<link rel="stylesheet" type="text/css" href="brat3/style-vis.css"/>
<link rel="stylesheet" type="text/css" href="brat4/style-vis.css"/>
<link rel="stylesheet" type="text/css" href="brat5/style-vis.css"/>
<link rel="stylesheet" type="text/css" href="brat6/style-vis.css"/>
<link rel="stylesheet" type="text/css" href="brat7/style-vis.css"/>
<link rel="stylesheet" type="text/css" href="brat8/style-vis.css"/>
<link rel="stylesheet" type="text/css" href="brat9/style-vis.css"/>
<link rel="stylesheet" type="text/css" href="brat10/style-vis.css"/>
<link rel="stylesheet" type="text/css" href="brat11/style-vis.css"/>
<link rel="stylesheet" type="text/css" href="brat12/style-vis.css"/>
<link rel="stylesheet" type="text/css" href="brat13/style-vis.css"/>
<link rel="stylesheet" type="text/css" href="brat14/style-vis.css"/>
<link rel="stylesheet" type="text/css" href="brat15/style-vis.css"/>
<link rel="stylesheet" type="text/css" href="brat16/style-vis.css"/>
ERROR: Entry has multiple types for annotationFieldnames[1]: line 5. Taking type to be String
ERROR: Entry has multiple types for annotationFieldnames[2]: line 10. Taking type to be boolean
ERROR: Entry has multiple types for annotationFieldnames[3]: line 3. Taking type to be double
ERROR: Entry has multiple types for annotationFieldnames[4]: line 7. Taking type to be long
ERROR: Entry has multiple types for annotationFieldnames[5]: line 2. Taking type to be float
ERROR: Entry has multiple types for annotationFieldnames[6]: line 6. Taking type to be byte
ERROR: Entry has multiple types for annotationFieldnames[7]: line 11. Taking type to be short
ERROR: Entry has multiple types for annotationFieldnames[8]: line 4. Taking type to be char
ERROR: Entry has multiple types for annotationFieldnames[9]: line 9. Taking type to be Object
ERROR: Entry has multiple types for annotationFieldnames[10]: line 8. Taking type to be List<Integer>
ERROR: Entry has multiple types for annotationFieldnames[11]: line 12. Taking type to be Map<String, String>
ERROR: Entry has multiple types for annotationFieldnames[12]: line 13. Taking type to be Set<Double>
ERROR: Entry has multiple types for annotationFieldnames[13]: line 14. Taking type to be MyClass
#feattags: 15
#feattags: 8
#feattags: 12
#feattags: 5
#feattags: 19
#feattags: 6
#feattags: 14
#feattags: 9
#feattags: 11
#feattags: 7
#feattags: 13
#feattags: 4
#feattags: 16
truecase.bias" - class bias of the true case model; default: 0.5
truecase.bias" - class bias of the true case model; default: -0.1
truecase.bias" - class bias of the true case model; default: 1.0
truecase.bias" - class bias of the true case model; default: -0.5
truecase.bias" - class bias of the true case model; default: -1.0
truecase.bias" - class bias of the true case model; default: 0.1
truecase.bias" - class bias of the true case model; default: 0.7
truecase.bias" - class bias of the true case model; default: 0.3
truecase.bias" - class bias of the true case model; default: -0.3
truecase.bias" - class bias of the true case model; default: 0.8
truecase.bias" - class bias of the true case model; default: -0.7
truecase.bias" - class bias of the true case model; default: 0.2
truecase.bias" - class bias of the true case model; default: -0.2
truecase.model" - path towards the true-casing model; default: model2
truecase.model" - path towards the true-casing model; default: model3
truecase.model" - path towards the true-casing model; default: model4
truecase.model" - path towards the true-casing model; default: model5
truecase.model" - path towards the true-casing model; default: model6
truecase.model" - path towards the true-casing model; default: model7
truecase.model" - path towards the true-casing model; default: model8
truecase.model" - path towards the true-casing model; default: model9
truecase.model" - path towards the true-casing model; default: model10
truecase.model" - path towards the true-casing model; default: model11
truecase.model" - path towards the true-casing model; default: model12
truecase.model" - path towards the true-casing model; default: model13
truecase.model" - path towards the true-casing model; default: model14
ERROR: Filesystem does not support encoding: com.example.FileNotFoundException
ERROR: Filesystem does not support encoding: com.example.InvalidFileTypeException
ERROR: Filesystem does not support encoding: com.example.FileAlreadyExistsException
ERROR: Filesystem does not support encoding: com.example.InvalidFileNameException
ERROR: Filesystem does not support encoding: com.example.DirectoryNotFoundException
ERROR: Filesystem does not support encoding: com.example.InvalidPathException
ERROR: Filesystem does not support encoding: com.example.PermissionDeniedException
ERROR: Filesystem does not support encoding: com.example.UnsupportedEncodingException
ERROR: Filesystem does not support encoding: com.example.MissingFilePermissionException
ERROR: Filesystem does not support encoding: com.example.DirectoryAlreadyExistsException
ERROR: Filesystem does not support encoding: com.example.EncodingNotSupportedException
ERROR: Filesystem does not support encoding: com.example.InvalidCharacterException
ERROR: Filesystem does not support encoding: com.example.InvalidBlockSizeException
# Loading embedding ...  word file = nltk_words.txt  vector file = nltk_vectors.bin
# Loading embedding ...  word file = custom_words.txt  vector file = custom_vectors.bin
# Loading embedding ...  word file = glove_words.txt  vector file = glove_vectors.bin
# Loading embedding ...  word file = fasttext_words.txt  vector file = fasttext_vectors.bin
# Loading embedding ...  word file = word2vec_words.txt  vector file = word2vec_vectors.bin
# Loading embedding ...  word file = embeddings_words.txt  vector file = embeddings_vectors.bin
# Loading embedding ...  word file = pretrained_words.txt  vector file = pretrained_vectors.bin
# Loading embedding ...  word file = data_words.txt  vector file = data_vectors.bin
# Loading embedding ...  word file = dict_words.txt  vector file = dict_vectors.bin
# Loading embedding ...  word file = vocab_words.txt  vector file = vocab_vectors.bin
# Loading embedding ...  word file = train_words.txt  vector file = train_vectors.bin
# Loading embedding ...  word file = input_words.txt  vector file = input_vectors.bin
# Loading embedding ...  word file = output_words.txt  vector file = output_vectors.bin
f1 (interm): 0.5714285714285714
f1 (interm): 0.6666666666666666
f1 (interm): 1.0
f1 (interm): 0.25
f1 (interm): 0.8571428571428571
f1 (interm): 0.5714285714285714
f1 (interm): 0.42857142857142855
f1 (interm): 0.6666666666666666
f1 (interm): 0.5
f1 (interm): 0.6
f1 (interm): 0.42857142857142855
f1 (interm): 0.5
f1 (interm): 0.6666666666666666
f1 (interm): 0.8571428571428571
After t = gappingTransform(t); t = t->left
After t = gappingTransform(t); t = t->right
After t = gappingTransform(t); t = t->parent
After t = gappingTransform(t); t = t->child
After t = gappingTransform(t); t = t->sibling
After t = gappingTransform(t); t = t->left_child
After t = gappingTransform(t); t = t->right_child
After t = gappingTransform(t); t = t->next_sibling
After t = gappingTransform(t); t = t->prev_sibling
After t = gappingTransform(t); t = t->first_child
After t = gappingTransform(t); t = t->last_child
After t = gappingTransform(t); t = t->next
After t = gappingTransform(t); t = t->prev
<h3>Annotated Text</h3> <em>(tagged using POS)</em>
<h3>Annotated Text</h3> <em>(tagged using Dependency Parsing)</em>
<h3>Annotated Text</h3> <em>(tagged using Sentiment Analysis)</em>
<h3>Annotated Text</h3> <em>(tagged using Named Entity Recognition)</em>
<h3>Annotated Text</h3> <em>(tagged using Part-of-Speech Tagging)</em>
<h3>Annotated Text</h3> <em>(tagged using Coreference Resolution)</em>
<h3>Annotated Text</h3> <em>(tagged using Syntax Analysis)</em>
<h3>Annotated Text</h3> <em>(tagged using Semantic Role Labeling)</em>
<h3>Annotated Text</h3> <em>(tagged using Phrase Chunking)</em>
<h3>Annotated Text</h3> <em>(tagged using Named Entity Extraction)</em>
<h3>Annotated Text</h3> <em>(tagged using Topic Modeling)</em>
<h3>Annotated Text</h3> <em>(tagged using Text Classification)</em>
<h3>Annotated Text</h3> <em>(tagged using Named Entity Tagging)</em>
debug-clusterCluster ID: ABC	Numbers: 10	Genders: Female	Animacies: Low
debug-clusterCluster ID: XYZ	Numbers: 3	Genders: Non-binary	Animacies: Medium
debug-clusterCluster ID: 456	Numbers: 8	Genders: Male	Animacies: High
debug-clusterCluster ID: DEF	Numbers: 2	Genders: Female	Animacies: Low
debug-clusterCluster ID: PQR	Numbers: 6	Genders: Non-binary	Animacies: Medium
debug-clusterCluster ID: 789	Numbers: 4	Genders: Male	Animacies: High
debug-clusterCluster ID: GHI	Numbers: 12	Genders: Female	Animacies: Low
debug-clusterCluster ID: STU	Numbers: 1	Genders: Non-binary	Animacies: Medium
debug-clusterCluster ID: 012	Numbers: 7	Genders: Male	Animacies: High
debug-clusterCluster ID: JKL	Numbers: 9	Genders: Female	Animacies: Low
debug-clusterCluster ID: VWX	Numbers: 15	Genders: Non-binary	Animacies: Medium
debug-clusterCluster ID: 234	Numbers: 11	Genders: Male	Animacies: High
debug-clusterCluster ID: MNO	Numbers: 14	Genders: Female	Animacies: Low